<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Storage on Tech Notebook</title><link>https://wiki.deimos.fr/tags/storage/</link><description>Recent content in Storage on Tech Notebook</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 28 Apr 2025 19:01:23 +0200</lastBuildDate><atom:link href="https://wiki.deimos.fr/tags/storage/index.xml" rel="self" type="application/rss+xml"/><item><title>LVM: Working with Logical Volume Management</title><link>https://wiki.deimos.fr/LVM_:_Utilisation_des_LVM/</link><pubDate>Mon, 15 Sep 2014 11:24:00 +0200</pubDate><guid>https://wiki.deimos.fr/LVM_:_Utilisation_des_LVM/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Logical Volume Management (LVM) is a method and software for partitioning, concatenating, and utilizing storage spaces on a server. It allows flexible management, security, and online optimization of storage spaces in UNIX/Linux-type operating systems.&lt;/p>
&lt;p>We also refer to it as Volume Manager.&lt;/p>
&lt;p>Since LVM is not very simple to use, and since I don&amp;rsquo;t handle it every day either, I thought a small documentation was essential. I&amp;rsquo;ll fill it in as needed.&lt;/p></description></item><item><title>Software RAID Configuration</title><link>https://wiki.deimos.fr/Configuration_d'un_Raid_logiciel/</link><pubDate>Fri, 08 Aug 2014 08:29:00 +0200</pubDate><guid>https://wiki.deimos.fr/Configuration_d'un_Raid_logiciel/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Not everyone can afford a RAID 5 card with proper disks. That&amp;rsquo;s why a small software RAID 5 can be a good solution, especially for home use!&lt;/p>
&lt;h2 id="creating-a-raid">Creating a RAID &lt;a href="#creating-a-raid" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;h3 id="raid-1">RAID 1 &lt;a href="#raid-1" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h3>&lt;p>To create a RAID 1, it&amp;rsquo;s simple. You just need 2 disks with 2 partitions of the same size, then run this command:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="6066813" class="language-bash ">
 &lt;code>mdadm --create --assume-clean --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;ul>
&lt;li>create: creating a raid&lt;/li>
&lt;li>assume-clean: allows having a directly usable raid, without a complete synchronization. This requires being 100% sure that both disks/partitions are blank.&lt;/li>
&lt;li>level: the type of raid (here RAID 1)&lt;/li>
&lt;li>raid-devices: the number of disks used&lt;/li>
&lt;/ul>
&lt;h2 id="monitoring">Monitoring &lt;a href="#monitoring" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>To see if everything is working properly, here are several solutions:&lt;/p></description></item><item><title>Ceph: Performance, Reliability and Scalability Storage Solution</title><link>https://wiki.deimos.fr/Ceph_:_performance,_reliability_and_scalability_storage_solution/</link><pubDate>Mon, 02 Jun 2014 15:06:00 +0200</pubDate><guid>https://wiki.deimos.fr/Ceph_:_performance,_reliability_and_scalability_storage_solution/</guid><description>&lt;table class="table table-hover table-striped">
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>Software version&lt;/strong>&lt;/td>
 &lt;td>0.72.2&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Operating System&lt;/strong>&lt;/td>
 &lt;td>Debian 7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Website&lt;/strong>&lt;/td>
 &lt;td>&lt;a href="https://ceph.com/" rel="external" target="_blank">Ceph Website&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Last Update&lt;/strong>&lt;/td>
 &lt;td>02/06/2014&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>

&lt;p>










&lt;img src="https://wiki.deimos.fr/images/ceph_logo.png" alt="Ceph" loading="lazy">

&lt;/p>
&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Ceph is an open-source, massively scalable, software-defined storage system which provides object, block and file system storage in a single platform. It runs on commodity hardware-saving you costs, giving you flexibility and because it&amp;rsquo;s in the Linux kernel, it&amp;rsquo;s easy to consume.&lt;/p></description></item><item><title>Installation and Configuration of DRBD</title><link>https://wiki.deimos.fr/Installation_et_configuration_de_DRBD/</link><pubDate>Sat, 07 Sep 2013 09:56:00 +0200</pubDate><guid>https://wiki.deimos.fr/Installation_et_configuration_de_DRBD/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;strong>DRBD&lt;/strong> is a system that allows you to create software RAID1 over a local network.
This enables high availability and resource sharing on a cluster without a disk array.&lt;/p>
&lt;p>Here we will install &lt;strong>DRBD8&lt;/strong>, with the goal of implementing a cluster filesystem (see documentation on OCFS2) which is not supported on DRBD7.
We&amp;rsquo;ll use the DRBD8 packages from Debian repositories. We&amp;rsquo;ll work on a 2-node cluster.&lt;/p>
&lt;h2 id="installation">Installation &lt;a href="#installation" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>First, install the following packages:&lt;/p></description></item><item><title>iSCSI: Setting up an iSCSI Server</title><link>https://wiki.deimos.fr/ISCSI_:_Mise_en_place_d'un_serveur_iSCSI/</link><pubDate>Tue, 04 Jun 2013 14:04:00 +0200</pubDate><guid>https://wiki.deimos.fr/ISCSI_:_Mise_en_place_d'un_serveur_iSCSI/</guid><description>&lt;p>










&lt;img src="https://wiki.deimos.fr/images/iscsi_logo.gif" alt="ISCSI" loading="lazy">

&lt;/p>
&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://fr.wikipedia.org/wiki/ISCSI" rel="external" target="_blank">iSCSI&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a> (internet SCSI) is an application layer protocol that enables the transport of SCSI commands over a TCP/IP network.&lt;/p>
&lt;p>This documentation was created on Red Hat 5 and is compatible with Red Hat 6.&lt;/p></description></item><item><title>Optimization of extX filesystems and RAID under Linux</title><link>https://wiki.deimos.fr/Optimisation_des_filesystems_extX_et_du_RAID_sous_Linux/</link><pubDate>Wed, 13 Feb 2013 13:13:00 +0200</pubDate><guid>https://wiki.deimos.fr/Optimisation_des_filesystems_extX_et_du_RAID_sous_Linux/</guid><description>&lt;p>










&lt;img src="https://wiki.deimos.fr/images/poweredbylinux.jpg" alt="Linux" loading="lazy">

&lt;/p>







&lt;table class="table table-hover table-striped">
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>Software version&lt;/strong>&lt;/td>
 &lt;td>Kernel 2.6.32+&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Operating System&lt;/strong>&lt;/td>
 &lt;td>Red Hat 6.3&lt;br>Debian 7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Website&lt;/strong>&lt;/td>
 &lt;td>&lt;a href="https://www.kernel.org" rel="external" target="_blank">Kernel Website&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Last Update&lt;/strong>&lt;/td>
 &lt;td>13/02/2013&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>

&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>All file operations on Linux are managed by VFS. The VFS layer is a common kernel interface for applications to access files. VFS handles the communication with different drivers. The inode and dentry cache is also managed by VFS. VFS is therefore capable of managing different types of filesystems, even network ones, through a common interface.&lt;/p></description></item><item><title>Parted: Solving Partitioning Problems on Large Filesystems</title><link>https://wiki.deimos.fr/Parted_:_r%C3%A9soudre_les_probl%C3%A8mes_de_partionnnement_sur_gros_filesystems/</link><pubDate>Wed, 13 Feb 2013 12:54:00 +0200</pubDate><guid>https://wiki.deimos.fr/Parted_:_r%C3%A9soudre_les_probl%C3%A8mes_de_partionnnement_sur_gros_filesystems/</guid><description>&lt;p>










&lt;img src="https://wiki.deimos.fr/images/parted_logo.png" alt="Parted logo" loading="lazy">

&lt;/p>







&lt;table class="table table-hover table-striped">
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>Software version&lt;/strong>&lt;/td>
 &lt;td>2.1&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Operating System&lt;/strong>&lt;/td>
 &lt;td>Debian 6&lt;br>Red Hat 6.3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Last Update&lt;/strong>&lt;/td>
 &lt;td>13/02/2013&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>

&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>GNU Parted is a program for creating, destroying, resizing, checking, and copying partitions, and the file systems on them. This is useful for creating space for new operating systems, reorganizing hard disk usage, copying data between hard disks, and disk imaging. It was written by Andrew Clausen and Lennert Buytenhek.&lt;/p>
&lt;p>It consists of a library, libparted, and a command-line frontend, parted, that also serves as reference implementation.&lt;/p></description></item><item><title>ZFS: The Filesystem Par Excellence</title><link>https://wiki.deimos.fr/ZFS_:_Le_FileSystem_par_excellence/</link><pubDate>Wed, 02 Jan 2013 13:30:00 +0200</pubDate><guid>https://wiki.deimos.fr/ZFS_:_Le_FileSystem_par_excellence/</guid><description>&lt;p>










&lt;img src="https://wiki.deimos.fr/images/zfs_logo.jpg" alt="ZFS" loading="lazy">

&lt;/p>
&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>ZFS or Z File System is an open-source filesystem under the CDDL license. The &amp;lsquo;Z&amp;rsquo; doesn&amp;rsquo;t officially stand for anything specific, but has been referred to in various ways in the press, such as Zettabyte (from the English unit zettabyte for data storage), or ZFS as &amp;ldquo;the last word in filesystems&amp;rdquo;.&lt;/p>
&lt;p>Produced by Sun Microsystems for Solaris 10 and above, it was designed by Jeff Bonwick&amp;rsquo;s team. Announced for September 2004, it was integrated into Solaris on October 31, 2005, and on November 16, 2005, as a feature of OpenSolaris build 27. Sun announced that ZFS was integrated into the Solaris update dated June 2006, a year after the opening of the OpenSolaris community.&lt;/p></description></item><item><title>Linux RAID Performance</title><link>https://wiki.deimos.fr/Linux_RAID_performances/</link><pubDate>Fri, 31 Aug 2012 20:45:00 +0200</pubDate><guid>https://wiki.deimos.fr/Linux_RAID_performances/</guid><description>&lt;p>I won&amp;rsquo;t discuss the different RAID types, but rather refer you to Wikipedia for that&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. For using software RAID under Linux, &lt;a href="Configuration_d'un_Raid_logiciel.html">I recommend this documentation&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. We&amp;rsquo;ll focus on performance since that&amp;rsquo;s our topic here. RAID 0 is the most performant of all RAID types, but it obviously has data security issues if a disk fails.&lt;/p>
&lt;p>The MTBF (Mean Time Between Failure) is also important for RAID systems. This is an estimate of how long the RAID will function properly before a disk is detected as failed.&lt;/p></description></item><item><title>BTRFS: Using the Ext4 Replacement</title><link>https://wiki.deimos.fr/BTRFS_:_Utilisation_du_rempla%C3%A7ant_de_l%27Ext4/</link><pubDate>Thu, 05 Jul 2012 21:08:00 +0200</pubDate><guid>https://wiki.deimos.fr/BTRFS_:_Utilisation_du_rempla%C3%A7ant_de_l%27Ext4/</guid><description>&lt;p>










&lt;img src="https://wiki.deimos.fr/images/btrfs_logo.png" alt="BTRFS" loading="lazy">

&lt;/p>







&lt;table class="table table-hover table-striped">
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>Software version&lt;/strong>&lt;/td>
 &lt;td>0.19&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Operating System&lt;/strong>&lt;/td>
 &lt;td>Debian 7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Website&lt;/strong>&lt;/td>
 &lt;td>&lt;a href="https://btrfs.wiki.kernel.org" rel="external" target="_blank">BTRFS Website&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Last Update&lt;/strong>&lt;/td>
 &lt;td>05/07/2012&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Others&lt;/strong>&lt;/td>
 &lt;td>Kernel used:&lt;br>3.2.0-2-amd64&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>

&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://btrfs.wiki.kernel.org/" rel="external" target="_blank">BTRFS&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a> is the perfect replacement for the aging ExtX filesystem. For those familiar with &lt;a href="zfs:_le_filesystem_par_excellence/" rel="external" target="_blank">the ZFS filesystem&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a>, BTRFS draws heavily from it.&lt;/p></description></item><item><title>Multipath: Configuring Multiple Paths for External Disk Access</title><link>https://wiki.deimos.fr/Multipath_:_configurer_plusieurs_chemins_pour_ses_acc%C3%A8s_disques_externe/</link><pubDate>Tue, 17 Apr 2012 09:46:00 +0200</pubDate><guid>https://wiki.deimos.fr/Multipath_:_configurer_plusieurs_chemins_pour_ses_acc%C3%A8s_disques_externe/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>We will discuss two topics here:&lt;/p>
&lt;ul>
&lt;li>Device mappers&lt;/li>
&lt;li>Multipathing&lt;/li>
&lt;/ul>
&lt;p>We need to understand how device mappers work before tackling multipathing, which is why there will be an explanation of both in this documentation.&lt;/p>
&lt;p>In the Linux kernel, the device mapper serves as a generic framework for mapping one block device (&amp;ldquo;mapping&amp;rdquo; the device) to another. It is the foundation for LVM2 and EVMS, software RAIDs, or disk encryption; and offers additional features such as file system snapshots.
The device mapper works by processing data transferred to it by a virtual block device (provided by itself), and passing the resulting data to another block device.&lt;/p></description></item><item><title>Trick Samba Share Size Display</title><link>https://wiki.deimos.fr/trick-samba-share-size-display/</link><pubDate>Thu, 16 Feb 2012 10:17:00 +0200</pubDate><guid>https://wiki.deimos.fr/trick-samba-share-size-display/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>A colleague of mine found himself in a rather delicate situation. Let me explain the scenario:&lt;/p>
&lt;ul>
&lt;li>2 mount points in /mnt, with one nested inside the other&lt;/li>
&lt;li>1 share on the primary mount point&lt;/li>
&lt;/ul>
&lt;p>When the primary mount point is full, you can&amp;rsquo;t copy anything anymore, even if the second nested mount point still has free space. For those who still don&amp;rsquo;t understand:&lt;/p>
&lt;ul>
&lt;li>/mnt/: 30 MB remaining&lt;/li>
&lt;li>/mnt/disk1: 10 GB remaining&lt;/li>
&lt;li>share: /mnt/&lt;/li>
&lt;/ul>
&lt;p>The share tells me that it can&amp;rsquo;t copy more than 30 MB, even into /share/disk1.&lt;/p></description></item><item><title>Installation and Configuration of SUN Cluster</title><link>https://wiki.deimos.fr/Installation_et_configuration_du_SUN_Cluster/</link><pubDate>Thu, 03 Nov 2011 00:24:00 +0200</pubDate><guid>https://wiki.deimos.fr/Installation_et_configuration_du_SUN_Cluster/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Solaris Cluster (sometimes Sun Cluster or SunCluster) is a high-availability cluster software product for the Solaris Operating System, created by Sun Microsystems.&lt;/p>
&lt;p>It is used to improve the availability of software services such as databases, file sharing on a network, electronic commerce websites, or other applications. Sun Cluster operates by having redundant computers or nodes where one or more computers continue to provide service if another fails. Nodes may be located in the same data center or on different continents.&lt;/p></description></item><item><title>Installing FreeBSD on ZFS</title><link>https://wiki.deimos.fr/Installation_FreeBSD_sur_ZFS/</link><pubDate>Tue, 21 Sep 2010 21:23:00 +0200</pubDate><guid>https://wiki.deimos.fr/Installation_FreeBSD_sur_ZFS/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>I love ZFS and for building a large NAS, I need FreeBSD which is capable of implementing ZFS and, most importantly, using it on the root partition as well.&lt;/p>
&lt;p>For this purpose, I needed 5 disks of the same capacity and the FreeBSD DVD (I emphasize the DVD because the livefs or CD versions don&amp;rsquo;t contain everything needed to boot from ZFS).&lt;/p>
&lt;h2 id="disk-formatting">Disk Formatting &lt;a href="#disk-formatting" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;h3 id="creating-partitions">Creating Partitions &lt;a href="#creating-partitions" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h3>&lt;p>Boot from the FreeBSD DVD and launch the Fixit menu.&lt;/p></description></item><item><title>AOE: Setting Up an ATA Over Ethernet Server</title><link>https://wiki.deimos.fr/aoe-mise-en-place-d-un-serveur-ata-over-ethernet/</link><pubDate>Sat, 28 Nov 2009 15:41:00 +0200</pubDate><guid>https://wiki.deimos.fr/aoe-mise-en-place-d-un-serveur-ata-over-ethernet/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/ATA_over_Ethernet" rel="external" target="_blank">ATA over Ethernet (AoE)&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a> is a network layer protocol that allows ATA commands to be transported over an Ethernet network.&lt;/p>
&lt;h2 id="resources">Resources &lt;a href="#resources" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;a href="https://wiki.deimos.fr/pdf/using_ata_over_ethernet_aoe.pdf">Using ATA Over Ethernet (AoE) On Debian Lenny (Initiator And Target)&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Creating an iSCSI Share Between Solaris and Debian</title><link>https://wiki.deimos.fr/Cr%C3%A9er_un_partage_iSCSI_entre_Solaris_et_Debian/</link><pubDate>Sat, 28 Nov 2009 15:28:00 +0200</pubDate><guid>https://wiki.deimos.fr/Cr%C3%A9er_un_partage_iSCSI_entre_Solaris_et_Debian/</guid><description>&lt;h2 id="1-introduction">1 Introduction &lt;a href="#1-introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Technology is beautiful, isn&amp;rsquo;t it! Here&amp;rsquo;s a guide to create an iSCSI share between Solaris and Debian. Be careful with performance as this remains iSCSI - I don&amp;rsquo;t recommend it for production environments unless you know what you&amp;rsquo;re doing.&lt;/p>
&lt;p>In this setup, Solaris will be our server and Debian our client.&lt;/p>
&lt;h2 id="2-installation">2 Installation &lt;a href="#2-installation" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;h3 id="21-solaris">2.1 Solaris &lt;a href="#21-solaris" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h3>&lt;p>Let&amp;rsquo;s verify that we have the required packages:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="021e662" class="language-bash ">
 &lt;code>$ pkginfo | grep iscsi
system SUNWiscsir Sun iSCSI Device Driver (root)
system SUNWiscsitgtr Sun iSCSI Target (Root)
system SUNWiscsitgtu Sun iSCSI Target (Usr)
system SUNWiscsiu Sun iSCSI Management Utilities (usr)&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;h3 id="22-debian">2.2 Debian &lt;a href="#22-debian" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h3>&lt;p>Let&amp;rsquo;s install Open iSCSI:&lt;/p></description></item><item><title>Multipathing Management on Solaris</title><link>https://wiki.deimos.fr/Multipathing_management_on_Solaris/</link><pubDate>Fri, 06 Nov 2009 16:40:00 +0200</pubDate><guid>https://wiki.deimos.fr/Multipathing_management_on_Solaris/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Multipathing allows for connection to multiple links. For example, a disk array connected via fiber to machines can have 2 fibers per machine.&lt;/p>
&lt;p>To manage this type of configuration, you need to use multipathing.&lt;/p>
&lt;h2 id="configuration">Configuration &lt;a href="#configuration" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>To check if your multipath is enabled, it&amp;rsquo;s simple. Look at your devices, they should look like this:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="d1b6073" class="language- ">
 &lt;code>/dev/dsk/c3t2000002037CD9F72d0s0&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;p>instead of this:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="ba6a140" class="language- ">
 &lt;code>/dev/dsk/c1t1d0s0&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;p>If this is not the case, then perform the actions that follow.&lt;/p></description></item><item><title>XenServer 5.0: Configuring XenServer with SUN Hardware</title><link>https://wiki.deimos.fr/XenServer_5.0_:_Configuration_d%27un_XenServer_avec_du_mat%C3%A9riel_SUN/</link><pubDate>Sun, 19 Apr 2009 09:28:00 +0200</pubDate><guid>https://wiki.deimos.fr/XenServer_5.0_:_Configuration_d%27un_XenServer_avec_du_mat%C3%A9riel_SUN/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>To perform installation on multiple XenServer, I needed to set up multiple hardware devices connected via Fiber channel. I completed two installations, both with this kind of hardware:&lt;/p>
&lt;ul>
&lt;li>SUN X4150&lt;/li>
&lt;li>SUN X4600&lt;/li>
&lt;li>SUN StorageTek ST2540&lt;/li>
&lt;li>SUN StorageTek ST6140&lt;/li>
&lt;/ul>
&lt;h2 id="hardware-connectivity">Hardware Connectivity &lt;a href="#hardware-connectivity" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>To setup connectivity, I used fiber channel. To make it work, you need to have Fiber Channel Switch and if possible, redundancy (so, 2 switches).&lt;/p>
&lt;p>Having this kind of configuration creates multiple multipath links. This is very beneficial for fault tolerance and load balancing.&lt;/p></description></item><item><title>Creating a RAID 1 (mirroring) on Solaris</title><link>https://wiki.deimos.fr/Creation_d'un_Raid_1_(mirroring)_sous_Solaris/</link><pubDate>Wed, 11 Feb 2009 16:15:00 +0200</pubDate><guid>https://wiki.deimos.fr/Creation_d'un_Raid_1_(mirroring)_sous_Solaris/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>The Solaris system includes the DiskSuite package that allows RAID 1 mirroring of a UFS filesystem using LVM. This tutorial explains how to achieve this.
It goes without saying that you need two disks of the same capacity.&lt;/p>
&lt;h2 id="procedure">Procedure &lt;a href="#procedure" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Here are the necessary steps:
Once we&amp;rsquo;re ready to mirror a disk, we display its partitions with the format command:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="a68044b" class="language-bash ">
 &lt;code>Format
 
Searching for disks...done
 
 
AVAILABLE DISK SELECTIONS:
 0. c1t0d0 &amp;lt;SUN146G cyl 14087 alt 2 hd 24 sec 848&amp;gt; root
 /pci@0/pci@0/pci@2/scsi@0/sd@0,0
 1. c1t1d0 &amp;lt;HITACHI-H101414SCSUN146G-SA25-136.73GB&amp;gt;
 /pci@0/pci@0/pci@2/scsi@0/sd@1,0&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;p>We choose the 1st disk (if it&amp;rsquo;s the one to be duplicated), then:&lt;/p></description></item><item><title>Migrating from Multipath to Powerpath on RedHat</title><link>https://wiki.deimos.fr/Migrer_de_Multipath_%C3%A0_Powerpath_sur_RedHat/</link><pubDate>Tue, 03 Jun 2008 12:10:00 +0200</pubDate><guid>https://wiki.deimos.fr/Migrer_de_Multipath_%C3%A0_Powerpath_sur_RedHat/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Powerpath is the multipathing solution for EMC arrays. The multipath package is so buggy on RedHat that you shouldn&amp;rsquo;t install it in production environments. This migration has been released on RedHat 4.6.EL.&lt;/p>
&lt;p>Reminder: Multipathing brings redundancy functionalities with 2 links on a disk array without having I/O errors.&lt;/p>
&lt;h2 id="uninstalling-multipath">Uninstalling multipath &lt;a href="#uninstalling-multipath" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Multipath package name (if installed) can be found this way:&lt;/p>



 
 
 

 
 
 
 

 

 &lt;div class="prism-codeblock ">
 &lt;pre id="2aa3fa6" class="language-bash ">
 &lt;code>rpm -qa | grep multipath&lt;/code>
 &lt;/pre>
 &lt;/div>
&lt;p>Next, you just need to use the package name and add it to the rpm command:&lt;/p></description></item><item><title>Solutions for Encrypted LVM System</title><link>https://wiki.deimos.fr/Solutions_pour_un_syst%C3%A8me_LVM_crypt%C3%A9/</link><pubDate>Sun, 25 May 2008 09:37:00 +0200</pubDate><guid>https://wiki.deimos.fr/Solutions_pour_un_syst%C3%A8me_LVM_crypt%C3%A9/</guid><description>&lt;h2 id="encrypting-the-root-partition-with-lvm">Encrypting the Root Partition with LVM &lt;a href="#encrypting-the-root-partition-with-lvm" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Here is documentation explaining how to encrypt the root partition with LVM:&lt;/p>
&lt;p>&lt;a href="https://wiki.deimos.fr/pdf/encrypted_root_lvm.pdf">Documentation on crypting root LVM&lt;/a>&lt;/p>
&lt;h2 id="migrating-an-encrypted-lvm-partition">Migrating an Encrypted LVM Partition &lt;a href="#migrating-an-encrypted-lvm-partition" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Here is documentation on encrypting LVM partitions with LUKS:&lt;/p>
&lt;p>&lt;a href="https://wiki.deimos.fr/pdf/how_to_migrate_to_a_full_encrypted_lvm_system.pdf">Documentation on How To Migrate to a full encrypted LVM system&lt;/a>&lt;br>
&lt;a href="https://wiki.deimos.fr/pdf/273_lefinnois.pdf">LVM2 Disk Replacement + Crypto and Data Migration&lt;/a>&lt;/p>
&lt;h2 id="resizing-an-encrypted-lvm">Resizing an Encrypted LVM &lt;a href="#resizing-an-encrypted-lvm" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Here is documentation for resizing an encrypted filesystem under LVM:&lt;/p></description></item><item><title>Setting up quotas on Linux</title><link>https://wiki.deimos.fr/Mise_en_place_des_quotas_sous_Linux/</link><pubDate>Fri, 23 May 2008 14:43:00 +0200</pubDate><guid>https://wiki.deimos.fr/Mise_en_place_des_quotas_sous_Linux/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>The assignment of quotas in a file system is a tool that allows control of disk space usage. Quotas consist of setting a space limit for a user or a group of users.&lt;/p>
&lt;p>For the creation of these quotas, &lt;strong>2 types of limits&lt;/strong> are defined:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The soft limit&lt;/strong>: indicates the maximum amount of space that a user can occupy on the file system. If this limit is reached, the user receives warning messages about exceeding their assigned quota. When used in combination with grace periods, if the user continues to exceed the soft limit after the grace period has elapsed, they will face the same restriction as when reaching a hard limit.&lt;/p></description></item><item><title>Recovering Data from a RAID1 LVM</title><link>https://wiki.deimos.fr/R%C3%A9cup%C3%A9rer_ses_donn%C3%A9es_depuis_un_RAID1_LVM/</link><pubDate>Fri, 02 Nov 2007 14:31:00 +0200</pubDate><guid>https://wiki.deimos.fr/R%C3%A9cup%C3%A9rer_ses_donn%C3%A9es_depuis_un_RAID1_LVM/</guid><description>&lt;p>Here is documentation explaining how to recover your data from a LVM in RAID1!&lt;/p>
&lt;p>&lt;a href="https://wiki.deimos.fr/pdf/recover_data_from_raid1_lvm_partitions.pdf">Documentation on Recovering Data From RAID1 LVM Partitions&lt;/a>&lt;/p></description></item><item><title>Initramfs: Fixing Kernel Boot Issues with Initramfs</title><link>https://wiki.deimos.fr/Initramfs_:_corriger_les_petits_probl%C3%A8mes_de_boot_kernel_gr%C3%A2ce_%C3%A0_initramfs/</link><pubDate>Tue, 18 Sep 2007 22:19:00 +0200</pubDate><guid>https://wiki.deimos.fr/Initramfs_:_corriger_les_petits_probl%C3%A8mes_de_boot_kernel_gr%C3%A2ce_%C3%A0_initramfs/</guid><description>&lt;h2 id="problem">Problem &lt;a href="#problem" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>I just installed a server with many SATA disks. The machine has 4.5TB of storage spread across 2 Areca ARC-1280ML controllers. The Debian/etch installation went without issues using kernel 2.6.18-5-686. After installing the system on 2 disks connected to the motherboard (ICH5R controller using the ata_piix driver), RAID5 volumes are created on the Areca cards (arcmsr driver). However, boot stops at an initramfs command prompt, unable to find the root partition:&lt;/p></description></item></channel></rss>