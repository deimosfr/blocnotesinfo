<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Storage on Tech Notebook</title><link>https://wiki.deimos.fr/docs/servers/highavailability/storage/</link><description>Recent content in Storage on Tech Notebook</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 01 May 2025 20:33:41 +0200</lastBuildDate><atom:link href="https://wiki.deimos.fr/docs/servers/highavailability/storage/index.xml" rel="self" type="application/rss+xml"/><item><title>Ceph: Performance, Reliability and Scalability Storage Solution</title><link>https://wiki.deimos.fr/Ceph_:_performance,_reliability_and_scalability_storage_solution/</link><pubDate>Mon, 02 Jun 2014 15:06:00 +0200</pubDate><guid>https://wiki.deimos.fr/Ceph_:_performance,_reliability_and_scalability_storage_solution/</guid><description>&lt;table class="table table-hover table-striped">
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>Software version&lt;/strong>&lt;/td>
 &lt;td>0.72.2&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Operating System&lt;/strong>&lt;/td>
 &lt;td>Debian 7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Website&lt;/strong>&lt;/td>
 &lt;td>&lt;a href="https://ceph.com/" rel="external" target="_blank">Ceph Website&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>Last Update&lt;/strong>&lt;/td>
 &lt;td>02/06/2014&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>

&lt;p>










&lt;img src="https://wiki.deimos.fr/images/ceph_logo.png" alt="Ceph" loading="lazy">

&lt;/p>
&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>Ceph is an open-source, massively scalable, software-defined storage system which provides object, block and file system storage in a single platform. It runs on commodity hardware-saving you costs, giving you flexibility and because it&amp;rsquo;s in the Linux kernel, it&amp;rsquo;s easy to consume.&lt;/p></description></item><item><title>Installation and Configuration of DRBD</title><link>https://wiki.deimos.fr/Installation_et_configuration_de_DRBD/</link><pubDate>Sat, 07 Sep 2013 09:56:00 +0200</pubDate><guid>https://wiki.deimos.fr/Installation_et_configuration_de_DRBD/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;strong>DRBD&lt;/strong> is a system that allows you to create software RAID1 over a local network.
This enables high availability and resource sharing on a cluster without a disk array.&lt;/p>
&lt;p>Here we will install &lt;strong>DRBD8&lt;/strong>, with the goal of implementing a cluster filesystem (see documentation on OCFS2) which is not supported on DRBD7.
We&amp;rsquo;ll use the DRBD8 packages from Debian repositories. We&amp;rsquo;ll work on a 2-node cluster.&lt;/p>
&lt;h2 id="installation">Installation &lt;a href="#installation" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>First, install the following packages:&lt;/p></description></item><item><title>GFS2: Red Hat Cluster Filesystem</title><link>https://wiki.deimos.fr/GFS2_:_Le_Filesystem_Cluster_de_Red_Hat/</link><pubDate>Tue, 06 Mar 2012 12:54:00 +0200</pubDate><guid>https://wiki.deimos.fr/GFS2_:_Le_Filesystem_Cluster_de_Red_Hat/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://fr.wikipedia.org/wiki/Global_File_System" rel="external" target="_blank">Global File System (GFS)&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a> is a shared file system designed for Linux or IRIX clusters. GFS and GFS2 are different from distributed file systems like AFS, Coda, or InterMezzo because they allow all nodes to have direct concurrent access to the same block storage device. Additionally, GFS and GFS2 can also be used as a local file system.&lt;/p></description></item><item><title>OCFS2: Oracle's Cluster File System</title><link>https://wiki.deimos.fr/OCFS2%C2%A0:%C2%A0Le_FileSystem_Cluster_d%27Oracle/</link><pubDate>Sat, 03 Mar 2012 15:37:00 +0200</pubDate><guid>https://wiki.deimos.fr/OCFS2%C2%A0:%C2%A0Le_FileSystem_Cluster_d%27Oracle/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>OCFS2 is a cluster filesystem that allows sharing filesystems between multiple machines with fault tolerance.&lt;/p>
&lt;p>This file system was initially created for Oracle databases and therefore has a locking mechanism designed for this type of application. You can use it as a file system, but the problem is that it may flush old locks after a certain period. If your idea is really to use a cluster filesystem to store files, it would be better to turn to GFS.&lt;/p></description></item><item><title>GlusterFS: High Availability Cluster Filesystem</title><link>https://wiki.deimos.fr/glusterfs-ha-cluster-filesystem/</link><pubDate>Mon, 11 Apr 2011 08:18:00 +0200</pubDate><guid>https://wiki.deimos.fr/glusterfs-ha-cluster-filesystem/</guid><description>&lt;h2 id="introduction">Introduction &lt;a href="#introduction" class="anchor" aria-hidden="true">&lt;i class="material-icons align-middle">link&lt;/i>&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://fr.wikipedia.org/wiki/GlusterFS" rel="external" target="_blank">GlusterFS&lt;svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">&lt;path fill="currentColor" d="M14 5c-.552 0-1-.448-1-1s.448-1 1-1h6c.552 0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1v-3.586l-7.293 7.293c-.391.39-1.024.39-1.414 0-.391-.391-.391-1.024 0-1.414l7.293-7.293h-3.586zm-9 2c-.552 0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552 0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1v4.563c0 1.657-1.343 3-3 3h-11c-1.657 0-3-1.343-3-3v-11c0-1.657 1.343-3 3-3h4.563c.552 0 1 .448 1 1s-.448 1-1 1h-4.563z"/>&lt;/svg>&lt;/a> is an open source distributed parallel file system capable of scaling to several petabytes.
GlusterFS is a cluster/network file system. GlusterFS comes with two components, a server and a client.
The storage server (or each server in a cluster) runs glusterfsd and clients use the mount command or glusterfs client to mount the file systems served, using FUSE.&lt;/p></description></item></channel></rss>